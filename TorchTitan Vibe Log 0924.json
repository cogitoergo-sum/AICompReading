{"threads":[{"_creationTime":1758739215635.4348,"_id":"jd7am1t9639rcx3wrb8kptn4vd7r7q82","createdAt":1758739215635,"generationStatus":"completed","lastMessageAt":1758739856480,"model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"pinned":false,"threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","title":"Triton compiler explanation","updatedAt":1758739856480,"userId":"google:104458561188320708185","userSetTitle":false,"visibility":"visible","id":"7b782838-0ad5-46c8-8d73-1edafa252319","last_message_at":1758739856480,"created_at":1758739215635,"updated_at":1758739856480,"status":"completed","user_edited_title":false}],"messages":[{"_creationTime":1758739215635.435,"_id":"j97aar6r4pjvp2areqga1qgjzx7r6pce","attachmentIds":[],"content":"Explain this paper to me in depth using the Feynman technique, as if you were its author.\n\nhttps://dl.acm.org/doi/10.1145/3315508.3329973\nTriton: an intermediate language and compiler for tiled neural network computations","created_at":1758739215635,"messageId":"ff8418c5-42b3-48a0-a457-f5175c01537b","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","updated_at":1758739215635,"userId":"google:104458561188320708185","id":"j97aar6r4pjvp2areqga1qgjzx7r6pce","providerMetadata":{}},{"_creationTime":1758739215635.4353,"_id":"j973rjxqjyy2mgkkqcjpg1mr0x7r63w0","attachmentIds":[],"byok":false,"content":"Speaking as the paper’s author, here’s the “Feynman technique” explanation: teach it like I’m explaining it to a smart colleague who doesn’t yet know the details, then peel back layers until we reach how and why it works.\n\n1) The problem we set out to solve (why Triton exists)\n- Most deep learning workloads run great if they can be expressed as a few standard kernels (GEMM via cuBLAS, and a handful of convolutions via cuDNN).\n- But research and production keep inventing new operators (custom attention patterns, structured sparsity, odd convolutions, fused epilogues). If your op isn’t exactly what the vendor libraries expect, you either:\n  - Accept poor performance, or\n  - Hand-write a custom CUDA kernel (hard, brittle, unportable).\n- Existing DSLs/compilers (Halide, TVM, Tensor Comprehensions, etc.) help, but they typically describe computation as loops over scalar indices. That’s good for some cases, but:\n  - It’s hard to express non-affine or irregular patterns cleanly.\n  - They often struggle to match vendor-library performance on the most important dense operators unless you manually drop to micro-kernels or rely on complex scheduling heuristics.\n\nWhat we want: a way to write custom GPU kernels that is both expressive and fast, without having to be a CUDA ninjutsu expert.\n\n2) The core idea in one sentence\nTreat tiles (small, statically shaped blocks of a tensor) as first-class values in the language and the compiler, and optimize at the tile level instead of the scalar or whole-tensor level.\n\nWhy “tiles”?\n- Tiles are the natural unit of locality and reuse on GPUs: a warp/block cooperates to load a block of A and a block of B, computes a block of C, and moves on.\n- Most high-performance kernels (GEMM, conv, attention matmul) are built from repeating tile operations. If tiles are the primitive, tiling is not an afterthought—it’s the program.\n\n3) What Triton is (the components)\n- Triton-C: a C-like kernel language whose variables can be tiles: statically-shaped multi-dimensional sub-arrays. You write loads/stores and math over tiles, not just scalars. You specify shapes via tunable meta-parameters.\n- Triton-IR: an LLVM-based IR that has tile types and tile ops. This IR is designed so the compiler can do tile-level analyses and transformations (e.g., hoist, fuse, vectorize, predicate) that are otherwise painful from scalarized code.\n- Triton-JIT: a just-in-time compiler:\n  - Machine-independent passes: simplify tile programs (algebraic simplifications, dead code elim, shape propagation) without committing to any specific GPU.\n  - Machine-dependent passes: lower tile ops to warp- and thread-level code, generate coalesced memory accesses, shared-memory staging, vector instructions, predication for edges, etc., and then down to LLVM-IR/PTX.\n  - Auto-tuner: searches the meta-parameters (tile sizes TM/TN/TK, number of warps, unroll factors) to hit the device sweet spot.\n\n4) How you write code in Triton (what it feels like)\n- You write one kernel where each “program instance” (akin to a CUDA block) computes a tile of the output, say C[TM, TN].\n- You construct index tiles (rm, rn, rk), compute pointer tiles (pa, pb), load A[TM, TK] and B[TN, TK] tiles, compute a dot-product tile, accumulate into C[TM, TN], advance pointers by TK, loop, and finally store C with boundary masks.\n- Crucially, all those are tile operations. The compiler is free to map them to threads/warps, choose vector widths, organize shared memory, and predicate the edges.\n\n5) Why this makes performance easier to get\n- Correct by construction locality: If your code loads tiles and multiplies them, the compiler knows the lifetimes and shapes. It can plan coalesced global loads into shared memory, synchronize, reuse the data across the TK loop, and emit the right vectorized FMAs.\n- Fewer ways to shoot yourself in the foot: You aren’t manually juggling thread indices, pointer arithmetic, or per-thread predication for every scalar—Triton abstracts that at tile granularity.\n- Tuning becomes structured: The main knobs are the tile sizes and a few scheduling choices. An auto-tuner can sweep TM/TN/TK and pick winners per GPU and per problem shape.\n\n6) What’s new compared to prior work\n- Tile-first IR: Many systems treat tiling as a schedule attached to a scalar loop nest. We encode the math directly over tiles, so the program already exposes reuse and memory footprints at the right granularity.\n- Expressivity beyond affine loops: Because tile indices and masks are first-class, irregular indexing, predicated edges, fused epilogues (e.g., bias+ReLU+scaling) are natural without contorting loop bounds.\n- Competitive with vendor libraries on dense ops while retaining flexibility: We show matmul and conv on par with cuBLAS/cuDNN, but the same approach readily implements non-standard kernels (e.g., shift convolutions) efficiently.\n\n7) How the compiler actually lowers tile code to a fast GPU kernel\nThink of one Triton program instance as one macro-kernel:\n- Mapping tiles to threads/warps:\n  - The compiler chooses how the [TM, TN] output tile is partitioned across warps/threads and how [TM, TK] and [TN, TK] input tiles are cooperatively loaded.\n- Memory:\n  - Computes pointer arithmetic for each lane so global loads are coalesced.\n  - Stages tiles in shared memory (or registers if appropriate).\n  - Applies masks to handle edge tiles safely without branches (predication).\n- Compute:\n  - Unrolls the TK loop to overlap memory and compute.\n  - Emits vectorized dot-product micro-ops (FMAs). On newer GPUs this can map to tensor-cores if extended, but the MAPL’19 work stays at standard cores.\n- Synchronization:\n  - Inserts barriers around shared-memory tiles where needed, tries to avoid redundant syncs.\n- Register pressure and occupancy:\n  - Balances tile sizes and unroll to avoid spilling while keeping enough active warps.\n\n8) Results we showed (what you get out)\n- Matmul: Triton kernels match cuBLAS across arithmetic intensities; substantially outperform contemporary DSLs (AutoTVM, Tensor Comprehensions, PlaidML) on realistic shapes from RNNs/Transformers.\n- Convolution: We reimplemented cuDNN’s IMPLICIT_GEMM-style conv and matched performance.\n- New ops: Implemented shift convolution efficiently—an example of something outside vendor templates but natural in Triton.\n\n9) What you as a kernel author control vs. what Triton handles\nYou pick:\n- The algorithm at the tile level: what tiles to load, how to combine them, any fused elementwise operations.\n- The meta-parameters (TM, TN, TK; sometimes number of warps).\n- Optional layout/stride decisions if you need something specific.\n\nTriton handles:\n- Mapping tiles to warps/threads.\n- Coalescing, shared-memory tiling, predication and vectorization.\n- Register allocation via LLVM and codegen to PTX.\n- Auto-tuning of the meta-parameters.\n\n10) Limits and future directions (as we wrote then)\n- We didn’t (in this paper) integrate with tensor-core intrinsics; later work can target them.\n- We focused on GPUs; a CPU/other-accelerator backend would require new lowering strategies.\n- Integration as a backend for higher-level frameworks/DSLs (TVM, XLA, MLIR) was an obvious next step so frontends can emit Triton-IR instead of reinventing tiling.\n- More automated scheduling/search (beyond tile sizes) and learned cost models could further reduce manual tuning.\n\nMental model you should keep\n- Imagine writing a matmul but the variables are little 2D blocks, not scalars.\n- Your program is a concise description of the classic “load A and B tiles, compute C tile” pattern.\n- The compiler “fills in” the GPU mechanics (lane mappings, masks, shared memory, vector widths).\n- Because your program already expresses the right unit of locality (tiles), the compiler can hit near-expert performance and still let you write new, weird ops.\n\nIf you want to quickly internalize it, read the matmul kernel in the paper: you’ll see index tiles, pointer tiles, masked loads, a dot(A, trans(B)) on tiles, a TK loop, and a masked store. That’s the whole story—compressed to the level GPUs like to run.","created_at":1758739215636,"messageId":"89455e27-eb33-4756-9336-819ce4090dcb","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":null,"queries":["Triton: an intermediate language and compiler for tiled neural network computations 2019 ACM paper 10.1145/3315508.3329973 pdf","Triton intermediate language compiler tiled neural network computations ICS 2019 Tillet"]},"result":[{"content":"![logo](data:,)\n\n## This website uses cookies\n\nWe occasionally run membership recruitment campaigns on social media channels and use cookies to track post-clicks. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you’ve provided to them or that they’ve collected from your use of their services. Use the check boxes below to choose the types of cookies you consent to have stored on your device.\n\nDo not sell or share my personal information\n\n[Use necessary cookies only](https://dl.acm.org/doi/abs/10.1145/3315508.3329973) [Allow all cookies](https://dl.acm.org/doi/abs/10.1145/3315508.3329973) [Show details](https://dl.acm.org/doi/abs/10.1145/3315508.3329973)\n\n[OK](https://dl.acm.org/doi/abs/10.1145/3315508.3329973)\n\n[Use necessary cookies only](https://dl.acm.org/doi/abs/10.1145/3315508.3329973) [Allow selected cookies](https://dl.acm.org/doi/abs/10.1145/3315508.3329973) [Allow all cookies](https://dl.acm.org/doi/abs/10.1145/3315508.3329973)\n\nNecessary\n\nPreferences\n\nStatistics\n\nMarketing\n\n[Show details](https://dl.acm.org/doi/abs/10.1145/3315508.3329973)\n\n[Cookie declaration](https://dl.acm.org/doi/abs/10.1145/3315508.3329973) [\\[#IABV2SETTINGS#\\]](https://dl.acm.org/doi/abs/10.1145/3315508.3329973) [About](https://dl.acm.org/doi/abs/10.1145/3315508.3329973)\n\n[Necessary (8)](https://dl.acm.org/doi/abs/10.1145/3315508.3329973) [Preferences (5)](https://dl.acm.org/doi/abs/10.1145/3315508.3329973) [Statistics (15)](https://dl.acm.org/doi/abs/10.1145/3315508.3329973) [Marketing (24)](https://dl.acm.org/doi/abs/10.1145/3315508.3329973) [Unclassified (5)](https://dl.acm.org/doi/abs/10.1145/3315508.3329973)\n\nNecessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. These cookies do not gather information about you that could be used for marketing purposes and do not remember where you have been on the internet.\n\n| Name | Provider | Purpose | Expiry | Type |\n| --- | --- | --- | --- | --- |\n| \\_\\_cf\\_bm \\[x2\\] | [ACM](https://www.acm.org/privacy-policy) | This cookie is used to distinguish between humans and bots. This is beneficial for the website, in order to make valid reports on the use of their website. | 1 day | HTTP Cookie |\n| \\_\\_jid | c.disquscdn.com | Used to add comments to the website and remember the user's Disqus login credentials across websites that use said service. | Session | HTTP Cookie |\n| disqusauth | c.disquscdn.com | Registers whether the user is logged in. This allows the website owner to make parts of the website inaccessible, based on the user's log-in status. | Session | HTTP Cookie |\n| \\_cfuvid | [ACM](https://www.acm.org/privacy-policy) | This cookie is a part of the services provided by Cloudflare - Including load-balancing, deliverance of website content and serving DNS connection for website operators. | Session | HTTP Cookie |\n| CookieConsent | [Cookiebot](https://www.cookiebot.com/goto/privacy-policy/) | Stores the user's cookie consent state for the current domain | 1 year | HTTP Cookie |\n| JSESSIONID | [ACM](https://www.acm.org/privacy-policy) | Preserves users states across page requests. | Session | HTTP Cookie |\n| 1.gif | [Cookiebot](https://www.cookiebot.com/goto/privacy-policy/) | Used to count the number of sessions to the website, necessary for optimizing CMP product delivery. | Session | Pixel Tracker |\n\nPreference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n| Name | Provider | Purpose | Expiry | Type |\n| --- | --- | --- | --- | --- |\n| aet-dismiss | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| drafts.queue | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| submitted\\_posts\\_cache | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| mopDeploy | [Mopinion](https://mopinion.com/privacy/) | Pending | Session | HTML Local Storage |\n| MACHINE\\_LAST\\_SEEN | [ACM](https://www.acm.org/privacy-policy) | Pending | 300 days | HTTP Cookie |\n\nStatistic cookies help website owners understand how visitors interact with websites by collecting and reporting information anonymously.\n\n| Name | Provider | Purpose | Expiry | Type |\n| --- | --- | --- | --- | --- |\n| \\_ga | [Google](https://business.safety.google/privacy/) | Registers a unique ID that is used to generate statistical data on how the visitor uses the website. | 2 years | HTTP Cookie |\n| \\_ga\\_# | [Google](https://business.safety.google/privacy/) | Used by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit. | 2 years | HTTP Cookie |\n| \\_gat | [Google](https://business.safety.google/privacy/) | Used by Google Analytics to throttle request rate | 1 day | HTTP Cookie |\n| \\_gid | [Google](https://business.safety.google/privacy/) | Registers a unique ID that is used to generate statistical data on how the visitor uses the website. | 1 day | HTTP Cookie |\n| \\_hjSession\\_# | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Collects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read. | 1 day | HTTP Cookie |\n| \\_hjSessionUser\\_# | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Collects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read. | 1 year | HTTP Cookie |\n| \\_hjTLDTest | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Registers statistical data on users' behaviour on the website. Used for internal analytics by the website operator. | Session | HTTP Cookie |\n| \\_hp2\\_# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user’s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| \\_hp2\\_hld#.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user’s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| \\_hp2\\_id.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user’s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 13 months | HTTP Cookie |\n| \\_hp2\\_ses\\_props.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user’s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| disqus\\_unique | c.disquscdn.com | Collects statistics related to the user's visits to the website, such as number of visits, average time spent on the website and loaded pages. | Session | HTTP Cookie |\n| collect | [Google](https://business.safety.google/privacy/) | Used to send data to Google Analytics about the visitor's device and behavior. Tracks the visitor across devices and marketing channels. | Session | Pixel Tracker |\n| hjActiveViewportIds | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | This cookie contains an ID string on the current session. This contains non-personal information on what subpages the visitor enters – this information is used to optimize the visitor's experience. | Persistent | HTML Local Storage |\n| hjViewportId | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Saves the user's screen size in order to adjust the size of images on the website. | Session | HTML Local Storage |\n\nMarketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\n\n| Name | Provider | Purpose | Expiry | Type |\n| --- | --- | --- | --- | --- |\n| badges-message | c.disquscdn.com | Collects data on the visitor’s use of the comment system on the website, and what blogs/articles the visitor has read. This can be used for marketing purposes. | Persistent | HTML Local Storage |\n| NID | [Google](https://business.safety.google/privacy/) | Pending | 6 months | HTTP Cookie |\n| api/telemetry | [Heap Analytics](https://heap.io/privacy) | Collects data on user behaviour and interaction in order to optimize the website and make advertisement on the website more relevant. | Session | Pixel Tracker |\n| h | [Heap Analytics](https://heap.io/privacy) | Collects data on user behaviour and interaction in order to optimize the website and make advertisement on the website more relevant. | Session | Pixel Tracker |\n| #-# | [YouTube](https://business.safety.google/privacy/) | Used to track user’s interaction with embedded content. | Session | HTML Local Storage |\n| iU5q-!O9@$ | [YouTube](https://business.safety.google/privacy/) | Registers a unique ID to keep statistics of what videos from YouTube the user has seen. | Session | HTML Local Storage |\n| LAST\\_RESULT\\_ENTRY\\_KEY | [YouTube](https://business.safety.google/privacy/) | Used to track user’s interaction with embedded content. | Session | HTTP Cookie |\n| LogsDatabaseV2:V#\\|\\|LogsRequestsStore | [YouTube](https://business.safety.google/privacy/) | Used to track user’s interaction with embedded content. | Persistent | IndexedDB |\n| nextId | [YouTube](https://business.safety.google/privacy/) | Used to track user’s interaction with embedded content. | Session | HTTP Cookie |\n| remote\\_sid | [YouTube](https://business.safety.google/privacy/) | Necessary for the implementation and fun","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdl.acm.org%2Fdoi%2Fabs%2F10.1145%2F3315508.3329973","id":"https://dl.acm.org/doi/abs/10.1145/3315508.3329973","publishedDate":"2019-06-22T00:00:00.000Z","summary":"This webpage discusses the use of cookies. It informs users that the website uses cookies to track post-clicks for membership recruitment campaigns on social media and shares information with social media, advertising, and analytics partners. Users can choose which types of cookies they consent to. The page then lists the different types of cookies used (Necessary, Preferences, Statistics, Marketing, and Unclassified) and provides details about each, including their purpose, provider, expiry, and type.\n","title":"Triton: an intermediate language and compiler for tiled neural network computations | Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages","url":"https://dl.acm.org/doi/abs/10.1145/3315508.3329973"},{"content":"Triton: An Intermediate Language and Compiler for\nTiled Neural Network Computations\nPhilippe Tillet\nHarvard University\nUSA\nH. T. Kung\nHarvard University\nUSA\nDavid Cox\nHarvard University, IBM\nUSA\nAbstract\nThe validation and deployment of novel research ideas in\nthe field of Deep Learning is often limited by the availability\nof efficient compute kernels for certain basic primitives. In\nparticular, operations that cannot leverage existing vendor\nlibraries (e.g., cuBLAS, cuDNN) are at risk of facing poor\ndevice utilization unless custom implementations are written\nby experts – usually at the expense of portability. For this\nreason, the development of new programming abstractions\nfor specifying custom Deep Learning workloads at a minimal\nperformance cost has become crucial.\nWe present Triton, a language and compiler centered around\nthe concept of tile, i.e., statically shaped multi-dimensional\nsub-arrays. Our approach revolves around (1) a C-based lan\u0002guage and an LLVM-based intermediate representation (IR)\nfor expressing tensor programs in terms of operations on\nparametric tile variables and (2) a set of novel tile-level opti\u0002mization passes for compiling these programs into efficient\nGPU code. We demonstrate how Triton can be used to build\nportable implementations of matrix multiplication and con\u0002volution kernels on par with hand-tuned vendor libraries\n(cuBLAS / cuDNN), or for efficiently implementing recent\nresearch ideas such as shift convolutions.\nCCS Concepts • Computing methodologies → Paral\u0002lel computing methodologies.\nKeywords compiler; neural networks; GPU\nACM Reference Format:\nPhilippe Tillet, H. T. Kung, and David Cox. 2019. Triton: An Inter\u0002mediate Language and Compiler for Tiled Neural Network Com\u0002putations. In Proceedings of the 3rd ACM SIGPLAN International\nWorkshop on Machine Learning and Programming Languages (MAPL\n’19), June 22, 2019, Phoenix, AZ, USA. ACM, New York, NY, USA,\n10 pages. https://doi.org/10.1145/3315508.3329973\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from permissions@acm.org.\nMAPL ’19, June 22, 2019, Phoenix, AZ, USA\n© 2019 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 978-1-4503-6719-6/19/06. . . $15.00\nhttps://doi.org/10.1145/3315508.3329973\n1 Introduction\nThe recent resurgence of Deep Neural Networks (DNNs)\nwas largely enabled [24] by the widespread availability of\nprogrammable, parallel computing devices. In particular, con\u0002tinuous improvements in the performance of many-core\narchitectures (e.g., GPUs) have played a fundamental role,\nby enabling researchers and engineers to explore an ever\u0002growing variety of increasingly large models, using more\nand more data. This effort was supported by a collection\nof vendor libraries (cuBLAS, cuDNN) aimed at bringing the\nlatest hardware innovations to practitioners as quickly as\npossible. Unfortunately, these libraries only support a re\u0002stricted set of tensor operations, leaving the implementation\nof novel primitives to experts [13, 17, 25].\nThis observation has led to the development of various\nDomain-Specific Languages (DSLs) for DNNs, based on poly\u0002hedral machinery (e.g., Tensor Comprehensions [43]) and/or\nloop synthesis techniques (e.g., Halide [37], TVM [10] and\nPlaidML[22]). But while these systems generally perform\nwell for certain classes of problems such as depthwise-separable\nconvolutions (e.g., MobileNet [20]), they are often much\nslower than vendor libraries in practice (see, e.g., Figure 1),\nand lack the expressivity necessary to implement structured\nsparsity patterns [28, 31, 47] that cannot be directly specified\nusing affine array indices in nested loops.\n101 102\nArithmetic Intensity (TFLOP/GB)\n10−1\n100\nPerformance (TFLOP/S)\nRoofline Model\ncuBLAS 10.0\nTriton\nAuto-TVM\nTensor Comprehensions\nPlaidML\nFigure 1. Performance of various implementations of\nC = ABT vs. Roofline [46] Model (NVIDIA GeForce\nGTX1070), where A ∈ R\n1760×1760 and B ∈ RN ×1760 as N\nmodulates arithmetic intensity.\nMAPL ’19, June 22, 2019, Phoenix, AZ, USA Philippe Tillet, H. T. Kung, and David Cox\nThese issues have often been addressed by the use of\nmicro-kernels [11, 21] – i.e., hand-written tile-level intrin\u0002sics – but this solution requires a lot of manual labor and\nlacks portability. And while several high-level programming\nabstractions for tiling have recently been proposed [23, 41],\nunderlying compiler backends still lack support for tile-level\noperations and optimizations. To this end we present Tri\u0002ton (Figure 2), an open-source1\nintermediate language and\ncompiler for specifying and compiling tile programs into\nefficient GPU code.\nTriton-C\nTriton-IR\nTriton-JIT\nAuto-Tuner\nMachine\n-Independent \nPasses\nMachine\n-Dependent \nPasses\nMachine-Code\nBenchmark\nInterface to existing \nDSLs\n(Not addressed \nin this paper)\nFigure 2. Overview of Triton\nThe main contributions of this paper are summarized as\nfollows:\n• Triton-C (Section 3): A C-like language for expressing\ntensor programs in terms of parametric tile variables.\nThe purpose of this language is to provide a stable in\u0002terface for existing DNN transcompilers (e.g., PlaidML,\nTensor Comprehensions) and programmers familiar\nwith CUDA. Listing 1 shows the Triton-C source code\nassociated with a simple matrix multiplication task.\n• Triton-IR (Section 4): An LLVM-based Intermediate\nRepresentation (IR) that provides an environment suit\u0002able for tile-level program analysis, transformation and\noptimization. Listing 5 shows the Triton-IR code for a\nRectified Linear Unit (ReLU) function. Here Triton-IR\nprograms are constructed directly from Triton-C dur\u0002ing parsing, but automatic generation from embedded\nDSLs or higher-level DNN compilers (e.g., TVM) could\nalso be explored in the future.\n• Triton-JIT (Section 5): A Just-In-Time (JIT) compiler\nand code generation backend for compiling Triton-IR\nprograms into efficient LLVM bitcode. This includes (1)\na set of tile-level, machine-independent passes aimed\nat simplifying input compute kernels independently of\n1http://triton-lang.org\nListing 1. C = A × B\nT\nin Triton-C. Keywords specific to\nTriton are shown in purple.\n/ / T i l e s h a p e s a r e p a r a m e t r i c and can be o p t i mi z e d\n/ / by c o m p i l a t i o n ba c k e n d s\nc o n s t t u n a b l e i n t TM = { 1 6 , 3 2 , 6 4 , 1 2 8 }\nc o n s t t u n a b l e i n t TN = { 1 6 , 3 2 , 6 4 , 1 2 8 }\nc o n s t t u n a b l e i n t TK = { 8 , 1 6 }\n/ / C = A ∗ B . T\nk e r n e l v oi d ma tmul_n t ( f l o a t ∗ a , f l o a t ∗ b , f l o a t ∗ c ,\ni n t M, i n N , i n t K)\n{\n/ / 1D t i l e o f i n d i c e s\ni n t rm [TM] = g e t _ g l o b a l _ r a n g e ( 0 ) ;\ni n t rn [TN ] = g e t _ g l o b a l _ r a n g e ( 1 ) ;\ni n t r k [TK ] = 0 . . . TK ;\n/ / 2D t i l e o f a c c u m u l a t o r s\nf l o a t C [TM, TN ] = 0 ;\n/ / 2D t i l e o f p o i n t e r s\nf l o a t ∗ pa [TM, TK ] = a + rm [ : , newaxi s ] + r k ∗ M;\nf l o a t ∗ pb [TN , TK ] = b + rn [ : , newaxi s ] + r k ∗ K ;\nf o r ( i n t k = K ; k >= 0 ; k −= TK) {\nb o ol che c k_ k [TK ] = r k < k ;\nb o ol c h e c k_a [TM, TK ] = ( rm < M) [ : , newaxi s ] && chec k_ k ;\nb o ol c he c k_b [TN , TK ] = ( rn < N) [ : , newaxi s ] && chec k_ k ;\n/ / l o a d t i l e o p e r a n d s\nf l o a t A[TM, TK ] = c h e c k_a ? ∗ pa : 0 ;\nf l o a t B [TN , TK ] = c he c k_b ? ∗ pb : 0 ;\n/ / a c c um ul a t e\nC += d o t (A , t r a n s ( B ) ) ;\n/ / u p d a t e p o i n t e r s\npa = pa + TK∗M;\npb = pb + TK∗N ;\n}\n/ / w ri t e −back a c c u m u l a t o r s\nf l o a t ∗ pc [TM, TN ] = c + rm [ : , newaxi s ] + rn ∗ M;\nb o ol c h e c k_ c [TM, TN ] = ( rm < M) [ : , newaxi s ] && ( rn < N) ;\n@check_c ∗ pc = C ;\n}\nany compilation target; (2) a set of tile-level machine\u0002dependent passes for generating efficient GPU-ready\nLLVM-IR; and (3) an auto-tuner that optimize any\nmeta-parameter associated with the above passes.\n• Numerical Experiments (Section 6): A numerical\nevaluation of Triton that demonstrates its ability to\n(1) generate matrix multiplication implementations\non par with cuBLAS and up to 3× faster than alterna\u0002tives DSLs on recurrent and transformer neural net\u0002works; (2) re-implement cuDNN’s IMPLICIT_GEMM algo\u0002rithm for dense convolution without performance loss;\nand (3) create efficient implementations of novel re\u0002search ideas such as shift-conv [47] modules.\nThis paper will be prefaced by a brief analysis of the exist\u0002ing related literature (Section 2) and concluded by a summary\nand directions of future work (Section 7).\n2 Related Work\nThe existence of frameworks [1, 9, 36] and libraries for\nDeep Learning has been critical to the emergence of novel\nneural network architectures and algorithms. But despite\nadvances in analytical [5, 48] and empirical [6, 30] heuris\u0002tics for linear algebra compilers, these software still invari\u0002ably rely on hand-optimized sub-routines (e.g., cuBLAS and\ncuDNN). This has led to development of various DSLs and\nTriton: An Intermediate Language for Tiled Neural Network Computations MAPL ’19, June 22, 2019, Phoenix, AZ, USA\ncompilers for DNNs, generally based on one of three distinct\napproaches:\n• Tensor-level IRs have been used by XLA [16] and\nGlow [38] to transform tensor programs into prede\u0002fined LLVM-IR and CUDA-C operation templates (e.g.,\ntensor contractions, element-wise operations, etc.) us\u0002ing pattern-matching.\n• The polyhedral model [18] has been used by Tensor\nComprehensions (TC) [43] and Diesel [14] to parame\u0002terize and automate the compilation of one or many\nDNN layers into LLVM-IR and CUDA-C progr","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.eecs.harvard.edu%2F~htk%2Fpublication%2F2019-mapl-tillet-kung-cox.pdf","id":"https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf","publishedDate":"2019-06-22T00:00:00.000Z","summary":"Triton is a language and compiler designed for tiled neural network computations. It uses a C-based language and an LLVM-based intermediate representation to express tensor programs, and employs tile-level optimization passes to generate efficient GPU code. Triton aims to provide portable implementations of matrix multiplication and convolution kernels that perform comparably to hand-tuned vendor libraries like cuBLAS and cuDNN.\n","title":"[PDF] Triton: An Intermediate Language and Compiler for Tiled Neural ...","url":"https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf"},{"content":"MLIR-based code generation for GPU tensor cores Computer Science, Engineering 2022 This work builds a transformation and lowering pipeline to automatically generate near-peak performance code for matrix-matrix multiplication (matmul) as well as matmul fused with simple pointwise operators targeting tensor cores on NVIDIA GPUs. CLBlast: A Tuned OpenCL BLAS Library C. Nugteren Computer Science 2018 ClBlast is an open-source BLAS library providing optimized OpenCL routines to accelerate dense linear algebra for a wide variety of devices and can combine multiple operations in a single batched routine, accelerating smaller problems significantly. [PDF]","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.semanticscholar.org%2Fpaper%2FTriton%253A-an-intermediate-language-and-compiler-for-Tillet-Kung%2F661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f","id":"https://www.semanticscholar.org/paper/Triton%3A-an-intermediate-language-and-compiler-for-Tillet-Kung/661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f","publishedDate":null,"summary":"This webpage discusses two research papers. The first paper focuses on automatically generating high-performance code for matrix-matrix multiplication and related operations on NVIDIA GPUs using a transformation and lowering pipeline. The second paper introduces CLBlast, an open-source BLAS library that provides optimized OpenCL routines for accelerating dense linear algebra on various devices.\n","title":"[PDF] Triton: an intermediate language and compiler for tiled neural ...","url":"https://www.semanticscholar.org/paper/Triton%3A-an-intermediate-language-and-compiler-for-Tillet-Kung/661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f"},{"content":"## Abstract\n\nThe validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts - usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial. We present Triton, a language and compiler centered around the concept of tile, i.e., statically shaped multi-dimensional sub-arrays. Our approach revolves around (1) a C-based language and an LLVM-based intermediate representation (IR) for expressing tensor programs in terms of operations on parametric tile variables and (2) a set of novel tile-level optimization passes for compiling these programs into efficient GPU code. We demonstrate how Triton can be used to build portable implementations of matrix multiplication and convolution kernels on par with hand-tuned vendor libraries (cuBLAS / cuDNN), or for efficiently implementing recent research ideas such as shift convolutions.\n\n## Related\n\nPaper\n\n### [Learning Brain Dynamics With Coupled Low-Dimensional Nonlinear Oscillators and Deep Recurrent Networks](https://research.ibm.com/publications/learning-brain-dynamics-with-coupled-low-dimensional-nonlinear-oscillators-and-deep-recurrent-networks)\n\nGermán Abrevaya, Guillaume Dumas, et al.\n\nNeural Computation\n\nConference paper\n\n### [Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts](https://research.ibm.com/publications/self-moe-towards-compositional-large-language-models-with-self-specialized-experts)\n\nJunmo Kang, Leonid Karlinsky, et al.\n\nICLR 2025\n\nPaper\n\n### [A neural network trained for prediction mimics diverse features of biological neurons and perception](https://research.ibm.com/publications/a-neural-network-trained-for-prediction-mimics-diverse-features-of-biological-neurons-and-perception)\n\nWilliam Lotter, Gabriel Kreiman, et al.\n\nNature Machine Intelligence\n\nConference paper\n\n### [Granite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning of Granular Tasks](https://research.ibm.com/publications/granite-function-calling-model-introducing-function-calling-abilities-via-multi-task-learning-of-granular-tasks)\n\nIbrahim Abdelaziz, Kinjal Basu, et al.\n\nEMNLP 2024\n\n[View all publications](https://research.ibm.com/publications)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fresearch.ibm.com%2Fpublications%2Ftriton-an-intermediate-language-and-compiler-for-tiled-neural-network-computations","id":"https://research.ibm.com/publications/triton-an-intermediate-language-and-compiler-for-tiled-neural-network-computations","publishedDate":"2019-06-22T00:00:00.000Z","summary":"Triton is a language and compiler designed for efficient GPU code generation for deep learning workloads. It uses a C-based language and an LLVM-based intermediate representation (IR) to express tensor programs using tile variables. Triton's tile-level optimization passes compile these programs into efficient GPU code, enabling the creation of portable implementations of matrix multiplication and convolution kernels that perform comparably to vendor libraries like cuBLAS and cuDNN.\n","title":"Triton: An intermediate language and compiler for tiled neural ...","url":"https://research.ibm.com/publications/triton-an-intermediate-language-and-compiler-for-tiled-neural-network-computations"},{"content":"[PLDI 2019](https://pldi19.sigplan.org)\n\nSat 22 - Wed 26 June 2019 [Phoenix, Arizona, United States](https://pldi19.sigplan.org/venue/pldi2019venue)\n\n[PLDI 2019](https://pldi19.sigplan.org) ( [series](https://pldi19.sigplan.org/series/pldi)) / MAPL 2019 ( [series](https://pldi19.sigplan.org/series/maps)) / [MAPL 2019](https://pldi19.sigplan.org/home/mapl-2019) /\n\n## Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations\n\nWho\n\n_[Philippe Tillet](https://pldi19.sigplan.org/profile/philippetillet)_, _[H. T. Kung](https://pldi19.sigplan.org/profile/htkung)_, _[David Cox](https://pldi19.sigplan.org/profile/davidcox)_\n\nTrack\n\n[MAPL 2019](https://pldi19.sigplan.org/home/mapl-2019)\n\nWhen\n\n**Sat 22 Jun 2019 10:20 - 11:00 at [105A](https://pldi19.sigplan.org/room/pldi2019venue-105a)** \\- [Session 1](https://pldi19.sigplan.org/home/mapl-2019#program)\n\nAbstract\n\nThe validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts – usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial. We present Triton, a language and compiler centered around the concept of tile, i.e., statically shaped multi-dimensional sub-arrays. Our approach revolves around (1) a C-based language and an LLVM-based intermediate representation (IR) for expressing tensor programs in terms of operations on parametric tile variables and (2) a set of novel tile-level optimization passes for compiling these programs into efficient GPU code. We demonstrate how Triton can be used to build portable implementations of matrix multiplication and convolution kernels on par with hand-tuned vendor libraries (cuBLAS / cuDNN), or for efficiently implementing recent research ideas such as shift convolutions.\n\n[**Philippe Tillet**](https://pldi19.sigplan.org/profile/philippetillet)\n\n[**H. T. Kung**](https://pldi19.sigplan.org/profile/htkung)\n\n[**David Cox**](https://pldi19.sigplan.org/profile/davidcox)\n\nSession Program\n\n#### Sat 22 Jun Displayed time zone: **Tijuana, Baja California** [change](https://pldi19.sigplan.org/pldi19.sigplan.org\\#userProgramSettings)\n\n| | |\n| --- | --- |\n| 09:00 - 11:00 | Session 1[MAPL](https://pldi19.sigplan.org/home/mapl-2019) at [105A](https://pldi19.sigplan.org/room/pldi2019venue-105a) |\n| 09:00 **40m** Talk | [Building Training Sets with Snorkel: Three Key Operators](https://pldi19.sigplan.org/pldi19.sigplan.org) MAPL [Alex Jason Ratner](https://pldi19.sigplan.org/profile/alexjasonratner) |\n| 09:40 **40m** Talk | [Machine Learning in Python with No Strings Attached](https://pldi19.sigplan.org/pldi19.sigplan.org) MAPL [Guillaume Baudart](https://pldi19.sigplan.org/profile/guillaumebaudart) IBM Research, [Martin Hirzel](https://pldi19.sigplan.org/profile/martinhirzel) IBM Research, [Kiran Kate](https://pldi19.sigplan.org/profile/kirankate), [Louis Mandel](https://pldi19.sigplan.org/profile/louismandel) IBM Research, [Avraham Shinnar](https://pldi19.sigplan.org/profile/avrahamshinnar) IBM Research |\n| 10:20 **40m** Talk | **[Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations](https://pldi19.sigplan.org/pldi19.sigplan.org)** MAPL [Philippe Tillet](https://pldi19.sigplan.org/profile/philippetillet), [H. T. Kung](https://pldi19.sigplan.org/profile/htkung), [David Cox](https://pldi19.sigplan.org/profile/davidcox) |\n\nx _Wed 24 Sep 20:40_","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpldi19.sigplan.org%2Fdetails%2Fmapl-2019-papers%2F1%2FTriton-An-Intermediate-Language-and-Compiler-for-Tiled-Neural-Network-Computations","id":"https://pldi19.sigplan.org/details/mapl-2019-papers/1/Triton-An-Intermediate-Language-and-Compiler-for-Tiled-Neural-Network-Computations","publishedDate":null,"summary":"This webpage is about the \"Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations\" paper presented at the MAPL 2019 track of PLDI 2019. The paper introduces Triton, a C-based language and LLVM-based intermediate representation (IR) for expressing tensor programs using tile variables. It also includes tile-level optimization passes for compiling these programs into efficient GPU code. The presentation took place on June 22, 2019, at 10:20 AM in room 105A. The authors are Philippe Tillet, H. T. Kung, and David Cox.\n","title":"Triton: An Intermediate Language and Compiler for Tiled Neural ...","url":"https://pldi19.sigplan.org/details/mapl-2019-papers/1/Triton-An-Intermediate-Language-and-Compiler-for-Tiled-Neural-Network-Computations"}],"status":"completed","toolCallId":"call_uDsmJArpuLyJxJHogXRigE9W","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Speaking as the paper’s author, here’s the “Feynman technique” explanation: teach it like I’m explaining it to a smart colleague who doesn’t yet know the details, then peel back layers until we reach how and why it works.\n\n1) The problem we set out to solve (why Triton exists)\n- Most deep learning workloads run great if they can be expressed as a few standard kernels (GEMM via cuBLAS, and a handful of convolutions via cuDNN).\n- But research and production keep inventing new operators (custom attention patterns, structured sparsity, odd convolutions, fused epilogues). If your op isn’t exactly what the vendor libraries expect, you either:\n  - Accept poor performance, or\n  - Hand-write a custom CUDA kernel (hard, brittle, unportable).\n- Existing DSLs/compilers (Halide, TVM, Tensor Comprehensions, etc.) help, but they typically describe computation as loops over scalar indices. That’s good for some cases, but:\n  - It’s hard to express non-affine or irregular patterns cleanly.\n  - They often struggle to match vendor-library performance on the most important dense operators unless you manually drop to micro-kernels or rely on complex scheduling heuristics.\n\nWhat we want: a way to write custom GPU kernels that is both expressive and fast, without having to be a CUDA ninjutsu expert.\n\n2) The core idea in one sentence\nTreat tiles (small, statically shaped blocks of a tensor) as first-class values in the language and the compiler, and optimize at the tile level instead of the scalar or whole-tensor level.\n\nWhy “tiles”?\n- Tiles are the natural unit of locality and reuse on GPUs: a warp/block cooperates to load a block of A and a block of B, computes a block of C, and moves on.\n- Most high-performance kernels (GEMM, conv, attention matmul) are built from repeating tile operations. If tiles are the primitive, tiling is not an afterthought—it’s the program.\n\n3) What Triton is (the components)\n- Triton-C: a C-like kernel language whose variables can be tiles: statically-shaped multi-dimensional sub-arrays. You write loads/stores and math over tiles, not just scalars. You specify shapes via tunable meta-parameters.\n- Triton-IR: an LLVM-based IR that has tile types and tile ops. This IR is designed so the compiler can do tile-level analyses and transformations (e.g., hoist, fuse, vectorize, predicate) that are otherwise painful from scalarized code.\n- Triton-JIT: a just-in-time compiler:\n  - Machine-independent passes: simplify tile programs (algebraic simplifications, dead code elim, shape propagation) without committing to any specific GPU.\n  - Machine-dependent passes: lower tile ops to warp- and thread-level code, generate coalesced memory accesses, shared-memory staging, vector instructions, predication for edges, etc., and then down to LLVM-IR/PTX.\n  - Auto-tuner: searches the meta-parameters (tile sizes TM/TN/TK, number of warps, unroll factors) to hit the device sweet spot.\n\n4) How you write code in Triton (what it feels like)\n- You write one kernel where each “program instance” (akin to a CUDA block) computes a tile of the output, say C[TM, TN].\n- You construct index tiles (rm, rn, rk), compute pointer tiles (pa, pb), load A[TM, TK] and B[TN, TK] tiles, compute a dot-product tile, accumulate into C[TM, TN], advance pointers by TK, loop, and finally store C with boundary masks.\n- Crucially, all those are tile operations. The compiler is free to map them to threads/warps, choose vector widths, organize shared memory, and predicate the edges.\n\n5) Why this makes performance easier to get\n- Correct by construction locality: If your code loads tiles and multiplies them, the compiler knows the lifetimes and shapes. It can plan coalesced global loads into shared memory, synchronize, reuse the data across the TK loop, and emit the right vectorized FMAs.\n- Fewer ways to shoot yourself in the foot: You aren’t manually juggling thread indices, pointer arithmetic, or per-thread predication for every scalar—Triton abstracts that at tile granularity.\n- Tuning becomes structured: The main knobs are the tile sizes and a few scheduling choices. An auto-tuner can sweep TM/TN/TK and pick winners per GPU and per problem shape.\n\n6) What’s new compared to prior work\n- Tile-first IR: Many systems treat tiling as a schedule attached to a scalar loop nest. We encode the math directly over tiles, so the program already exposes reuse and memory footprints at the right granularity.\n- Expressivity beyond affine loops: Because tile indices and masks are first-class, irregular indexing, predicated edges, fused epilogues (e.g., bias+ReLU+scaling) are natural without contorting loop bounds.\n- Competitive with vendor libraries on dense ops while retaining flexibility: We show matmul and conv on par with cuBLAS/cuDNN, but the same approach readily implements non-standard kernels (e.g., shift convolutions) efficiently.\n\n7) How the compiler actually lowers tile code to a fast GPU kernel\nThink of one Triton program instance as one macro-kernel:\n- Mapping tiles to threads/warps:\n  - The compiler chooses how the [TM, TN] output tile is partitioned across warps/threads and how [TM, TK] and [TN, TK] input tiles are cooperatively loaded.\n- Memory:\n  - Computes pointer arithmetic for each lane so global loads are coalesced.\n  - Stages tiles in shared memory (or registers if appropriate).\n  - Applies masks to handle edge tiles safely without branches (predication).\n- Compute:\n  - Unrolls the TK loop to overlap memory and compute.\n  - Emits vectorized dot-product micro-ops (FMAs). On newer GPUs this can map to tensor-cores if extended, but the MAPL’19 work stays at standard cores.\n- Synchronization:\n  - Inserts barriers around shared-memory tiles where needed, tries to avoid redundant syncs.\n- Register pressure and occupancy:\n  - Balances tile sizes and unroll to avoid spilling while keeping enough active warps.\n\n8) Results we showed (what you get out)\n- Matmul: Triton kernels match cuBLAS across arithmetic intensities; substantially outperform contemporary DSLs (AutoTVM, Tensor Comprehensions, PlaidML) on realistic shapes from RNNs/Transformers.\n- Convolution: We reimplemented cuDNN’s IMPLICIT_GEMM-style conv and matched performance.\n- New ops: Implemented shift convolution efficiently—an example of something outside vendor templates but natural in Triton.\n\n9) What you as a kernel author control vs. what Triton handles\nYou pick:\n- The algorithm at the tile level: what tiles to load, how to combine them, any fused elementwise operations.\n- The meta-parameters (TM, TN, TK; sometimes number of warps).\n- Optional layout/stride decisions if you need something specific.\n\nTriton handles:\n- Mapping tiles to warps/threads.\n- Coalescing, shared-memory tiling, predication and vectorization.\n- Register allocation via LLVM and codegen to PTX.\n- Auto-tuning of the meta-parameters.\n\n10) Limits and future directions (as we wrote then)\n- We didn’t (in this paper) integrate with tensor-core intrinsics; later work can target them.\n- We focused on GPUs; a CPU/other-accelerator backend would require new lowering strategies.\n- Integration as a backend for higher-level frameworks/DSLs (TVM, XLA, MLIR) was an obvious next step so frontends can emit Triton-IR instead of reinventing tiling.\n- More automated scheduling/search (beyond tile sizes) and learned cost models could further reduce manual tuning.\n\nMental model you should keep\n- Imagine writing a matmul but the variables are little 2D blocks, not scalars.\n- Your program is a concise description of the classic “load A and B tiles, compute C tile” pattern.\n- The compiler “fills in” the GPU mechanics (lane mappings, masks, shared memory, vector widths).\n- Because your program already expresses the right unit of locality (tiles), the compiler can hit near-expert performance and still let you write new, weird ops.\n\nIf you want to quickly internalize it, read the matmul kernel in the paper: you’ll see index tiles, pointer tiles, masked loads, a dot(A, trans(B)) on tiles, a TK loop, and a masked store. That’s the whole story—compressed to the level GPUs like to run.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d43b180d9081a19376bc77a2e7787e05f2b4ad821b73c1"}},"resumableStreamId":"STREAM:772cca8f-0e15-4048-94e1-1d66c0c5f395","role":"assistant","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","timeToFirstToken":0.81,"tokens":1855,"tokensPerSecond":34.32515450945561,"updated_at":1758739215636,"userId":"google:104458561188320708185","id":"j973rjxqjyy2mgkkqcjpg1mr0x7r63w0"},{"_creationTime":1758739387120.7542,"_id":"j970n7bt8efwbsh47pxtmsngfx7r7ez0","attachmentIds":[],"content":"explain where triton lie in the compiiler workflow","created_at":1758739387120,"messageId":"3c401da8-773d-4c5d-ad51-8f0bd9a44800","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","updated_at":1758739387120,"userId":"google:104458561188320708185","id":"j970n7bt8efwbsh47pxtmsngfx7r7ez0","providerMetadata":{}},{"_creationTime":1758739387120.7544,"_id":"j97dwy4zkw3k10btky8bg54mpn7r7f03","attachmentIds":[],"byok":false,"content":"Short answer: Triton sits at the “kernel-level DSL + compiler” layer between a deep learning framework and the GPU back end. It replaces hand-written CUDA kernels or vendor libraries for custom ops, and lowers to LLVM/PTX for execution on GPUs.\n\nA typical workflow stack with Triton looks like this:\n\n- Model level (PyTorch/JAX/TF)\n  - You write your model in a framework. Most ops call into standard libraries (cuBLAS/cuDNN) or framework kernels.\n  - For ops you want to customize or fuse, you write a Triton kernel (Python-decorated function using Triton’s language).\n\n- Triton frontend (DSL and JIT)\n  - You express the kernel in Triton’s tile-oriented language (Triton-C/Python API). Each “program instance” computes a tile of the output.\n  - You set meta-parameters (tile sizes, num warps, etc.). An autotuner may search these.\n\n- Triton compiler (middle end and backend)\n  - Lowers your tile-level code to Triton-IR (with first-class tile types/ops).\n  - Performs tile-aware optimizations (shape propagation, algebraic simplification, mask/predicate handling, memory coalescing plans).\n  - Lowers tiles to threads/warps, generates shared-memory staging, vectorization, synchronization.\n  - Emits LLVM IR → PTX (NVIDIA) and invokes the driver JIT to produce SASS for the target GPU.\n\n- Runtime\n  - The compiled kernel is launched from Python (or a framework integration) with problem sizes and pointers/strides to tensors.\n  - The framework can call it like any other kernel, and cache per-shape compiled binaries.\n\nSo, conceptually:\n\n- Compared to CUDA: Triton replaces you writing CUDA C++ by hand. You still write a kernel, but at a higher abstraction (tiles instead of per-thread scalars); Triton compiles it to CUDA-level machine code via LLVM/PTX.\n\n- Compared to vendor libraries (cuBLAS/cuDNN): Triton sits beside/above them. If your op matches a library primitive, you may use the library. If not (custom attention, fused epilogues, odd layouts), you implement it in Triton and get near-library performance.\n\n- Compared to whole-graph compilers (XLA, TVM, MLIR-based stacks):\n  - Those systems schedule and fuse at the graph or loop level and then need a kernel codegen backend.\n  - Triton can act as that kernel backend for GPU: the higher-level compiler lowers a fused op into a Triton kernel, and Triton handles the GPU-specific tiling/mapping/codegen.\n\nIn one line: Triton is a kernel-level DSL and compiler that you invoke from a framework or a higher-level compiler to generate high-performance GPU kernels, lowering through LLVM/PTX to the hardware.","created_at":1758739387121,"messageId":"81387fa0-2544-4696-8a4f-4b5268182601","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Short answer: Triton sits at the “kernel-level DSL + compiler” layer between a deep learning framework and the GPU back end. It replaces hand-written CUDA kernels or vendor libraries for custom ops, and lowers to LLVM/PTX for execution on GPUs.\n\nA typical workflow stack with Triton looks like this:\n\n- Model level (PyTorch/JAX/TF)\n  - You write your model in a framework. Most ops call into standard libraries (cuBLAS/cuDNN) or framework kernels.\n  - For ops you want to customize or fuse, you write a Triton kernel (Python-decorated function using Triton’s language).\n\n- Triton frontend (DSL and JIT)\n  - You express the kernel in Triton’s tile-oriented language (Triton-C/Python API). Each “program instance” computes a tile of the output.\n  - You set meta-parameters (tile sizes, num warps, etc.). An autotuner may search these.\n\n- Triton compiler (middle end and backend)\n  - Lowers your tile-level code to Triton-IR (with first-class tile types/ops).\n  - Performs tile-aware optimizations (shape propagation, algebraic simplification, mask/predicate handling, memory coalescing plans).\n  - Lowers tiles to threads/warps, generates shared-memory staging, vectorization, synchronization.\n  - Emits LLVM IR → PTX (NVIDIA) and invokes the driver JIT to produce SASS for the target GPU.\n\n- Runtime\n  - The compiled kernel is launched from Python (or a framework integration) with problem sizes and pointers/strides to tensors.\n  - The framework can call it like any other kernel, and cache per-shape compiled binaries.\n\nSo, conceptually:\n\n- Compared to CUDA: Triton replaces you writing CUDA C++ by hand. You still write a kernel, but at a higher abstraction (tiles instead of per-thread scalars); Triton compiles it to CUDA-level machine code via LLVM/PTX.\n\n- Compared to vendor libraries (cuBLAS/cuDNN): Triton sits beside/above them. If your op matches a library primitive, you may use the library. If not (custom attention, fused epilogues, odd layouts), you implement it in Triton and get near-library performance.\n\n- Compared to whole-graph compilers (XLA, TVM, MLIR-based stacks):\n  - Those systems schedule and fuse at the graph or loop level and then need a kernel codegen backend.\n  - Triton can act as that kernel backend for GPU: the higher-level compiler lowers a fused op into a Triton kernel, and Triton handles the GPU-specific tiling/mapping/codegen.\n\nIn one line: Triton is a kernel-level DSL and compiler that you invoke from a framework or a higher-level compiler to generate high-performance GPU kernels, lowering through LLVM/PTX to the hardware.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d43bbba364819ca3a09a402d1300d205b85698447cac0c"}},"resumableStreamId":"STREAM:cdda2772-35c0-4412-af8e-92c2b78d31a1","role":"assistant","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","timeToFirstToken":0.645,"tokens":600,"tokensPerSecond":57.46025665581306,"updated_at":1758739387121,"userId":"google:104458561188320708185","id":"j97dwy4zkw3k10btky8bg54mpn7r7f03"},{"_creationTime":1758739434811.139,"_id":"j972hf8nsbng8hbfbypfyccgd17r6k3a","attachmentIds":[],"content":"how well does triton perform as a cuda compiler?","created_at":1758739434811,"messageId":"af40e966-210e-466e-8908-3fbd6c294cda","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","updated_at":1758739434811,"userId":"google:104458561188320708185","id":"j972hf8nsbng8hbfbypfyccgd17r6k3a","providerMetadata":{}},{"_creationTime":1758739434811.1392,"_id":"j97asdhw70d2wde0rjsyt671jn7r7dvf","attachmentIds":[],"byok":false,"content":"Short answer: Very good for many DL kernels, often near cuBLAS/cuDNN on the shapes it’s tuned for, and sometimes beats them for specific cases or fusions—but it isn’t a general drop‑in replacement for NVIDIA’s full CUDA toolchain. Performance depends on kernel type, data type, GPU arch, and how well you tune the tile sizes/pipelining.\n\nWhat Triton does well as a “CUDA compiler”\n- Kernel-level productivity with high performance:\n  - Matmul/attention/convolution-style tiles: With proper BLOCK_M/N/K, num_warps, and pipelining, Triton kernels typically reach near-library throughput on CUDA cores and, on newer releases, map to tensor core paths (WMMA/WGMMA) for FP16/FP8.\n  - Elementwise/fused epilogues and memory-bound ops: Easy to fuse; large wins vs naïve PyTorch and often competitive with bespoke CUDA when memory traffic is the bottleneck.\n  - Real results reported publicly:\n    - PyTorch team’s “CUDA‑free” inference: End-to-end Llama3/Granite using only Triton kernels ran at 0.76–0.78× of CUDA stacks on H100 and 0.62–0.82× on A100 before deeper kernel tuning; their tuned FP8/FP16 GEMMs and flash-attention close much of that gap.\n    - Triton FP8 TK‑GEMM on H100: Up to 1.94× over Triton’s base matmul, and as high as 1.87× faster than cuBLAS FP8 for small‑batch attention shapes (after Split‑K and WGMMA tuning).\n    - Numerous case studies (GPTQ dequant, normalization, Split‑K GEMM) show Triton kernels matching or exceeding hand-written CUDA baselines after attention to coalescing, swizzled tiling, vectorized loads, and software pipelining.\n  - Architecture tracking: Active work with NVIDIA/AMD means day‑0 or early support for new instructions (e.g., Hopper WGMMA; Blackwell tensor cores; AMD MFMA/WMMA), so kernels using tl.dot can benefit without source changes.\n\nWhere Triton may lag or need extra work\n- Best-in-class vendor kernels still hard to beat broadly:\n  - cuBLAS/cuDNN/TensorRT-LLM include deep arch-specific tricks (TMA, threadblock clusters, graph-captured micro-schedules, kernel catalogs) that are costly to re-create. For “big square GEMM” and highly engineered convs, libraries are tough to surpass out of the box.\n- Shape sensitivity and tuning cost:\n  - You often need per-layer tuning (BLOCK_{M,N,K}, num_warps, stages, Split‑K) to hit peak. Without it, expect 0.6–0.9× of vendor kernels on key paths.\n- Launch/JIT overhead:\n  - JIT compile + Python launch overhead can dominate short kernels unless you use caching and CUDA Graphs.\n- Feature coverage:\n  - Some newest hardware features (e.g., Hopper TMA, threadblock clusters) or exotic data types/layouts may trail vendor stacks, though support keeps improving.\n\nPractical guidance\n- Use Triton when:\n  - You need custom/fused kernels (attention variants, epilogues, unusual layouts) with near‑CUDA performance.\n  - You want fast iteration in Python and auto-vectorized/coalesced codegen to PTX.\n  - You’re inside PyTorch 2.x (Inductor) or similar stacks that already generate Triton and you can swap in hand-tuned kernels for hotspots.\n- Prefer vendor libraries when:\n  - Your op matches a highly optimized primitive (classic GEMM/conv) and you need top speed immediately across many shapes and GPUs.\n- How to get “CUDA-like” performance in Triton:\n  - Tune tiles and pipeline stages; consider Split‑K for small‑M or large‑K shapes.\n  - Ensure coalesced loads/stores, swizzled tile ordering for L2 locality, vectorized memory ops, shared-memory staging, and overlap load/compute.\n  - Target tensor cores (FP16/BF16/FP8) where applicable; verify WGMMA usage via Nsight Compute.\n  - Use CUDA Graphs to amortize launch overhead; cache JIT artifacts.\n\nBottom line\n- As a CUDA-targeting compiler, Triton can generate kernels that are within a few percent of, equal to, or occasionally faster than hand-tuned CUDA/library kernels—especially for workload-specific shapes and fusions—provided you tune it. For the most polished vendor primitives across all shapes, NVIDIA libraries still set the ceiling, but the gap is narrow and shrinking on many transformer-critical kernels.","created_at":1758739434812,"messageId":"40f44be7-2863-41dc-90b7-7b482c05e34f","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"news","domains":null,"queries":["Triton performance vs CUDA cuBLAS cuDNN benchmarks 2023 2024 2025","OpenAI Triton compiler performance matmul benchmark PTX CUDA 2022 2024","PyTorch Triton kernels performance compared to CUDA kernels 2024","Triton 2.0 tensor cores performance benchmark 2023"]},"result":[{"content":"[Discord](https://pxl.to/ivxz41s)[Linkedin](https://www.linkedin.com/company/marktechpost/?viewAsMember=true)[Reddit](https://www.reddit.com/r/machinelearningnews/)[Twitter](https://twitter.com/Marktechpost)\n\n- [Home](https://www.marktechpost.com/)\n- [Open Source/Weights](https://www.marktechpost.com/category/technology/open-source/)\n- [Enterprise AI](https://www.marktechpost.com/category/editors-pick/enterprise-ai/)\n- [Robotics](https://www.marktechpost.com/category/robotics/)\n- [AI Agents](https://www.marktechpost.com/category/editors-pick/ai-agents/)\n- [MCP](https://www.marktechpost.com/category/editors-pick/model-context-protocol-mcp/)\n- [Tutorials](https://www.marktechpost.com/category/tutorials/)\n- [Voice AI](https://www.marktechpost.com/category/technology/artificial-intelligence/voice-ai/)\n- [Sponsorship](https://calendly.com/marktechpost/marktechpost-promotion-call)\n\nSearch\n\n[NewsHub](https://www.marktechpost.com/)\n\n[NewsHub](https://www.marktechpost.com/)\n\n[Premium Content](https://www.marktechpost.com/www.marktechpost.com)\n\n[Read our exclusive articles](https://www.marktechpost.com/www.marktechpost.com)\n\n[Facebook](https://www.marktechpost.com/www.marktechpost.com)\n\n[Instagram](https://www.marktechpost.com/www.marktechpost.com)\n\n[Twitter](https://www.marktechpost.com/www.marktechpost.com)\n\n- [Home](https://www.marktechpost.com/)\n- [Open Source/Weights](https://www.marktechpost.com/category/technology/open-source/)\n- [Enterprise AI](https://www.marktechpost.com/category/editors-pick/enterprise-ai/)\n- [Robotics](https://www.marktechpost.com/category/robotics/)\n- [AI Agents](https://www.marktechpost.com/category/editors-pick/ai-agents/)\n- [MCP](https://www.marktechpost.com/category/editors-pick/model-context-protocol-mcp/)\n- [Tutorials](https://www.marktechpost.com/category/tutorials/)\n- [Voice AI](https://www.marktechpost.com/category/technology/artificial-intelligence/voice-ai/)\n- [Sponsorship](https://calendly.com/marktechpost/marktechpost-promotion-call)\n\n[NewsHub](https://www.marktechpost.com/)\n\nSearch\n\n- [Home](https://www.marktechpost.com/)\n- [Open Source/Weights](https://www.marktechpost.com/category/technology/open-source/)\n- [Enterprise AI](https://www.marktechpost.com/category/editors-pick/enterprise-ai/)\n- [Robotics](https://www.marktechpost.com/category/robotics/)\n- [AI Agents](https://www.marktechpost.com/category/editors-pick/ai-agents/)\n- [MCP](https://www.marktechpost.com/category/editors-pick/model-context-protocol-mcp/)\n- [Tutorials](https://www.marktechpost.com/category/tutorials/)\n- [Voice AI](https://www.marktechpost.com/category/technology/artificial-intelligence/voice-ai/)\n- [Sponsorship](https://calendly.com/marktechpost/marktechpost-promotion-call)\n\n[Home](https://www.marktechpost.com/)[Artificial Intelligence](https://www.marktechpost.com/category/technology/artificial-intelligence/)[AI Infrastructure](https://www.marktechpost.com/category/technology/artificial-intelligence/ai-infrastructure/)Software Frameworks Optimized for GPUs in AI: CUDA, ROCm, Triton, TensorRT—Compiler Paths...\n\n- [Artificial Intelligence](https://www.marktechpost.com/category/technology/artificial-intelligence/)\n- [AI Infrastructure](https://www.marktechpost.com/category/technology/artificial-intelligence/ai-infrastructure/)\n- [Technology](https://www.marktechpost.com/category/technology/)\n- [AI Shorts](https://www.marktechpost.com/category/technology/ai-shorts/)\n- [Editors Pick](https://www.marktechpost.com/category/editors-pick/)\n- [Staff](https://www.marktechpost.com/category/editors-pick/staff/)\n- [Tech News](https://www.marktechpost.com/category/tech-news/)\n\n### **Table of contents**\n\n- [What actually determines performance on modern GPUs](https://www.marktechpost.com/www.marktechpost.com#h-what-actually-determines-performance-on-modern-gpus)\n- [CUDA: nvcc/ptxas, cuDNN, CUTLASS, and CUDA Graphs](https://www.marktechpost.com/www.marktechpost.com#h-cuda-nvcc-ptxas-cudnn-cutlass-and-cuda-graphs)\n- [ROCm: HIP/Clang toolchain, rocBLAS/MIOpen, and the 6.x series](https://www.marktechpost.com/www.marktechpost.com#h-rocm-hip-clang-toolchain-rocblas-miopen-and-the-6-x-series)\n- [Triton: a DSL and compiler for custom kernels](https://www.marktechpost.com/www.marktechpost.com#h-triton-a-dsl-and-compiler-for-custom-kernels)\n- [TensorRT (and TensorRT-LLM): builder-time graph optimization for inference](https://www.marktechpost.com/www.marktechpost.com#h-tensorrt-and-tensorrt-llm-builder-time-graph-optimization-for-inference)\n- [Practical guidance: choosing and tuning the stack](https://www.marktechpost.com/www.marktechpost.com#h-practical-guidance-choosing-and-tuning-the-stack)\n\nDeep-learning throughput hinges on how effectively a compiler stack maps tensor programs to GPU execution: thread/block schedules, memory movement, and instruction selection (e.g., Tensor Core MMA pipelines). In this article we will focus on four dominant stacks—CUDA, ROCm, Triton, and TensorRT—from the compiler’s perspective and explains which optimizations move the needle in practice.\n\n### **What actually determines performance on modern GPUs**\n\n**Across vendors, the same levers recur:**\n\n- **Operator scheduling & fusion:** reduce kernel launches and round-trips to HBM; expose longer producer→consumer chains for register/shared-memory reuse. TensorRT and cuDNN “runtime fusion engines” exemplify this for attention and conv blocks.\n- **Tiling & data layout:** match tile shapes to Tensor Core/WGMMA/WMMA native fragment sizes; avoid shared-memory bank conflicts and partition camping. CUTLASS documents warp-level GEMM tiling for both Tensor Cores and CUDA cores.\n- **Precision & quantization:** FP16/BF16/FP8 for training/inference; INT8/INT4 (calibrated or QAT) for inference. TensorRT automates calibration and kernel selection under these precisions.\n- **Graph capture & runtime specialization:** graph execution to amortize launch overheads; dynamic fusion of common subgraphs (e.g., attention). cuDNN 9 added graph support for attention fusion engines.\n- **Autotuning:** search tile sizes, unroll factors, and pipelining depths per arch/SKU. Triton and CUTLASS expose explicit autotune hooks; TensorRT performs builder-time tactic selection.\n\nWith that lens, here’s how each stack implements the above.\n\n### **CUDA: nvcc/ptxas, cuDNN, CUTLASS, and CUDA Graphs**\n\n**Compiler path.** CUDA code compiles through **nvcc** into PTX, then **ptxas** lowers PTX to SASS (arch-specific machine code). Controlling optimization requires feeding flags to both host and device phases; for kernels the key is `-Xptxas`. Developers often miss that `-O3` alone affects only host code.\n\n**Kernel generation & libraries.**\n\n- **CUTLASS** provides parametric templates for GEMM/conv, implementing warp-level tiling, Tensor Core MMA pipelines, and smem iterators designed for conflict-free access—canonical references for writing peak kernels, including Hopper’s **WGMMA** path.\n- **cuDNN 9** introduced runtime fusion engines (notably for attention blocks), native CUDA Graph integration for those engines, and updates for new compute capabilities—materially reducing dispatch overheads and improving memory locality in Transformer workloads.\n\n**Performance implications.**\n\n- Moving from unfused PyTorch ops to cuDNN attention fusion typically cuts kernel launches and global memory traffic; combined with **CUDA Graphs**, it reduces CPU bottlenecks in short-sequence inference.\n- On Hopper/Blackwell, aligning tile shapes to WGMMA/Tensor Core native sizes is decisive; CUTLASS tutorials quantify how mis-sized tiles waste tensor-core throughput.\n\n**When CUDA is the right tool.** You need maximum control over instruction selection, occupancy, and smem choreography; or you’re extending kernels beyond library coverage while staying on NVIDIA GPUs.\n\n### **ROCm: HIP/Clang toolchain, rocBLAS/MIOpen, and the 6.x series**\n\n**Compiler path.** ROCm uses **Clang/LLVM** to compile **HIP** (CUDA-like) into GCN/RDNA ISA. The 6.x series has focused on perf and framework coverage; release notes track component-level optimizations and HW/OS support.\n\n**Libraries and kernels.**\n\n- **rocBLAS** and **MIOpen** implement GEMM/conv primitives with arch-aware tiling and algorithm selection similar in spirit to cuBLAS/cuDNN. The consolidated changelog highlights iterative perf work across these libraries.\n- Recent ROCm workstream includes better **Triton** enablement on AMD GPUs, enabling Python-level kernel authoring while still lowering through LLVM to AMD backends.\n\n**Performance implications.**\n\n- On AMD GPUs, matching LDS (shared memory) bank widths and vectorized global loads to matrix tile shapes is as pivotal as smem bank alignment on NVIDIA. Compiler-assisted fusion in frameworks (e.g., attention) plus library autotuning in rocBLAS/MIOpen typically closes a large fraction of the gap to handwritten kernels, contingent on architecture/driver. Release documentation indicates continuous tuner improvements in 6.0–6.4.x.\n\n**When ROCm is the right tool.** You need native support and optimization on AMD accelerators, with HIP portability from existing CUDA-style kernels and a clear LLVM toolchain.\n\n### **Triton: a DSL and compiler for custom kernels**\n\n**Compiler path.** Triton is a Python-embedded DSL that lowers via **LLVM**; it handles vectorization, memory coalescing, and register allocation while giving explicit control over block sizes and program IDs. Build docs show the LLVM dependency and custom builds; NVIDIA’s developer materials discuss Triton’s tuning for newer architectures (e.g., Blackwell) with FP16/FP8 GEMM improvements.\n\n**Optimizations.**\n\n- **Autotuning** over tile sizes, `num_warps`, and pipelining stages; static **masking** for boundary conditions without scalar fallbacks; **shared-memory** staging and software pipelining to overlap global loads with compute.\n- Triton’s design aims to **automate** the error-prone parts of CUDA-level optimization while leaving block-level tiling choices to t","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.marktechpost.com%2F2025%2F09%2F14%2Fsoftware-frameworks-optimized-for-gpus-in-ai-cuda-rocm-triton-tensorrt-compiler-paths-and-performance-implications%2F","id":"https://www.marktechpost.com/2025/09/14/software-frameworks-optimized-for-gpus-in-ai-cuda-rocm-triton-tensorrt-compiler-paths-and-performance-implications/","publishedDate":"2025-09-14T00:00:00.000Z","summary":"This webpage from Marktechpost.com discusses software frameworks optimized for GPUs in AI, including CUDA, ROCm, Triton, and TensorRT. It covers compiler paths and performance implications. The page also includes links to various categories like Open Source, Enterprise AI, Robotics, and AI Agents, as well as links to the site's social media presence.\n","title":"Software Frameworks Optimized for GPUs in AI: CUDA, ROCm ...","url":"https://www.marktechpost.com/2025/09/14/software-frameworks-optimized-for-gpus-in-ai-cuda-rocm-triton-tensorrt-compiler-paths-and-performance-implications/"},{"content":"[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7aaa32dac5ae&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40hexiangnan%2Fgpus-for-large-language-models-kernels-triton-memory-coalescing-and-the-execution-hierarchy-7aaa32dac5ae&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40hexiangnan%2Fgpus-for-large-language-models-kernels-triton-memory-coalescing-and-the-execution-hierarchy-7aaa32dac5ae&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# GPUs for Large Language Models: Kernels, Triton, Memory Coalescing, and the Execution Hierarchy\n\n[Shawn](https://medium.com/@hexiangnan?source=post_page---byline--7aaa32dac5ae---------------------------------------)\n\n6 min read\n\n·\n\n1 day ago\n\n--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\n## Introduction\n\nLarge Language Models (LLMs) like GPT-4, LLaMA, and Mixtral are computationally demanding. Their massive scale — billions or even trillions of parameters — necessitates highly parallelized hardware to achieve practical training and inference times. That hardware is almost always the **GPU (Graphics Processing Unit)**.\n\nGPUs were originally built for rendering graphics but have evolved into massively parallel processors capable of accelerating matrix multiplications, convolutions, and tensor operations — the backbone of deep learning. In fact, the rise of transformer-based models directly coincided with advances in GPU architectures and software frameworks like CUDA, cuDNN, and more recently, Triton.\n\nIn this blog, we’ll unpack the technical foundations of how GPUs accelerate LLMs. We’ll dive into:\n\n- GPU programming concepts (blocks, threads, warps, waves).\n- **Kernels** and how they orchestrate computation.\n- **Memory coalescing**, the key to efficient bandwidth usage.\n- **Triton**, a modern kernel programming language for AI workloads.\n- Examples of how these concepts come together in training and inference of LLMs.\n\nBy the end, you’ll have a deep understanding of why GPUs are the backbone of LLMs, and how software and hardware interact to maximize efficiency.\n\n## 1\\. GPU Basics: Why GPUs Are Ideal for LLMs\n\nLLMs rely heavily on **linear algebra operations**: matrix multiplications, dot products, layer normalizations, and tensor reshaping. Training involves both forward passes (computing activations) and backward passes (gradients), with vast amounts of matrix multiplications.\n\nA GPU is fundamentally different from a CPU:\n\n- **CPU:** Few cores (e.g., 8–64), optimized for sequential, general-purpose tasks.\n- **GPU:** Thousands of smaller cores, optimized for massively parallel tasks.\n\nMatrix multiplication is embarrassingly parallel: each output element can be computed independently by summing products of input pairs. This aligns perfectly with the GPU architecture.\n\n## 2\\. CUDA Programming Model\n\nAt the heart of GPU programming is **CUDA (Compute Unified Device Architecture)**, NVIDIA’s programming interface. CUDA abstracts the GPU as a device where developers can write **kernels** that execute across thousands of threads simultaneously.\n\n## 2.1 Threads, Blocks, and Grids\n\nCUDA organizes computation hierarchically:\n\n- **Thread:** The smallest unit of execution. Each thread performs computation on one piece of data.\n- **Block:** A group of threads (1D, 2D, or 3D). Threads in a block can cooperate via shared memory.\n- **Grid:** A collection of blocks. Grids can also be multi-dimensional.\n\nFor example:\n\n```\n__global__ void vector_add(float *a, float *b, float *c, int N) { int idx = blockIdx.x * blockDim.x + threadIdx.x; if (idx < N) { c[idx] = a[idx] + b[idx]; }}\n```\n\nHere:\n\n- threadIdx.x → thread’s index inside the block.\n- blockIdx.x → block’s index inside the grid.\n- blockDim.x → number of threads per block.\n\nThe **global thread ID** is computed as blockIdx.x \\* blockDim.x + threadIdx.x.\n\nThis design allows CUDA to scale: if you launch 1024 threads per block and 1000 blocks, you get **1 million threads** in parallel.\n\n## 2.2 Warps and Waves\n\nThreads are not executed independently. GPUs group **32 threads into a warp** (on NVIDIA hardware). All threads in a warp execute the same instruction at the same time (SIMT: Single Instruction, Multiple Threads).\n\nIf threads in a warp diverge (e.g., some take an if branch, others don’t), performance drops because the warp serializes execution. This is called **warp divergence**.\n\nOn AMD GPUs, the term **wavefront** (often 64 threads) is used instead of warp.\n\nUnderstanding warps/waves is critical for writing efficient kernels.\n\n## 3\\. Kernels: The Workhorses of LLMs\n\nA **kernel** is a GPU function executed by many threads simultaneously. Deep learning frameworks like PyTorch, TensorFlow, and JAX rely on pre-optimized kernels (e.g., from cuBLAS, cuDNN) for heavy lifting.\n\nFor LLMs, the most critical kernels include:\n\n- **Matrix multiplication (GEMM)** — core of transformer attention and MLP layers.\n- **Softmax kernels** — for attention probability normalization.\n- **Layer normalization kernels** — stabilize training.\n- **Activation functions** (ReLU, GeLU, SwiGLU, etc.).\n\nEach of these kernels is tuned at the hardware level to exploit GPU parallelism and memory hierarchy.\n\n## 4\\. Memory Hierarchy and Coalescing\n\nGPUs have a **memory hierarchy**:\n\n1. **Registers** — private to each thread, fastest.\n2. **Shared memory** — shared across threads in a block, very fast (on-chip).\n3. **Global memory (VRAM)** — accessible to all threads, but slower (off-chip).\n\nEfficient GPU programming requires minimizing expensive global memory accesses.\n\n## 4.1 Memory Coalescing\n\nWhen a warp of 32 threads loads data, the GPU fetches memory in chunks (e.g., 128 bytes). If each thread accesses consecutive memory addresses, the requests are **coalesced** into one transaction.\n\nExample:\n\n- Good (coalesced): Threads 0–31 load A\\[0\\]..A\\[31\\] (consecutive).\n- Bad (non-coalesced): Threads 0–31 load A\\[0\\], A\\[1000\\], A\\[2000\\]… (scattered).\n\nNon-coalesced memory access wastes bandwidth, slowing performance.\n\n**LLMs are particularly sensitive** here because matrix multiplications and attention involve massive amounts of memory movement.\n\n## 5\\. Triton: A New Way to Write Kernels\n\nTraditionally, writing GPU kernels in CUDA is complex. Enter **Triton**, an open-source programming model developed by OpenAI. Triton lets you write custom GPU kernels in Python with performance close to hand-written CUDA.\n\nExample: Matrix multiplication kernel in Triton:\n\n```\nimport tritonimport triton.language as tl@triton.jitdef matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE: tl.constexpr): pid = tl.program_id(axis=0) m = pid // N n = pid % N a = tl.load(a_ptr + m * stride_am + tl.arange(0, BLOCK_SIZE) * stride_ak) b = tl.load(b_ptr + n * stride_bn + tl.arange(0, BLOCK_SIZE) * stride_bk) c = tl.sum(a * b, axis=0) tl.store(c_ptr + m * stride_cm + n * stride_cn, c)\n```\n\nWhy Triton matters:\n\n- Higher productivity (Python API).\n- Explicit control over memory layout.\n- Integration with PyTorch 2.0 (torch.compile uses Triton).\n\nLLMs benefit because attention, normalization, and activation kernels can be tuned more easily with Triton.\n\n## 6\\. Putting It Together: GPU Execution in LLM Training\n\nLet’s walk through what happens when you train an LLM step-by-step on GPUs.\n\n## 6.1 Forward Pass\n\n1. **Token embeddings** loaded from memory.\n2. **Matrix multiplications** (Q, K, V projections in attention).\n3. **Softmax kernel** normalizes attention scores.\n4. **Weighted sum** computed via GEMM.\n5. **MLP layers**: large matrix multiplications with GeLU/SwiGLU activations.\n6. **Layer normalization**.\n\nEach step invokes optimized GPU kernels.\n\n## 6.2 Backward Pass\n\n- Requires storing intermediate activations (big memory cost).\n- Computes gradients using kernels (often similar to forward kernels).\n- Optimizer step updates weights.\n\nBackward is usually **2–3× more expensive** than forward, because gradients and optimizer states are involved.\n\n## 7\\. GPU Concepts Applied to LLMs\n\nLet’s connect the GPU fundamentals to LLM workloads.\n\n## 7.1 Parameters\n\n- Stored in GPU VRAM.\n- Accessed repeatedly during forward and backward passes.\n- Memory coalescing ensures high throughput when loading weights.\n\n## 7.2 Activations\n\n- Stored during forward pass for use in backpropagation.\n- Often the **largest memory consumer**.\n- Activation checkpointing trades memory for compute.\n\n## 7.3 Optimizer States\n\n- Adam stores **two extra states per parameter** (momentum and variance).\n- Triples memory usage relative to raw parameters.\n\n## 7.4 Gradient Accumulation\n\n- Large models don’t fit full batches in memory.\n- Gradients accumulated over micro-batches before weight update.\n- Kernels handle partial gradient updates efficiently.\n\n## 8\\. Advanced Optimizations\n\n## 8.1 Mixed Precision\n\n- Use **FP16/BF16** for activations and weights.\n- Reduces memory footprint and doubles throughput.\n- Requires care with numerical stability.\n\n## 8.2 Kernel Fusion\n\n- Combine multiple operations into one kernel (e.g., bias add + activation).\n- Saves memory bandwidth.\n- Triton is excellent for writing fused kernels.\n\n## 8.3 Multi-GPU and Model Parallelism\n\n- **Tensor parallelis","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fmedium.com%2F%40hexiangnan%2Fgpus-for-large-language-models-kernels-triton-memory-coalescing-and-the-execution-hierarchy-7aaa32dac5ae","id":"https://medium.com/@hexiangnan/gpus-for-large-language-models-kernels-triton-memory-coalescing-and-the-execution-hierarchy-7aaa32dac5ae","publishedDate":"2025-09-21T18:43:58.851Z","summary":"This blog post explains how GPUs are essential for Large Language Models (LLMs) like GPT-4 due to their ability to handle the computationally intensive linear algebra operations LLMs rely on. It will cover GPU programming concepts, including kernels, memory coalescing, and Triton, a modern kernel programming language for AI workloads. The post will also explain the CUDA programming model, which organizes computation hierarchically using threads, blocks, and grids.\n","title":"Kernels, Triton, Memory Coalescing, and the Execution Hierarchy ...","url":"https://medium.com/@hexiangnan/gpus-for-large-language-models-kernels-triton-memory-coalescing-and-the-execution-hierarchy-7aaa32dac5ae"},{"content":"[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7ed8017e25f5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Ftop-python-libraries%2Fpytorch-officially-accelerates-inference-without-cuda-is-the-triton-era-coming-7ed8017e25f5&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Ftop-python-libraries%2Fpytorch-officially-accelerates-inference-without-cuda-is-the-triton-era-coming-7ed8017e25f5&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**Top Python Libraries**](https://medium.com/top-python-libraries?source=post_page---publication_nav-d565f18bf45f-7ed8017e25f5---------------------------------------)\n\n·\n\nPython is widely used in fields such as data analysis, machine learning, and web development. Sharing these skills will help you advance further in your career. [https://join.slack.com/t/aidisruptiont-9307882/shared\\_invite/zt-2vb2pzkqq-oTJmcTR\\_v0AWgJ31ZPfOGA](https://join.slack.com/t/aidisruptiont-9307882/shared_invite/zt-2vb2pzkqq-oTJmcTR_v0AWgJ31ZPfOGA)\n\n## NVIDIA CUDA is Becoming Obsolete\n\n# PyTorch Officially Accelerates Inference Without CUDA: Is the Triton Era Coming?\n\n## PyTorch explores non-CUDA inference with Triton kernels, challenging NVIDIA’s dominance in large model training and inference. Discover the future of AI computing.\n\n[Meng Li](https://medium.com/@mengyoupanshan?source=post_page---byline--7ed8017e25f5---------------------------------------)\n\n7 min read\n\n·\n\nSep 9, 2024\n\n--\n\nListen\n\nShare\n\nRecently, PyTorch shared insights on implementing non-CUDA computations, including micro-benchmark comparisons of different kernels and discussing future improvements to Triton kernels to close the gap with CUDA.\n\nFor training, fine-tuning, and inference of large language models (LLMs), NVIDIA GPUs and CUDA are commonly used.\n\nIn broader machine learning computation, CUDA is also heavily relied upon, offering significant performance boosts for accelerated models.\n\nWhile CUDA dominates the field of accelerated computing, it has also become one of NVIDIA’s key advantages.\n\nHowever, other efforts are emerging to challenge CUDA. One example is Triton, introduced by OpenAI, which shows advantages in usability, memory efficiency, and AI compiler stack development.\n\nRecently, PyTorch announced its plans for large model inference without using NVIDIA CUDA.\n\nPyTorch explained why they are exploring 100% Triton, saying:\n\n> “Triton offers a path to run large models on various GPUs, including those from NVIDIA, AMD, Intel, and other GPU-based accelerators. It also provides a higher-level abstraction for GPU programming in Python, making it faster to write high-performance kernels with PyTorch than vendor-specific APIs.”\n\nPress enter or click to view image in full size\n\nIn a PyTorch blog post, they discussed how to achieve FP16 inference using popular LLMs like Meta’s Llama3–8B and IBM’s Granite-8B Code, with all computations fully executed using OpenAI’s Triton language.\n\nFor token generation time using Triton-based models, PyTorch achieved 0.76–0.78x the performance of CUDA-dominant workflows on NVIDIA H100 GPUs and 0.62–0.82x on A100 GPUs for Llama and Granite.\n\nPress enter or click to view image in full size\n\nPerhaps the era of NVIDIA domination is coming to an end.\n\n## Transformer Block Composition\n\nThe PyTorch team first broke down the computations in Transformer-based models.\n\nThe diagram below shows a typical Transformer block “kernel.”\n\nPress enter or click to view image in full size\n\nLlama3’s core operations include:\n\n- RMSNorm\n- Matrix Multiplication: Fused QKV\n- RoPE\n- Attention\n- Matrix Multiplication: Output Projection\n- RMSNorm\n- Matrix Multiplication: Fused Gate + Up Projection\n- Activation: SiLU\n- Element Wise Multiplication\n- Matrix Multiplication: Down Projection\n\nEach operation is executed through one or more kernels on the GPU.\n\nWhile the kernel details vary across Transformer models, the core operations remain the same.\n\nFor example, IBM’s Granite 8B Code model uses bias in its MLP layer, which differs from Llama3. Such changes require kernel adjustments.\n\nA typical model stacks these Transformer blocks, connected via embedding layers.\n\n## Model Inference\n\nThe typical model architecture shares code with the python \\`model.py\\` file initiated by PyTorch.\n\nIn default PyTorch Eager Execution mode, these kernels are executed using CUDA.\n\nTo achieve 100% Triton for end-to-end Llama3–8B and Granite-8B inference, handwritten Triton kernels were integrated, along with torch.compile for Triton operations.\n\nFirst, PyTorch replaced smaller operations with compiler-generated Triton kernels. Then, they replaced more complex computations, like matrix multiplications and flash attention, with handwritten Triton kernels.\n\nTorch.compile automatically generates Triton kernels for RMSNorm, RoPE, SiLU, and element-wise multiplication.\n\nUsing tools like Nsight Systems, they observed these generated kernels, appearing as tiny deep green kernels between matrix multiplication and attention.\n\nPress enter or click to view image in full size\n\nFor Llama3–8B models, the PyTorch team noticed that 80% of the end-to-end latency comes from matrix multiplication and attention kernels, both still CUDA-based.\n\nTo close the remaining gap, PyTorch replaced the matmul and attention kernels with handwritten Triton kernels.\n\n## Triton SplitK GEMM Kernel\n\nFor matrix multiplication in linear layers, PyTorch wrote a custom FP16 Triton GEMM (General Matrix-Matrix Multiplication) kernel, leveraging the SplitK work decomposition.\n\n## Triton SplitK GEMM Kernel\n\nTo optimize performance, the PyTorch team used an exhaustive search to tune the SplitK GEMM kernel. Granite-8B and Llama3–8B models have linear layers with different weight matrix shapes.\n\nPress enter or click to view image in full size\n\nTo achieve the best performance, the Triton kernel must be adjusted for each shape.\n\nAfter tuning for each layer, PyTorch achieved a 1.20x E2E speedup on Llama3–8B and Granite-8B compared to untuned Triton kernels.\n\n## Flash Attention Kernel\n\nThe PyTorch team evaluated existing Triton flash attention kernels, including:\n\n- AMD Flash\n- OpenAI Flash\n- Dao AI Lab Flash\n- XFormers Flash\n- PyTorch FlexAttention\n\nThey assessed the text generation quality of each kernel in eager and compiled modes. The comparison is shown in Figure 5.\n\nPress enter or click to view image in full size\n\nThe chart summarizes out-of-the-box performance, with kernels 2 to 5 expected to meet standards after modifications.\n\nHowever, this highlights that having a benchmarkable kernel is just the first step to making it a production-ready solution.\n\nThe team chose to use the AMD flash attention kernel in further tests, compiled via torch.compile, producing clear outputs in both eager and compiled modes.\n\nTo ensure compatibility with torch.compile, the team defined it as a custom torch operator.\n\nThe process of wrapping the flash attention kernel involves two steps:\n\n1. Wrapping the function as a custom torch operator.\n\nPress enter or click to view image in full size\n\n2\\. Adding a FakeTensor kernel to calculate the output shape of the flash kernel given the input tensors (q, k, v).\n\nPress enter or click to view image in full size\n\nBy defining Triton flash kernels as custom ops, the team successfully compiled and ran them end-to-end.\n\nPress enter or click to view image in full size\n\nThe diagram shows that with the SplitK matmul kernel integrated, the flash attention kernel, wrapped in a torch op, can run torch.compile with 100% Triton kernels for forward passes.\n\n## End-to-End Benchmarking\n\nPyTorch tested the end-to-end performance of Granite-8B and Llama3–8B on NVIDIA H100 and A100 (single GPU) with two configurations:\n\nTriton kernel setup:\n\n- Triton SplitK GEMM\n- AMD Triton Flash Attention\n\nCUDA kernel setup:\n\n- cuBLAS GEMM\n- cuDNN Flash Attention — Scaled Dot-Product Attention (SDPA)\n\nThe throughput and inter-token latency in both eager and compiled modes under typical inference settings are shown in the following chart.\n\nPress enter or click to view image in full size\n\nOverall, Triton models achieved up to 78% of CUDA performance on H100 and 82% on A100. The gaps were caused by matmul and flash attention kernel delays.\n\n## Micro-Benchmarking\n\nFigure 8 compares Triton and CUDA kernel latencies (running Llama3–8B on NVIDIA H100).\n\nWith a random prompt (batch size = 1, prompt sequence length = 44), the decoding delay was measured.\n\nResults showed Triton matmul kernels were 1.2 to 1.4 times slower than CUDA, and AMD Triton Flash Attention was 1.6 times slower than CUDA SDPA.\n\nThese results highlight the need for further improvements to core kernel performance in GEMM and Flash Attention.\n\nRecent work (e.g., FlashAttention-3, FlexAttention) has proposed better ways to utilize hardware and Triton, and PyTorch aims to build on this to achieve faster performance.\n\nThe team compared FlexAttention with SDPA and AMD’s Triton Flash kernels.\n\nThey are currently validating FlexAttention’s end-to-end performance.\n\nInitial micro-benchmarks suggest that with smaller query vectors, FlexAttention could handle longer contexts and decoding shapes.\n\nPress enter or click to view image in full size\n\n## Future Work\n\nPyTorch plans to","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fmedium.com%2Ftop-python-libraries%2Fpytorch-officially-accelerates-inference-without-cuda-is-the-triton-era-coming-7ed8017e25f5","id":"https://medium.com/top-python-libraries/pytorch-officially-accelerates-inference-without-cuda-is-the-triton-era-coming-7ed8017e25f5","publishedDate":"2024-09-09T00:00:00.000Z","summary":"This article discusses PyTorch's exploration of non-CUDA inference using Triton kernels, potentially challenging NVIDIA's dominance in large model training and inference. PyTorch is exploring 100% Triton to run large models on various GPUs, including those from NVIDIA, AMD, and Intel. Benchmarks show that Triton-based models achieve 0.76-0.78x the performance of CUDA-dominant workflows on NVIDIA H100 GPUs and 0.62-0.82x on A100 GPUs for Llama and Granite models.\n","title":"PyTorch Officially Accelerates Inference Without CUDA: Is the Triton Era Coming?","url":"https://medium.com/top-python-libraries/pytorch-officially-accelerates-inference-without-cuda-is-the-triton-era-coming-7ed8017e25f5"},{"content":"[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fac80f4d13885&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fellyralte%2Fsupercharging-gpu-performance-with-triton-writing-custom-kernels-made-easy-ac80f4d13885&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fellyralte%2Fsupercharging-gpu-performance-with-triton-writing-custom-kernels-made-easy-ac80f4d13885&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# **Supercharging GPU Performance with Triton: Writing Custom Kernels Made Easy**\n\n[Fellyralte](https://medium.com/@fellyralte?source=post_page---byline--ac80f4d13885---------------------------------------)\n\n5 min read\n\n·\n\nNov 15, 2024\n\n--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nWhere Triton works\n\nWhen it comes to accelerating computations on GPUs, most developers lean on well-optimized libraries like cuDNN or cuBLAS. These libraries, while powerful, cater to general use cases and often fall short when dealing with specialized or non-standard workloads. Enter **Triton**, an open-source library that empowers developers to write **custom GPU kernels** with ease, all while achieving performance on par with or better than handcrafted CUDA implementations.\n\nIn this blog, we’ll explore **Triton**, why it’s a game-changer, and how it stacks up against industry staples like **cuDNN**. We’ll also dive into the mechanics of Triton, its control over GPU memory, the advantages of custom kernels, and when to consider this approach for your GPU workloads.\n\n## What Is Triton?\n\n**Triton** is an open-source library developed by OpenAI that simplifies the creation of highly efficient custom GPU kernels. At its core, Triton provides a high-level programming model that abstracts away much of the complexity of CUDA programming while offering fine-grained control over GPU resources. This makes it ideal for developers who want to:\n\n- Optimize non-standard operations.\n- Develop domain-specific computations.\n- Push the performance boundaries of existing libraries.\n\nTriton leverages **just-in-time (JIT)** compilation to generate GPU kernels during runtime, ensuring that the code is optimized for the specific hardware and workload.\n\n## Why Write Custom Kernels?\n\nLibraries like **cuDNN** and **cuBLAS** are excellent tools for common deep learning operations (e.g., convolutions, matrix multiplications). However, they come with limitations:\n\n- **Lack of flexibility**: Custom operations or fused kernels may not be supported.\n- **Overhead for specific workloads**: Generic implementations may not be as efficient as task-specific optimizations.\n- **Suboptimal memory access patterns**: Applications with unique memory layouts can suffer from poor performance.\n\nCustom kernels allow developers to address these challenges by tailoring operations to the exact requirements of their workload.\n\n## The Magic of `@triton.jit` and Memory Optimization\n\nTriton introduces the `@triton.jit` decorator, which transforms a Python function into an optimized GPU kernel. Let’s break down how this works:\n\n## 1\\. Kernel Compilation\n\nWhen you annotate a function with `@triton.jit`, Triton compiles it into GPU-executable code. The JIT compiler optimizes the code based on parameters like tile sizes, thread-block configurations, and memory usage patterns.\n\n## 2\\. Efficient Memory Management\n\nTriton doesn’t directly interact with GPU memory like **SRAM** or specific cache levels; instead, it allows you to control how data is loaded into and accessed from these memory levels within a kernel. This enables:\n\n- **Coalesced memory access**: Aligning memory reads/writes with GPU architecture to reduce access latency from **global memory (DRAM)**.\n- **Shared memory optimization**: Managing **L1 cache (shared memory)** to avoid bank conflicts and maximize throughput within **Streaming Multiprocessors (SMs)**.\n- **Prefetching**: Loading data into registers or shared memory before it’s needed, reducing access times from slower memory tiers like DRAM.\n\n## 3\\. Automatic Parallelization\n\nTriton automatically handles parallelism by breaking computations into **blocks** and **threads**, mapping them efficiently onto the GPU’s Streaming Multiprocessors (SMs). Each kernel operates on a specific **grid of blocks and threads**, similar to what you might set up in a CUDA program, but without needing to handle the low-level details manually.\n\n## How Triton Fits into the GPU Architecture\n\nWhile Triton doesn’t “sit” on any specific memory level, it does provide a high-level interface to manage data across the GPU memory hierarchy:\n\n- **Registers**: The fastest memory, typically used within individual threads.\n- **L1 cache (shared memory)**: Accessible by all threads within a block, suitable for frequently accessed data in small blocks.\n- **L2 cache** and **DRAM (global memory)**: Slower, used for storing larger data. Triton helps optimize access patterns to these layers to ensure efficient memory usage and avoid bottlenecks.\n\nThis memory control gives Triton an edge over libraries like cuDNN, where memory management is predetermined and optimized for standard operations.\n\nTriton vs. cuDNN: A Comparison\n\nPress enter or click to view image in full size\n\n## Use Case Example: Optimizing Convolution Operations\n\n- **cuDNN**: Designed for standard convolution operations and excels in training neural networks.\n- **Triton**: Ideal if you need fused operations, such as combining convolution with activation and normalization in a single kernel.\n\n## Writing a Triton Kernel: An Example\n\nLet’s look at a simple example of a custom kernel that performs **element-wise addition** using Triton:\n\n## Code Example: Element-Wise Addition\n\n```\nimport tritonimport triton.language as tl\n```\n\n```\n@triton.jitdef add_kernel(X, Y, Z, N, BLOCK_SIZE: tl.constexpr): # Identify thread block idx = tl.program_id(0) # Compute offsets for this block offsets = idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE) # Mask to handle out-of-bound accesses mask = offsets < N # Load data x = tl.load(X + offsets, mask=mask) y = tl.load(Y + offsets, mask=mask) # Perform addition z = x + y # Store result tl.store(Z + offsets, z, mask=mask)\n```\n\nTo run the kernel:\n\n```\nBLOCK_SIZE = 128n_elements = 1024X = torch.randn(n_elements, device='cuda')Y = torch.randn(n_elements, device='cuda')Z = torch.empty_like(X)\n```\n\n```\nadd_kernel[(n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE]( X, Y, Z, n_elements, BLOCK_SIZE)\n```\n\nThis simple kernel highlights the ease of writing and optimizing GPU computations with Triton.\n\n## Key Benefits of Triton\n\n1. **Ease of Use**: Write GPU kernels in Python without needing deep CUDA expertise.\n2. **Memory Control**: Triton lets you programmatically manage memory usage across L1, L2, and DRAM for optimized performance.\n3. **Customizability**: Create operations that standard libraries don’t support.\n4. **Cross-Hardware Compatibility**: Works seamlessly across different GPU vendors.\n5. **Seamless Integration**: Integrates with PyTorch, NumPy, and other frameworks.\n\n## When to Use Triton\n\n- **Custom Operations**: When your workload involves non-standard computations.\n- **Kernel Fusion**: When combining multiple operations into a single kernel reduces memory overhead.\n- **Optimizing Memory Bottlenecks**: When memory access patterns dominate performance.\n- **Research & Experimentation**: When developing new methods that aren’t supported by existing libraries.\n\n## Conclusion\n\nTriton bridges the gap between high performance and developer productivity, making custom GPU kernel development accessible to a broader audience. While cuDNN and similar libraries remain the go-to tools for standard deep learning workloads, Triton shines in scenarios requiring **flexibility** and **fine-tuned optimization**.\n\nIf you’re tackling unique computational challenges or striving for cutting-edge performance, Triton is your key to unlocking the full potential of GPUs — without the steep learning curve of CUDA.\n\n## References\n\n[https://openai.com/index/triton/](https://openai.com/index/triton/)\n\n[**Custom GPU Kernels via Triton - PyTorch/XLA master documentation** \\\n\\\n**PyTorch/XLA now supports Triton kernels, enabling high-performance deep learning model execution on GPUs. Triton, a…**\\\n\\\npytorch.org](https://pytorch.org/xla/master/features/triton.html?source=post_page-----ac80f4d13885---------------------------------------)\n\n[https://triton-lang.org/main/index.html](https://triton-lang.org/main/index.html)\n\n[Gpu](https://medium.com/tag/gpu?source=post_page-----ac80f4d13885---------------------------------------)\n\n[OpenAI](https://medium.com/tag/openai?source=post_page-----ac80f4d13885---------------------------------------)\n\n[High Performace Computing](https://medium.com/tag/high-performace-computing?source=post_page-----ac80f4d13885---------------------------------------)\n\n[Deep Learning](https://medium.com/tag/deep-learning?source=post_page-----ac80f4d13885---------------------------------------)\n\n[Computer Vision](https://medium.com/tag/computer-vision?source=post_page-----ac80f4d13885---------------------------------------)\n\n[**Written by Fellyralte**](https://medium.com/@fellyralte?source=post_page---post_author_info--ac80f4d13885-------------------","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fmedium.com%2F%40fellyralte%2Fsupercharging-gpu-performance-with-triton-writing-custom-kernels-made-easy-ac80f4d13885","id":"https://medium.com/@fellyralte/supercharging-gpu-performance-with-triton-writing-custom-kernels-made-easy-ac80f4d13885","publishedDate":"2024-11-15T00:00:00.000Z","summary":"This blog post introduces Triton, an open-source library developed by OpenAI that simplifies writing custom GPU kernels for optimized performance. Triton allows developers to create efficient, custom GPU kernels, offering fine-grained control over GPU resources and achieving performance comparable to or better than handcrafted CUDA implementations. It's particularly useful for optimizing non-standard operations, developing domain-specific computations, and pushing the performance boundaries of existing libraries, especially when compared to the limitations of libraries like cuDNN and cuBLAS.\n","title":"Supercharging GPU Performance with Triton: Writing Custom Kernels Made Easy","url":"https://medium.com/@fellyralte/supercharging-gpu-performance-with-triton-writing-custom-kernels-made-easy-ac80f4d13885"},{"content":"[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F95d99ed1422b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40official.indrajit.kar%2Fhigh-performance-deep-learning-part-3-custom-kernel-optimization-for-pytorch-exploring-cudnn-and-95d99ed1422b&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40official.indrajit.kar%2Fhigh-performance-deep-learning-part-3-custom-kernel-optimization-for-pytorch-exploring-cudnn-and-95d99ed1422b&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# High Performance Deep learning Part 3:Custom Kernel Optimization for PyTorch, Exploring CuDNN and Triton\n\n[Indrajit](https://medium.com/@official.indrajit.kar?source=post_page---byline--95d99ed1422b---------------------------------------)\n\n7 min read\n\n·\n\nOct 28, 2024\n\n--\n\nListen\n\nShare\n\n**By Indrajit Kar and Zonunfeli Ralte**\n\nPress enter or click to view image in full size\n\n## Introduction\n\nIn today’s world of AI, where models are continuously growing in size and complexity, the demand for efficient GPU utilization has reached an all-time high. From natural language processing to computer vision, the computational requirements of deep learning have put unprecedented strain on available hardware resources. Efficient GPU processing is more crucial than ever for developers and researchers, as it directly affects the speed of innovation and the practicality of AI solutions in real-world applications.\n\nOne of the key components of GPU efficiency is the kernel used to perform basic operations like matrix multiplication, convolution, and activation functions. Kernels serve as the underlying computational engines in deep learning frameworks, providing optimized routines for processing large datasets in parallel. However, not all kernels are created equal. CuDNN, NVIDIA’s deep learning library, is the default in frameworks like PyTorch, offering highly optimized kernels for standard operations. On the other hand, Triton — a more recent open-source approach — offers developers the flexibility to create custom kernels, potentially unlocking even greater performance for specific tasks but at the cost of additional programming complexity.\n\nThis blog post compares two implementations of a Convolutional Neural Network (CNN) to explore the performance impacts of using different kernel backends. **Code 1** uses PyTorch’s standard operations with CuDNN acceleration, while **Code 2** leverages Triton kernels for custom GPU operations. Both implementations are designed to train and evaluate an MNIST digit classifier but differ significantly in how core operations are executed. By examining these differences, we aim to shed light on the potential trade-offs between pre-built and custom kernels in the GPU-hungry AI landscape.\n\nThe codes for this Blog can be found [here](https://github.com/Feli4AI/High-Performance-Deep-learning/blob/main/Triton.ipynb)\n\n**Kernels**\n\nIn computing, a **kernel** is a function or small program designed to execute a specific computation on a processing unit, like a CPU or GPU. In the context of GPUs, a kernel is executed by thousands of threads in parallel, each processing data in a highly efficient, parallel manner. Kernels are central to GPU programming in frameworks like CUDA, Triton, and OpenCL, where they handle tasks such as matrix multiplication, image processing, and neural network operations. They allow developers to offload computationally intensive tasks to GPUs, exploiting their massive parallelism for faster execution on large datasets.\n\n**Triton**\n\nTriton is a programming language and compiler for writing high-performance GPU code, particularly for deep learning applications. It relies on the LLVM (Low Level Virtual Machine) compiler infrastructure to generate optimized machine code for GPUs. When you write Triton kernels in Python, Triton’s compiler transforms this code into an intermediate representation (IR). This IR is then lowered to LLVM’s IR, where LLVM’s suite of optimization passes enhances the code. Finally, LLVM’s backend targets generate the machine code specific to the GPU architecture. By leveraging LLVM, Triton benefits from its robust optimization capabilities and hardware support, enabling efficient execution of custom GPU kernels.\n\n**About LLVM**\n\nLLVM (Low Level Virtual Machine) is an open-source compiler infrastructure designed to optimize and generate machine code for various hardware architectures. It originally started as a way to create a “virtual machine” for executing programs in a low-level, language-agnostic form. Now, LLVM is widely used for building compilers, thanks to its modular, flexible design.\n\nKey components of LLVM include:\n\n1. **Intermediate Representation (IR)**: A platform-independent, low-level code format that allows optimizations and transformations before converting it to machine code.\n2. **Optimization Passes**: A series of transformations that improve performance and efficiency (e.g., inlining, loop unrolling).\n3. **Backend Code Generators**: LLVM can target many architectures (x86, ARM, GPUs) by converting IR into optimized assembly or machine code for each hardware.\n\nLLVM powers many language compilers, like Clang (C/C++) and Rust, and is widely used in GPUs and hardware-specific tools, such as Triton.\n\n**CuDNN**\n\ncuDNN (CUDA Deep Neural Network library) does not use LLVM. cuDNN is a highly optimized library developed by NVIDIA specifically for accelerating deep learning operations on NVIDIA GPUs. It is built directly on CUDA, NVIDIA’s parallel computing platform and application programming interface (API) for GPU computing.\n\ncuDNN’s optimizations are tailored to deep learning primitives (like convolutions, pooling, and activation functions) and leverage CUDA-specific optimizations and low-level GPU instructions directly, bypassing the need for LLVM’s intermediate representation and general-purpose compilation process. Therefore, cuDNN operates closer to the hardware through CUDA, without relying on LLVM for code generation or optimization.\n\n## Table of Contents\n\n1. [Introduction](https://medium.com/medium.com#introduction)\n2. [Kernels](https://medium.com/medium.com#kernels)\n3. [Triton](https://medium.com/medium.com#triton)\n4. [LLVM](https://medium.com/medium.com#llvm)\n5. [CuDNN](https://medium.com/medium.com#cudnn)\n6. [Comparison of PyTorch and Triton Implementations for CNNs](https://medium.com/medium.com#comparison-of-pytorch-and-triton-implementations-for-cnns)\n\n[Code 1: PyTorch CNN Model](https://medium.com/medium.com#code-1-pytorch-cnn-model)\n\n- [Model Architecture](https://medium.com/medium.com#model-architecture)\n- [Data Loading and Preprocessing](https://medium.com/medium.com#data-loading-and-preprocessing)\n- [Training Loop](https://medium.com/medium.com#training-loop)\n\n[Code 2: CNN with Triton Kernels](https://medium.com/medium.com#code-2-cnn-with-triton-kernels)\n\n- [Using Triton for ReLU Activation](https://medium.com/medium.com#using-triton-for-relu-activation)\n- [Custom Triton 2D Convolution Kernel](https://medium.com/medium.com#custom-triton-2d-convolution-kernel)\n- [Linear Layers and Forward Pass](https://medium.com/medium.com#linear-layers-and-forward-pass)\n- [Training Loop](https://medium.com/medium.com#training-loop-triton)\n\n[7\\. CuDNN vs. Triton Kernels](https://medium.com/medium.com#cudnn-vs-triton-kernels)\n\n- [Performance and Efficiency](https://medium.com/medium.com#performance-and-efficiency)\n- [Flexibility and Optimization](https://medium.com/medium.com#flexibility-and-optimization)\n\n[8\\. Conclusion](https://medium.com/medium.com#conclusion)\n\n## Comparison of PyTorch and Triton Implementations for CNNs\n\n## Code 1: PyTorch CNN Model\n\n## Model Architecture\n\nOur first implementation is a basic CNN model using PyTorch. This model is structured with two convolutional layers, each followed by a ReLU activation function and a flattening layer before moving into fully connected layers.\n\n```\nimport torchimport torch.nn as nn\n```\n\n```\nclass CNNModel(nn.Module): def __init__(self): super(CNNModel, self).__init__() self.conv1 = nn.Conv2d(1, 16, 3, stride=1) self.relu1 = nn.ReLU() self.conv2 = nn.Conv2d(16, 32, 3, stride=1) self.relu2 = nn.ReLU() # Flattened size calculation after convolutions self.flattened_size = self._get_flattened_size() self.fc1 = nn.Linear(self.flattened_size, 128) self.relu3 = nn.ReLU() self.fc2 = nn.Linear(128, 10) def _get_flattened_size(self): with torch.no_grad(): x = torch.zeros(1, 1, 28, 28) x = self.conv1(x) x = self.conv2(self.relu1(x)) return x.numel() def forward(self, x): x = self.conv1(x) x = self.relu1(x) x = self.conv2(x) x = self.relu2(x) x = x.view(x.size(0), -1) x = self.fc1(x) x = self.relu3(x) x = self.fc2(x) return x\n```\n\n## Data Loading and Preprocessing\n\nThe model uses the MNIST dataset for training and testing. The data is normalized to improve convergence.\n\n```\nfrom torchvision import datasets, transformsfrom torch.utils.data import DataLoader\n```\n\n```\ntransform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])train_dataset = datasets.MNIST('./data', train=True, transform=transform, download=True)test_dataset = datasets.MNIST('./data', train=False, transform=transform)train_loader = DataLoader(train","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fmedium.com%2F%40official.indrajit.kar%2Fhigh-performance-deep-learning-part-3-custom-kernel-optimization-for-pytorch-exploring-cudnn-and-95d99ed1422b","id":"https://medium.com/@official.indrajit.kar/high-performance-deep-learning-part-3-custom-kernel-optimization-for-pytorch-exploring-cudnn-and-95d99ed1422b","publishedDate":"2024-12-03T00:00:00.000Z","summary":"This blog post discusses the importance of efficient GPU utilization in deep learning, especially with the increasing size and complexity of AI models. It compares two implementations of a Convolutional Neural Network (CNN) for MNIST digit classification, one using PyTorch's standard operations with CuDNN acceleration and the other using Triton kernels for custom GPU operations. The post aims to highlight the trade-offs between pre-built and custom kernels in deep learning. The code for the blog can be found on GitHub.\n","title":"High Performance Deep learning Part 3:Custom Kernel Optimization for PyTorch, Exploring CuDNN and Triton","url":"https://medium.com/@official.indrajit.kar/high-performance-deep-learning-part-3-custom-kernel-optimization-for-pytorch-exploring-cudnn-and-95d99ed1422b"},{"content":"[Technical Blog](https://developer.nvidia.com/blog)\n\n[Subscribe](https://developer.nvidia.com/email-signup)\n\n[Related Resources](https://developer.nvidia.com/developer.nvidia.com#main-content-end)\n\n[Generative AI](https://developer.nvidia.com/blog/category/generative-ai/)\nEnglish한국어中文\n\n# OpenAI Triton on NVIDIA Blackwell Boosts AI Performance and Programmability\n\nFeb 05, 2025\n\nBy [Pradeep Ramani](https://developer.nvidia.com/blog/author/prramani/), [Jason Knight](https://developer.nvidia.com/blog/author/jaknight/), [Philippe Tillet](https://developer.nvidia.com/blog/author/philopenai/), [Thomas Raoux](https://developer.nvidia.com/blog/author/thomasraoux/), [Paweł Szczerbuk](https://developer.nvidia.com/blog/author/pawelszczerbuk/) and [Peter Bell](https://developer.nvidia.com/blog/author/peterbell10/)\n\nLike\n\n[Discuss (0)](https://developer.nvidia.com/developer.nvidia.com#entry-content-comments)\n\n- [L](https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fopenai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability%2F)\n- [T](https://twitter.com/intent/tweet?text=OpenAI+Triton+on+NVIDIA+Blackwell+Boosts+AI+Performance+and+Programmability+%7C+NVIDIA+Technical+Blog+https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fopenai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability%2F)\n- [F](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fopenai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability%2F)\n- [R](https://www.reddit.com/submit?url=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fopenai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability%2F&title=OpenAI+Triton+on+NVIDIA+Blackwell+Boosts+AI+Performance+and+Programmability+%7C+NVIDIA+Technical+Blog)\n- [E](mailto://developer.nvidia.com?subject=I'd like to share a link with you&body=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fopenai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability%2F)\n\nAI-Generated Summary\n\nLike\n\nDislike\n\n- The Triton compiler now supports NVIDIA BLACKWELL architecture, enabling developers to leverage its advanced features through a Python-based compiler, thanks to NVIDIA's collaboration with OpenAI.\n- The NVIDIA BLACKWELL architecture introduces a new Tensor Core that improves throughput and energy efficiency for matrix multiplications, with Triton's compiler infrastructure automatically exploiting these capabilities for FP8 and FP16 GEMM operations.\n- Triton's support for new precision formats, such as the Open Computing Project's microscaling formats, provides higher average precision at higher performance, with MXFP8 and MXFP4 formats offering exceptional performance and new operating points in the precision-performance trade-off space.\n\nAI-generated content may summarize information incompletely. Verify important information. [Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\n\nMatrix multiplication and attention mechanisms are the computational backbone of modern AI workloads. While libraries like [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) provide highly optimized implementations, and frameworks such as [CUTLASS](https://github.com/NVIDIA/cutlass) offer deep customization, many developers and researchers need a middle ground that combines performance with programmability.\n\nThe [open-source Triton compiler](https://openai.com/index/triton/) on the [NVIDIA Blackwell architecture](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/) addresses this need by exposing Blackwell’s advanced features through an intuitive programming model.\n\nAs a result of NVIDIA’s ongoing collaboration with OpenAI, the Triton compiler now supports the NVIDIA Blackwell architecture. This ensures that developers and researchers can use the latest and greatest features from Blackwell architecture easily from the comfort of a Python-based compiler such as Triton.\n\n## Performance advances on NVIDIA Blackwell\n\nThe NVIDIA Blackwell architecture introduces substantial improvements in both raw computing power and architectural innovations. NVIDIA’s collaboration with OpenAI has focused on leveraging these capabilities transparently through Triton’s compiler infrastructure, particularly in two key areas:\n\n- Matrix multiplications including flash attention\n- New precision formats\n\n### Matrix multiplications\n\nThe NVIDIA Blackwell architecture adds a brand-new Tensor Core designed from the ground up for improved throughput and [energy efficiency](https://www.nvidia.com/en-us/glossary/energy-efficiency/).\n\nBy extending Triton’s Matrix Multiply-Accumulate (MMA) pipelining machinery, we’ve enabled automatic exploitation of NVIDIA Blackwell’s new Tensor Cores. This required careful analysis of memory access patterns and sophisticated compiler transformations to ensure correct and efficient compute / data-movement overlap.\n\nThe result is exceptional performance for both FP8 and FP16 GEMM operations out of the box, with these optimizations automatically applying to any kernel using Triton’s \\`tl.dot\\` primitive. Overall, Triton manages to achieve near-optimal performance, comparable to library implementations across several critical use cases.\n\n_Figure 1.  Performance improvements with Triton on NVIDIA Blackwell_\n\nFigure 1 shows that Triton optimizations on NVIDIA Blackwell architecture bring hardware performance improvements to users in both FP16 and FP8 in this _K_ sweep analysis for a typical generative AI size of GEMM kernel, as provided in the Triton tutorials.\n\n#### Flash attention\n\nFlash attention, a crucial primitive in modern transformer architectures, sees significant speedups on NVIDIA Blackwell through Triton, with up to 1.5x for FP16 attention over the NVIDIA Hopper GPU architecture.\n\nWhile we continue to optimize absolute performance through ongoing compiler enhancements on FP8 and other precisions, the current work helps customers readily transition to NVIDIA Blackwell on Day 0 for existing products.\n\nAnother important aspect to note here is the ability to deliver this performance gain “for free” with existing Triton flash attention implementations, requiring no code changes.\n\n_Figure 2. Large performance gains for more complex workloads_\n\nFigure 2 shows that more complex workloads, such as the flash attention kernel as provided in the Triton tutorials, again show the large performance gains of the NVIDIA Blackwell architecture when unlocked with Triton compiler improvements. Some improvements from this work have improved NVIDIA Hopper attention performance as well and don’t show up in this data.\n\n### New precision formats\n\nNVIDIA Blackwell introduces revolutionary block-scaled floating point formats, including the [Open Computing Project’s microscaling formats](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf), which Triton now unlocks forNVIDIABlackwell-powered hardware acceleration.\n\nThese formats provide higher average precision at higher performance than the non-native block-scaling techniques emulated frequently in LLM inference projects today.\n\nFor OCP format support, MXFP8 GEMMs on Triton showcase exceptional performance similar to the FP8 GEMMs performance accelerated and shown earlier in this post, while natively allowing for scaling in the Tensor Core.\n\nSimilarly, MXFP4 provides a new operating point in the precision-performance trade-off space but while offering double the hardware-accelerated performance of FP8 and MXFP8 GEMMs.\n\nTo learn more about the new block-scaled floating point support, take a look at the new [Triton tutorial](https://github.com/triton-lang/triton/pull/5813) dedicated to this functionality.\n\n## Areas of improvement going forward\n\nThe layout and packing of sub-byte datatype formats like MXFP4 still require care by the end user. We look forward to working with the community to improve the ergonomics for kernel authors and seamless framework integrations.\n\nThe earlier-referenced matrix multiplication kernels across all data types still achieve relatively low utilization when `GEMM_K` is small. This can be mitigated through manual sub-tiling in the kernel itself and has been implemented in the GEMM tutorials as an example. It will eventually be addressed transparently in the compiler through automatic warp-specialization.\n\n## More information\n\nPhillippe Tillet, the creator of Triton, and NVIDIA will be diving into the details of this NVIDIA Blackwell work and the resulting performance at the NVIDIA GTC conference on March 17.\n\n[Register to attend GTC 2025 virtually or or attend live](https://www.nvidia.com/gtc/pricing/?nvid=nv-int-bnr-346123).\n\nThis release establishes a powerful foundation for NVIDIA Blackwell support in Triton—but it’s just the beginning. Here’s how you can help shape what’s next:\n\n- Explore the complete implementation in the [/triton-lang/triton](https://github.com/triton-lang/triton) GitHub repo.\n- Join the vibrant [GPU MODE Discord](https://discord.com/invite/gpumode) community.\n- Dive into our growing [collection of examples and tutorials](https://developer.nvidia.com/blog/tag/ai-inference-microservices/).\n\nStart building with Triton on NVIDIA Blackwell today and unlock the full potential of NVIDIA’s latest architecture while maintaining complete control over your development.\n\nHave ideas or encountered issues? Contact our NVIDIA product manager [Matthew Nicely](https://github.com/manicely) by tagging him on GitHub.\n\n[Discuss (0)](https://developer.nvidia.com/developer.nvidia.com#entry-content-comments)\n\nLike\n\n## Tags\n\n[Data Center / Cloud](https://developer.nvidia.com/blog/category/data-center-cloud/) \\| [Generative AI](https://developer.nvidia.com/blog/category/generative-ai/) \\| [Cloud Services](https://developer.nvidia.com/blog/recent-posts/?industry=Cloud+Services) \\| [Blackwell](https://developer.nvidia.com/blog/recent-posts/?products=Blackwell) \\| [Hopper](https://dev","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fopenai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability%2F","id":"https://developer.nvidia.com/blog/openai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability/","publishedDate":"2025-02-05T00:00:00.000Z","summary":"This NVIDIA technical blog post announces that the Triton compiler now supports the NVIDIA Blackwell architecture. This collaboration between NVIDIA and OpenAI allows developers to leverage Blackwell's advanced features through a Python-based compiler. The Blackwell architecture introduces a new Tensor Core that improves throughput and energy efficiency for matrix multiplications. Triton's support for new precision formats provides higher average precision at higher performance.\n","title":"OpenAI Triton on NVIDIA Blackwell Boosts AI Performance and ...","url":"https://developer.nvidia.com/blog/openai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability/"},{"content":"[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fba2babce8cc5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fflymy-ai%2Fhow-to-outperform-pytorch-kernels-with-openais-triton-compiler-ba2babce8cc5&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fflymy-ai%2Fhow-to-outperform-pytorch-kernels-with-openais-triton-compiler-ba2babce8cc5&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**FlyMy.AI**](https://medium.com/flymy-ai?source=post_page---publication_nav-9343f2050356-ba2babce8cc5---------------------------------------)\n\n·\n\nThe Agentic Cloud. We make your agents fly — slashing costs and latency for AI at scale.\n\nFeatured\n\n# How to Outperform PyTorch Kernels with OpenAI’s Triton Compiler\n\n[Denis Timonin](https://medium.com/@denistimonin?source=post_page---byline--ba2babce8cc5---------------------------------------)\n\n10 min read\n\n·\n\nNov 13, 2024\n\n--\n\nListen\n\nShare\n\n> By FlyMy.AI team: Mateus, Denis\\| 01/11/2024\n\nAt FlyMy.AI, we’re building the fastest Compound AI platform, delivering unparalleled inference speed for all types of neural networks and AI applications. We take pride in our in-depth system implementation, optimizing every component from SIMD kernels to distillation techniques and protocols. This relentless focus on optimization is how we achieved the [world’s fastest real-time demo of Diffusion](http://flymy.ai) [NN](http://flymy.ai) , maintaining high image quality without compromising on FPS\n\nFor some parts in our system we use Triton, a language and compiler developed by OpenAI, aims to provide better performance than CUDA while using a higher-level language. This allows us to rewrite certain neural network algorithms in Triton, outperforming those in popular Python libraries like PyTorch — while still writing in Python.\n\nIn this post, we’ll demonstrate how well-known kernels can be implemented in Triton, showcasing its significant performance improvements. Specifically, we’ll focus on a kernel related to input normalization — **Group Normalization**.\n\n## 1\\. Types of Normalization Techniques\n\nNormalization techniques are essential in AI because they stabilize and accelerate training by ensuring that inputs to each layer are on a consistent scale, making learning more efficient. In Generative AI (GenAI), these techniques are critical for producing high-quality, realistic outputs. They maintain balance in the data, preventing issues like exploding or vanishing gradients, which can hinder model performance.\n\n**Batch Normalization** has been the most used algorithm for normalizing layers in deep learning models for quite a while now. It consists of normalizing the channels by using the mean and variance over all batches, however efficiency is diminished if the amount of batches isn’t largeenough (usually at least 32). After the mean and variance of a specific channel are computed, the channel is normalized, then scaled and shifted accordingly..\n\n**Layer Normalization** does the same thing, however over all channels of a specific batch. **Instance Normalization**, on the other hand, performs the calculations only over a channel in a batch. **Group Normalization** is a mid-term between these methods, in which the calculations are performed over a group of channels of a specific batch; this amount of channels in which the algorithm runs through has to be divisible by the amount of channels in total. The following image describes the relations between each method.\n\nPress enter or click to view image in full size\n\nImage 1: the most popular normalization techniques in AI\n\n## **2\\. Group Normalization in GenerativeAI**\n\nBatch Normalization is a widely used technique known for delivering high-quality results, but Group Normalization (GN) is particularly important in Generative AI and diffusion models. GN enhances the stability and quality of neural networks during training and overcomes the limitations of BatchNorm, which can be problematic in image generation processes.\n\nSpecifically:\n\n- **_Batch Size Independence_**: GroupNorm works well with any batch size, making it ideal for tasks with small or variable batches, such as diffusion models.\n- **_Consistent Stability_**: GroupNorm offers more reliable stability across different tasks and architectures, and it generalizes better in non-i.i.d. settings, such as when data distribution varies across batches that may not be from the same source.\n- **_No Running Statistics_**: Unlike BatchNorm, GroupNorm doesn’t require running statistics, simplifying the inference process.\n\n## 3\\. Step by Step of the Algorithm\n\nWe’re going to follow using an example of a tensor, whose batch size is 2, with each batch consisting of 4 channels, height 2 and width 2. As such, the example has the shape (N, C, H, W) = (2, 4, 2, 2)And also, let’s define the following tensors as the weights (also known as gamma) and the bias (also known as beta). These two tensors must have the same length as the amount of channels in the input tensor, as they are applied independently for each channel.\n\n```\nγ = [0.2963, −0.9232, 1.4572, −1.0557]β = [0.0984, 1.2018, −1.0154, 0.0261]\n```\n\nFinally, we define what’s the amount of groups that we’re going to work with. This value needs to be a divisor of the number of channels of the input, and so for this case we must choose between 1, 2 and 4. If we choose 1, we’re pretty much doing the same as layer normalization, and if wechoose 4, it’s going to be the same as instance normalization. To demonstrate the middle-ground that is group normalization, I’m going to choose 2 for this example.Therefore, the code using the builtin group normalization function from PyTorch should look something like this:\n\n```\nimport torcha = torch.tensor( [[[[0.0127, 0.6262], [-0.3884, -0.9077]], [[-0.5615, -0.4755], [-1.2070, 0.2371]], [[1.2504, 0.0138], [-0.5010, 0.3860]], [[-0.2113, 0.8218], [-0.4039, 1.7963]]], [[[0.6527, 1.4151], [-0.3092, -0.4007]], [[0.4586, -1.1754], [-0.3117, -0.9210]], [[-2.5762, 0.6758], [0.4550, 0.1805]], [[-0.3143, -0.8534], [0.8388, 0.7529]]]], device='cuda')groups = 2gamma = torch.tensor([0.2963, -0.9232, 1.4572, -1.0557], device='cuda')beta = torch.tensor([0.0984, 1.2018, -1.0154, 0.0261], device='cuda')res = torch.nn.functional.group_norm(a, groups, gamma, beta, 1e-5)\n```\n\nAs such, let’s calculate the value of the element (0, 0, 0, 0) (remembering that the tensor is 0-indexed), by doing the following:\n\n- First of all, let’s calculate the mean. Since we’re working with element (0, 0, 0, 0), then we must calculate the mean and variance of the first group of the first channel of the first batch. By looking at the example, we can see that we’ll be working with the following values: _0.0127, 0.6262, −0.3884, −0.9077, −0.5615, −0.4755, −1.2070, 0.2371_ In the formula below _xi_ are the elements and _n_ is the amount of elements. In this case, the mean is _µ_ = −0.3330125\n\n- Then we need to calculate the variance, which is simply the average of the square difference to the mean. We find that _σ2_ = 0.31679136\n\n- Now, we’re able to find out the new value of _xi. U_ sually we use that ϵ = 10−5, and so for our first element, we find out that _ˆxi_ = 0.6142166:\n\n- Now we can’t forget about the gammas and the betas. _γ_ is the gamma value associated to that channel, and β is also the one associated to that channel. Since we’re working with the first channel, we see that γ = 0.2963 and β = 0.0984. Plugging back in, we find that yi = 0.280392:\n\n## 4\\. Triton Implementation\n\nFirst of all, the implementation of the algorithm will vary depending on how the tensor is configured in memory. This depends on whether or not the format is in channels last or not; for a better understanding [check out here](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html). For simplicity reasons, we’re going to work as if the input tensor isin classic contiguous storage, however the difference to the other one is how we will iterate through the memory.\n\n### 4.1 Welford Combine\n\nLoading everything into GPU memory for calculations isn’t efficient, especially when working with large datasets. Some algorithms calculate variance in an online manner, meaning you don’t need the entire dataset at once. This is useful because, for each group where we calculate the mean and variance, dividing it into smaller blocks is faster than processing everything at once.\n\nThe Welford Combine algorithm is straightforward. It works by storing the running mean and the running squared difference from the mean, similar to the naive method. Here’s how it can be understood:\n\nPress enter or click to view image in full size\n\nOf course, implementing this linearly would be a bit costly, as you’d have to apply the algorithm having a separate function call for every element, so what we can do is do a bunch at the same time (size being the block size that we’re going to work with) and at the end combine them all by using the function triton.language.reduce. The Welford Combine should then look something like this:\n\n```\n@triton.jitdef welford_combine(mean1, m21, weight1, mean2, m22, weight2): delta = mean2 - mean1 new_weight = weight1 + weight2 w2_over_w = tl.where(new_weight == 0.0, 0.0, weight2/new_weight) return ( mean1 +","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fmedium.com%2Fflymy-ai%2Fhow-to-outperform-pytorch-kernels-with-openais-triton-compiler-ba2babce8cc5","id":"https://medium.com/flymy-ai/how-to-outperform-pytorch-kernels-with-openais-triton-compiler-ba2babce8cc5","publishedDate":"2025-02-12T00:00:00.000Z","summary":"This article from FlyMy.AI discusses how they use OpenAI's Triton compiler to optimize neural network algorithms, specifically focusing on achieving better performance than PyTorch kernels. The article highlights the use of Triton for implementing and improving the performance of Group Normalization, a technique used to stabilize and accelerate AI model training.\n","title":"How to Outperform PyTorch Kernels with OpenAI’s Triton Compiler","url":"https://medium.com/flymy-ai/how-to-outperform-pytorch-kernels-with-openais-triton-compiler-ba2babce8cc5"},{"content":"[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7ad93fe4a54c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40michael.diggin%2Fimplementing-a-split-k-matrix-multiplication-kernel-in-triton-7ad93fe4a54c&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40michael.diggin%2Fimplementing-a-split-k-matrix-multiplication-kernel-in-triton-7ad93fe4a54c&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# Implementing a Split-K Matrix Multiplication Kernel in Triton\n\n## Iteratively writing a cuBLAS level kernel in Triton\n\n[Michael Diggin](https://medium.com/@michael.diggin?source=post_page---byline--7ad93fe4a54c---------------------------------------)\n\n13 min read\n\n·\n\nMar 1, 2025\n\n--\n\n1\n\nListen\n\nShare\n\n### Introduction\n\nMatrix multiplication is at the heart of many computational tasks, powering applications from deep learning to scientific simulations. Modern GPUs are outrageously fast at this operation, with peak performance approaching 1000 trillion floating-point operations per second. There has been decades of research put in to achieving this level of efficiency, with multiple sophisticated algorithms and optimised low level kernel code.\n\nThanks to [OpenAI’s Triton](https://triton-lang.org/main/index.html), a language and compiler for high-performance GPU programming, engineers and researchers now have the tools to write custom GPU kernels with relative ease — making it easier than ever to reach and match the performance of industry-standard libraries like cuBLAS.\n\nIn this article, we’ll explore one such advanced algorithm: **Split-K**, seeing what it is, where it can be useful, and how to write a Triton kernel implementing it that achieves performance comparable to cuBLAS.\n\nSome familiarity with Triton is assumed, the [tutorials](https://triton-lang.org/main/getting-started/tutorials/index.html) on their documentation site are great resources for understanding how to write kernels.\n\n### Tiled Matrix Multiplication Algorithm Overview\n\nTo understand the motivation behind Split-K it’s helpful to have an understanding of a ‘typical’ matrix multiplication algorithm for computing C = A @ B. Often called a ‘blocked’ or ‘tiled’ algorithm, the idea is to split the output matrix C into blocks, and compute each block independently. In the below image, to compute the block (1, 1) of C, we need to iterate over the second row of A and the second column of B. It exploits the fact that the matrix multiplication to compute this can be decomposed into a sum of smaller matrix multiplications over these blocks of A and B.\n\nPress enter or click to view image in full size\n\nThe yellow output block is the sum of smaller matrix multiplications of ‘tiles’ of A and B\n\nModern GPUs are extremely fast at computing these smaller matrix multiplications thanks to their tensor cores — a hardware component build for this purpose.\n\nThis algorithm is precisely what the [Matrix Multiplication tutorial](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py) from Triton implements and shows that for square matrices it can match the performance of cuBLAS, that is, when M = N = K. We won’t go through the full tutorial here, however in this Triton kernel the important part is:\n\n```\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)): # Load the next block of A and B, generate a mask by checking the K dimension. # If it is out of bounds, set it to 0. a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0) b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0) # We accumulate along the K dimension. accumulator += tl.dot(a, b) # Advance the ptrs to the next K block. a_ptrs += BLOCK_SIZE_K * stride_ak b_ptrs += BLOCK_SIZE_K * stride_bk\n```\n\nEach ‘program instance’ that Triton launches will compute a block of size (BLOCK\\_M, BLOCK\\_N) within C. On every iteration it loads a block of size (BLOCK\\_M, BLOCK\\_K) from A, (BLOCK\\_K, BLOCK\\_N) from B, computes their matrix multiplication using `tl.dot` and adds it to the accumulator. Then it moves the pointers for A and B along the K dimension so it can load the next block. This accumulator is later stored in the output matrix C.\n\nTriton launches a grid of these program instances, one for each block, which get computed in parallel. There’s a trade off between having larger blocks but fewer, versus many blocks but each one is smaller. However, in the case of square matrices when the shape is fairly large it’s incredibly efficient and is on par with cuBLAS and a typical PyTorch implementation.\n\n### Problems when K is large\n\nBut what about when the matrices aren’t square — the cases where the inner dimension K is much larger than M and N? With the above algorithm, each program instance that Triton launches would iterate over the K dimension for a long time, and there’s less of an advantage taken with parallelism, as these iterations are all sequential. Since M and N are smaller, the number of output blocks, and hence the grid size, is smaller as well so there are fewer program instances to fully utilise the GPU.\n\nPress enter or click to view image in full size\n\nTiled approach when K >>> M, N\n\nIn the above image, the output matrix C only has 4 output blocks, so the tiled approach would launch 4 program instances. Each of these instances has 16 blocks to load (8 from each of A and B) and to perform computations on.\n\nWe can see that this is an issue by using the same algorithm and benchmark set up from the Triton tutorial, Setting M, N = 256 and letting K get as large at 4096 gives the following plot:\n\nBenchmark of torch.matmul and the tiled Triton kernel. On an A100 GPU with float16 datatypes\n\nThe x-axis of the plot is K which varies while M and N stay fixed. The y-axis is the terra-flops per second, or number of trillions of floating point operations that the kernel could compute per second. For a matrix multiply, this is `2*M*N*K / duration_in_seconds` and a higher value is better.\n\nIt’s extremely clear that it doesn’t take very long for this specific algorithm to be suboptimal with these inputs. Compared to the native PyTorch operation of `torch.matmul(A, B)` this algorithm can’t keep up as K gets larger and larger.\n\n### Split-K Algorithm Overview\n\nSplit-K provides a way to overcome the shortcomings of a typical tiled approach specifically for these kinds of inputs. When M and N are smaller, the above approach limits the amount of parallelism, and hence the GPU utilisation, that can be achieved. Instead, Split-K’s insight is to also parallelise along the K dimension, breaking up the long loop over the contracting dimension. Different program instances would contribute to the same output block in C, each computing a partial sum and finally synchronise reducing this partial sum.\n\nThere are a few small nuances to this approach. Typically in Triton kernels, the output tensor is saved or written to memory using `tl.store`, however that doesn’t work here as program instances would overwrite each other. Instead we need to use `tl.atomic_add` to synchronise atomic updates to the memory location. This adds a small bit of overhead but that can be ‘hidden’ as we get greater parallelism, there’s more work that can be done by the GPU cores. Due to the use of atomics, the data type of C has to be either float16 or float32 when performing the atomic addition.\n\nWe’ll start with a simple approach to this algorithm, and iterate on it until we reach something that is comparable with PyTorch and cuBLAS. The source code for all kernels below can be found [here](https://github.com/michael-diggin/triton-kernels/tree/main/split_k).\n\n### Attempt 1 — Naive\n\nThe first naive attempt is to just split the K dimension into equal chunks and have each program instance compute the partial sum of their chunk before reducing it using `tl.atomic_add`. This looks something like\n\nPress enter or click to view image in full size\n\nNaive Split-K: K dimension is split into two chunks\n\nIn the above image, `SPLIT_K=2` and so the K-dimension is chunked into two equal chunks. For the output block in yellow, one program instance computes the partial output using the blue blocks, and another computes the partial output using the green blocks. The full kernel is below — the important part is the inner loop:\n\n```\nimport tritonimport triton.language as tlimport torch@triton.jitdef _first_kernel(A, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, ): # matrix multiplication pid_m = tl.program_id(0) pid_n = tl.program_id(1) num_pid_m, num_pid_n = tl.num_programs(1), tl.num_programs(2) pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_M) pid_k = tl.program_id(2) rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M) rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N) rm = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M) rn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N) CHUNK_SIZE = K // SPLIT_K rk = pid_k * CHUNK_SIZE + tl.arange(0, BLOCK","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fmedium.com%2F%40michael.diggin%2Fimplementing-a-split-k-matrix-multiplication-kernel-in-triton-7ad93fe4a54c","id":"https://medium.com/@michael.diggin/implementing-a-split-k-matrix-multiplication-kernel-in-triton-7ad93fe4a54c","publishedDate":"2025-04-15T00:00:00.000Z","summary":"This article discusses implementing a Split-K matrix multiplication kernel in Triton, a language and compiler for high-performance GPU programming. It aims to achieve performance comparable to cuBLAS, a standard library. The article assumes some familiarity with Triton and provides an overview of the tiled matrix multiplication algorithm, which is the basis for Split-K.\n","title":"Implementing a Split-K Matrix Multiplication Kernel in Triton","url":"https://medium.com/@michael.diggin/implementing-a-split-k-matrix-multiplication-kernel-in-triton-7ad93fe4a54c"},{"content":"[Skip to main content](https://pytorch.org/pytorch.org#ajax-content-wrap)\n\nHit enter to search or ESC to closeSearch\n\n[Close Search](https://pytorch.org/pytorch.org)\n\n[Blog](https://pytorch.org/blog/category/blog/)\n\n# CUDA-Free Inference for LLMs\n\nBy [Adnan Hoque, Less Wright, Raghu Ganti and Mudhakar Srivatsa](https://pytorch.org/pytorch.org)September 4, 2024November 11th, 2024[No Comments](https://pytorch.org/blog/cuda-free-inference-for-llms/#respond)\n\nIn this blog, we discuss the methods we used to achieve FP16 inference with popular LLM models such as [Meta’s Llama3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B) and [IBM’s Granite-8B Code](https://huggingface.co/ibm-granite/granite-8b-code-base), where **100%** of the computation is performed using [OpenAI’s Triton Language](https://github.com/triton-lang/triton).For single token generation times using our Triton kernel based models, we were able to approach **0.76-0.78x** performance relative to the CUDA kernel dominant workflows for both Llama and Granite on Nvidia H100 GPUs, and **0.62-0.82x** on Nvidia A100 GPUs.\n\nWhy explore using 100% Triton? Triton provides a path for enabling LLMs to run on different types of GPUs – NVIDIA, AMD, and in the future Intel and other GPU based accelerators. It also provides a higher layer of abstraction in Python for programming GPUs and has allowed us to write performant kernels faster than authoring them using vendor specific APIs. In the rest of this blog, we will share how we achieve CUDA-free compute, micro-benchmark individual kernels for comparison, and discuss how we can further improve future Triton kernels to close the gaps.\n\n**Figure 1. Inference throughput benchmarks with Triton and CUDA variants of Llama3-8B and Granite-8B, on NVIDIA H100 and A100** _Settings: batch size = 2, input sequence length = 512, output sequence length = 256_\n\n**2.0 Composition of a Transformer Block**\n\nWe start with a breakdown of the computations that happen in Transformer-based models. The figure below shows the “kernels” of a typical Transformer block.\n\n**Figure 2.** Transformer Block by core kernels\n\nThe core operations for a Llama3 architecture are summarized in this list:\n\n01. RMSNorm\n02. Matrix multiplication: Fused QKV\n03. RoPE\n04. Attention\n05. Matrix multiplication: Output Projection\n06. RMSNorm\n07. Matrix multiplication: Fused Gate + Up Projection\n08. Activation function: SiLU\n09. Element Wise Multiplication\n10. Matrix multiplication: Down Projection\n\nEach of these operations is computed on the GPU through the execution of one (or multiple) kernels. While the specifics of each of these kernels can vary across different transformer models, the core operations remain the same. For example, IBM’s Granite 8B Code model uses bias in the MLP layer, different from Llama3. Such changes do require modifications to the kernels. A typical model is a stack of these transformer blocks wired together with embedding layers.\n\n**3.0 Model Inference**\n\nTypical model architecture code is shared with a python model.py file that is launched by PyTorch. In the default PyTorch [eager execution](https://pytorch.org/blog/optimizing-production-pytorch-performance-with-graph-transformations/) mode, these kernels are all executed with CUDA. To achieve 100% Triton for end-to-end Llama3-8B and Granite-8B inference we need to write and integrate handwritten Triton kernels as well as leverage torch.compile (to generate Triton ops). First, we replace smaller ops with compiler generated Triton kernels, and second, we replace more expensive and complex computations (e.g. matrix multiplication and flash attention) with handwritten Triton kernels.\n\nTorch.compile generates Triton kernels automatically for RMSNorm, RoPE, SiLU and Element Wise Multiplication. Using tools like [Nsight Systems](https://developer.nvidia.com/nsight-systems) we can observe these generated kernels; they appear as tiny dark green kernels in-between the matrix multiplications and attention.\n\n**Figure 3.** Trace of Llama3-8B with torch.compile, showing CUDA kernels being used for matrix multiplications and flash attention\n\nFor the above trace, we note that the two major ops that make up **80%** of the E2E latency in a Llama3-8B style model are matrix multiplication and attention kernels and both remain CUDA kernels. Thus to close the remaining gap, we replace both matmul and attention kernels with handwritten Triton kernels.\n\n**4.0 Triton SplitK GEMM Kernel**\n\nFor the matrix multiplications in the linear layers, we wrote a custom FP16 Triton GEMM (General Matrix-Matrix Multiply) kernel that leverages a [SplitK work decomposition](https://pytorch.org/blog/accelerating-moe-model//#30-work-decomposition---splitk). We have previously discussed this parallelization in other blogs as a way to accelerate the decoding portion of LLM inference.\n\n**5.0 GEMM Kernel Tuning**\n\nTo achieve optimal performance we used the exhaustive search approach to tune our SplitK GEMM kernel. Granite-8B and Llama3-8B have linear layers with the following shapes:\n\n| Linear Layer | Shape (in\\_features, out\\_features) |\n| --- | --- |\n| Fused QKV Projection | (4096, 6144) |\n| Output Projection | (4096, 4096) |\n| Fused Gate + Up Projection | (4096, 28672) |\n| Down Projection | (14336, 4096) |\n\n**Figure 4.** Granite-8B and Llama3-8B Linear Layer Weight Matrix Shapes\n\nEach of these linear layers have different weight matrix shapes. Thus, for optimal performance the Triton kernel must be tuned for each of these shape profiles. After tuning for each linear layer we were able to achieve **1.20x** E2E speedup on Llama3-8B and Granite-8B over the untuned Triton kernel.\n\n**6.0 Flash Attention Kernel**\n\nWe evaluated a suite of existing Triton flash attention kernels with different configurations, namely:\n\n1. [AMD Flash](https://github.com/ROCm/triton/blob/triton-mlir/python/perf-kernels/flash-attention.py)\n2. [OpenAI Flash](https://github.com/triton-lang/triton/blob/main/python/tutorials/06-fused-attention.py)\n3. [Dao AI Lab Flash](https://github.com/Dao-AILab/flash-attention/blob/3669b25206d5938e3cc74a5f7860e31c38af8204/flash_attn/flash_attn_triton.py#L812)\n4. [XFormers Flash](https://github.com/facebookresearch/xformers/blob/fae0ceb195a41f2ab762d89449c6012fbcf2ffda/xformers/ops/fmha/triton_splitk.py#L96)\n5. [PyTorch FlexAttention](https://github.com/pytorch/pytorch/blob/e7b870c88bc3b854a95399a96a274d2f1f908172/torch/nn/attention/flex_attention.py#L800)\n\nWe evaluated the text generation quality of each of these kernels, first, in eager mode and then (if we were able to torch.compile the kernel with standard methods) compile mode. For kernels 2-5, we noted the following:\n\n| Kernel | Text Generation Quality | Torch.compile | Support for Arbitrary Sequence Length |\n| --- | --- | --- | --- |\n| AMD Flash | Coherent | Yes | Yes |\n| OpenAI Flash | Incoherent | Did not evaluate. WIP to debug precision in eager mode first | No |\n| Dao AI Lab Flash | Incoherent | Did not evaluate. WIP to debug precision in eager mode first | Yes |\n| Xformers FlashDecoding | Hit a compilation error before we were able to evaluate text quality | WIP | No (This kernel is optimized for decoding) |\n| PyTorch FlexAttention | Coherent | WIP | WIP |\n\n**Figure 5.** Table of combinations we tried with different Flash Attention Kernels\n\nThe above table summarizes what we observed out-of-the box. With some effort we expect that kernels 2-5 can be modified to meet the above criteria. However, this also shows that having a kernel that works for benchmarking is often only the start of having it usable as an end to end production kernel.We chose to use the AMD flash attention kernel in our subsequent tests as it can be compiled via torch.compile and produces legible output in both eager and compiled mode.\n\nTo satisfy torch.compile compatibility with the AMD flash attention kernel, we had to define it as a torch custom operator. This process is explained in detail [here](https://pytorch.org/tutorials/advanced/python/_custom/_ops.html). The tutorial link discusses how to wrap a simple image crop operation. However, we note that wrapping a more complex flash attention kernel follows a similar process. The two step approach is as follows:\n\n1. Wrap the function into a PyTorch Custom Operator\n\n1. Add a FakeTensor Kernel to the operator, which given the shapes of the input tensors of flash (q, k and v) provides a way to compute the output shape of the flash kernel\n\nAfter defining the Triton flash kernel as a custom op, we were able to successfully compile it for our E2E runs.\n\n**Figure 6.** Trace of Llama3-8B with torch.compile, after swapping in Triton matmul and Triton flash attention kernels\n\nFrom Figure 5, we note that now, after integrating both the SplitK matrix multiplication kernel, the torch op wrapped flash attention kernel, and then running torch.compile, we are able to achieve a forward pass that uses 100% Triton computation kernels.\n\n**7.0 End-to-End Benchmarks**\n\nWe performed end-to-end measurements on NVIDIA H100s and A100s (single GPU) with Granite-8B and Llama3-8B models. We performed our benchmarks with two different configurations.\n\nThe Triton kernel configuration uses:\n\n1. Triton SplitK GEMM\n2. AMD Triton Flash Attention\n\nThe CUDA Kernel configuration uses:\n\n1. cuBLAS GEMM\n2. cuDNN Flash Attention – Scaled Dot-Product Attention (SDPA)\n\nWe found the following throughput and inter-token latencies for both eager and torch compiled modes, with typical inference settings:\n\n| GPU | Model | Kernel Config | Median Latency (Eager) \\[ms/tok\\] | Median Latency (Compiled) \\[ms/tok\\] |\n| --- | --- | --- | --- | --- |\n| H100 | Granite-8B | Triton | 27.42 | 11.59 |\n| CUDA | 18.84 | 9.50 |\n| Llama3-8B | Triton | 20.36 | 10.61 |\n| CUDA | 16.59 | 8.59 |\n| A100 | Granite-8B | Triton | 53.44 | 16.88 |\n| CUDA | 37.13 | 14.25 |\n| Llama3-8B | Triton | 44.44 | 17.94 |\n| CUDA | 32.45 | 12.96 |\n\n**Figure 7.** Granite-8B and Llama3-8B Single Token Generation Latency on H10","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpytorch.org%2Fblog%2Fcuda-free-inference-for-llms%2F","id":"https://pytorch.org/blog/cuda-free-inference-for-llms/","publishedDate":"2024-09-04T00:00:00.000Z","summary":"This blog post discusses methods for achieving CUDA-free FP16 inference with popular LLM models like Meta's Llama3-8B and IBM's Granite-8B Code, using OpenAI's Triton Language for 100% of the computation. The authors achieved performance close to CUDA-based workflows on NVIDIA H100 and A100 GPUs. They explain the core operations in a Transformer block and how they replaced CUDA kernels with Triton kernels, including those generated by torch.compile. This approach allows LLMs to run on various GPUs (NVIDIA, AMD, and potentially Intel) and simplifies GPU programming.\n","title":"CUDA-Free Inference for LLMs - PyTorch","url":"https://pytorch.org/blog/cuda-free-inference-for-llms/"},{"content":"[Skip to main content](https://pytorch.org/pytorch.org#ajax-content-wrap)\n\nHit enter to search or ESC to closeSearch\n\n[Close Search](https://pytorch.org/pytorch.org)\n\n[Blog](https://pytorch.org/blog/category/blog/)\n\n# Accelerating Triton Dequantization Kernels for GPTQ\n\nBy [Less Wright, Adnan Hoque (IBM)](https://pytorch.org/pytorch.org)January 16, 2024November 14th, 2024[No Comments](https://pytorch.org/blog/accelerating-triton/#respond)\n\n## TL;DR\n\nLeveraging a first principles approach, we showcase a step by step process undertaken to accelerate the current Triton GPTQ kernels by 3x (core GPTQ) and 6x (AutoGPTQ). Example: 275us to 47us on a typical Llama style inference input. The goal is to provide a helpful template for accelerating any given Triton kernel. We provide a background on Triton and GPTQ quantization and dequantization process, showcase the impact of coalesced memory access to improve shared and global memory throughput, highlight changes made to reduce warp stalling to improve total throughput, and an overview on integrating Triton kernels into PyTorch code. Longer term, we hope to surpass the existing CUDA native GPTQ kernel with our Triton kernel.\n\n_Fig 1: Performance benchmarking the optimized AutoGTPQ kernel vs the current AutoGPTQ kernel on H100_\n\n_Fig 2: Performance benchmarking the newly optimized AutoGTPQ kernel vs the current AutoGPTQ kernel on A100_\n\n_Fig 3: Even with these improvements, there remains a gap between our optimized Triton kernel and the CUDA native AutoGTPQ kernel on A100. More to come…_\n\n## 1.0 Introduction to Triton\n\nThe [Triton framework](https://openai.com/research/triton) provides a hardware agnostic way of programming and targeting GPUs, currently supporting both NVIDIA and AMD, with support for additional hardware vendors in progress. Triton is now a mainstay for PyTorch 2.0 as torch.compile decomposes eager PyTorch and re-assembles it into a high percentage of Triton kernels with PyTorch connecting code.\n\nAs Triton becomes more widely adopted, it will be essential that programmers understand how to systematically step through the Triton stack (from the high level Python down to the low-level SASS) to address performance bottlenecks in order to optimize GPU efficiency for algorithms that go beyond torch.compile generated kernels.\n\nIn this post, we will introduce some core concepts of the Triton programming language, how to identify common performance limiters in GPU kernels, and in parallel, tune a quantization kernel used in AutoGPTQ that can be used for high throughput inference applications.\n\n### Intro to GPTQ Quantization and Dequantization\n\n[GPTQ](https://arxiv.org/abs/2210.17323) is a quantization algorithm that is able to compress ultra-large (175B+) LLMs efficiently to int4 bit representation, via approximate second order information (Hessian inverse). [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) is a framework built on GPTQ, allowing for rapid dequantization and inference/serving of LLMs that have been quantized with GPTQ.\n\nAs part of the AutoGPTQ stack, they provide a Triton GPTQ kernel to handle the dequantization of a model for inference.\n\nThe basic process for INT quantization is shown below and involves determining the scale and zero point, and then computing the quantized 4bit Weight using the Scale and Zero point:\n\nWe thus store the 4 Bit weights along with the meta information of Scale and ZeroPoint for each group of weights.\n\nTo ‘dequant’ these weights, we do the following:\n\nAnd then proceed to **Matrix Multiply** the dequantized weights with the dense input feature matrix for this linear layer.\n\n## 2.0 Identify the Bottlenecks – Optimizing Matrix Multiplication\n\nAs it turns out, making a fast matrix multiplication kernel is not trivial. A naively implemented matrix multiply will rarely reach peak throughput performance on highly parallel machines like GPUs. So – we need to tackle our compute and memory subsystems in our GPU in an hierarchical fashion to make sure we are maximally utilizing each resource.\n\nWe start our optimization process, by running the unoptimized Triton Kernel, through the Nvidia Nsight Compute tool and taking a note of some important metrics and warnings:\n\nFig xy (todo)\n\nWe notice first that both compute and memory throughput are low, 7.40% and 21.19% respectively (fig xy) . Knowing that for typical inference matrix problem sizes, we are in the memory bound regime, we will attempt to optimize the kernel by applying code changes that target the memory subsystem of our A100 GPU.\n\nThe three topics this post will cover are:\n\n1. L2 Optimization\n2. Vectorized Load\n3. Warp Stalling\n\nLet’s walk through each topic, make the appropriate changes, and see its corresponding impact on our Triton Kernel. This Triton kernel is a fused dequantization kernel that dequantizes a packed int32 weight (we will refer to this as the B Matrix) Tensor into int4 weights, performs matrix multiplication with the activation tensor (refer to as the A matrix) in FP16 mode, and then stores the results back to a matrix C.\n\nThe above is referred to as W4A16 quantization. Keep in mind that the process we describe can and should be used for the development of any GPU kernel, as these are common bottlenecks in any unoptimized kernel.\n\n## 3.0 L2 Optimization\n\nThis optimization already exists in the AutoGPTQ kernel, but we’d like to dedicate a section to this to help readers better understand how mapping and execution order of thread blocks is handled in Triton. Thus, we will step through a naive mapping and then a more optimal mapping to see its corresponding impact.\n\nLet’s build up our kernel naively, starting with a “linear” load from global memory and then compare it to a more optimized “swizzled” load. Linear vs Swizzled determines the execution order of our grid of work on the GPU. Let’s take a look at the hints that the [Nvidia Nsight Compute Tool](https://developer.nvidia.com/nsight-compute) provides regarding our kernels shared memory access pattern in the naive case:\n\nTo tackle this issue we can use an approach referred to as “tile-swizzling.” The idea of this method is to launch our thread blocks in a more L2 cache friendly order.\n\nLet’s take a step back and familiarize ourselves with some Triton semantics and make a simple CUDA analogy to understand the concept better. Triton kernels launch “programs”. These so-called programs map to the concept of a Thread Block in CUDA and it is the basic unit of parallelism in a Triton Kernel. Every program has with it associated a “pid” and all the threads in a program are guaranteed to be executing the same instruction.\n\nThe Triton programs will be distributed onto your SMs in a naive-way if you do a simple linear mapping of “pid” to a 2D grid location of your output matrix C.\n\nThis 2D grid location is determined by pid\\_m and pid\\_n in Triton. We would like to exploit data and cache locality in the L2 cache of our GPU, when we distribute our grid of work. To do this in Triton we can make the following changes:\n\nThe code highlighted in red would be the naive “linear” tile ordering, and the code highlighted in green is the “swizzled” tile ordering. This type of launch promotes a sense of locality. Here is a visual to help understand this better.\n\nAfter incorporating this change, the profiler no longer complains about uncoalesced memory accesses. Let’s take a look at how our memory throughput has changed:\n\nThis change was tested on a simple load store kernel. Looking at the GPU speed of light statistics section in the profiler we also see a 112.07% increase in the memory throughput of the simple load kernel, which is what we were after with this optimization. Again, this optimization already exists in the AutoGPTQ kernel, but is the boilerplate logic that every Triton Kernel programmer will have to write in the beginning of their kernel, before any of the exciting dequantization or matrix multiply logic. It is thus important to understand that:\n\n1. This mapping is not unique\n2. Triton does not automatically handle this kind of optimization for the programmer, and careful thought must be taken to ensure your kernel is optimally handling shared memory accesses\n\nThese are not obvious for those new to Triton, as much of the shared memory access optimization is handled by the Triton compiler. However, in the cases where these are not handled by the compiler, it is important to be able to understand what tools and methods are available to us to be able to influence memory behavior.\n\n## 4.0 Vectorized Load\n\nNow, back to the original complaints of our unoptimized kernel. We want to optimize the global memory access pattern of our kernel. From the details page of the Nvidia Nsight compute tool, we see the following note, where the profiler is complaining about uncoalesced global memory accesses.\n\nLet’s dig deeper and take a look at the SASS (Assembly) Code load for an unoptimized memory read:\n\nThis load operation resulted in 32 global load operations that are 16 bit wide. This is not optimal.\n\nWe would like to do our global memory loads in a vectorized way so that it results in the least amount of load instructions. To combat this we can give the Triton Compiler some help.\n\nThe green highlighted lines above act as a compiler hint. It tells the compiler that these elements are contiguous in memory and that this load operation can be coalesced.\n\nLet’s see the effect in assembly after adding these lines.\n\nThe load is now performed in 4 global load operations that are each 128 bit wide, instead of 32 16 bit global load operations. This means 28 fewer memory fetch instructions, and importantly a coalesced memory access. This can be seen from the fact that a single thread is not accessing consecutive memory addresses, which without the compiler hint, was the behavior.\n\nThe resulting effect is 73x speedup in an isolated load operation, and after incorporating it in the full dequantization kernel we were able to see another 6% ","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpytorch.org%2Fblog%2Faccelerating-triton%2F","id":"https://pytorch.org/blog/accelerating-triton/","publishedDate":"2024-01-16T00:00:00.000Z","summary":"This blog post details the process of accelerating Triton GPTQ kernels, used for dequantization in large language models, by up to 6x. The authors provide a step-by-step guide to optimizing Triton kernels, focusing on improving memory access and reducing warp stalling. They benchmark their optimized kernels against existing ones, demonstrating significant performance gains, and aim to surpass CUDA native kernels in the future. The post also includes background information on Triton, GPTQ quantization, and the dequantization process.\n","title":"Accelerating Triton Dequantization Kernels for GPTQ - PyTorch","url":"https://pytorch.org/blog/accelerating-triton/"},{"content":"[Skip to main content](https://pytorch.org/pytorch.org#ajax-content-wrap)\n\nHit enter to search or ESC to closeSearch\n\n[Close Search](https://pytorch.org/pytorch.org)\n\n[Blog](https://pytorch.org/blog/category/blog/)\n\n# Accelerating Llama3 FP8 Inference with Triton Kernels\n\nBy [Adnan Hoque, Less Wright, Chih Chieh Yang](https://pytorch.org/pytorch.org)May 1, 2024November 13th, 2024[No Comments](https://pytorch.org/blog/accelerating-llama3/#respond)\n\n## 1.0 Summary\n\nWe present an optimized Triton FP8 GEMM (General Matrix-Matrix Multiply) kernel TK-GEMM, which leverages SplitK parallelization. For small batch size inference, TK-GEMM delivers up to **1.94x** over the base Triton matmul implementation, **1.87x** speedup over cuBLAS FP8 and **1.71x** over cuBLAS FP16 for Llama3-70B inference problem sizes on NVIDIA H100 GPUs.\n\n**Figure 1.** TK-GEMM Speedup over PyTorch (calling cuBLAS) for Llama3-70B Attention Layer Matrix Shapes (N=K=8192)\n\nIn this blog, we will cover how we designed an optimized kernel using [Triton](https://github.com/openai/triton) for FP8 inference and tuned it for Lama3-70B inference. We will cover FP8 (8-bit floating point), a new datatype supported by Hopper generation GPUs (SM90), the key SM90 features that Triton supports, and how we modified the parallelization to be able to maximize memory throughput for memory-bound (inference) problem sizes.\n\nWe also dedicate a section on CUDA graphs, an important technology that will help materialize kernel level speedups and enable developers who want to use Triton kernels in production settings to get additional performance gain.\n\nRepo and code available at: [https://github.com/pytorch-labs/applied-ai](https://github.com/pytorch-labs/applied-ai)\n\n## 2.0 FP8 Datatype\n\nThe FP8 datatype was [introduced](https://arxiv.org/pdf/2209.05433.pdf) jointly by Nvidia, Arm and Intel and serves as a successor to 16-bit floating point types. With half the bit count, it has the potential to provide significant throughput improvements over its predecessors for Transformer networks. The FP8 datatype consists of 2 formats:\n\n**E4M3** (4-bit exponent and 3-bit mantissa). Able to store +/ 448 and nan. **E5M2** (5-bit exponent and 2-bit mantissa). Able to store +/- 57,334, nan and inf.\n\n**Above:** _BF16, FP16, FP8 E4M3 and FP8 E5M2._ _To show precision differences, the closest representation to 0.3952 is shown in each format._ _Image Credit: [Nvidia](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html)_\n\nWe use E4M3 in inference and forward pass training due its higher precision and E5M2 in training backward pass due to its higher dynamic range. Nvidia has designed their H100 FP8 Tensor Core to provide a peak of 3958 TFLOPS, **2x** the FLOPS of the FP16 Tensor Core.\n\nWe designed our Triton kernel with these hardware innovations in mind and in the rest of the blog we will discuss methods to leverage and verify that these features are indeed being utilized by the Triton compiler.\n\n## 3.0 Triton Hopper Support and FP8 Tensor Core Instruction\n\nThe Hopper GPU architecture has added the following [new features](https://arxiv.org/abs/2402.13499) that we can expect will accelerate FP8 GEMM.\n\n- TMA (Tensor Memory Accelerator) Hardware Unit\n- WGMMA (Warp Group Matrix Multiply-Accumulate Instruction)\n- Threadblock Clusters\n\nTriton currently takes advantage of one of these features, the _wgmma_ instruction, whereas PyTorch (calling cuBLAS) leverages all 3 which makes these speedups even more impressive. To fully take advantage of the Hopper FP8 Tensor Core, the wgmma is necessary even though the older mma.sync instruction is still supported.\n\nThe key difference between the mma and wgmma instructions is that instead of 1 CUDA warp being responsible for an output shard, an entire warp group, 4 CUDA warps, _asynchronously_ contributes to an output shard.\n\nTo see what this instruction looks like in practice, and to verify that our Triton Kernel is indeed utilizing this feature we analyzed the PTX and SASS assembly using [nsight compute](https://developer.nvidia.com/nsight-compute).\n\n**Figure 2.** PTX Assembly\n\nThis instruction is further lowered into a QGMMA instruction in SASS.\n\n**Figure 3.** SASS Assembly\n\nBoth instructions tell us that we are multiplying two FP8 E4M3 input tensors and accumulating in F32, which confirms that the TK-GEMM Kernel is utilizing the FP8 Tensor Core and the lowering is being done correctly.\n\n## 4.0 SplitK Work Decomposition\n\n**Figure 4.** TK-GEMM vs Base Triton GEMM TFLOPS for M = 1-64\n\nThe base Triton FP8 GEMM implementation does [not perform](https://github.com/openai/triton/issues/3104) well for the small M regime, where for a matrix multiplication of A ( _MxN_) x B ( _NxK_), _M_ < _N_, _K_. To optimize for this type matrix profile we applied a SplitK work decomposition instead of the Data Parallel decomposition found in the base Triton kernel. This greatly improved latencies for the small M regime.\n\nFor background, SplitK launches additional thread blocks along the k dimension to calculate partial output sums. The partial results from each thread block are then summed using an atomic reduction. This allows for finer grained work decomposition with resultant performance improvements. More details on SplitK are available in our [arxiv paper](https://arxiv.org/abs/2402.00025).\n\nAfter carefully tuning the other relevant hyperparameters for our kernel such as tile sizes, number of warps and the number of pipeline stages to Llama3-70B problem sizes we were able to produce up to **1.94x** speedup over the Triton [base implementation](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html). For a more comprehensive introduction to hyperparameter tuning, see our [blog](https://pytorch.org/blog/accelerating-moe-model/#30-work-decomposition---splitk).\n\n**Above**: _NCU profiler times for TK-GEMM under varying batch sizes, and compared with PyTorch (calling cuBLAS) FP8 and FP16._\n\nNote that starting at M=32, the cuBLAS FP8 kernel starts to outperform TK-GEMM. For M >= 32, we suspect that hyperparameters we found are not optimal, and thus another set of experiments is required to determine the optimal parameters for the mid-sized M regime.\n\n## 5.0 CUDA Graphs to Enable End-to-End Speedup\n\nTo be able to realize these speedups in an end-to-end setting, we must take into account both the kernel execution time (GPU duration) as well as the wall time (CPU+GPU) duration. Triton kernels, which are handwritten (as opposed to torch compile generated) are known to suffer from high-kernel launch latencies. If we use [torch profiler](https://pytorch.org/docs/stable/profiler.html) to trace the TK-GEMM kernel we can see the call stack on the CPU side to pinpoint exactly what is causing the slowdown.\n\n**Figure 5.** CPU Launch Overhead: 2.413ms\n\nFrom above, we see that the majority of the wall time of our optimized kernel is dominated by JIT (Just-in-Time) compilation overhead. To combat this we can use CUDA graphs.\n\n**Figure 6.** CUDA Graphs Visualization _Image Credit: [PyTorch](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/)_\n\nThe key idea is instead of multiple kernel launches, we instead can create and instantiate a graph (1 time cost) and then submit that instance of the graph for execution. To illustrate this point we simulate a Llama3-70B Attention layer, As shown in the below figure generated using [nsight systems](https://developer.nvidia.com/nsight-systems), the time between each GEMM is **_165us_** compared to the **_12us_** spent on the actual matmul due the CPU kernel launch overhead. This means that **_92%_** of the time of the time in an Attention layer the GPU is idle and not doing any work.\n\n**Figure 7.** Simulated Llama3-70B Attention Layer with TK-GEMM\n\nTo show the impact of CUDA graphs, we then created a graph of the TK-GEMM kernel in the toy Attention layer and replayed the graph. Below, we can see that the gaps between kernel executions are reduced to 6.65us.\n\n**Figure 8.** Simulated Llama3-70B Attention Layer with TK-GEMM and CUDA Graphs\n\nIn practice, this optimization would result in a **6.4x** speedup of a single attention layer in Llama3-70B, over naively using TK-GEMM in a model without CUDA graphs.\n\n## 6.0 Potential Future Optimization Paths\n\n**Figure 9.** TMA Hardware Unit _Image Credit: [Nvidia](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/)_\n\nThe Nvidia H100 features a TMA hardware unit. The dedicated TMA unit frees up registers and threads to do other work, as address generation is completely handled by the TMA. For memory bound problem sizes, this can provide even further gain when Triton enables support for this feature.\n\n**Figure 10.** Tensor Core Utilization (Arrows Indicate Degrees of Freedom)\n\nTo identify how well we are utilizing the Tensor Core, we can analyze the roofline chart. Notice that we are in the memory-bound region as expected for small M. To improve kernel latency we can either increase the arithmetic intensity, which with a fixed problem size can only be achieved through exploiting data locality and other loop [optimizations](https://www.codee.com/is-your-algorithm-running-at-peak-performance-the-roofline-model/) or increasing the memory throughput. This requires either a more optimal parallel algorithm specialized for the FP8 datatype as well as the type of problem size characteristics we expect to see in FP8 inference.\n\n**Figure 11.** DRAM Throughput Circled, 1.65TB/s vs Peak 3.35TB/s on H100 (M=16, N=8192, K=8192)\n\nLastly, we can see that we are only achieving around **50%** of peak DRAM throughput on the NVIDIA H100. High performance GEMM kernels typically achieve around **70-80%** of peak throughput. This means that there is still a lot of room to improve and the techniques mentioned above (loop unrolling, optimized parallelization) are needed for additional gain.\n\n## 7.0 Future Work\n\nFor future research, we would li","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpytorch.org%2Fblog%2Faccelerating-llama3%2F","id":"https://pytorch.org/blog/accelerating-llama3/","publishedDate":"2024-05-01T00:00:00.000Z","summary":"This blog post details the development of an optimized Triton FP8 GEMM (General Matrix-Matrix Multiply) kernel, TK-GEMM, for accelerating Llama3 FP8 inference. TK-GEMM achieves up to 1.94x speedup over the base Triton matmul implementation, 1.87x over cuBLAS FP8, and 1.71x over cuBLAS FP16 on NVIDIA H100 GPUs for Llama3-70B inference. The post covers FP8 datatypes (E4M3 and E5M2), Hopper GPU architecture features (TMA, WGMMA, Threadblock Clusters), and how the Triton kernel leverages these features for performance gains. The code is available on GitHub.\n","title":"Accelerating Llama3 FP8 Inference with Triton Kernels – PyTorch","url":"https://pytorch.org/blog/accelerating-llama3/"},{"content":"[Skip to main content](https://pytorch.org/pytorch.org#ajax-content-wrap)\n\nHit enter to search or ESC to closeSearch\n\n[Close Search](https://pytorch.org/pytorch.org)\n\n[Blog](https://pytorch.org/blog/category/blog/)\n\n# Triton Kernel Compilation Stages\n\nBy [Sara Kokkila-Schumacher\\*, Brian Vaughan\\*, Raghu Ganti\\*, and Less Wright+ (\\*IBM Research, +Meta)](https://pytorch.org/pytorch.org)October 30, 2024May 5th, 2025[No Comments](https://pytorch.org/blog/triton-kernel-compilation-stages/#respond)\n\nThe Triton open-source programming language and compiler offers a high-level, python-based approach to create efficient GPU code. In this blog, we highlight the underlying details of how a triton program is compiled and the intermediate representations. For an introduction to Triton, we refer readers to this [blog](https://openai.com/index/triton/).\n\n## Triton Language and Compilation\n\nThe Triton programming language supports different types of modern GPUs and follows a blocked programming approach. As an example, we will follow the [Triton vector add tutorial](https://github.com/triton-lang/triton/blob/main/python/tutorials/01-vector-add.py) with minor modifications. The vector addition kernel and helper function is defined as:\n\n```\nimport torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(x_ptr, # *Pointer* to first input vector.\n y_ptr, # *Pointer* to second input vector.\n output_ptr, # *Pointer* to output vector.\n n_elements,\n BLOCK_SIZE: tl.constexpr,\n ):\n\n pid = tl.program_id(axis=0)\n block_start = pid * BLOCK_SIZE\n offsets = block_start + tl.arange(0, BLOCK_SIZE)\n\n mask = offsets < n_elements\n\n x = tl.load(x_ptr + offsets, mask=mask)\n y = tl.load(y_ptr + offsets, mask=mask)\n output = x + y\n tl.store(output_ptr + offsets, output, mask=mask)\n\ndef add(x: torch.Tensor, y: torch.Tensor):\n output = torch.empty_like(x)\n assert x.is_cuda and y.is_cuda and output.is_cuda\n n_elements = output.numel()\n\n grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n triton_kernel=add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n torch.cuda.synchronize()\n\n # Save compilation stages - some of the stages identified here are specific to NVIDIA devices:\n with open('triton_IR.txt', 'w') as f:\n print(triton_kernel.asm['ttir'], file=f)\n with open('triton_TTGIR.txt', 'w') as f:\n print(triton_kernel.asm['ttgir'], file=f)\n with open('triton_LLVMIR.txt', 'w') as f:\n print(triton_kernel.asm['llir'], file=f)\n with open('triton_PTX.ptx', 'w') as f:\n print(triton_kernel.asm['ptx'], file=f)\n with open('triton_cubin.txt', 'w') as f:\n print(triton_kernel.asm['cubin'], file=f)\n\n return output\n\ntorch.manual_seed(0)\nsize = 98432\nx = torch.rand(size, device='cuda')\ny = torch.rand(size, device='cuda')\noutput_torch = x + y\noutput_triton = add(x, y)\nprint(output_torch)\nprint(output_triton)\nprint(f'The maximum difference between torch and triton is '\n f'{torch.max(torch.abs(output_torch - output_triton))}')\n\n```\n\nThe Triton vector add kernel includes the `@triton.jit` decorator. The Triton compiler will compile functions marked by `@triton.jit`, which lowers the function through multiple compilation stages. The helper function `add` allocates the output tensor, computes the appropriate GPU grid size, and additionally saves the intermediate compilation stages.\n\nFocusing on the compilation process, the Triton kernel is lowered to device specific assembly through a series of stages outlined in the following figure.\n\nThe kernel is compiled by first walking the abstract syntax tree (AST) of the decorated python function to create the Triton Intermediate Representation (Triton-IR). The Triton-IR is an unoptimized, machine independent intermediate representation. It introduces tile-level programming requirements and is based on the open-source LLVM compiler project. Next the Triton compiler optimizes and converts the Triton-IR into the stages Triton-GPU IR (Triton-TTGIR) and then LLVM-IR. Both the Triton-IR and Triton-GPUIR representations are written as MLIR dialects, where MLIR is a subproject of LLVM that aims to improve compilation for heterogeneous hardware.\n\nFor the Triton vector add tutorial kernel, the example Triton IR snippet is:\n\n```\nmodule {\n tt.func public @add_kernel(%arg0: !tt.ptr {tt.divisibility = 16 : i32} loc(\"/u/saraks/triton_blog/01-vector-add.py\":28:0), %arg1: !tt.ptr {tt.divisibility = 16 : i32} loc(\"/u/saraks/triton_blog/01-vector-add.py\":28:0), %arg2: !tt.ptr {tt.divisibility = 16 : i32} loc(\"/u/saraks/triton_blog/01-vector-add.py\":28:0), %arg3: i32 {tt.divisibility = 16 : i32} loc(\"/u/saraks/triton_blog/01-vector-add.py\":28:0)) attributes {noinline = false} {\n %c1024_i32 = arith.constant 1024 : i32 loc(#loc1)\n %0 = tt.get_program_id x : i32 loc(#loc2)\n %1 = arith.muli %0, %c1024_i32 : i32 loc(#loc3)\n %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32> loc(#loc4)\n %3 = tt.splat %1 : i32 -> tensor<1024xi32> loc(#loc5)\n %4 = arith.addi %3, %2 : tensor<1024xi32> loc(#loc5)\n %5 = tt.splat %arg3 : i32 -> tensor<1024xi32> loc(#loc6)\n %6 = arith.cmpi slt, %4, %5 : tensor<1024xi32> loc(#loc6)\n %7 = tt.splat %arg0 : !tt.ptr -> tensor<1024x!tt.ptr > loc(#loc7)\n %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr >, tensor<1024xi32> loc(#loc7)\n %9 = tt.load %8, %6 : tensor<1024x!tt.ptr > loc(#loc8)\n %10 = tt.splat %arg1 : !tt.ptr -> tensor<1024x!tt.ptr > loc(#loc9)\n %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr >, tensor<1024xi32> loc(#loc9)\n %12 = tt.load %11, %6 : tensor<1024x!tt.ptr > loc(#loc10)\n %13 = arith.addf %9, %12 : tensor<1024xf32> loc(#loc11)\n %14 = tt.splat %arg2 : !tt.ptr -> tensor<1024x!tt.ptr > loc(#loc12)\n %15 = tt.addptr %14, %4 : tensor<1024x!tt.ptr >, tensor<1024xi32> loc(#loc12)\n tt.store %15, %13, %6 : tensor<1024x!tt.ptr > loc(#loc13)\n tt.return loc(#loc14)\n } loc(#loc)\n} loc(#loc)\n\n```\n\nNotice that the main functions in the Triton kernel are now represented as:\n\n| | |\n| --- | --- |\n| **Triton kernel** | **Triton IR** |\n| x = tl.load(x\\_ptr + offsets, mask=mask) | %9 = tt.load %8, %6 : tensor<1024x!tt.ptr > loc(#loc8) |\n| y = tl.load(y\\_ptr + offsets, mask=mask) | %12 = tt.load %11, %6 : tensor<1024x!tt.ptr > loc(#loc10) |\n| output = x + y | %13 = arith.addf %9, %12 : tensor<1024xf32> loc(#loc11) |\n| tl.store(output\\_ptr + offsets, output, mask=mask) | tt.store %15, %13, %6 : tensor<1024x!tt.ptr > loc(#loc13) |\n\nAt the Triton IR stage, the `%arg0: !tt.ptr&lt;f32>` and the following tensor references show that the intermediate representation is already specialized by the data type.\n\nWe ran this example on a Tesla V100-SXM2-32GB GPU with CUDA Version 12.2, Python version 3.11.9, and PyTorch 2.4.1 with the default version of Triton that is installed with PyTorch. On this device, the simple vector addition has the following Triton GPU IR snippet with lines omitted for clarity:\n\n```\n#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\nmodule attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, triton_gpu.target = \"cuda:70\", \"triton_gpu.threads-per-warp\" = 32 : i32} {\n tt.func public @add_kernel(%arg0: !tt.ptr {tt.divisibility = 16 : i32}\n ⋮\n %9 = tt.load %8, %6 : tensor<1024x!tt.ptr, #blocked> loc(#loc8)\n ⋮\n %12 = tt.load %11, %6 : tensor<1024x!tt.ptr, #blocked> loc(#loc10)\n %13 = arith.addf %9, %12 : tensor<1024xf32, #blocked> loc(#loc11)\n ⋮\n tt.store %15, %13, %6 : tensor<1024x!tt.ptr, #blocked> loc(#loc13)\n ⋮\n } loc(#loc)\n} loc(#loc)\n\n```\n\nAt this stage, some of the hardware specific information is included. For example, the compute capability is included along with details on how the tensors are distributed to cores and warps or for AMD GPUs on wavefronts. In this example, the tensors are represented as a `#blocked` layout. In this encoding, each warp owns a contiguous portion of the tensor. Currently, other possible memory optimizations include layouts such as `slice` (restructures and distributes a tensor along a dimension), `dot_op`(optimized layout for block matrix product), `shared`(indicates GPU shared memory), `nvidia_mma` (produced by NVIDIA tensor cores), `amd_mfma` (produced by AMD MFMA matrix core), and `amd_wmma` (produced by AMD WMMA matrix core). As announced at the recent Triton conference, this layout representation will transition to a new linear layout to unify layouts within and across backends. The stage from Triton-GPUIR to LLVM-IR converts the Triton-GPUIR to LLVM’s representation. At this time, Triton has third-party backend support for NVIDIA and AMD devices, but other device support is under active development by the open-source community.\n\nA small subset of the LLVM-IR vector add arguments shown below for illustration:\n\n```\n %19 = extractvalue { i32, i32, i32, i32 } %18, 0, !dbg !16\n %39 = extractvalue { i32, i32, i32, i32 } %38, 0, !dbg !18\n %23 = bitcast i32 %19 to float, !dbg !16\n %43 = bitcast i32 %39 to float, !dbg !18\n %56 = fadd float %23, %43, !dbg !19\n\n```\n\nAfter some pointer arithmetic and an inline assembly call to retrieve the data from global memory, the vector elements are extracted and cast to the correct type. Finally they are added together and later written to global memory through an inline assembly expression.\n\nThe final stages of the Triton compilation process lower the LLVM-IR to a device specific binary. For the example vector add, on an NVIDIA GPU, the next intermediate is PTX (Parallel Thread Execution). The low-level PTX syntax specifies the execution at the thread level of NVIDIA devices, starting with the CUDA 1.0 release. For an in-depth guide on PTX, see [NVIDIA’s documentation](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html). In the vector add, the kernel parameters are passed from the host to the kernel, addresses are assigned and `mov` instructions facilitate the thread-level data access, ultimately representing the element addition calls with `add.f32` such as the example below:\n\n```\n add.f32 %f17, %f1, %f","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpytorch.org%2Fblog%2Ftriton-kernel-compilation-stages%2F","id":"https://pytorch.org/blog/triton-kernel-compilation-stages/","publishedDate":"2024-10-30T00:00:00.000Z","summary":"This blog post details the compilation process of Triton kernels, a high-level, Python-based approach for creating efficient GPU code. The Triton compiler transforms functions marked with `@triton.jit` through several stages, starting with the creation of a Triton Intermediate Representation (Triton-IR) from the abstract syntax tree (AST). This unoptimized, machine-independent representation introduces tile-level programming requirements.\n","title":"Triton Kernel Compilation Stages","url":"https://pytorch.org/blog/triton-kernel-compilation-stages/"},{"content":"Switch to\n\n- [ChatGPT(opens in a new window)](https://chatgpt.com/?openaicom-did=ccd3bbce-7b71-44f7-b720-4e2723d303eb&openaicom_referred=true)\n- [Sora(opens in a new window)](https://sora.com/)\n- [API Platform(opens in a new window)](https://platform.openai.com/)\n\nOpenAI\n\nJuly 28, 2021\n\n[Release](https://openai.com/research/index/release/)\n\n# Introducing Triton: Open-source GPU programming for neural networks\n\n[View code(opens in a new window)](https://github.com/openai/triton) [Read documentation(opens in a new window)](https://triton-lang.org/main/index.html)\n\nLoading…\n\nShare\n\nWe’re releasing Triton 1.0, an open-source Python-like programming language which enables researchers with no CUDA experience to write highly efficient GPU code—most of the time on par with what an expert would be able to produce.\n\n## Why it matters\n\nTriton makes it possible to reach peak hardware performance with relatively little effort; for example, it can be used to write FP16 matrix multiplication kernels that match the performance of cuBLAS—something that many GPU programmers can’t do—in under 25 lines of code. Our researchers have already used it to produce kernels that are up to 2x more efficient than equivalent Torch implementations, and we’re excited to work with the community to make GPU programming more accessible to everyone.\n\nNovel research ideas in the field of Deep Learning are generally implemented using a combination of native framework operators. While convenient, this approach often requires the creation (and/or movement) of many temporary tensors, which can hurt the performance of neural networks at scale. These issues can be mitigated by writing specialized GPU kernels, but doing so can be surprisingly difficult due to the many intricacies of GPU programming.[1](https://openai.com/openai.com#citation-bottom-1), [2](https://openai.com/openai.com#citation-bottom-2), [3](https://openai.com/openai.com#citation-bottom-3) And, although a variety of systems have recently emerged[4](https://openai.com/openai.com#citation-bottom-4), [5](https://openai.com/openai.com#citation-bottom-5) to make this process easier, we have found them to be either too verbose, lack flexibility or generate code noticeably slower than our hand-tuned baselines. This has led us to extend and improve Triton[6](https://openai.com/openai.com#citation-bottom-6), a recent language and compiler whose original creator now works at OpenAI.\n\n## The challenges of GPU programming\n\nThe architecture of modern GPUs can be roughly divided into three major components—DRAM, SRAM and ALUs—each of which must be considered when optimizing CUDA code:\n\n- Memory transfers from DRAM must be _coalesced_ into large transactions to leverage the large bus width of modern memory interfaces.\n- Data must be manually stashed to SRAM prior to being re-used, and managed so as to minimize shared memory bank conflicts upon retrieval.\n- Computations must be partitioned and scheduled carefully, both across and within Streaming Multiprocessors (SMs), so as to promote instruction/thread-level parallelism and leverage special-purpose ALUs (e.g., tensor cores).\n\nBasic architecture of a GPU.\n\nReasoning about all these factors can be challenging, even for seasoned CUDA programmers with many years of experience. The purpose of Triton is to fully automate these optimizations, so that developers can better focus on the high-level logic of their parallel code. Triton aims to be broadly applicable, and therefore does not automatically schedule work across SMs -- leaving some important algorithmic considerations (e.g. tiling, inter-SM synchronization) to the discretion of developers.\n\n| | | |\n| --- | --- | --- |\n| **CUDA** | **TRITON** |\n| Memory Coalescing | Manual | Automatic |\n| Shared Memory Management | Manual | Automatic |\n| Scheduling (Within SMs) | Manual | Automatic |\n| Scheduling (Across SMs) | Manual | Manual |\n\nCompiler optimizations in CUDA vs Triton.\n\n## Programming model\n\nOut of all the Domain Specific Languages and JIT-compilers available, Triton is perhaps most similar to Numba: kernels are defined as decorated Python functions, and launched concurrently with different `program_id`’s on a grid of so-called _instances_. However, as shown in the code snippet below, the resemblance stops there: Triton exposes intra-instance parallelism via operations on _blocks_—small arrays whose dimensions are powers of two—rather than a Single Instruction, Multiple Thread (SIMT)[7](https://openai.com/openai.com#citation-bottom-7) execution model. In doing so, Triton effectively abstracts away all the issues related to concurrency _within_ CUDA thread blocks (e.g., memory coalescing, shared memory synchronization/conflicts, tensor core scheduling).\n\nLoading...\n\nWhile this may not be particularly helpful for embarrassingly parallel (i.e., element-wise) computations, it can greatly simplify the development of more complex GPU programs.\n\nConsider for example the case of a fused softmax kernel (below) in which each instance normalizes a different row of the given input tensor _X\\_∈R\\_M\\_×\\_N_. Standard CUDA implementations of this parallelization strategy can be challenging to write, requiring explicit synchronization between threads as they concurrently reduce the same row of _X_. Most of this complexity goes away with Triton, where each kernel instance loads the row of interest and normalizes it sequentially using NumPy-like primitives.\n\n#### Python\n\n`\n1\nimport triton\n2\nimport triton.language as tl\n3\n4\n@triton.jit\n5\ndef softmax(Y, stride_ym, stride_yn, X, stride_xm, stride_xn, M, N):\n6\n# row index\n7\n m = tl.program_id(0)\n8\n# col indices\n9\n# this specific kernel only works for matrices that\n10\n# have less than BLOCK_SIZE columns\n11\n BLOCK_SIZE = 1024\n12\n n = tl.arange(0, BLOCK_SIZE)\n13\n# the memory address of all the elements\n14\n# that we want to load can be computed as follows\n15\n X = X + m * stride_xm + n * stride_xn\n16\n# load input data; pad out-of-bounds elements with 0\n17\n x = tl.load(X, mask=n < N, other=-float('inf'))\n18\n# compute numerically-stable softmax\n19\n z = x - tl.max(x, axis=0)\n20\n num = tl.exp(z)\n21\n denom = tl.sum(num, axis=0)\n22\n y = num / denom\n23\n# write back to Y\n24\n Y = Y + m * stride_ym + n * stride_yn\n25\n tl.store(Y, y, mask=n < N)\n26\n27\nimport torch\n28\n# Allocate input/output tensors\n29\nX = torch.normal(0, 1, size=(583, 931), device='cuda')\n30\nY = torch.empty_like(X)\n31\n# SPMD launch grid\n32\ngrid = (X.shape[0], )\n33\n# enqueue GPU kernel\n34\nsoftmax[grid](Y, Y.stride(0), Y.stride(1),\n35\n X, X.stride(0), X.stride(1),\n36\n X.shape[0] , X.shape[1])`\n\nNote that the Triton JIT treats X and Y as _pointers_ rather than tensors; we felt like retaining low-level control of memory accesses was important to address more complex data structures (e.g., block-sparse tensors).\n\nImportantly, this particular implementation of softmax keeps the rows of _X_ in SRAM throughout the entire normalization process, which maximizes data reuse when applicable (~<32K columns). This differs from PyTorch’s internal CUDA code, whose use of temporary memory makes it more general but significantly slower (below). The bottom line here is not that Triton is inherently better, but that it simplifies the development of specialized kernels that can be much faster than those found in general-purpose libraries.\n\nLoading...\n\nThe lower performance of the Torch (v1.9) JIT highlights the difficulty of automatic CUDA code generation from sequences of high-level tensor operations.\n\n#### Python\n\n`\n1\n@torch.jit.script\n2\ndef softmax(x):\n3\n x_max = x.max(dim=1)[0]\n4\n z = x - x_max[:, None]\n5\n numerator = torch.exp(x)\n6\n denominator = numerator.sum(dim=1)\n7\nreturn numerator / denominator[:, None]`\n\nFused softmax with the Torch JIT.\n\n## Matrix multiplication\n\nBeing able to write fused kernels for element-wise operations and reductions is important, but not sufficient given the prominence of matrix multiplication tasks in neural networks. As it turns out, Triton also works very well for those, achieving peak performance with just ~25 lines of Python code. On the other hand, implementing something similar in CUDA would take [a lot more effort⁠(opens in a new window)](https://github.com/openai/blocksparse/blob/master/src/matmul_op_gpu.cu) and would even be likely to achieve lower performance.\n\n#### Python\n\n`\n1\n@triton.jit\n2\ndef matmul(A, B, C, M, N, K, stride_am, stride_ak,\n3\n stride_bk, stride_bn, stride_cm, stride_cn,\n4\n **META):\n5\n# extract metaparameters\n6\n BLOCK_M, GROUP_M = META['BLOCK_M'], META['GROUP_M']\n7\n BLOCK_N = META['BLOCK_N']\n8\n BLOCK_K = META['BLOCK_K']\n9\n# programs are grouped together to improve L2 hit rate\n10\n _pid_m = tl.program_id(0)\n11\n _pid_n = tl.program_id(1)\n12\n pid_m = _pid_m // GROUP_M\n13\n pid_n = (_pid_n * GROUP_M) + (_pid_m % GROUP_M)\n14\n# rm (resp. rn) denotes a range of indices\n15\n# for rows (resp. col) of C\n16\n rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n17\n rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n18\n# rk denotes a range of indices for columns\n19\n# (resp. rows) of A (resp. B)\n20\n rk = tl.arange(0, BLOCK_K)\n21\n# the memory addresses of elements in the first block of\n22\n# A and B can be computed using numpy-style broadcasting\n23\n A = A + (rm[:, None] * stride_am + rk[None, :] * stride_ak)\n24\n B = B + (rk [:, None] * stride_bk + rn[None, :] * stride_bn)\n25\n# initialize and iteratively update accumulator\n26\n acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n27\nfor k in range(K, 0, -BLOCK_K):\n28\n a = tl.load(A)\n29\n b = tl.load(B)\n30\n# block level matrix multiplication\n31\n acc += tl.dot(a, b)\n32\n# increment pointers so that the next blocks of A and B\n33\n# are loaded during the next iteration\n34\n A += BLOCK_K * stride_ak\n35\n B += BLOCK_K * stride_bk\n36\n# fuse leaky ReLU if desired\n37\n# acc = tl.where(acc >= 0, acc, alpha * acc)\n38\n# write back result\n39\n C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n40\n mask = (rm[:, None] < M) & (rn[None, :] < N)\n41\n tl.store(C, acc, mask=mask)`\n\n","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fopenai.com%2Fblog%2Ftriton%2F","id":"https://openai.com/blog/triton/","publishedDate":"2023-02-08T00:00:00.000Z","summary":"Triton 1.0 is an open-source, Python-like programming language that allows researchers with no CUDA experience to write highly efficient GPU code. It aims to make GPU programming more accessible by automating optimizations like memory coalescing and shared memory management, allowing developers to focus on the high-level logic of their parallel code. Triton can produce kernels that are up to 2x more efficient than equivalent Torch implementations.\n","title":"Introducing Triton: Open-Source GPU Programming for Neural Networks","url":"https://openai.com/blog/triton/"},{"content":"January 16, 2023\n\n## How Nvidia’s CUDA Monopoly In Machine Learning Is Breaking – OpenAI Triton And PyTorch 2.0\n\n17 minutes\n\n[30 comments on How Nvidia’s CUDA Monopoly In Machine Learning Is Breaking – OpenAI Triton And PyTorch 2.0](https://semianalysis.com/2023/01/16/nvidiaopenaitritonpytorch/#comments)\n\nBy\n\n[Dylan Patel](https://semianalysis.com/author/dylanpatel/)\n\n- [Share using Native toolsShareCopied to clipboard](https://semianalysis.com/2023/01/16/nvidiaopenaitritonpytorch/)\n- [Click to share on LinkedIn (Opens in new window)LinkedIn](https://semianalysis.com/2023/01/16/nvidiaopenaitritonpytorch/?share=linkedin&nb=1)\n- [Click to share on X (Opens in new window)X](https://semianalysis.com/2023/01/16/nvidiaopenaitritonpytorch/?share=x&nb=1)\n- [Click to share on Facebook (Opens in new window)Facebook](https://semianalysis.com/2023/01/16/nvidiaopenaitritonpytorch/?share=facebook&nb=1)\n\nTable of Contents\n\n01. [TensorFlow vs. PyTorch](https://semianalysis.com/semianalysis.com#tensorflow-vs-pytorch)\n02. [Machine Learning Training Components](https://semianalysis.com/semianalysis.com#machine-learning-training-components)\n03. [The Memory Wall](https://semianalysis.com/semianalysis.com#the-memory-wall)\n04. [Operator Fusion – The Workaround](https://semianalysis.com/semianalysis.com#operator-fusion-%e2%80%93-the-workaround)\n05. [Nvidia Is King](https://semianalysis.com/semianalysis.com#nvidia-is-king)\n06. [PyTorch 2.0](https://semianalysis.com/semianalysis.com#pytorch-2-0)\n07. [PrimTorch](https://semianalysis.com/semianalysis.com#primtorch)\n08. [TorchDynamo](https://semianalysis.com/semianalysis.com#torchdynamo)\n09. [TorchInductor](https://semianalysis.com/semianalysis.com#torchinductor)\n10. [OpenAI Triton](https://semianalysis.com/semianalysis.com#openai-triton)\n11. [Subscribe for full access](https://semianalysis.com/semianalysis.com#semianalysis-blocks-login-to-view-content)\n\nOver the last decade, the landscape of machine learning software development has undergone significant changes. Many frameworks have come and gone, but most have relied heavily on leveraging Nvidia’s CUDA and performed best on Nvidia GPUs. However, with the arrival of PyTorch 2.0 and OpenAI’s Triton, Nvidia’s dominant position in this field, mainly due to its software moat, is being disrupted.\n\nThis report will touch on topics such as why Google’s TensorFlow lost out to PyTorch, why Google hasn’t been able to capitalize publicly on its early leadership of AI, the major components of machine learning model training time, the memory capacity/bandwidth/cost wall, model optimization, why other AI hardware companies haven’t been able to make a dent in Nvidia’s dominance so far, why hardware will start to matter more, how Nvidia’s competitive advantage in CUDA is wiped away, and a major win one of Nvidia’s competitors has at a large cloud for training silicon.\n\nThe 1,000-foot summary is that the default software stack for machine learning models will no longer be Nvidia’s closed-source CUDA. The ball was in Nvidia’s court, and they let OpenAI and Meta take control of the software stack. That ecosystem built its own tools because of Nvidia’s failure with their proprietary tools, and now Nvidia’s moat will be permanently weakened.\n\n## TensorFlow vs. PyTorch\n\nA handful of years ago, the framework ecosystem was quite fragmented, but TensorFlow was the frontrunner. Google looked poised to control the machine learning industry. They had a first movers’ advantage with the most commonly used framework, TensorFlow, and by designing/deploying the only successful AI application-specific accelerator, TPU.\n\n[https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/)\n\nInstead, PyTorch won. Google failed to convert its first mover’s advantage into dominance of the nascent ML industry. Nowadays, Google is somewhat isolated within the machine learning community because of its lack of use of PyTorch and GPUs in favor of its own software stack and hardware. In typical Google fashion, they even have a 2nd framework called Jax that competes directly with TensorFlow.\n\nThere’s even endless talk of Google’s dominance in search and natural language processing waning due to large language models, particularly those from OpenAI and the various startups that utilize OpenAI APIs or are building similar foundational models. While we believe this doom and gloom is overblown, that story is for another day. Despite these challenges, Google is still at the forefront of [the most advanced machine learning models](https://arxiv.org/abs/2212.13138). They invented transformers and remain state-of-the-art in many areas (PaLM, LaMBDA, Chinchilla, MUM, TPU).\n\nBack to why PyTorch won. While there was an element of wrestling control away from Google, it was primarily due to its increased flexibility and usability of PyTorch versus TensorFlow. If we boil it down to a first principal level, PyTorch differed from TensorFlow in using “ **Eager mode**” rather than “ **Graph Mode**.”\n\nEager mode can be thought of as a standard scripting execution method. The deep learning framework executes each operation immediately, as it is called, line by line, like any other piece of Python code. This makes debugging and understanding your code more accessible, as you can see the results of intermediate operations and see how your model behaves.\n\nIn contrast, graph mode has two phases. The first phase is the definition of a computation graph representing the operations to perform. A computation graph is a series of interconnected nodes representing operations or variables, and the edges between nodes represent the data flow between them. The second phase is the deferred execution of an optimized version of the computation graph.\n\nThis two-phase approach makes it more challenging to understand and debug your code, as you cannot see what is happening until the end of the graph execution. This is analogous to “interpreted” vs. “compiled” languages, like python vs. C++. It’s easier to debug Python, largely since it’s interpreted.\n\nWhile TensorFlow now has Eager mode by default, the research community and most large tech firms have settled around PyTorch. This is exemplified by the fact that nearly ever generative AI model that made the news, being based on PyTorch. The Google generative AI models are based on Jax, not TensorFlow.\n\nOf course there is a long tail of image nets using other frameworks like TensorFlow and Keras, but the compute budgets for new model development is all flowing to PyTorch models. For a deeper explanation of why PyTorch won, see [here](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/). In general, if you walk around the halls of NeurIPS (the main AI conference), all generative AI, the non-Google work is with PyTorch.\n\n## Machine Learning Training Components\n\nIf we boil machine learning model training to its most simplistic form, there are two major time components in a machine learning model’s training time.\n\n1. Compute (FLOPS):  Running dense matrix multiplication within each layer\n\n2. Memory (Bandwidth): Waiting for data or layer weights to get to the compute resources. Common examples of bandwidth-constrained operations are various [normalizations](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html), [pointwise operations](https://pytorch.org/docs/stable/torch.html), [SoftMax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html), and [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html).\n\n\nIn the past, the dominant factor in machine learning training time was compute time, waiting for matrix multiplies. As Nvidia’s GPUs continued to develop, this quickly faded away from being the primary concern.\n\nNvidia’s FLOPS have increased multiple orders of magnitude by leveraging Moore’s Law, but primarily architectural changes such as the tensor core and lower precision floating point formats. In contrast, [memory has not followed the same path](https://semianalysis.com/cxl-enables-microsoft-azure-to-cut).\n\nIf we go back to 2018, when the BERT model was state of the art, and the Nvidia V100 was the most advanced GPU, we could see that matrix multiplication was no longer the primary factor for improving a model’s performance. Since then, the most advanced models have grown 3 to 4 orders of magnitude in parameter count, and the fastest GPUs have grown an order of magnitude in FLOPS.\n\nhttps://arxiv.org/pdf/2007.00072.pdf\n\nEven in 2018, purely compute-bound workloads made up 99.8% of FLOPS but only 61% of the runtime. The normalization and pointwise ops achieve 250x less FLOPS and 700x less FLOPS than matrix multiplications, respectively, yet they consume nearly 40% of the model’s runtime.\n\n## The Memory Wall\n\nAs models continue to soar in size, large language models take 100s gigabytes, if not terabytes, for the model weights alone. Production recommendation networks deployed by Baidu and Meta require dozens of terabytes of memory for their massive embedding tables. A huge chunk of the time in large model training/inference is not spent computing matrix multiplies, but rather waiting for data to get to the compute resources. The obvious question is why don’t architects put more memory closer to the compute. The answer is $$$.\n\nMemory follows a hierarchy from close and fast to slow and cheap. The nearest shared memory pool is on the same chip and is generally made of SRAM. Some machine-learning ASICs attempt to utilize huge pools of SRAM to hold model weights, but there are issues with this approach. [Even Cerebras’ ~$2,500,000 wafer scale chips](https://semianalysis.com/gpt-model-training-competition-heats) only have 40GB of SRAM on the chip. There isn’t enough memory capacity to hold the weights of a 100B+ parameter model.\n\nNvidi","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fsemianalysis.com%2F2023%2F01%2F16%2Fnvidiaopenaitritonpytorch%2F","id":"https://semianalysis.com/2023/01/16/nvidiaopenaitritonpytorch/","publishedDate":"2023-01-16T00:00:00.000Z","summary":"This article discusses how Nvidia's dominance in machine learning, primarily due to its CUDA software, is being challenged by the emergence of PyTorch 2.0 and OpenAI's Triton. It highlights the shift from Nvidia's closed-source CUDA as the default software stack for machine learning models, with OpenAI and Meta taking control of the software stack due to Nvidia's shortcomings. The article touches on the evolution of machine learning frameworks, the components of model training, memory constraints, and the reasons behind Nvidia's current market position. It also mentions Google's TensorFlow initially leading the field but losing out to PyTorch.\n","title":"How Nvidia's CUDA Monopoly In Machine Learning Is Breaking","url":"https://semianalysis.com/2023/01/16/nvidiaopenaitritonpytorch/"},{"content":"[Technical Blog](https://developer.nvidia.com/blog)\n\n[Subscribe](https://developer.nvidia.com/email-signup)\n\n[Related Resources](https://developer.nvidia.com/developer.nvidia.com#main-content-end)\n\n[Generative AI](https://developer.nvidia.com/blog/category/generative-ai/)\nEnglish한국어\n\n# Achieving Top Inference Performance with the NVIDIA H100 Tensor Core GPU and NVIDIA TensorRT-LLM\n\nDec 14, 2023\n\nBy [Dave Salvator](https://developer.nvidia.com/blog/author/dsalvator/) and [Ashraf Eassa](https://developer.nvidia.com/blog/author/aeassa/)\n\nLike\n\n[Discuss (1)](https://developer.nvidia.com/developer.nvidia.com#entry-content-comments)\n\n- [L](https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fachieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm%2F)\n- [T](https://twitter.com/intent/tweet?text=Achieving+Top+Inference+Performance+with+the+NVIDIA+H100+Tensor+Core+GPU+and+NVIDIA+TensorRT-LLM+%7C+NVIDIA+Technical+Blog+https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fachieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm%2F)\n- [F](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fachieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm%2F)\n- [R](https://www.reddit.com/submit?url=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fachieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm%2F&title=Achieving+Top+Inference+Performance+with+the+NVIDIA+H100+Tensor+Core+GPU+and+NVIDIA+TensorRT-LLM+%7C+NVIDIA+Technical+Blog)\n- [E](mailto://developer.nvidia.com?subject=I'd like to share a link with you&body=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fachieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm%2F)\n\nAI-Generated Summary\n\nLike\n\nDislike\n\n- NVIDIA's TensorRT-LLM software enables the NVIDIA H100 GPU to execute models like Llama 2 70B using accelerated FP8 operations while maintaining inference accuracy.\n- The NVIDIA DGX H100 server with eight NVIDIA H100 GPUs achieved a measured performance of processing a single Llama 2 70B inference in 1.7 seconds using a batch size of one.\n- By setting a fixed 2.5-second response time budget, an 8-GPU DGX H100 server can process over five Llama 2 70B inferences per second, significantly improving data center throughput.\n\nAI-generated content may summarize information incompletely. Verify important information. [Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\n\nBest-in-class AI performance requires an efficient parallel computing architecture, a productive tool stack, and deeply optimized algorithms. NVIDIA released the open-source [NVIDIA TensorRT-LLM,](https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/) which includes the latest kernel optimizations for the [NVIDIA Hopper architecture](https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/) at the heart of the [NVIDIA H100 Tensor Core GPU](https://www.nvidia.com/en-us/data-center/h100/). These optimizations enable models like Llama 2 70B to execute using accelerated FP8 operations on H100 GPUs while maintaining inference accuracy.\n\nAt a recent launch event, AMD talked about the inference performance of the H100 GPU compared to that of its MI300X chip. The results shared did not use optimized software, and the H100, if benchmarked properly, is 2x faster.\n\nThe following is the actual measured performance of a single NVIDIA DGX H100 server with eight NVIDIA H100 GPUs on the Llama 2 70B model. This includes results for both “Batch-1” where an inference request is processed one at a time, as well as results using fixed response-time processing.\n\n_Figure 1. Llama 2 70B server inference performance in queries per second with 2,048 input tokens and 128 output tokens for “Batch 1” and various fixed response time settings_\n\n_AMD’s implied claims for H100 are measured based on the configuration taken from AMD launch presentation footnote #MI300-38. Using vLLM v.02.2.2 inference software with NVIDIA DGX H100 system, Llama 2 70B query with an input sequence length of 2,048 and output sequence length of 128. They claimed relative performance compared to DGX H100 with 8x GPU MI300X system._\n\n_For NVIDIA measured data, DGX H100 with 8x NVIDIA H100 Tensor Core GPUs with 80 GB HBM3 with publicly available NVIDIA TensorRT-LLM, v0.5.0 for batch 1 and v0.6.1 for latency threshold measurements. Workload details same as footnote #MI300-38._\n\nDGX H100 can process a single inference in 1.7 seconds using a batch size of one—in other words, one inference request at a time. A batch size of one results in the fastest possible response time for serving a model. To optimize both response time and data center throughput, cloud services set a fixed response time for a particular service. This enables them to combine multiple inference requests into larger “batches” and increase the overall inferences per second of the server. Industry-standard benchmarks like MLPerf also measure performance with this fixed response time metric.\n\nSmall tradeoffs in response time can yield x-factors in the number of inference requests that a server can process in real time. Using a fixed 2.5-second response time budget, an 8-GPU DGX H100 server can process over five Llama 2 70B inferences per second compared to less than one per second with batch one.\n\nAI is moving fast and the NVIDIA CUDA ecosystem enables us to optimize our stack quickly and continuously. We look forward to continuing to improve AI performance with every update of our software, so be sure to check out our [performance pages](https://developer.nvidia.com/deep-learning-performance-training-inference) and [GitHub sites](https://github.com/NVIDIA/TensorRT-LLM) for the latest.\n\n## _How to reproduce these AI inference results_\n\n_DGX H100 AMD Footnote_ was measured by NVIDIA in vLLM based on the configurations provided by AMD in their footnotes using [vLLM](https://github.com/vllm-project/vllm/tree/main) and its provided [benchmarking script](https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_latency.py) with the following command lines:\n\n```\n$ python benchmarks/benchmark_latency.py --model \"meta-llama/Llama-2-70b-hf\" --input-len 2048 --output-len 128 --batch-size 1 -tp 8\n\n```\n\n_MI300X 8-Chip System_ is the inferred data based on AMD’s claimed speedup over _DGX H100 AMD Footnote_ measured vLLM results.\n\n_DGX H100 Measured_ was measured by NVIDIA using publicly available versions of TensorRT-LLM available on [GitHub](https://github.com/NVIDIA/TensorRT-LLM) and using the command lines outlined in the TensorRT-LLM benchmarking [guide for Llama 2](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance.md#llama2-70b).\n\n```\n// Build TensorRT optimized Llama-2-70b for H100 fp8 tensorcore\n$ python examples/llama/build.py --remove_input_padding --enable_context_fmha --parallel_build --output_dir DTYPE.float16_TP.8_BS.14_ISL.2048_OSL.128 --dtype float16 --use_gpt_attention_plugin float16 --world_size 8 --tp_size 8 --pp_size 1 --max_batch_size 14 --max_input_len 2048 --max_output_len 128 --enable_fp8 --fp8_kv_cache --strongly_typed --n_head 64 --n_kv_head 8 --n_embd 8192 --inter_size 28672 --vocab_size 32000 --n_positions 4096 --hidden_act silu --ffn_dim_multiplier 1.3 --multiple_of 4096 --n_layer 80\n\n// Benchmark Llama-70B\n$ mpirun -n 8 --allow-run-as-root --oversubscribe ./cpp/build/benchmarks/gptSessionBenchmark --model llama_70b --engine_dir DTYPE.float16_TP.8_BS.14_ISL.2048_OSL.128 --warm_up 1 --batch_size 14 --duration 0 --num_runs 5 --input_output_len 2048,1;2048,128\n\n```\n\n[Discuss (1)](https://developer.nvidia.com/developer.nvidia.com#entry-content-comments)\n\nLike\n\n## Tags\n\n[Data Center / Cloud](https://developer.nvidia.com/blog/category/data-center-cloud/) \\| [Generative AI](https://developer.nvidia.com/blog/category/generative-ai/) \\| [Cloud Services](https://developer.nvidia.com/blog/recent-posts/?industry=Cloud+Services) \\| [DGX](https://developer.nvidia.com/blog/recent-posts/?products=DGX) \\| [H100](https://developer.nvidia.com/blog/recent-posts/?products=H100) \\| [Hopper](https://developer.nvidia.com/blog/recent-posts/?products=Hopper) \\| [TensorRT](https://developer.nvidia.com/blog/recent-posts/?products=TensorRT) \\| [Triton Inference Server](https://developer.nvidia.com/blog/recent-posts/?products=Triton+Inference+Server) \\| [Advanced Technical](https://developer.nvidia.com/blog/recent-posts/?learning_levels=Advanced+Technical) \\| [Benchmark](https://developer.nvidia.com/blog/recent-posts/?content_types=Benchmark) \\| [AI Inference / Inference Microservices](https://developer.nvidia.com/blog/tag/ai-inference-microservices/) \\| [featured](https://developer.nvidia.com/blog/tag/featured/) \\| [LLMs](https://developer.nvidia.com/blog/tag/large-language-models/) \\| [Performance Optimization](https://developer.nvidia.com/blog/tag/optimization/)\n\n## About the Authors\n\n**About Dave Salvator** Dave Salvator is a director of product marketing with the NVIDIA Accelerated Computing group. He focuses on hyperscale computing, deep learning, and bringing the NVIDIA data center platform to the cloud.\n\n[View all posts by Dave Salvator](https://developer.nvidia.com/blog/author/dsalvator/)\n\n**About Ashraf Eassa** Ashraf Eassa is a senior product marketing manager at NVIDIA, focusing on deep learning, training and inference. He holds bachelor's degrees in computer science and mathematics from the University of Vermont.\n\n[View all posts by Ashraf Eassa](https://developer.nvidia.com/blog/author/aeassa/)\n\n## Comments\n\n## Related posts\n\n### Deploying NVIDIA H200 NVL at Scale with New Enterprise Reference Architecture\n\n[Deploying NVIDIA H200 NVL at Scale with New Enterprise Reference Architecture](https://developer.nvidia.com/blog/deploying-nvidi","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fachieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm%2F","id":"https://developer.nvidia.com/blog/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/","publishedDate":"2023-12-14T00:00:00.000Z","summary":"This NVIDIA technical blog post discusses achieving top inference performance with the NVIDIA H100 Tensor Core GPU and NVIDIA TensorRT-LLM. Key takeaways include:\n\n*   NVIDIA's TensorRT-LLM software enables the NVIDIA H100 GPU to execute models like Llama 2 70B using accelerated FP8 operations while maintaining inference accuracy.\n*   A single NVIDIA DGX H100 server with eight NVIDIA H100 GPUs achieved a measured performance of processing a single Llama 2 70B inference in 1.7 seconds using a batch size of one.\n*   By setting a fixed 2.5-second response time budget, an 8-GPU DGX H100 server can process over five Llama 2 70B inferences per second, significantly improving data center throughput.\n","title":"Achieving Top Inference Performance with the NVIDIA H100 Tensor ...","url":"https://developer.nvidia.com/blog/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/"},{"content":"Tuesday, June 6th 2023\n\n# [NVIDIA Triton Inference Server Running A100 Tensor Core GPUs Boosts Bing Advert Delivery](https://www.techpowerup.com/309697/nvidia-triton-inference-server-running-a100-tensor-core-gpus-boosts-bing-advert-delivery)\n\nPress Releaseby[T0@st](https://www.techpowerup.com/forums/members/228819/)Jun 6th, 2023 11:15\n[Discuss (9 Comments)](https://www.techpowerup.com/309697/nvidia-triton-inference-server-running-a100-tensor-core-gpus-boosts-bing-advert-delivery#comments)\n\nInference software enables shift to NVIDIA A100 Tensor Core GPUs, delivering 7x throughput for the search giant. Jiusheng Chen's team just got accelerated. They're delivering personalized ads to users of Microsoft Bing with 7x throughput at reduced cost, thanks to [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) running on [NVIDIA A100 Tensor Core GPUs](https://www.nvidia.com/en-us/data-center/a100/). It's an amazing achievement for the principal software engineering manager and his crew.\n**Tuning a Complex System**\nBing's ad service uses hundreds of models that are constantly evolving. Each must respond to a request within as little as 10 milliseconds, about 10x faster than the blink of an eye. The latest speedup got its start with two innovations the team delivered to make AI models run faster: [Bang](http://proceedings.mlr.press/v139/qi21a/qi21a.pdf) and [EL-Attention](http://proceedings.mlr.press/v139/yan21a.html). Together, they apply sophisticated techniques to do more work in less time with less computer memory. Model training was based on Azure Machine Learning for efficiency. **Flying With NVIDIA A100 MIG**\nNext, the team upgraded the ad service from [NVIDIA T4](https://www.nvidia.com/en-us/data-center/tesla-t4/) to A100 GPUs. The latter's Multi-Instance GPU ( [MIG](https://blogs.nvidia.com/blog/2020/05/14/multi-instance-gpus/)) feature lets users split one GPU into several instances. Chen's team maxed out the MIG feature, transforming one physical A100 into seven independent ones. That let the team reap a 7x throughput per GPU with inference response in 10 ms.\n**Flexible, Easy, Open Software**\nTriton enabled the shift, in part, because it lets users simultaneously run different runtime software, frameworks and AI modes on isolated instances of a single GPU. The inference software comes in a software container, so it's easy to deploy. And open-source Triton - also available with enterprise-grade security and support through [NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/) \\- is backed by a community that makes the software better over time.\nAccelerating Bing's ad system with Triton on A100 GPUs is one example of what Chen likes about his job. He gets to witness breakthroughs with AI.\nWhile the scenarios often change, the team's goal remains the same - creating a win for its users and advertisers.\n\nSource:\n[NVIDIA BlogNVIDIA A100 Tensor Core GPUs](https://blogs.nvidia.com/blog/2023/06/05/microsoft-bing-triton/)\n\n### Related News\n\n- Tags:\n- [A100](https://www.techpowerup.com/news-tags/A100)\n- [Advertising](https://www.techpowerup.com/news-tags/Advertising)\n- [Azure](https://www.techpowerup.com/news-tags/Azure)\n- [Bing](https://www.techpowerup.com/news-tags/Bing)\n- [Enterprise](https://www.techpowerup.com/news-tags/Enterprise)\n- [GPU](https://www.techpowerup.com/news-tags/GPU)\n- [Machine Learning](https://www.techpowerup.com/news-tags/Machine%20Learning)\n- [Microsoft](https://www.techpowerup.com/news-tags/Microsoft)\n- [NVIDIA](https://www.techpowerup.com/news-tags/NVIDIA)\n- [Triton](https://www.techpowerup.com/news-tags/Triton)\n\n- Feb 25th 2025 [NVIDIA GeForce RTX 50 Cards Spotted with Missing ROPs, NVIDIA Confirms the Issue, Multiple Vendors Affected](https://www.techpowerup.com/332884/nvidia-geforce-rtx-50-cards-spotted-with-missing-rops-nvidia-confirms-the-issue-multiple-vendors-affected) (519)\n- Dec 25th 2024 [NVIDIA GeForce RTX 5070 and RTX 5070 Ti Final Specifications Seemingly Confirmed](https://www.techpowerup.com/330206/nvidia-geforce-rtx-5070-and-rtx-5070-ti-final-specifications-seemingly-confirmed) (153)\n- Jan 12th 2025 [AMD Radeon RX 9070 XT Tested in Cyberpunk 2077 and Black Myth: Wukong](https://www.techpowerup.com/331015/amd-radeon-rx-9070-xt-tested-in-cyberpunk-2077-and-black-myth-wukong) (177)\n- Jan 24th 2025 [New Leak Reveals NVIDIA RTX 5080 Is Slower Than RTX 4090](https://www.techpowerup.com/331599/new-leak-reveals-nvidia-rtx-5080-is-slower-than-rtx-4090) (260)\n- Jul 28th 2025 [NVIDIA to Debut GeForce RTX 50-series SUPER GPUs by Christmas](https://www.techpowerup.com/339358/nvidia-to-debut-geforce-rtx-50-series-super-gpus-by-christmas) (112)\n- Sep 26th 2024 [NVIDIA GeForce RTX 5090 and RTX 5080 Specifications Surface, Showing Larger SKU Segmentation](https://www.techpowerup.com/327026/nvidia-geforce-rtx-5090-and-rtx-5080-specifications-surface-showing-larger-sku-segmentation) (185)\n- Dec 16th 2024 [NVIDIA GeForce RTX 5070 Ti Leak Tips More VRAM, Cores, and Power Draw](https://www.techpowerup.com/329873/nvidia-geforce-rtx-5070-ti-leak-tips-more-vram-cores-and-power-draw) (161)\n- Jan 6th 2025 [NVIDIA 2025 International CES Keynote: Liveblog](https://www.techpowerup.com/330611/nvidia-2025-international-ces-keynote-liveblog) (470)\n- Jan 3rd 2025 [NVIDIA GeForce RTX 5090 Features 575 W TDP, RTX 5080 Carries 360 W TDP](https://www.techpowerup.com/330459/nvidia-geforce-rtx-5090-features-575-w-tdp-rtx-5080-carries-360-w-tdp) (217)\n- Apr 28th 2025 [NVIDIA Reportedly Working on GeForce RTX 5080 SUPER 24 GB & RTX 5070 SUPER 18 GB Designs](https://www.techpowerup.com/336037/nvidia-reportedly-working-on-geforce-rtx-5080-super-24-gb-rtx-5070-super-18-gb-designs) (76)\n\n[Add your own comment](https://www.techpowerup.com/forums/threads/309697/reply)\n\n## **9 Comments** on [NVIDIA Triton Inference Server Running A100 Tensor Core GPUs Boosts Bing Advert Delivery](https://www.techpowerup.com/forums/threads/309697/)\n\n[#1](https://www.techpowerup.com/forums/posts/5034057)\n\n> T0@stThey're delivering personalized ads to users of Microsoft Bing with 7x throughput at reduced cost, thanks to [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) running on [NVIDIA A100 Tensor Core GPUs](https://www.nvidia.com/en-us/data-center/a100/). It's an amazing achievement for the principal software engineering manager and his crew.\n\nGreat for people that like watching/clicking ads lol.....\n\n[#2](https://www.techpowerup.com/forums/posts/5034087)\n\nMankind creating most advanced chips to date only to use to serve ads better.\nThis world is doomed.\n\n[#3](https://www.techpowerup.com/forums/posts/5034144)\n\nOh yay and just when I was getting worried about the viability of advertising, in swoops our savior.\n\n[#4](https://www.techpowerup.com/forums/posts/5034172)\n\nTry out the new & improved Bing!\nNow serving you even more customized ads, seven times more efficiently :clap:\n\n[#5](https://www.techpowerup.com/forums/posts/5034226)\n\nThe only way AI can make Bing advertising better is a Skynet situation where Bing is the target.\n\n[#6](https://www.techpowerup.com/forums/posts/5034295)\n\nGood for the heads up, I'll double down on making sure I never use Bing.\n\n[#7](https://www.techpowerup.com/forums/posts/5034513)\n\n> Haile SelassieMankind creating most advanced chips to date only to use to serve ads better.\n> This world is doomed.\n\nthat is what it's all about. I look at the new apple AR ski googles and all I see is a more efficient ad delivery system. all silicon things are quickly becoming that, ad delivery platforms.\n\n[#8](https://www.techpowerup.com/forums/posts/5034925)\n\n> Haile SelassieMankind creating most advanced chips to date only to use to serve ads better.\n> This world is doomed.\n\nIf you have a fully functional AI system you can replace all the human workers. Profit because that's where less salary overall is paid out.\nAI is going to kill alot of jobs. All the human interaction will be gone in the future. All led by AI models.\n\n[#9](https://www.techpowerup.com/forums/posts/5035038)\n\n> JismIf you have a fully functional AI system you can replace all the human workers. Profit because that's where less salary overall is paid out.\n> AI is going to kill alot of jobs. All the human interaction will be gone in the future. All led by AI models.\n\nLike steam power. Like electrification. Like air conditioning. Like computers. And so on...\n\nSep 7th, 2025 08:01 PDT [change timezone](https://www.techpowerup.com/www.techpowerup.com)\n\n[Sign in](https://www.techpowerup.com/forums/login) / [Register](https://www.techpowerup.com/forums/register)\n\n# Latest GPU Drivers\n\n- [NVIDIA GeForce 581.15 WHQL](https://www.techpowerup.com/download/nvidia-geforce-graphics-drivers/)\n- [AMD Radeon 25.8.1 WHQL](https://www.techpowerup.com/download/amd-radeon-graphics-drivers/)\n- [Intel Arc & iGPU 101.7028 Beta](https://www.techpowerup.com/download/intel-integrated-graphics-drivers/)\n\n# New Forum Posts\n\n- 07:57\nby\nLogic\n[UPDATE:Nvme Health decreasing now with 3% for 5 days](https://www.techpowerup.com/forums/threads/update-nvme-health-decreasing-now-with-3-for-5-days.327343/unread) (252)\n- 07:54\nby\nContra\n[Question about Intel Optane SSDs](https://www.techpowerup.com/forums/threads/question-about-intel-optane-ssds.334174/unread) (154)\n- 07:53\nby\nKabal81\n[DTS DCH Driver for Realtek HDA \\[DTS:X APO4 + DTS Interactive\\]](https://www.techpowerup.com/forums/threads/dts-dch-driver-for-realtek-hda-dts-x-apo4-dts-interactive.279972/unread) (2253)\n- 07:52\nby\nr00tcz\n[Front Panel Temp Display Not Working](https://www.techpowerup.com/forums/threads/front-panel-temp-display-not-working.340737/unread) (3)\n- 07:34\nby\nRUSerious\n[What are you playing?](https://www.techpowerup.com/forums/threads/what-are-you-playing.204644/unread) (24169)\n- 07:16\nby\nFoulOnWhite\n[What's your latest tech purchase?](https://www.techpowerup.com/forums/threads/whats-your-latest-tech-purchase.2258","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.techpowerup.com%2F309697%2Fnvidia-triton-inference-server-running-a100-tensor-core-gpus-boosts-bing-advert-delivery","id":"https://www.techpowerup.com/309697/nvidia-triton-inference-server-running-a100-tensor-core-gpus-boosts-bing-advert-delivery","publishedDate":"2023-06-06T00:00:00.000Z","summary":"NVIDIA Triton Inference Server, running on NVIDIA A100 Tensor Core GPUs, has enabled Microsoft Bing to deliver personalized ads with 7x throughput at a reduced cost. The upgrade from NVIDIA T4 to A100 GPUs, utilizing the Multi-Instance GPU (MIG) feature, allowed the team to split one A100 into seven independent instances, achieving a 7x throughput per GPU with inference response in 10 ms. Triton's flexibility in running different software and AI models on isolated GPU instances, along with its open-source nature, facilitated this acceleration.\n","title":"NVIDIA Triton Inference Server Running A100 Tensor Core GPUs ...","url":"https://www.techpowerup.com/309697/nvidia-triton-inference-server-running-a100-tensor-core-gpus-boosts-bing-advert-delivery"},{"content":"Findings of the Association for Computational Linguistics: ACL 2025, pages 23053–23066\nJuly 27 - August 1, 2025 ©2025 Association for Computational Linguistics\nTRITONBENCH: Benchmarking Large Language Model Capabilities for\nGenerating Triton Operators\nJianling Li1,∗, Shangzhan Li2,∗, Zhenye Gao3, Qi Shi4,†, Yuxuan Li4,†, Zefan Wang4,\nJiacheng Huang4, Haojie Wang4, Jianrong Wang1, Xu Han4, Zhiyuan Liu4, Maosong Sun4\n1 Tianjin University, Tianjin, China 2 Harbin Institute of Technology, Harbin, China\n3 The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China\n4 Tsinghua University, Beijing, China\n{jianlingl@tju,szli@ir.hit}.edu.cn, {qshi9510,yxuanl1995}@gmail.com\nAbstract\nTriton, a high-level Python-like language de\u0002signed for building efficient GPU kernels, is\nwidely adopted in deep learning frameworks\ndue to its portability, flexibility, and accessi\u0002bility. However, programming and parallel op\u0002timization still require considerable trial and\nerror from Triton developers. Despite advances\nin large language models (LLMs) for conven\u0002tional code generation, these models struggle\nto generate accurate, performance-optimized\nTriton code, as they lack awareness of its speci\u0002fications and the complexities of GPU program\u0002ming. More critically, there is an urgent need\nfor systematic evaluations tailored to Triton. In\nthis work, we introduce TRITONBENCH, the\nfirst comprehensive benchmark for Triton oper\u0002ator generation. TRITONBENCH features two\nevaluation channels: a curated set of 184 real\u0002world operators from GitHub and a collection\nof 166 operators aligned with PyTorch inter\u0002faces. Unlike conventional code benchmarks\nprioritizing functional correctness, TRITON\u0002BENCH also profiles efficiency performance\non widely deployed GPUs aligned with indus\u0002try applications. Our study reveals that current\nstate-of-the-art code LLMs struggle to gener\u0002ate efficient Triton operators, highlighting a\nsignificant gap in high-performance code gen\u0002eration. TRITONBENCH is available at https:\n//github.com/thunlp/TritonBench.\n1 Introduction\nTriton (Tillet et al., 2019) language, a high-level\nPython-like programming language designed for\nimplementing efficient GPU kernels, is playing an\nincreasingly pivotal role in the ever-scaling deep\nlearning ecosystems (Abadi et al., 2016; Paszke\net al., 2019). Due to the superior portability, flex\u0002ibility, lightweight design, and accessibility to\nless proficient programmers, Triton is prevalently\nadopted in modern Large Language Model (LLM)\n*Equal contribution.\n†Corresponding authors.\nframeworks such as vLLM (Kwon et al., 2023),\nLightLLM (ModelTC, 2025), Liger-kernel (Hsu\net al., 2024) and unsloth (Daniel Han and team,\n2023). However, crafting high-performance oper\u0002ators remains challenging, especially for the intri\u0002cate balance between memory hierarchy manage\u0002ment, parallel thread coordination, and hardware\u0002specific optimizations. Even though Triton ab\u0002stracts away many complexities of low-level pro\u0002gramming architectures like CUDA, it still requires\ndevelopers to manually handle critical aspects such\nas pointer arithmetic and memory access patterns,\nmaking performance tuning a labor-intensive pro\u0002cess that often involves extensive trial and error.\nCurrent research in AI-assisted coding has\nreached a human-competitive level (Hui et al.,\n2024; Zhu et al., 2024), yet it is primarily restricted\nto general-purpose languages like Python. How\u0002ever, LLMs still face challenges in generating Do\u0002main Specific Language (DSL) code. Specifically\nfor Triton, current models might be unfamiliar with\nTriton specification and the intricacies of GPU pro\u0002gramming (Nichols et al., 2024). Most importantly,\nthe ability of these models to produce high-quality\nTriton code remains unassessed. Therefore, a high\u0002quality benchmark paired with performance-aware\nmetrics is urgently required.\nIn this study, we present TRITONBENCH, a\nperformance-aware benchmark framework for Tri\u0002ton generation, which contains two channels,\nnamely TRITONBENCH-G and TRITONBENCH\u0002T. Specifically, TRITONBENCH-G contains 184\ncarefully curated operators from existing GitHub\nrepositories, reflecting the realistic demand for\nTriton operator development. As a complement,\nTRITONBENCH-T is composed of operator de\u0002velopment tasks aligned with PyTorch interfaces,\ncovering operators under-represented by public\nsources. Moreover, unlike the majority of code\nbenchmarks merely prioritizing functional correct\u0002ness (Chen et al., 2021; Austin et al., 2021a), TRI\u000223053\nTRITONBENCH-T\nEvaluation\nCodeBLEU\nConstruction\nCall Execution\nAccuracy\nSpeed Up\nPerformance\nGPU Efficiency\nReference Operator Generated Operator\nCollection\nOperator\nStatis Sample\nFusing\nUncommon\nCommon\n…\nTRITONBENCH-G\nGPU\nRuntime\nQuality\nRating\nInstruction\nTest Code Operator\nCrawl Filter Extract\nCollection\nOutput\nOutput\nInstruction\nTest Code\n@triton.jit\ndef add_kernel(x_ptr, …)\n...\ndef add_wrapper(x, y)\n...\nSimilarity\n@triton.jit\ndef add_kernel(x_p, …)\n...\ndef add_wrapper(x, y)\n...\nFigure 1: Illustration of the construction and evaluation of TRITONBENCH.\nTONBENCH emphasizes efficiency performance\nprofiling against reference programs on NVIDIA\nGPUs, better aligning industrial demands.\nAs shown in Figure 1, for TRITONBENCH\u0002G, we follow three steps: 1) scrape and collect\nhigh-quality operators, 2) generate instructions via\nprompts, and 3) annotate test code with LLMs.\nMoreover, HPC experts evaluate GPU performance\nfor all triton codes. For TRITONBENCH-T, we\nprovide operator generation tasks aligned with Py\u0002Torch. To construct these tasks, we first perform a\nfrequency analysis to select torch operators, com\u0002bine them into diverse sets, and provide paired\ninstructions and test code. Our evaluation met\u0002rics include similarity, call and execution accuracy,\nspeed up, and GPU efficiency.\nWe conduct extensive experiments across a\nbroad range of LLMs. Overall, the difficulty\nof TRITONBENCH-G is greater than that of\nTRITONBENCH-T. The highest execution accuracy\non TRITONBENCH-G can reach 23.91%, while\non TRITONBENCH-T, it can reach 53.01%. For\nall correctly executed operators generated by the\nmodels, the best speed up on TRITONBENCH-G\nis 1.56×, whereas, on TRITONBENCH-T, it is\n1.91×. Additionally, we perform in-depth analyses\nof LLMs’ behavior on TRITONBENCH and summa\u0002rize the challenges in Triton generation. The results\nreveal that current LLMs are not yet fully capable\nof handling TRITONBENCH, underscoring the chal\u0002lenge of enabling LLMs to generate Triton code\neffectively. We hope this work initiates evaluation\nin this under-explored area and fosters advance\u0002ments in LLM-driven operator development.\n2 Related Work\n2.1 Triton Development\nTriton (Mitkov et al., 2021) is an open-source,\nPython-like language and a compiler designed to\nsimplify GPU programming in AI and HPC. It\nabstracts the complexities of CUDA by introduc\u0002ing a block-based programming model, automat\u0002ing low-level optimizations such as memory coa\u0002lescing and tensor core utilization, and making it\nmore accessible to researchers without HPC back\u0002ground. Nonetheless, Triton provides explicit con\u0002trol over memory access patterns and parallelism.\nThis balance of productivity and flexibility makes\nit prevalently adopted in both academia and indus\u0002try (Kwon et al., 2023; ModelTC, 2025; Hsu et al.,\n2024; Daniel Han and team, 2023). However, Tri\u0002ton developers must still laboriously tune critical\nparameters to exploit hardware capabilities. LLM\ncode generation poses prospects for automating\nTriton development, which calls for a systematic\nevaluation of generated operators.\n2.2 Code Benchmarks\nThe demand for proper measurement of coding ca\u0002pability arises as the program synthesis research\nadvances. The primary practice of coding bench\u0002marks is functional correctness testing, usually re\u0002alized by test case construction and sandbox exe\u0002cution. For example, HUMANEVAL (Belz et al.,\n2021) curate hand-written programs and test cases,\nand MBPP (Austin et al., 2021b) create program\u0002ming problems by crowd-sourcing. The function\u0002ality test has recently extended to automated test\ngeneration for better coverage (Liu et al., 2023)\nand broader applications, including software en\u0002gineering (Jimenez et al., 2024). Another vital\n23054\naspect of coding benchmarking is performance\nprofiling (Shypula et al.; Liu et al., 2023; Huang\net al., 2024; Qiu et al., 2024). However, most\nexisting frameworks focus on competition-style,\nsingle-process execution. While there are some\nframeworks for evaluating parallel programming\non CPUs (Nichols et al., 2024; Chaturvedi et al.,\n2024), benchmarks targeting GPU code remain\nscarce. As the deployment of deep learning models\nscales up, a comprehensive evaluation framework\nthat considers both correctness and performance on\nGPU code becomes increasingly necessary.\n2.3 LLMs for Code Generation\nLLMs have recently demonstrated impressive ca\u0002pabilities in generating code from natural lan\u0002guage instructions, as evidenced by models such\nas DeepSeek-Coder (Guo et al., 2024; Zhu et al.,\n2024) and Qwen-Coder (Hui et al., 2024), which\nhave achieved strong performance on broad coding\nbenchmarks. Despite their versatility, they often\nstruggle with Domain-Specific Languages (DSLs)\ndesigned for higher levels of abstraction and im\u0002proved efficiency in targeted contexts (W ˛asowski\nand Berger, 2023). The main reason for this sta\u0002tus is the limited availability of DSL datasets and\nbenchmarks (Cassano et al., 2024; Pujar et al.,\n2023), coupled with their unique syntax and se\u0002mantics (Pujar et al., 2023), posing significant chal\u0002lenges for LLMs (Buscemi, 2023). In this work,\nwe focus on DSLs within the high-performance\ncomputing domain where the challenges we men\u0002tioned are more pronounced for involving the par\u0002allel programming model. We introduce the first\ncomprehensive benchmark for Triton generation,\nproviding a systematic evaluation framework that\naims to guide future improvements in DSL-centric\nLLM code generation.\n3 TRITONBENCH-G\nTriton (Tille","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Faclanthology.org%2F2025.findings-acl.1183.pdf","id":"https://aclanthology.org/2025.findings-acl.1183.pdf","publishedDate":"2025-07-27T00:00:00.000Z","summary":"TRITONBENCH is a new benchmark for evaluating large language models (LLMs) in generating Triton operators. It includes 184 real-world operators from GitHub and 166 operators aligned with PyTorch interfaces. The benchmark also profiles efficiency performance on GPUs. The study reveals that current state-of-the-art code LLMs struggle to generate efficient Triton operators. TRITONBENCH is available at https://github.com/thunlp/TritonBench.\n","title":"[PDF] TRITONBENCH: Benchmarking Large Language Model Capabilities ...","url":"https://aclanthology.org/2025.findings-acl.1183.pdf"}],"status":"completed","toolCallId":"call_4fVBoDulIX3E7q4jMcviGynP","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Short answer: Very good for many DL kernels, often near cuBLAS/cuDNN on the shapes it’s tuned for, and sometimes beats them for specific cases or fusions—but it isn’t a general drop‑in replacement for NVIDIA’s full CUDA toolchain. Performance depends on kernel type, data type, GPU arch, and how well you tune the tile sizes/pipelining.\n\nWhat Triton does well as a “CUDA compiler”\n- Kernel-level productivity with high performance:\n  - Matmul/attention/convolution-style tiles: With proper BLOCK_M/N/K, num_warps, and pipelining, Triton kernels typically reach near-library throughput on CUDA cores and, on newer releases, map to tensor core paths (WMMA/WGMMA) for FP16/FP8.\n  - Elementwise/fused epilogues and memory-bound ops: Easy to fuse; large wins vs naïve PyTorch and often competitive with bespoke CUDA when memory traffic is the bottleneck.\n  - Real results reported publicly:\n    - PyTorch team’s “CUDA‑free” inference: End-to-end Llama3/Granite using only Triton kernels ran at 0.76–0.78× of CUDA stacks on H100 and 0.62–0.82× on A100 before deeper kernel tuning; their tuned FP8/FP16 GEMMs and flash-attention close much of that gap.\n    - Triton FP8 TK‑GEMM on H100: Up to 1.94× over Triton’s base matmul, and as high as 1.87× faster than cuBLAS FP8 for small‑batch attention shapes (after Split‑K and WGMMA tuning).\n    - Numerous case studies (GPTQ dequant, normalization, Split‑K GEMM) show Triton kernels matching or exceeding hand-written CUDA baselines after attention to coalescing, swizzled tiling, vectorized loads, and software pipelining.\n  - Architecture tracking: Active work with NVIDIA/AMD means day‑0 or early support for new instructions (e.g., Hopper WGMMA; Blackwell tensor cores; AMD MFMA/WMMA), so kernels using tl.dot can benefit without source changes.\n\nWhere Triton may lag or need extra work\n- Best-in-class vendor kernels still hard to beat broadly:\n  - cuBLAS/cuDNN/TensorRT-LLM include deep arch-specific tricks (TMA, threadblock clusters, graph-captured micro-schedules, kernel catalogs) that are costly to re-create. For “big square GEMM” and highly engineered convs, libraries are tough to surpass out of the box.\n- Shape sensitivity and tuning cost:\n  - You often need per-layer tuning (BLOCK_{M,N,K}, num_warps, stages, Split‑K) to hit peak. Without it, expect 0.6–0.9× of vendor kernels on key paths.\n- Launch/JIT overhead:\n  - JIT compile + Python launch overhead can dominate short kernels unless you use caching and CUDA Graphs.\n- Feature coverage:\n  - Some newest hardware features (e.g., Hopper TMA, threadblock clusters) or exotic data types/layouts may trail vendor stacks, though support keeps improving.\n\nPractical guidance\n- Use Triton when:\n  - You need custom/fused kernels (attention variants, epilogues, unusual layouts) with near‑CUDA performance.\n  - You want fast iteration in Python and auto-vectorized/coalesced codegen to PTX.\n  - You’re inside PyTorch 2.x (Inductor) or similar stacks that already generate Triton and you can swap in hand-tuned kernels for hotspots.\n- Prefer vendor libraries when:\n  - Your op matches a highly optimized primitive (classic GEMM/conv) and you need top speed immediately across many shapes and GPUs.\n- How to get “CUDA-like” performance in Triton:\n  - Tune tiles and pipeline stages; consider Split‑K for small‑M or large‑K shapes.\n  - Ensure coalesced loads/stores, swizzled tile ordering for L2 locality, vectorized memory ops, shared-memory staging, and overlap load/compute.\n  - Target tensor cores (FP16/BF16/FP8) where applicable; verify WGMMA usage via Nsight Compute.\n  - Use CUDA Graphs to amortize launch overhead; cache JIT artifacts.\n\nBottom line\n- As a CUDA-targeting compiler, Triton can generate kernels that are within a few percent of, equal to, or occasionally faster than hand-tuned CUDA/library kernels—especially for workload-specific shapes and fusions—provided you tune it. For the most polished vendor primitives across all shapes, NVIDIA libraries still set the ceiling, but the gap is narrow and shrinking on many transformer-critical kernels.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d43bf65c5c819e9f4af1450798f33909cfa59badc4836b"}},"resumableStreamId":"STREAM:8e8b3abf-0623-4072-a6df-33da2214735c","role":"assistant","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","timeToFirstToken":0.795,"tokens":997,"tokensPerSecond":27.574952981524504,"updated_at":1758739434812,"userId":"google:104458561188320708185","id":"j97asdhw70d2wde0rjsyt671jn7r7dvf"},{"_creationTime":1758739499423.6943,"_id":"j97248jg2ejq1sr70cjtkvva0h7r72j9","attachmentIds":[],"content":"how well does triton work with AMD GPUs","created_at":1758739499423,"messageId":"1684a5e9-027d-4982-8d02-f8ffe6bb58bc","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","updated_at":1758739499423,"userId":"google:104458561188320708185","id":"j97248jg2ejq1sr70cjtkvva0h7r72j9","providerMetadata":{}},{"_creationTime":1758739499423.6946,"_id":"j9798w9rf90hrprmsx47sgyzgn7r7vv8","attachmentIds":[],"byok":false,"content":"Short answer: Triton works well on modern AMD GPUs via the ROCm stack, and support has matured a lot. You can write a single Triton kernel and run it on NVIDIA or AMD; on AMD it lowers through LLVM’s AMDGPU backend to HSACO. For many DL kernels (matmul/attention/epilogues), performance is competitive when you use AMD-tuned configs and MFMA paths. The main caveats are version alignment (ROCm, PyTorch, Triton versions), and sometimes needing AMD‑specific tuning to hit peak.\n\nWhat “works” today\n- Backends: Triton targets AMD via LLVM AMDGPU; kernels JIT-compile to HSACO and run on ROCm.\n- Architectures: MI200/MI300 series are well supported; ROCm 6.x/7.0 focus on MI300(X/series) and newer.\n- Instructions/cores: Triton maps tile dots to AMD MFMA/WMMA where available; layouts like amd_mfma and amd_wmma are in Triton’s AMD dialect.\n- Tooling: PyTorch 2.x Inductor emits Triton for AMD; ROCm profilers (ROCProfiler, Compute Profiler) and docs cover Triton kernel tuning.\n- Distributed: There’s active work (e.g., Triton-Distributed on AMD) to overlap comm/compute on CDNA3.\n\nPerformance picture\n- Matmul/attention-style kernels: With proper BLOCK_M/N/K, num_warps, stages (software pipelining), and MFMA usage, Triton kernels on MI300-class GPUs can reach near-library throughput on many transformer shapes.\n- Memory-bound and fused ops: Triton makes it easy to fuse epilogues or odd layouts; these often match/best HIP/CUDA-style baselines because memory traffic dominates.\n- Autotuning matters: Peak results typically require autotuning tile sizes and sometimes Split-K; AMD provides passes like OptimizeLDSUsage, BlockPingpong, and buffer ops conversion to improve coalescing and LDS usage.\n\nPractical requirements and gotchas\n- Versions:\n  - Use a Triton build that matches your ROCm (e.g., Triton 3.3 with ROCm 7.0; check release notes).\n  - PyTorch Inductor on ROCm needs compatible PyTorch/ROCm wheels.\n- Hardware/OS: Stick to ROCm-supported SKUs/OS combos from the ROCm docs (Ubuntu 24.04/Rocky 9 for ROCm 7.0).\n- Tuning:\n  - Ensure MFMA paths are chosen (tt.dot → MFMA). Check assembly or profiler counters.\n  - Balance LDS usage vs occupancy; vectorize global loads; use pipelined stages to overlap memory/compute.\n- Feature gaps:\n  - Brand-new AMD hardware features land first in ROCm libraries; Triton usually follows but may lag briefly on cutting-edge ops or datatypes.\n  - Porting highly NVIDIA-specific kernels (e.g., relying on TMA/cluster features) may need algorithmic tweaks on AMD.\n\nIf you’re deciding whether to use Triton on AMD\n- Yes, use it when you want custom/fused GPU kernels in PyTorch or standalone with competitive performance on MI300+.\n- For “classic” GEMM/conv where rocBLAS/MIOpen already shine for your shapes, start with the libraries; use Triton for fusions or nonstandard layouts.\n- Bake in an autotune step per workload and cache the best configs; profile with ROCm tools to confirm MFMA utilization and coalesced accesses.\n\nReferences to explore\n- AMD ROCm docs: Triton kernel optimization guides (autotuning, MLIR/ISA analysis, occupancy).\n- AMD blog posts on “Unlock Peak Performance on AMD GPUs with Triton” and “Triton-Distributed on AMD.”\n- Triton GitHub: third_party/amd backend, layouts (amd_mfma), and tuning examples.","created_at":1758739499424,"messageId":"a4581826-d2e0-4c8a-802b-221e7ad7da49","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"news","domains":null,"queries":["Triton AMD GPUs ROCm support status 2025","PyTorch Inductor Triton AMD 2025 H100 MI300 performance triton","OpenAI Triton AMD backend ROCm documentation 2025","Triton language AMDGPU LLVM backend status 2024 2025"]},"result":[{"content":"[Skip to main content](https://rocm.blogs.amd.com/rocm.blogs.amd.com#main-content)\nBack to top\n\n`Ctrl` + `K`\n\n[ROCm blogs](https://rocm.blogs.amd.com/)\n\n# Unlock Peak Performance on AMD GPUs with Triton Kernel Optimizations\n\n## Contents\n\n# Unlock Peak Performance on AMD GPUs with Triton Kernel Optimizations [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#unlock-peak-performance-on-amd-gpus-with-triton-kernel-optimizations)\n\nApril 10, 2025 by [Ning Zhang](https://rocm.blogs.amd.com/authors/ning-zhang.html), [George Wang](https://rocm.blogs.amd.com/authors/george-wang.html).\n\n4 min read. \\| 1007 total words.\n\n[Software tools & optimizations](https://rocm.blogs.amd.com/blog/category/software-tools-optimizations.html)\n\n[AI/ML](https://rocm.blogs.amd.com/blog/tag/aiml.html)\n\n[AI](https://rocm.blogs.amd.com/ai.html)\n\n[Triton](https://github.com/triton-lang/triton) is a domain-specific programming language designed to simplify GPU programming for high-performance tasks, particularly in AI applications. It provides an open-source environment that enables users to write high-level Triton code with greater productivity compared to Nvidia CUDA or AMD HIP. The Triton compiler translates Triton code into optimized GPUs instructions, effectively compiling tensor operations into low-level GPU code. It achieves high efficiency through multiple optimizations passes and leverages the underlying architecture of the GPU. To optimize GPU performance, it is important to have a solid understanding of the Triton compiler and the role it plays in kernel performance. In this blog, we will deep dive into the AMD Triton compiler, introduce Triton kernel compilation, and provide insights on how to create an efficient Triton kernel code.\n\n## AMD Triton Compilation Flow [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#amd-triton-compilation-flow)\n\nTriton compiler includes 3 key modules, Frontend, Optimizer, and Backend machine code generation, as shown in Figure 1:\n\nFigure 1. Triton Complier Block Diagram [#](https://rocm.blogs.amd.com/rocm.blogs.amd.com#id1)\n\nFrontend module walks the abstract syntax tree (AST) of python Triton kernel function, which includes the @triton.jit decorator to create the Triton Intermediate Representation (Triton-IR). The Optimizer module applies multiple optimizations and converts the Triton-IR into Triton-GPU IR (Triton-TTGIR) and then LLVM-IR. At this stage, optimization includes two core components: layout and passes, which are hardware dependent.\n\n1. When generating Triton GPU IR, the hardware compute capability is included along with layout. Layout represents memory optimizations details on how the tensors are distributed to cores and warps for AMD GPUs on wavefronts. Currently, Triton supports multiple layouts like blocked (each warp owns a contiguous portion of the tensor), slice (restructures and distributes a tensor along a dimension), dot\\_op (optimized layout for block matrix product), shared (indicates GPU shared memory, amd\\_mfma (for AMD MFMA matrix core), amd\\_wmma (for AMD WMMA matrix core), and linear layout (to unify layouts within and across backends).\n\n2. Various optimization passes are applied to the IRs. These optimizations include loop unrolling, memory access optimizations, constant folding, CSE, etc. to improve the performance of the generated codes. Optimization passes can be divided into 3 categories:\n\n1. MLIR general optimization passes, like CSE, DCE, Inlining, etc.\n\n2. GPU specific optimization passes, like Pipeline, Prefetch, Matmul accelerate, Coalesce, etc.\n\n3. Vender specific GPU optimization passes, for example Nvidia provides TMA, Async Dot, etc. AMD provides OptimizeLDSUsage, BlockPingpong, etc.\n\nThe final stages of the Triton compilation process lower LLVM-IR to a device specific binary, which is done by backend machine code generation modules. Triton leverages LLVM to perform further optimizations and facilitate the generation of machine code. On AMD platforms, the AMD LLVM compiler module first translates the LLVM-IR to AMDGCN assembly codes then AMDGCN codes compile into a hsaco binary, which uses AMD JIT compiler.\n\n## AMD Triton Compilation Flow Example [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#amd-triton-compilation-flow-example)\n\nLet’s walk the triton compilation flow implementation on AMD GPUs using an example of [Vector Add](https://github.com/triton-lang/triton/tree/main/python/tutorials)\n\n1. Frontend module of AMD Triton Compiler\n\n\nIn the frontend module codes of Triton compiler, the Trition Kernel function converts into Triton-IR. In this example, Triton kernel function is add\\_kernel, which is marked by @triton.jit decorator. [JIT decorator function](https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L782) first checks the TRITON\\_INTERPRET environment variable , if True, InterpretedFunction will be invoked, which is used to run Triton kernel in Interpreter mode; otherwise, it will run into JITFunction, where we will compile and run Triton kernel on real device.\n\nThe kernel compilation entry point is [Triton compile function](https://github.com/triton-lang/triton/blob/main/python/triton/compiler/compiler.py#L214), which is invoked with the target device and compilation options information. It will create the kernel cache manager, launch the compilation pipeline, and populate the kernel meta data. It also loads the backend specific dialects, e.g. TritonAMDGPUDialect on the AMD platform, and backend specific LLVM modules to handle LLVM-IR compilation. When all are ready, it will invoke ast\\_to\\_ttir function to generate Triton-IR file of this kernel.\n\n1. Optimizer module of AMD Triton Compiler\n\n\n```\n i. Triton-IR optimization\n\n```\n\nOn AMD platforms, Triton-IR optimization passes are defined in [make\\_ttir function](https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py.).The optimization passes at this stage are hardware independent, like inline optimization,\nCSE(Common subexpression elimination), Canonicalization, DCE(Dead code elimination), LICM(Loop Invariant Code Motion), and Loop Unrolling.\n\n```\n ii. Triton-GPU IR optimization\n\n```\n\nOn AMD platforms, Triton-IR optimization passes are defined in make\\_ttgir function , and are designed for GPU platform to boost its performance. Based on AMD GPU hardware features and kernel optimization experience, AMD also developed some AMD GPU specific optimization passes for this stage, which are listed in table 1, below.\n\n| TTGIR AMD Specific Optimization Pass | Description |\n| --- | --- |\n| AMD GPU accelerate Matmul | Optimize the input/output layout of \\`dot\\` instruction to make them compatible AMD matrix cores |\n| AMD GPU Optimize Epilogue | Store accumulators directly without going through SMEM in epilogue. |\n| AMD GPU Stream Pipeline | Pipeline global loads through registers to shared memory while computing on previous tile |\n| AMD GPU insert instruction sched hints | Insert instruction scheduling hints after the dot ops in the main loop |\n| AMD GPU Reorder Instructions | This pass reorder instructions, so as to (1) decrease register pressure (e.g., by moving conversions from shared memory before their first use) and (2) promote LLVM instruction order more friendly to AMDGCN assembly generation. |\n| AMD GPU Block Ping pong | This pass reorder instructions to interleave instructions from two warps on the same SIMD unit. We call this a ping-pong scheduling pattern, where two warps run concurrently in the synchronized fashion.This block ping-pong pattern could be beneficial under few conditions including occupancy and number of warps |\n| AMD GPU Canonicalize Pointers | Canonicalize pointers means to rewrite pointers passed to load/store operation as a \\` \\` pair.This pass pushes all the constant pointer arithmetic on a scalar basePtr, while all the vector pointer arithmetic to a vector offset.when we load the data, we splat the scalar pointer, add the “variable” offset and then issue the load |\n| AMD GPU Convert To Buffer Ops | This pass converts memory and atomic operations (e.g., tt.load/tt.store/tt.atomic\\_rmw) to amdgpu buffer operations, if possible |\n\nTable 1. AMD GPU-IR specific optimization\n\nThese optimization passes first convert Triton-IR to Triton-GPU IR. At this step, layout information is added into the IR. In this example, the tensors are represented as a #blocked layout.\n\nIf we try another [Triton Matrix Multiplication](https://github.com/ROCm/triton/blob/main_perf/python/tutorials/03-matrix-multiplication.py) example, the shared memory access is introduced by the above optimization passes to boost performance, which is a popular optimization solution for Matrix Multiplication, and amd\\_mfma layout is also used, which is designed for AMD MFMA accelerator.\n\n```\n iii. LLVM-IR optimization\n\n```\n\nOn AMD platform, optimization passes are defined in make\\_llir function. This function includes two components: IR level optimization and AMD GPU LLVM compiler configuration.\nFor IR-level optimization, AMD GPU specific optimization passes are included, like LDS/shared memory related optimization, and LLVM-IR level generic optimization, as shown in table 2.\n\n| LLVM-IR AMD Specific Optimization Pass | Description |\n| --- | --- |\n| Decompose Unsupported AMD Conversions | Decompose conversions that are not supported by AMD GPU during the step of converting TritonGPU-IR to LLVM-IR. |\n| Optimize AMD lds/shared memory usage | Minimize LDS usage: find operations with peak LDS consumption , and then try to transform candidate operations to fit them into LDS |\n| AMD GPU Lower Instruction Sched Hints | Lower instruction scheduling hints to LLVM intrinsics |\n| AMD GPU Convert Builtin Func To LLVM | Convert the Builtin Functions to LLVM |\n\nTable 2. LLVM-IR AMD Specific Optimization Pass\n\nFor AMD GPU LLVM compiler configuration, the LLVM target library and context are initialized, sets various control constants on the LLVM module, sets calling convention for A","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Frocm.blogs.amd.com%2Fsoftware-tools-optimization%2Fkernel-development-optimizations-with-triton-on-%2FREADME.html","id":"https://rocm.blogs.amd.com/software-tools-optimization/kernel-development-optimizations-with-triton-on-/README.html","publishedDate":"2025-04-10T00:00:00.000Z","summary":"This blog post discusses how to optimize AMD GPU performance using the Triton programming language. The Triton compiler, which translates high-level Triton code into optimized GPU instructions, is key to achieving high efficiency. The compilation flow includes a Frontend, Optimizer, and Backend. The Optimizer module applies various optimizations, including layout and passes, which are hardware-dependent. Layouts optimize memory distribution, while passes include MLIR general, GPU-specific, and vendor-specific optimizations. The final stage uses LLVM to generate device-specific machine code.\n","title":"Unlock Peak Performance on AMD GPUs with Triton Kernel Optimizations","url":"https://rocm.blogs.amd.com/software-tools-optimization/kernel-development-optimizations-with-triton-on-/README.html"},{"content":"[Skip to main content](https://rocm.blogs.amd.com/rocm.blogs.amd.com#main-content)\nBack to top\n\n`Ctrl` + `K`\n\n[ROCm blogs](https://rocm.blogs.amd.com/)\n\n# Unleash Full GPU Potential: Overlap Communication and Computation with Triton-Distributed\n\n## Contents\n\n# Unleash Full GPU Potential: Overlap Communication and Computation with Triton-Distributed [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#unleash-full-gpu-potential-overlap-communication-and-computation-with-triton-distributed)\n\nMay 06, 2025 by Lei Zhang, [George Wang](https://rocm.blogs.amd.com/authors/george-wang.html), [Fan Wu](https://rocm.blogs.amd.com/authors/fan-wu.html), Peng Sun, Kyle Wang, [Anshul Gupta](https://rocm.blogs.amd.com/authors/anshul-gupta.html).\n\n2 min read. \\| 516 total words.\n\n[Software tools & optimizations](https://rocm.blogs.amd.com/blog/category/software-tools-optimizations.html)\n\n[Compiler](https://rocm.blogs.amd.com/blog/tag/compiler.html)\n\n[Developers](https://rocm.blogs.amd.com/developers.html)\n\nIn distributed computing, AI workloads demand both massive parallelism\nand efficient data movement. A primary challenge lies in effectively\noverlapping computation with communication to maximize performance. GPUs\nare excellent at crunching numbers. However, their full potential often\nremains untapped due to relatively long inter-GPU communication. This\nresults in their computing units staying idle for large amounts of time\nwhile waiting for data transfer from other nodes. In this blog, we will\nshow how you can use the Triton-Distributed framework to generate\nkernels that overlap communication and computation, resulting in\nperformance that can rival highly optimized libraries.\n\n## What is the Triton-Distributed Compiler? [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#what-is-the-triton-distributed-compiler)\n\nTriton-Distributed is a novel extension of the OpenAI Triton framework\nthat enables concurrent execution of computation and cross-GPU data\ntransfers. It aims to transform how developers tackle the fundamental\ntrade-off between performance and compiler-induced latency overhead. Triton-Distributed\nempowers developers to write kernels that automatically optimize both\nlocal computations and cross-device communication patterns. For example,\na single Triton-Distributed kernel can dynamically adjust memory access\npatterns to hide latency during operations like all-reduced.\nTriton-Distributed Compiler inherits the strengths of OpenAI Triton, a\ncompiler renowned for simplifying GPU programming. In CUDA/HIP, crafting\noptimized kernels requires deep expertise in hardware microarchitecture\nand low-level programming, which requires intricate hand-tuning\nlow-level code (error-prone and time-consuming) or rely on rigid\nlibraries that limit flexibility. Triton changes this equation and\noffers a higher level of abstraction while maintaining flexibility,\nefficiency and productivity.\n\nBy encapsulating complex optimizations, such as shared-memory\nmanagement, tensor/matrix core usage, and warp level parallelism into\nreusable compiler passes, Triton-Distributed lets developers focus on algorithmic\ninnovation rather than low level architectural specifics. Imagine\nwriting a matrix multiplication kernel once and achieving performance\nrivaling hand-tuned libraries for multiple input configurations across\nmultiple GPU nodes. Triton-Distributed aims to deliver: efficient\nkernels comparable to highly-optimized libraries\nincluding [Distributed-GEMM](https://github.com/NVIDIA/cutlass/tree/main/examples/65_distributed_gemm),\ncuBLASMp and [FLUX](https://github.com/bytedance/flux/blob/main/README.md).\nWith its high-level abstractions and productivity-focused design, Triton-Distributed brings performance portability to a broader audience, abstracting away the complexity of distributed GPU programming into an accessible, extensible compiler stack.\nLet’s explore how to get started with Triton-Distributed.\n\n## Getting Started For Triton-Distributed [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#getting-started-for-triton-distributed)\n\nBelow are the steps to install and use Triton-Distributed on AMD GPUs.\n\n**Step 1**. Install Triton-Distributed from source:\n\nRefer to the [Build\\\nGuide](https://github.com/ByteDance-Seed/Triton-distributed/blob/main/docs/distributed/build.md)\nfor best practices on compiling and configuring Triton-Distributed for AMD GPUs.\n\n**Step 2** How to use Triton-Distributed\n\nTriton-Distributed provides a set of ease-of-use primitives to support\nthe development of distributed compute-communication overlapping\nkernels. All the primitives are exposed by triton.distributed.language,\nwhich are additional features that add-on to the main Triton features,\nmaking Triton totally composable while not modifying core Triton\nThe primitives are grouped into low-level and high-level sets. While the high-level primitives (as described in the [MLSys 2025 paper](https://mlsys.org/virtual/2025/poster/3248)) will be released in the future, the current release includes the following low-level primitives:\n\nLow-level primitives - Context Querying Primitives\n\n```\nrank(axis=-1,_builder=None)\nnum_ranks(axis=-1,_builder=None)\nsymm_at(ptr,rank,_builder=None)\n```\n\nLow-level primitives - Singal Control Primitives\n\n```\nwait(barrierPtrs,numBarriers,scope:str,semantic:str,_builder=None)\nconsume_token(value,token,_builder=None)\nnotify(ptr,rank,signal=1,sig_op=\"set\",comm_scope=\"inter_node\",_builder=None)\n```\n\nUsers can combine the [communication part with computation\\\npart](https://github.com/ByteDance-Seed/Triton-distributed/tree/main/third_party/distributed/distributed/kernels)\nto design overlapping kernels.\n\n## Test and Reproduce Performance with Triton-Distributed on AMD CDNA3 [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#test-and-reproduce-performance-with-triton-distributed-on-amd-cdna3)\n\nBelow are the steps to test and reproduce the performance with\nTriton-Distributed on AMD CDNA3 for GEMM ReduceScatter on single node.\n\n### GEMM ReduceScatter on Single Node [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#gemm-reducescatter-on-single-node)\n\nBelow is the script for testing and reproducing the performance of ReduceScatter GEMM on a Single Node:\n\n```\n#!/bin/bash\nset-e\nSHAPES=(\n\"8192 4096 12288\"\n\"8192 4096 14336\"\n\"8192 3584 14336\"\n\"8192 4608 36864\"\n\"8192 8192 28672\"\n\"8192 8192 30720\"\n)\nforshapein\"${SHAPES[@]}\";do\nread-rmnk<<<\"$shape\"\necho\"Testing GEMM_RS shape: m=$m, n=$n, k=$k\"\nbash./third_party/distributed/launch_amd.sh./third_party/distributed/distributed/test/amd/test_gemm_rs_intra_node.py${m}${n}${k}--warmup5--iters20\ndone\n```\n\nFigure 1 below shows the performance boost when using Triton distribution vs PyTorch+RCCL. On tuned cases, we see a 30% better with this approach.\n\nFigure 1: Performance of Intra-node GEMM ReduceScatter on AMD GPUs [^1](https://rocm.blogs.amd.com/rocm.blogs.amd.com#endnotes) [#](https://rocm.blogs.amd.com/rocm.blogs.amd.com#id1)\n\n### AllGather GEMM on Single Node [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#allgather-gemm-on-single-node)\n\nBelow is the script for testing and reproducing the performance of AllGather GEMM on a Single Node\n\n```\n#!/bin/bash\nset-e\nSHAPES=(\n\"8192 4096 12288\"\n\"8192 4096 14336\"\n\"8192 3584 14336\"\n\"8192 4608 36864\"\n\"8192 8192 28672\"\n\"8192 8192 30720\"\n)\nforshapein\"${SHAPES[@]}\";do\nread-rmnk<<<\"$shape\"\necho\"Testing AG_GEMM shape: m=$m, n=$n, k=$k\"\nbash./third_party/distributed/launch_amd.sh./third_party/distributed/distributed/test/amd/test_ag_gemm_intra_node.py${m}${n}${k}--warmup5--iters20\ndone\n```\n\nFigure 2 below shows the performance boost when using Triton distribution vs PyTorch+RCCL. On tuned cases, we see a 30 -40% better with this approach.\n\nFigure 2: Performance of Intra-node AllGather GEMM on AMD GPUs [^2](https://rocm.blogs.amd.com/rocm.blogs.amd.com#endnotes) [#](https://rocm.blogs.amd.com/rocm.blogs.amd.com#id2)\n\n## Summary [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#summary)\n\nTriton-Distributed is a compiler built on OpenAI’s Triton framework and\nrepresents a breakthrough in distributed GPU computing. It addresses one of the most pressing challenges in AI workloads—efficiently balancing computation with inter-GPU communication.\nBy enabling the overlap of computation and communication,\nTriton-Distributed allows kernels to continue processing while data is transferred, thereby maximizing GPU utilization, reducing idle cycles, and improving system-wide throughput.\nTriton-Distributed inherits Triton’s high-level abstractions, eliminating the need for deep hardware expertise or rigid libraries.\nIt enables the creation of portable, high-performance distributed kernels—like Distributed-GEMM—that rival hand-optimized CUDA implementations.\nBy encapsulating complex\noptimizations such as shared memory management and warp-level\nparallelism into compiler passes, it empowers developers to focus on\nalgorithmic design rather than low-level details. Triton-Distributed\nbrings these capabilities to AMD GPUs, broadening access and performance\nportability across hardware platforms for developers. It provides a\ncutting-edge solution that enables developers to unlock the full\npotential of AMD GPU hardware, push the boundaries of distributed performance, and\naccelerate innovation in AI systems.\n\n## Acknowledgement [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#acknowledgement)\n\nTriton-Distributed Compiler on AMD GPUs is the result of a close collaboration\nbetween the AMD ROCm software engineering team and the\n[ByteDance Seed Team](https://team.doubao.com/) ; we sincerely\nappreciate the great efforts and contributions of everyone involved in\nmaking this collaboration a success. We also greatly appreciate OpenAI’s\nopen-source Triton project, which this effort is based upon.\n\n## Additional Resource [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#additional-resource)\n\n[Triton-Distributed on AMD GPUs platform](https://github.com/ByteDance-Seed/Triton-distributed/tree/main/third_party/amd)\n\n[Byt","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Frocm.blogs.amd.com%2Fsoftware-tools-optimization%2Ftriton-distributed-c%2FREADME.html","id":"https://rocm.blogs.amd.com/software-tools-optimization/triton-distributed-c/README.html","publishedDate":"2025-05-06T00:00:00.000Z","summary":"This blog post introduces Triton-Distributed, a framework that enables concurrent execution of computation and cross-GPU data transfers to maximize performance in distributed computing for AI workloads. It's an extension of the OpenAI Triton framework, designed to help developers write kernels that automatically optimize both local computations and cross-device communication. Triton-Distributed aims to deliver efficient kernels comparable to highly-optimized libraries, abstracting away the complexity of distributed GPU programming into an accessible, extensible compiler stack.\n","title":"Unleash Full GPU Potential: Overlap Communication and Computation with Triton-Distributed","url":"https://rocm.blogs.amd.com/software-tools-optimization/triton-distributed-c/README.html"},{"content":"[About](https://www.youtube.com/about/) [Press](https://www.youtube.com/about/press/) [Copyright](https://www.youtube.com/about/copyright/) [Contact us](https://www.youtube.com/t/contact_us/) [Creators](https://www.youtube.com/creators/) [Advertise](https://www.youtube.com/ads/) [Developers](https://developers.google.com/youtube) [Cancel Memberships](https://support.google.com/youtube/contact/de_cancellation?hl=fr) [Terms](https://www.youtube.com/t/terms) [Privacy](https://www.youtube.com/t/privacy) [Policy & Safety](https://www.youtube.com/about/policies/) [How YouTube works](https://www.youtube.com/howyoutubeworks?utm_campaign=ytgen&utm_source=ythp&utm_medium=LeftNav&utm_content=txt&u=https%3A%2F%2Fwww.youtube.com%2Fhowyoutubeworks%3Futm_source%3Dythp%26utm_medium%3DLeftNav%26utm_campaign%3Dytgen) [Test new features](https://www.youtube.com/new)\n\n© 2025 Google LLC","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DrgKbX4xi8IY","id":"https://www.youtube.com/watch?v=rgKbX4xi8IY","publishedDate":"2025-05-09T00:00:00.000Z","summary":"This webpage is a YouTube page with links to various sections like \"About\", \"Press\", \"Copyright\", \"Contact us\", \"Creators\", \"Advertise\", \"Developers\", \"Cancel Memberships\", \"Terms\", \"Privacy\", \"Policy & Safety\", \"How YouTube works\", and \"Test new features\". It also includes a copyright notice from Google LLC.\n","title":"Triton on AMD GPUs","url":"https://www.youtube.com/watch?v=rgKbX4xi8IY"},{"content":"[Skip to main content](https://rocm.blogs.amd.com/rocm.blogs.amd.com#main-content)\nBack to top\n\n`Ctrl` + `K`\n\n[ROCm blogs](https://rocm.blogs.amd.com/)\n\n# Triton Inference Server with vLLM on AMD GPUs\n\n## Contents\n\n# Triton Inference Server with vLLM on AMD GPUs [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#triton-inference-server-with-vllm-on-amd-gpus)\n\nJanuary 08, 2025 by [Fabricio Flores](https://rocm.blogs.amd.com/authors/fabricio-flores.html), [Tiffany Mintz](https://rocm.blogs.amd.com/authors/tiffany-mintz.html), [Eliot Li](https://rocm.blogs.amd.com/authors/eliot-li.html), [Yao Liu](https://rocm.blogs.amd.com/authors/yao-liu.html), Ted Themistokleous, Brian Pickrell, [Vish Vadlamani](https://rocm.blogs.amd.com/authors/vish-vadlamani.html).\n\n5 min read. \\| 1130 total words.\n\n[Applications & models](https://rocm.blogs.amd.com/blog/category/applications-models.html)\n\n[LLM](https://rocm.blogs.amd.com/blog/tag/llm.html), [AI/ML](https://rocm.blogs.amd.com/blog/tag/aiml.html)\n\n[AI](https://rocm.blogs.amd.com/ai.html)\n\nTriton Inference Server is an open-source platform designed to streamline AI inferencing. It supports the deployment, scaling, and inference of trained AI models from various machine learning and deep learning frameworks including Tensorflow, PyTorch, and vLLM, making it adaptable for diverse AI workloads. It is designed to work across multiple environments, including cloud, data centers and edge devices.\n\nSome of the Triton Inference Server capabilities include:\n\n- **Framework flexibility**: Allows for the deployment of models from different frameworks (see [Triton Inference Server Backend](https://github.com/triton-inference-server/backend)) regardless of underlying infrastructure. This flexibility allows for running multiple models or multiple instances of the same model on the same hardware, improving resource utilization.\n\n- **Hardware and deployment versatility**: It is optimized for both GPU and CPU-based environments, which allows for its deployment on a variety of hardware. Triton Inference Server can be used in the cloud, data centers or on edge, making it highly versatile.\n\n- **Performance optimization**: It enhances inference performance through dynamic batching, which aggregates smaller inference requests to optimize processing and enable concurrent model execution. This capability allows multiple models to run simultaneously, making it crucial for real-time applications that require minimal latency.\n\n\nIn this blog we will show you, step-by-step, how to set up a Triton Inference Server with [vLLM](https://docs.vllm.ai/en/latest/) backend on AMD GPUs, using ROCm. We start by briefly introducing some of the key aspects of using [vLLM](https://docs.vllm.ai/en/latest/) as a backend for the Triton Inference Server. We then present a detailed how-to guide showing you how to set up the Triton Inference Server with vLLM backend, with inference testing performed on 3 LLMs: `microsoft/phi-2`, `mistral-7b-instruct` and `meta-llama/Meta-Llama-3-8B-Instruct.`\n\n## Requirements [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#requirements)\n\n- AMD GPU: See the [ROCm documentation page](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html) for supported hardware and operating systems. This blog was tested on a machine with 8 AMD Instinct MI210 GPUs.\n\n- ROCm 6.1+: See the [ROCm installation for Linux](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/index.html) for installation instructions.\n\n- Docker: See [Install Docker Engine on Ubuntu](https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository) for installation instructions.\n\n- Hugging Face Access Token: This blog requires a [Hugging Face](https://huggingface.co/) account with a newly generated [User Access Token](https://huggingface.co/docs/hub/security-tokens).\n\n- Access to the `mistral` and `Llama-3` models on Hugging Face. These are [gated models](https://huggingface.co/docs/hub/models-gated) on Hugging Face, to request access see [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) and [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).\n\n\nYou can find the files related to this blog post in this\n[GitHub folder](https://github.com/ROCm/rocm-blogs/tree/release/blogs/artificial-intelligence/triton_server_vllm).\n\n## Triton Inference Server: vLLM Backend [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#triton-inference-server-vllm-backend)\n\nA Triton Inference Server backend refers to the component responsible for executing the AI model during inference. A backend is a wrapper around a specific machine learning framework such as [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), [vLLM](https://docs.vllm.ai/en/latest/), or others. Each backend is implemented as a shared library and models can be configured to use specific backends. For example, if a model is using PyTorch, the backend will be configured to interact with PyTorch libraries.\n\nThe Triton Inference Server project provides a set of supported backends that are tested and updated with each release. For a list of the supported backends, see [Where can I find all the backends that are available for Triton?](https://github.com/triton-inference-server/backend#where-can-i-find-all-the-backends-that-are-available-for-triton). This blog focuses on the [vLLM](https://docs.vllm.ai/en/latest/) backend.\n\nUsing vLLM as a backend enables serving of large language models (LLMs) with features designed for high throughput and low latency. vLLM is a specialized engine optimized for handling LLM inference, particularly in scenarios where continuous batching and memory efficiency are critical.\n\nThese are some of the key aspects of vLLM with Triton Inference Server:\n\n- **vLLM integration**: vLLM is integrated into Triton Inference Server starting from the 23.10 release. It can be used through pre-built containers that include the vLLM backend or by building a custom container. This integration allows serving models like Facebook’s OPT series, LLaMA models, and others through Triton Inference Server’s flexible and scalable architecture.\n\n- **Configuration and deployment**: When setting up vLLM as a backend, it is necessary to configure the model repository. This repository includes the `model.json` and `config.pbtxt` files. These configurations define model parameters such as memory utilization, batch sizes, and model-specific settings.\n\n- **Performance features**: vLLM’s backend in Triton Inference Server supports asynchronous inference, which is crucial for tasks like large-scale text generation and processing. Features such as tensor parallelism and paged attention enhance multi-GPU performance, making vLLM suitable for handling large models across distributed systems.\n\n- **Deployment options**: vLLM-backend models can be deployed on various platforms, including cloud environments. The containerized deployment ensures that models can be scaled horizontally based on performance demands, with support for Kubernetes and other orchestration systems.\n\n\nUsing vLLM as the backend for Triton Inference Server provides access to a highly optimized serving engine tailored to the specific demands of LLMs, while also leveraging Triton Inference server’s robust infrastructure for scalable inference.\n\n## Setting up Triton Inference Server with vLLM backend [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#setting-up-triton-inference-server-with-vllm-backend)\n\nTo perform inference with large language models using the Triton Inference Server and vLLM backend, follow these steps:\n\n- **Set up Triton Inference Server with vLLM backend**: We are configuring a `docker compose` file that includes a Triton Inference Server container with the vLLM backend. The `docker compose` refers to a Docker image with Triton Inference Server pre-installed (the Docker image can be built from source or pulled from a registry), defines GPU access, sets the repository path, and exposes the necessary ports.\n\n- **Prepare the model repository**: A model repository is a directory or set of directories that contain the models that will be served for inference. Each model is organized in a specific structure within the repository. This structure is scanned and loaded every time Triton Inference Server starts.\n\nThe structure of the model repository is as follows:\n\n\n\n```\nmodel_repository/\n ├── /\n │ ├── config.pbtxt # Configuration file that describes the model\n │ ├── 1/ # Version directory (Triton Inference Server supports versioning)\n │ │ └── model.onnx # The actual model file (e.g., ONNX, PyTorch, vLLM)\n │ └── 2/\n │ └── model.onnx\n ├── /\n │ ├── config.pbtxt\n │ ├── 1/\n │ │ └── model.json\n │ └── 2/\n │ └── model.json\n\n```\n\n\n\nThe `model_repository` is the root directory that contains one or more subdirectories, each representing a model. Each model is organized into a **Model directory** ( ` `), where the directory name corresponds to the model’s name. Within the model directory, there are **Version directories** ( `1/`, `2/`) allowing for multiple versions of the same model to exist. Each version directory contains the actual model files. These files enable Triton Inference Server to identify and serve the correct version. The **Model file** ( `model.onnx`, `model.json`, and so on) stores the model architecture and inference parameters. Finally, **Configuration file** ( `config.pbtxt`) defines the input and output tensor names, shapes, data types, and other configurations.\n\n- **Define the model configuration and model files**: When using the vLLM backend, the model configuration file must specify the backend type in addition to the data types and shapes. A simplified version of a `config.pbtxt` would look like:\n\n\n\n```\nbackend:\"vllm\"\n\ninput[\n{\nname:\"text_input\"\ndata_type:TYPE_STRING\ndims:[1]\n}\n]\n\noutput[\n{\nname:\"text_output\"\ndata_type:TYPE_STRING\ndims:[-1]\n}","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Frocm.blogs.amd.com%2Fartificial-intelligence%2Ftriton_server_vllm%2FREADME.html","id":"https://rocm.blogs.amd.com/artificial-intelligence/triton_server_vllm/README.html","publishedDate":"2025-01-08T00:00:00.000Z","summary":"This blog post details how to set up a Triton Inference Server with a vLLM backend on AMD GPUs using ROCm. It covers the key aspects of using vLLM with Triton, and provides a step-by-step guide, including inference testing on `microsoft/phi-2`, `mistral-7b-instruct`, and `meta-llama/Meta-Llama-3-8B-Instruct` LLMs. The post outlines the requirements, including an AMD GPU, ROCm 6.1+, Docker, and a Hugging Face access token.\n","title":"Triton Inference Server with vLLM on AMD GPUs — ROCm Blogs","url":"https://rocm.blogs.amd.com/artificial-intelligence/triton_server_vllm/README.html"},{"content":"[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F912d96e68e45&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40nzhangnju%2Fa-deep-dive-into-amd-triton-compilation-912d96e68e45&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40nzhangnju%2Fa-deep-dive-into-amd-triton-compilation-912d96e68e45&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# A Deep Dive Into AMD Triton Compilation\n\n[Nzhangnju](https://medium.com/@nzhangnju?source=post_page---byline--912d96e68e45---------------------------------------)\n\n7 min read\n\n·\n\nApr 13, 2025\n\n--\n\nListen\n\nShare\n\n**Overview**\n\nTriton is a domain-specific programming language designed to simplify GPU programming for high-performance tasks, particularly for AI. Triton provides an open-source environment for user to write high-level Triton code which with higher productivity than Nvidia CUDA or AMD HIP or DSLs.\n\nTriton compiler translates Triton code into GPUs instructions, it compiles high-level tensor operations into low-level GPUs code with high efficiency by using multiple optimization passes and leveraging the underlying architecture of the GPUs.\n\nAMD GPUs support Triton, in this blog [https://rocm.blogs.amd.com/artificial-intelligence/triton/README.html](https://rocm.blogs.amd.com/artificial-intelligence/triton/README.html) talks about a good example of how Triton kernel running on AMD GPUs.\n\nSince Triton is a compiler-based GPU kernel solution, compiler also plays a key role in Triton kernel performance, have a good understanding of the Triton compiler is important to optimize the performance. AMD open sourced the upstream support in OpenAI Triton repo [https://github.com/triton-lang/triton/tree/main/third\\_party/amd](https://github.com/triton-lang/triton/tree/main/third_party/amd) , including the AMD specific optimization passes and layouts.\n\nIn this blog, we will deep dive into AMD Triton compiler, introduce the details of Triton kernel compilation, which helps developers to understand AMD Triton compiler better and develop more efficient Triton kernel codes.\n\n**AMD Triton compilation flow**\n\nSimilar with other AI compilers, Triton compiler includes 3 key modules: frontend, optimizer and backend machine code generation.\n\nFrontend module walks the abstract syntax tree (AST) of python Triton kernel function, which includes the @triton.jit decorator, to create the Triton Intermediate Representation (Triton-IR). Triton-IR is an unoptimized, machine independent intermediate representation, which is based on tile-level programming design and leverage multiple MLIR dialects, like Arith, Math, ControlFlow and StructuredControlFlow dialects.\n\nOptimizer module applies multiple optimizations and converts the Triton-IR into Triton-GPU IR (Triton-TTGIR) and then LLVM-IR. At this stage, optimization includes two core components: layout and passes, which are hardware dependent.\n\n1. When generating Triton GPU IR, the hardware compute capability is included along with layout, the memory optimizations details on how the tensors are distributed to cores and warps or for AMD GPUs on wavefronts. Currently, Triton supports multiple layouts ,like blocked (each warp owns a contiguous portion of the tensor), slice (restructures and distributes a tensor along a dimension), dot\\_op(optimized layout for block matrix product), shared (indicates GPU shared memory,amd\\_mfma (for AMD MFMA matrix core), amd\\_wmma (for AMD WMMA matrix core), and linear layout (to unify layouts within and across backends).\n2. Various optimization passes are applied to the IRs. These optimizations include loop unrolling, memory access optimizations, constant folding, CSE, etc. to improve the performance of the generated codes. Optimization Passes can be divided into 3 categories:\n\n- MLIR general optimization passes, like CSE, DCE, Inlining, etc.\n- _GPU_ specific optimization passes, like Pipeline, Prefetch, Matmul accelerate, Coalesce, etc.\n- Vender specific GPU optimization passes, for example Nvidia provides TMA, Async Dot, etc. AMD provides OptimizeLDSUsage, BlockPingpong, etc.\n\nThe final stages of the Triton compilation process lower LLVM-IR to a device specific binary, which is done by backend machine code generation module. Triton leverages LLVM to perform further optimizations and to facilitate the generation of machine code. On AMD platform, AMD LLVM compiler module first translate the LLVM IR to AMDGCN assembly codes. AMDGCN codes compiled into a hsaco binary, which uses AMD JIT compiler.\n\nThe whole flow as shown in below diagram.\n\n**AMD Triton Compilation example**\n\nWe take Vector Add [https://github.com/triton-lang/triton/tree/main/python/tutorials](https://github.com/triton-lang/triton/tree/main/python/tutorials) as the example to introduce the compilation details on AMD GPUs.\n\n1. Frontend module of AMD Triton Compiler\n\nLet’s first walk through the frontend module codes of Triton compiler, which converted the Trition Kernel function into Triton IR.\n\nIn this example, Triton kernel function is add\\_kernel, which is marked by @triton.jit decorator. JIT decorator function [https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L782](https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L782) , first checks TRITON\\_INTERPRET environment variable , if True, InterpretedFunction will be invoked, which is used to run Triton kernel in Interpreter mode; otherwise, it will run into JITFunction, where we will compile and run Triton kernel on real device.\n\nInside JITFunction module [https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L432](https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L432) , Triton JIT will first check the device property information through Triton runtime API, our test device is AMD MI300X GPU, so the captured info is shown as below. this info will be kept for Triton performance optimization.\n\nPress enter or click to view image in full size\n\nJIT then checks whether current kernel existed in the kernel cache, if not, it will load the compilation options for Triton kernel function. In this example, the compilation options are shown as below.\n\nPress enter or click to view image in full size\n\nThe kernel compilation entry point is Triton compile function (https://github.com/triton-lang/triton/blob/main/python/triton/compiler/compiler.py#L214), which is invoked with above target device and compilation options information. It will create kernel cache manager, launch the compilation pipeline and populate the kernel meta data. It also loads the backend specific dialects, e.g. TritonAMDGPUDialect on AMD platform, and backend specific LLVM modules to handle LLVM-IR compilation. When all are ready, it will invoke ast\\_to\\_ttir function to generate Triton-IR file of this kernel. Now, the work of Triton compiler frontend part has been done.\n\n2\\. Optimizer module of AMD Triton Compiler\n\nOnce Triton-IR file is ready, compilation pipeline begins to work, and the registered optimization passes will apply on the IRs in sequence.\n\n1) Triton-IR optimization\n\nOn AMD platform, Triton-IR optimization passes are defined in make\\_ttir function, [https://github.com/triton-lang/triton/blob/main/third\\_party/amd/backend/compiler.py](https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py). The optimization passes at this stage are hardware independent, like inline optimization, CSE(Common subexpression elimination), Canonicalization, DCE(Dead code elimination), LICM(Loop Invariant Code Motion), and Loop Unrolling. After applying these optimization passes, we dump the optimized Triton-IR as below:\n\nPress enter or click to view image in full size\n\nWe can notice that the main functions of in the vector add Triton kernel are now represented by Triton-IR operations as below:\n\nPress enter or click to view image in full size\n\n2) Triton-GPU IR optimization\n\nOn AMD platform, Triton-IR optimization passes are defined in make\\_ttgir function , and are designed for GPU platform to boost its performance. Based on AMD GPU hardware features and kernel optimization experience, AMD also developed some AMD GPU specific optimization passes for this stage, which are listed in the below table.\n\nPress enter or click to view image in full size\n\nThese optimization passes first convert Triton-IR to Triton-GPU IR. At this step, layout information is added into the IR. In this example, the tensors are represented as a #blocked layout, as shown in below table.\n\nPress enter or click to view image in full size\n\nBecause vector add Triton kernel is some simple, optimization passes don’t have the obvious effect on the IR. Here we used another Triton Matrix Multiplication example [https://github.com/ROCm/triton/blob/main\\_perf/python/tutorials/03-matrix-multiplication.py](https://github.com/ROCm/triton/blob/main_perf/python/tutorials/03-matrix-multiplication.py) . By dumping this example’s TTIR and TTGIR file, these GPU IR optimization passes add the shared memory access to boost performance, which is a popular optimization solution for Matrix Multiplication, and used amd\\_mfma layout, which is designed for AMD MFMA accelerator.\n\nPress enter or click to view image i","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fmedium.com%2F%40nzhangnju%2Fa-deep-dive-into-amd-triton-compilation-912d96e68e45","id":"https://medium.com/@nzhangnju/a-deep-dive-into-amd-triton-compilation-912d96e68e45","publishedDate":"2025-04-13T00:00:00.000Z","summary":"This blog post provides an overview of AMD's Triton compiler, a key component for running Triton kernels on AMD GPUs. The Triton compiler translates high-level Triton code into efficient low-level GPU instructions. The compilation flow involves a frontend that creates Triton Intermediate Representation (IR), an optimizer that applies hardware-dependent optimizations (including layout and memory optimizations), and a backend for machine code generation. AMD has open-sourced its Triton compiler support, including specific optimization passes and layouts, allowing developers to understand and optimize their Triton kernel code for AMD GPUs.\n","title":"A Deep Dive Into AMD Triton Compilation","url":"https://medium.com/@nzhangnju/a-deep-dive-into-amd-triton-compilation-912d96e68e45"},{"content":"[Discord](https://pxl.to/ivxz41s)[Linkedin](https://www.linkedin.com/company/marktechpost/?viewAsMember=true)[Reddit](https://www.reddit.com/r/machinelearningnews/)[Twitter](https://twitter.com/Marktechpost)\n\n- [Home](https://www.marktechpost.com/)\n- [Open Source/Weights](https://www.marktechpost.com/category/technology/open-source/)\n- [Enterprise AI](https://www.marktechpost.com/category/editors-pick/enterprise-ai/)\n- [Robotics](https://www.marktechpost.com/category/robotics/)\n- [AI Agents](https://www.marktechpost.com/category/editors-pick/ai-agents/)\n- [MCP](https://www.marktechpost.com/category/editors-pick/model-context-protocol-mcp/)\n- [Tutorials](https://www.marktechpost.com/category/tutorials/)\n- [Voice AI](https://www.marktechpost.com/category/technology/artificial-intelligence/voice-ai/)\n- [Sponsorship](https://calendly.com/marktechpost/marktechpost-promotion-call)\n\nSearch\n\n[NewsHub](https://www.marktechpost.com/)\n\n[NewsHub](https://www.marktechpost.com/)\n\n[Premium Content](https://www.marktechpost.com/www.marktechpost.com)\n\n[Read our exclusive articles](https://www.marktechpost.com/www.marktechpost.com)\n\n[Facebook](https://www.marktechpost.com/www.marktechpost.com)\n\n[Instagram](https://www.marktechpost.com/www.marktechpost.com)\n\n[Twitter](https://www.marktechpost.com/www.marktechpost.com)\n\n- [Home](https://www.marktechpost.com/)\n- [Open Source/Weights](https://www.marktechpost.com/category/technology/open-source/)\n- [Enterprise AI](https://www.marktechpost.com/category/editors-pick/enterprise-ai/)\n- [Robotics](https://www.marktechpost.com/category/robotics/)\n- [AI Agents](https://www.marktechpost.com/category/editors-pick/ai-agents/)\n- [MCP](https://www.marktechpost.com/category/editors-pick/model-context-protocol-mcp/)\n- [Tutorials](https://www.marktechpost.com/category/tutorials/)\n- [Voice AI](https://www.marktechpost.com/category/technology/artificial-intelligence/voice-ai/)\n- [Sponsorship](https://calendly.com/marktechpost/marktechpost-promotion-call)\n\n[NewsHub](https://www.marktechpost.com/)\n\nSearch\n\n- [Home](https://www.marktechpost.com/)\n- [Open Source/Weights](https://www.marktechpost.com/category/technology/open-source/)\n- [Enterprise AI](https://www.marktechpost.com/category/editors-pick/enterprise-ai/)\n- [Robotics](https://www.marktechpost.com/category/robotics/)\n- [AI Agents](https://www.marktechpost.com/category/editors-pick/ai-agents/)\n- [MCP](https://www.marktechpost.com/category/editors-pick/model-context-protocol-mcp/)\n- [Tutorials](https://www.marktechpost.com/category/tutorials/)\n- [Voice AI](https://www.marktechpost.com/category/technology/artificial-intelligence/voice-ai/)\n- [Sponsorship](https://calendly.com/marktechpost/marktechpost-promotion-call)\n\n[Home](https://www.marktechpost.com/)[Artificial Intelligence](https://www.marktechpost.com/category/technology/artificial-intelligence/)[AI Infrastructure](https://www.marktechpost.com/category/technology/artificial-intelligence/ai-infrastructure/)Software Frameworks Optimized for GPUs in AI: CUDA, ROCm, Triton, TensorRT—Compiler Paths...\n\n- [Artificial Intelligence](https://www.marktechpost.com/category/technology/artificial-intelligence/)\n- [AI Infrastructure](https://www.marktechpost.com/category/technology/artificial-intelligence/ai-infrastructure/)\n- [Technology](https://www.marktechpost.com/category/technology/)\n- [AI Shorts](https://www.marktechpost.com/category/technology/ai-shorts/)\n- [Editors Pick](https://www.marktechpost.com/category/editors-pick/)\n- [Staff](https://www.marktechpost.com/category/editors-pick/staff/)\n- [Tech News](https://www.marktechpost.com/category/tech-news/)\n\n### **Table of contents**\n\n- [What actually determines performance on modern GPUs](https://www.marktechpost.com/www.marktechpost.com#h-what-actually-determines-performance-on-modern-gpus)\n- [CUDA: nvcc/ptxas, cuDNN, CUTLASS, and CUDA Graphs](https://www.marktechpost.com/www.marktechpost.com#h-cuda-nvcc-ptxas-cudnn-cutlass-and-cuda-graphs)\n- [ROCm: HIP/Clang toolchain, rocBLAS/MIOpen, and the 6.x series](https://www.marktechpost.com/www.marktechpost.com#h-rocm-hip-clang-toolchain-rocblas-miopen-and-the-6-x-series)\n- [Triton: a DSL and compiler for custom kernels](https://www.marktechpost.com/www.marktechpost.com#h-triton-a-dsl-and-compiler-for-custom-kernels)\n- [TensorRT (and TensorRT-LLM): builder-time graph optimization for inference](https://www.marktechpost.com/www.marktechpost.com#h-tensorrt-and-tensorrt-llm-builder-time-graph-optimization-for-inference)\n- [Practical guidance: choosing and tuning the stack](https://www.marktechpost.com/www.marktechpost.com#h-practical-guidance-choosing-and-tuning-the-stack)\n\nDeep-learning throughput hinges on how effectively a compiler stack maps tensor programs to GPU execution: thread/block schedules, memory movement, and instruction selection (e.g., Tensor Core MMA pipelines). In this article we will focus on four dominant stacks—CUDA, ROCm, Triton, and TensorRT—from the compiler’s perspective and explains which optimizations move the needle in practice.\n\n### **What actually determines performance on modern GPUs**\n\n**Across vendors, the same levers recur:**\n\n- **Operator scheduling & fusion:** reduce kernel launches and round-trips to HBM; expose longer producer→consumer chains for register/shared-memory reuse. TensorRT and cuDNN “runtime fusion engines” exemplify this for attention and conv blocks.\n- **Tiling & data layout:** match tile shapes to Tensor Core/WGMMA/WMMA native fragment sizes; avoid shared-memory bank conflicts and partition camping. CUTLASS documents warp-level GEMM tiling for both Tensor Cores and CUDA cores.\n- **Precision & quantization:** FP16/BF16/FP8 for training/inference; INT8/INT4 (calibrated or QAT) for inference. TensorRT automates calibration and kernel selection under these precisions.\n- **Graph capture & runtime specialization:** graph execution to amortize launch overheads; dynamic fusion of common subgraphs (e.g., attention). cuDNN 9 added graph support for attention fusion engines.\n- **Autotuning:** search tile sizes, unroll factors, and pipelining depths per arch/SKU. Triton and CUTLASS expose explicit autotune hooks; TensorRT performs builder-time tactic selection.\n\nWith that lens, here’s how each stack implements the above.\n\n### **CUDA: nvcc/ptxas, cuDNN, CUTLASS, and CUDA Graphs**\n\n**Compiler path.** CUDA code compiles through **nvcc** into PTX, then **ptxas** lowers PTX to SASS (arch-specific machine code). Controlling optimization requires feeding flags to both host and device phases; for kernels the key is `-Xptxas`. Developers often miss that `-O3` alone affects only host code.\n\n**Kernel generation & libraries.**\n\n- **CUTLASS** provides parametric templates for GEMM/conv, implementing warp-level tiling, Tensor Core MMA pipelines, and smem iterators designed for conflict-free access—canonical references for writing peak kernels, including Hopper’s **WGMMA** path.\n- **cuDNN 9** introduced runtime fusion engines (notably for attention blocks), native CUDA Graph integration for those engines, and updates for new compute capabilities—materially reducing dispatch overheads and improving memory locality in Transformer workloads.\n\n**Performance implications.**\n\n- Moving from unfused PyTorch ops to cuDNN attention fusion typically cuts kernel launches and global memory traffic; combined with **CUDA Graphs**, it reduces CPU bottlenecks in short-sequence inference.\n- On Hopper/Blackwell, aligning tile shapes to WGMMA/Tensor Core native sizes is decisive; CUTLASS tutorials quantify how mis-sized tiles waste tensor-core throughput.\n\n**When CUDA is the right tool.** You need maximum control over instruction selection, occupancy, and smem choreography; or you’re extending kernels beyond library coverage while staying on NVIDIA GPUs.\n\n### **ROCm: HIP/Clang toolchain, rocBLAS/MIOpen, and the 6.x series**\n\n**Compiler path.** ROCm uses **Clang/LLVM** to compile **HIP** (CUDA-like) into GCN/RDNA ISA. The 6.x series has focused on perf and framework coverage; release notes track component-level optimizations and HW/OS support.\n\n**Libraries and kernels.**\n\n- **rocBLAS** and **MIOpen** implement GEMM/conv primitives with arch-aware tiling and algorithm selection similar in spirit to cuBLAS/cuDNN. The consolidated changelog highlights iterative perf work across these libraries.\n- Recent ROCm workstream includes better **Triton** enablement on AMD GPUs, enabling Python-level kernel authoring while still lowering through LLVM to AMD backends.\n\n**Performance implications.**\n\n- On AMD GPUs, matching LDS (shared memory) bank widths and vectorized global loads to matrix tile shapes is as pivotal as smem bank alignment on NVIDIA. Compiler-assisted fusion in frameworks (e.g., attention) plus library autotuning in rocBLAS/MIOpen typically closes a large fraction of the gap to handwritten kernels, contingent on architecture/driver. Release documentation indicates continuous tuner improvements in 6.0–6.4.x.\n\n**When ROCm is the right tool.** You need native support and optimization on AMD accelerators, with HIP portability from existing CUDA-style kernels and a clear LLVM toolchain.\n\n### **Triton: a DSL and compiler for custom kernels**\n\n**Compiler path.** Triton is a Python-embedded DSL that lowers via **LLVM**; it handles vectorization, memory coalescing, and register allocation while giving explicit control over block sizes and program IDs. Build docs show the LLVM dependency and custom builds; NVIDIA’s developer materials discuss Triton’s tuning for newer architectures (e.g., Blackwell) with FP16/FP8 GEMM improvements.\n\n**Optimizations.**\n\n- **Autotuning** over tile sizes, `num_warps`, and pipelining stages; static **masking** for boundary conditions without scalar fallbacks; **shared-memory** staging and software pipelining to overlap global loads with compute.\n- Triton’s design aims to **automate** the error-prone parts of CUDA-level optimization while leaving block-level tiling choices to t","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.marktechpost.com%2F2025%2F09%2F14%2Fsoftware-frameworks-optimized-for-gpus-in-ai-cuda-rocm-triton-tensorrt-compiler-paths-and-performance-implications%2F","id":"https://www.marktechpost.com/2025/09/14/software-frameworks-optimized-for-gpus-in-ai-cuda-rocm-triton-tensorrt-compiler-paths-and-performance-implications/","publishedDate":"2025-09-14T00:00:00.000Z","summary":"This webpage discusses software frameworks optimized for GPUs in AI, including CUDA, ROCm, Triton, and TensorRT. It covers compiler paths and performance implications. The article also includes links to related content such as AI infrastructure and technology news.\n","title":"Software Frameworks Optimized for GPUs in AI: CUDA, ROCm, Triton, TensorRT—Compiler Paths and Performance Implications","url":"https://www.marktechpost.com/2025/09/14/software-frameworks-optimized-for-gpus-in-ai-cuda-rocm-triton-tensorrt-compiler-paths-and-performance-implications/"},{"content":"[Sign in / up](https://account.theregister.com/register/)\n\n[The Register](https://www.theregister.com/)\n\n## Topics\n\n[Special Features](https://go.theregister.com/go.theregister.com#subnav-box-nav-special_features)\n\n## Special Features\n\n## Vendor Voice\n\n[Resources](https://go.theregister.com/go.theregister.com#subnav-box-nav-resources)\n\n## Resources\n\n#### [Systems](https://go.theregister.com/on_prem/systems/)\n\n[**5**](https://forums.theregister.com/forum/all/2025/09/17/amd_rocm_7_chases_nvidia_cuda/)\n\n# AMD tries to catch CUDA with performance-boosting ROCm 7 software\n\n[**5**](https://forums.theregister.com/forum/all/2025/09/17/amd_rocm_7_chases_nvidia_cuda/)\n\n## House of Zen promises 3.5x improvement in inference and 3x uplift in training perf over last-gen software\n\n[Tobias Mann](https://go.theregister.com/Author/Tobias-Mann)\n\nWed 17 Sep 2025 // 20:40 UTC\n\nAMD closed the performance gap with Nvidia's Blackwell accelerators with the launch of the MI355X this spring. Now the company just needs to overcome Nvidia's CUDA software advantage and make that perf more accessible to developers.\n\nThe [release](https://rocm.blogs.amd.com/ecosystems-and-partners/rocm-7.0-blog/README.html) of AMD's ROCm 7.0 software platform this week is a step in that direction, promising major improvements in inference and training performance that benefit not only its latest chips but its older MI300-series parts as well. The so-called [CUDA moat](https://www.theregister.com/2024/12/17/nvidia_cuda_moat/) could be getting narrower.\n\nROCm, if you're not familiar, is a suite of software libraries and development tools, including HIP frameworks, that provides developers a low-level programming interface for running high-performance computing (HPC) and AI workloads on GPUs. The software stack is reminiscent in many ways of the CUDA runtime, but for AMD GPUs rather than Nvidia.\n\nSince the launch of the MI300X, its first truly AI-optimized graphics accelerator, back in 2023, AMD has extended support for new datatypes, improved compatibility with popular runtimes and frameworks, and introduced hardware-specific optimizations through its ROCm runtime.\n\nROCm 7 is arguably AMD's biggest update yet. Compared to ROCm 6, AMD says that customers can expect a roughly 3.5x uplift in inference performance on the MI300X. Meanwhile, the company says it has managed to boost the effective floating point performance achieved in model training by 3x.\n\nAMD claims that these software enhancements combined give its latest and greatest GPU, the MI355X, a 1.3x edge in inference workloads over Nvidia's B200 when running DeepSeek R1 in SGLang. As usual, you should take all vendor performance claims with a grain of salt.\n\nWhile the MI350X and MI355X are roughly on par with the [B200](https://www.theregister.com/2024/03/18/nvidia_turns_up_the_ai/?td=rt-9c) in terms of floating point performance, achieving 9.2 and 10 petaFLOPS of dense FP4 to Nvidia's 9 petaFLOPs, the AMD parts boast 108 GB more HBM3e.\n\nThe AMD MI355X's main competitor is actually Nvidia's B300, which [packs](https://www.theregister.com/2025/03/18/nvidia_blackwell_ultra/) 288 GB of HBM3e and manages 14 petaFLOPS of dense FP4 performance, which on paper could give it an edge in inference workloads.\n\nSpeaking of FP4 support, the MI350 series is AMD's first generation of GPUs to offer hardware acceleration for OCP's microscaling datatypes, which we looked at in more detail around OpenAI's [gpt-oss](https://www.theregister.com/2025/08/07/run_openai_gpt_oss_locally/) launch last month.\n\nThese smaller formats have major implications for inference and training performance boosting throughput and cutting memory requirements by a factor of 2 to 4x. ROCm 7.0.0 extends broader support for these low precision datatypes with AMD saying its Quark quantization framework is now production ready.\n\nThis is a big improvement compared to giving the card FP8 support which [trailed](https://www.theregister.com/2024/08/06/amd_unleashes_mi300s_full_potential/) the release of the MI300 by the better part of a year.\n\nAlongside the datatypes, ROCm 7.0.0 also introduces AMD's AI Tensor Engine, or AITER for short, which features specialized operators tuned for maximum GenAI performance.\n\nFor inference, AMD says AITER can boost MLA decode operations by 17x and MHA prefill ops by 14x. When applied to models like DeepSeek R1, the GPU slinger says AITER can boost throughput by more than 2x.\n\nMore importantly, AITER and the MXFP4 datatype have already been merged into popular inference serving engines like vLLM and SGLang. AMD tells us that enabling the feature is as simple as installing the dependencies and setting the appropriate environment variables.\n\nOther improvements include support for the latest Ubuntu 24.04.3 LTS release as well as Rocky Linux 9 and KVM passthrough for those that want to add GPU acceleration to virtual machines.\n\nROCm 7 also adds native support for PyTorch 2.7 and 2.9, TensorFlow 2.19.1, and JAX 0.6.\n\n- [Ruh-roh. DDR5 memory vulnerable to new Rowhammer attack](https://www.theregister.com/2025/09/17/ddr5_dram_rowhammer/)\n- [SK Hynix cranks up the HBM4 assembly line to prep for next-gen GPUs](https://www.theregister.com/2025/09/12/sk_hynix_hbm4_mass_production/)\n- [If Broadcom is helping OpenAI build AI chips, here's what they might look like](https://www.theregister.com/2025/09/05/openai_broadcom_ai_chips/)\n- [AMD Ryzen CPUs fry twice in the face of heavy math load, GMP says](https://www.theregister.com/2025/08/29/amd_ryzen_twice_fails_in/)\n\nFinally, for those deploying large quantities of Instinct accelerators in production, AMD is rolling out a pair of new dashboards designed to make managing large clusters of GPUs easier. AMD's Resource Manager provides detailed telemetry on the performance and utilization of the cluster, as well as access controls and the ability to set project quotas so that one team doesn't end up hogging all the compute.\n\nAlongside the resource manager, AMD is also rolling out an AI Workbench which is designed to streamline the process of training or fine tuning popular foundation models.\n\nROCm 7.0 is available for [download](https://rocm.docs.amd.com/en/latest/) from AMD's support site, as well as in pre-baked container [images](https://hub.docker.com/u/rocm) on Docker Hub. ®\n\nShare\n\n#### More about\n\n- [AI](https://go.theregister.com/Tag/AI/)\n- [AMD](https://go.theregister.com/Tag/AMD/)\n- [Datacenter](https://go.theregister.com/Tag/Datacenter/)\n\nMore like these\n\n×\n\n### More about\n\n- [AI](https://go.theregister.com/Tag/AI/)\n- [AMD](https://go.theregister.com/Tag/AMD/)\n- [Datacenter](https://go.theregister.com/Tag/Datacenter/)\n- [GPU](https://go.theregister.com/Tag/GPU/)\n\n### Narrower topics\n\n- [AIOps](https://go.theregister.com/Tag/AIOps/)\n- [Apple M1](https://go.theregister.com/Tag/Apple%20M1/)\n- [DeepSeek](https://go.theregister.com/Tag/DeepSeek/)\n- [Disaster recovery](https://go.theregister.com/Tag/Disaster%20recovery/)\n- [Epyc](https://go.theregister.com/Tag/Epyc/)\n- [Gemini](https://go.theregister.com/Tag/Gemini/)\n- [Google AI](https://go.theregister.com/Tag/Google%20AI/)\n- [GPT-3](https://go.theregister.com/Tag/GPT-3/)\n- [GPT-4](https://go.theregister.com/Tag/GPT-4/)\n- [GTC](https://go.theregister.com/Tag/GTC/)\n- [Large Language Model](https://go.theregister.com/Tag/Large%20Language%20Model/)\n- [Machine Learning](https://go.theregister.com/Tag/Machine%20Learning/)\n- [MCubed](https://go.theregister.com/Tag/MCubed/)\n- [Neural Networks](https://go.theregister.com/Tag/Neural%20Networks/)\n- [NLP](https://go.theregister.com/Tag/NLP/)\n- [Open Compute Project](https://go.theregister.com/Tag/Open%20Compute%20Project/)\n- [PUE](https://go.theregister.com/Tag/PUE/)\n- [Retrieval Augmented Generation](https://go.theregister.com/Tag/Retrieval%20Augmented%20Generation/)\n- [Software defined data center](https://go.theregister.com/Tag/Software%20defined%20data%20center/)\n- [Star Wars](https://go.theregister.com/Tag/Star%20Wars/)\n- [Tensor Processing Unit](https://go.theregister.com/Tag/Tensor%20Processing%20Unit/)\n- [TOPS](https://go.theregister.com/Tag/TOPS/)\n- [Xilinx](https://go.theregister.com/Tag/Xilinx/)\n\n### Broader topics\n\n- [Alder Lake](https://go.theregister.com/Tag/Alder%20Lake/)\n- [Hardware](https://go.theregister.com/Tag/Hardware/)\n- [Self-driving Car](https://go.theregister.com/Tag/Self-driving%20Car/)\n\n#### More about\n\nShare\n\n[**5**\\\nCOMMENTS](https://forums.theregister.com/forum/all/2025/09/17/amd_rocm_7_chases_nvidia_cuda/)\n\n#### More about\n\n- [AI](https://go.theregister.com/Tag/AI/)\n- [AMD](https://go.theregister.com/Tag/AMD/)\n- [Datacenter](https://go.theregister.com/Tag/Datacenter/)\n\nMore like these\n\n×\n\n### More about\n\n- [AI](https://go.theregister.com/Tag/AI/)\n- [AMD](https://go.theregister.com/Tag/AMD/)\n- [Datacenter](https://go.theregister.com/Tag/Datacenter/)\n- [GPU](https://go.theregister.com/Tag/GPU/)\n\n### Narrower topics\n\n- [AIOps](https://go.theregister.com/Tag/AIOps/)\n- [Apple M1](https://go.theregister.com/Tag/Apple%20M1/)\n- [DeepSeek](https://go.theregister.com/Tag/DeepSeek/)\n- [Disaster recovery](https://go.theregister.com/Tag/Disaster%20recovery/)\n- [Epyc](https://go.theregister.com/Tag/Epyc/)\n- [Gemini](https://go.theregister.com/Tag/Gemini/)\n- [Google AI](https://go.theregister.com/Tag/Google%20AI/)\n- [GPT-3](https://go.theregister.com/Tag/GPT-3/)\n- [GPT-4](https://go.theregister.com/Tag/GPT-4/)\n- [GTC](https://go.theregister.com/Tag/GTC/)\n- [Large Language Model](https://go.theregister.com/Tag/Large%20Language%20Model/)\n- [Machine Learning](https://go.theregister.com/Tag/Machine%20Learning/)\n- [MCubed](https://go.theregister.com/Tag/MCubed/)\n- [Neural Networks](https://go.theregister.com/Tag/Neural%20Networks/)\n- [NLP](https://go.theregister.com/Tag/NLP/)\n- [Open Compute Project](https://go.theregister.com/Tag/Open%20Compute%20Project/)\n- [PUE](https://go.theregister.com/Tag/PUE/)\n- [Retrieval Augmented Generation](https://go.theregister.com/Tag/Retrieval%20Augme","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fgo.theregister.com%2Ffeed%2Fwww.theregister.com%2F2025%2F09%2F17%2Famd_rocm_7_chases_nvidia_cuda%2F","id":"https://go.theregister.com/feed/www.theregister.com/2025/09/17/amd_rocm_7_chases_nvidia_cuda/","publishedDate":"2025-09-17T00:00:00.000Z","summary":"AMD's ROCm 7.0 software platform promises significant performance improvements for its GPUs, particularly in inference and training tasks. Compared to ROCm 6, AMD claims a 3.5x uplift in inference performance on the MI300X and a 3x boost in model training. This update aims to narrow the gap with Nvidia's CUDA advantage, with the MI355X potentially outperforming Nvidia's B200 in inference workloads. ROCm 7.0 also extends support for low-precision datatypes, like OCP's microscaling formats, which can boost throughput and reduce memory requirements.\n","title":"AMD tries to catch CUDA with performance-boosting ROCm 7 software","url":"https://go.theregister.com/feed/www.theregister.com/2025/09/17/amd_rocm_7_chases_nvidia_cuda/"},{"content":"- [Articles & Reviews](https://www.phoronix.com/reviews)\n- [News Archive](https://www.phoronix.com/news)\n- [Forums](https://www.phoronix.com/forums/)\n- [Premium Ad-Free](https://www.phoronix.com/phoronix-premium)\n- [Contact](https://www.phoronix.com/contact)\n- Popular Categories\n- Close\n\n- [Articles & Reviews](https://www.phoronix.com/reviews)\n- [News Archive](https://www.phoronix.com/news)\n- [Forums](https://www.phoronix.com/forums/)\n- [Premium](https://www.phoronix.com/phoronix-premium)\n- [Contact](https://www.phoronix.com/contact)\n- Categories\n[Computers](https://www.phoronix.com/reviews/Computers) [Display Drivers](https://www.phoronix.com/reviews/Display+Drivers) [Graphics Cards](https://www.phoronix.com/reviews/Graphics+Cards) [Linux Gaming](https://www.phoronix.com/reviews/Linux+Gaming) [Memory](https://www.phoronix.com/reviews/Memory) [Motherboards](https://www.phoronix.com/reviews/Motherboards) [Processors](https://www.phoronix.com/reviews/Processors) [Software](https://www.phoronix.com/reviews/Software) [Storage](https://www.phoronix.com/reviews/Storage) [Operating Systems](https://www.phoronix.com/reviews/Operating+Systems) [Peripherals](https://www.phoronix.com/reviews/Peripherals)\n\n\n# AMD ROCm 7.0 Officially Released With Many Significant Improvements\n\nWritten by [Michael Larabel](https://www.michaellarabel.com/) in [AMD](https://www.phoronix.com/linux/AMD) on 16 September 2025 at 09:00 AM EDT. [32 Comments](https://www.phoronix.com/forums/node/1577749)\n\nOvernight [the AMD ROCm 7.0 release tags](https://www.phoronix.com/news/AMD-ROCm-7.0-Rolling-Out) began appearing within the public Git repositories. Now AMD ROCm 7.0 is officially released as a very significant step forward for AMD's open-source GPU compute stack for better competing against NVIDIA's CUDA ecosystem.\nROCm 7.0.0 is officially out and all of the ROCm 7.0 documentation has also been published along with the binaries being available via the AMD repository.\nThe key highlights of ROCm 7.0 include:\n\\- AMD Instinct MI350X and Instinct MI355X are now officially supported.\n\\- Ubuntu 24.04.3 LTS and Rocky Linux 9 with Linux 5.14 are now officially supported combinations.\n\\- ROCm 7.0 supports KVM Passthrough for MI350X and MI355X GPUs.\n\\- ROCm 7.0 supports PyTorch 2.7, integrated Fused Rope kernels in APEX, Python C++ extension support with amdclang++, TensorFlow 2.19.1 support, ONNX 1.22 support, Triton 3.3, and support for JAX 0.6.0.\n\\- ROCm now supports Ray as a unified framework for scaling AI and Python applications.\n\\- Official support for Llama.cpp.\n\\- The AMD GPU kernel driver code is now distributed separately from the ROCm stack.\n\\- HIP Runtime support for Open Compute Project FP4, FP6, and FP8 data types and APIs.\n\\- Support for the AMD Next-Gen Fortran Compiler (llvm-clang / new-flang).\n\\- ROCgdb debugger enhancements.\n\\- The ROCm Compute Profiler brings an interactive command line with TUI.\nDownloads and more details on the ROCm 7.0 release via [rocm.docs.amd.com](https://rocm.docs.amd.com/en/latest/). Details on all of the ROCm 7.0 specific changes can also be found now via [this GitHub page](https://github.com/ROCm/ROCm/releases/tag/rocm-7.0.0). I will be working on some AMD ROCm 7.0 benchmarks shortly.\n\n[32 Comments](https://www.phoronix.com/forums/node/1577749)\n\n[Tweet](https://twitter.com/share)\n\n[AMD Versal NET DDR EDAC Driver Ready For Linux 6.18](https://www.phoronix.com/news/AMD-Versal-NET-EDAC-Linux-6.18)\n\n[AMD ISP4 Driver Still Pending Review For The Linux Kernel](https://www.phoronix.com/news/AMD-ISP4-Driver-Pending-Review)\n\n[AMDKFD Compute Driver Sees Patches For S0ix Standby Support](https://www.phoronix.com/news/AMDKFD-Compute-S0ix)\n\n[AMD \"GFX1251\" Target Added To LLVM As Latest RDNA 4.5 APU](https://www.phoronix.com/news/AMD-GFX1251-LLVM-Target)\n\n[AMD Hardware Would Ideally Be Supported By ROCm For ~10 Years](https://www.phoronix.com/news/AMD-ROCm-Hardware-Length)\n\n[A Quick Look At The AMD Instinct MI355X With ROCm 7.0](https://www.phoronix.com/news/AMD-Instinct-MI355X-ROCm-7.0)\n\nMichael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via [Twitter](https://twitter.com/MichaelLarabel), [LinkedIn](https://www.linkedin.com/in/michaellarabel/), or contacted via [MichaelLarabel.com](https://www.michaellarabel.com/).\n\n[A Major Trading Firm Has Open-Sourced The Latest Linux File-System: TernFS](https://www.phoronix.com/news/TernFS-File-System-Open-Source)\n\n[Multi-Kernel Architecture Proposed For The Linux Kernel](https://www.phoronix.com/news/Linux-Multi-Kernel-Patches)\n\n[Git Developers Debate Making Rust Mandatory](https://www.phoronix.com/news/Git-Weighs-Mandatory-Rust)\n\n[AMD Officially Confirms The End Of The AMDVLK Driver](https://www.phoronix.com/news/AMDVLK-Discontinued)\n\n[AMD ROCm 7.0 Officially Released With Many Significant Improvements](https://www.phoronix.com/news/AMD-ROCm-7.0-Released)\n\n[Linux Ready To Upstream Support For Google's PSP Encryption For TCP Connections](https://www.phoronix.com/news/PSP-Encryption-Linux-6.18)\n\n[Godot 4.5 Open-Source Game Engine Released With A Multitude Of Improvements](https://www.phoronix.com/news/Godot-4.5-Released)\n\n[Rust 1.90 Released With LLD Default On Linux x86\\_64 While macOS x86\\_64 Demoted](https://www.phoronix.com/news/Rust-1.90-Released)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.phoronix.com%2Fnews%2FAMD-ROCm-7.0-Released","id":"https://www.phoronix.com/news/AMD-ROCm-7.0-Released","publishedDate":"2025-09-16T00:00:00.000Z","summary":"AMD has officially released ROCm 7.0, a significant update to its open-source GPU compute stack. Key highlights include support for AMD Instinct MI350X and MI355X GPUs, Ubuntu 24.04.3 LTS and Rocky Linux 9 with Linux 5.14, and KVM Passthrough for MI350X and MI355X GPUs. ROCm 7.0 also supports various AI frameworks and tools like PyTorch 2.7, TensorFlow 2.19.1, and Llama.cpp. The AMD GPU kernel driver is now distributed separately, and there are enhancements to the HIP Runtime, AMD Next-Gen Fortran Compiler, ROCgdb debugger, and ROCm Compute Profiler.\n","title":"AMD ROCm 7.0 Officially Released With Many Significant Improvements","url":"https://www.phoronix.com/news/AMD-ROCm-7.0-Released"},{"content":"AMD 正加速推進開源軟體堆疊 ROCm 7 的發布計畫。根據外媒報導，AMD 已將 ROCm 7 的版本標籤（release tag）上架至 GitHub，包含 HIP、AOMP 與 ROCm Libraries，顯示該軟體堆疊已接近最終版本，市場預期將於數週內正式推出。\n\n在 AI 軟體生態中，NVIDIA 的 CUDA 長期占據主導地位，形成上下游高度綁定，使競爭者難以切入。AMD 此次強調 ROCm 7 的開源特性，並號稱能提供更完整的框架支援、硬體相容性與開發工具，力圖挑戰 CUDA 的市場地位。\n\n在先前 Advancing AI 活動上，AMD 公布了 ROCm 7 的效能數據：相較於上一代 ROCm 6，AI 推理效能最高提升 3.5 倍。此外，基於 CDNA 4 架構的 Instinct MI355X GPU 在 DeepSeek R1 測試中，FP8 吞吐量表現比 NVIDIA  B200 高出 30%。不過，這項數據目前仍僅為 AMD 公布的內部測試，尚未經第三方驗證。\n\n外界認為，ROCm 7 的登場有望吸引更多開發者，突破 CUDA 的壟斷優勢，但要撼動 NVIDIA 在產業內累積多年的龐大生態，挑戰仍相當嚴峻。\n\n- [AMD ROCm 7 Announced: MI350 Support, New Algorithms, Models & Advanced Features For AI Added, Focus on Inference With 3.5x Uplfit](https://wccftech.com/amd-rocm-7-mi350-support-new-algorithms-models-advanced-features-ai-3-5x-uplift/)\n\n（首圖來源： [AMD](https://www.amd.com/zh-tw/corporate/leadership/lisa-su/photo-gallery.html)）\n\n文章看完覺得有幫助，何不給我們一個鼓勵\n\n請我們喝杯咖啡\n\n## 想請我們喝幾杯咖啡？\n\n### 每杯咖啡 65 元\n\nx1\n\nx3\n\nx5\n\nx\n\n您的咖啡贊助將是讓我們持續走下去的動力\n\n**總金額共新臺幣 0 元**\n\n[《關於請喝咖啡的 Q & A》](https://technews.tw/aboutus/buy-me-a-coffee-policy/)\n\n### 留給我們的話\n\n[取消](javascript:void(0)) [確認](javascript:void(0))\n\n[從這裡可透過《Google 新聞》追蹤 TechNews](https://news.google.com/publications/CAAqBwgKMJfS3gow8arWAQ)\n\n科技新知，時時更新\n\n[科技新報粉絲團](https://www.facebook.com/technewsinside) [加入好友](https://lin.ee/nuCjuzt) [訂閱免費電子報](https://member.technews.tw/user/newsPaperManager)\n\n關鍵字: [AMD](https://technews.tw/tag/amd/) , [B200](https://technews.tw/tag/b200/) , [Cuda](https://technews.tw/tag/cuda/) , [ROCm 7](https://technews.tw/tag/rocm-7/) , [閉源](https://technews.tw/tag/%e9%96%89%e6%ba%90/) , [開源](https://technews.tw/tag/%e9%96%8b%e6%ba%90/)\n\n- 我們偵測\n到您有啟用\nAD Block\n- 請您暫停使用AD Block，以支持我們持續能提供更多新聞資訊與優質的閱讀環境。\n\n請您暫停使用AD Block，以支持我們持續能提供更多新聞資訊與優質的閱讀環境。","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Ftechnews.tw%2F2025%2F09%2F17%2Famd-rocm-7-github%2F","id":"https://technews.tw/2025/09/17/amd-rocm-7-github/","publishedDate":"2025-09-17T00:00:00.000Z","summary":"AMD is nearing the final release of ROCm 7, its open-source software stack, with version tags already uploaded to GitHub. ROCm 7 aims to challenge NVIDIA's CUDA dominance in the AI software ecosystem by offering improved framework support, hardware compatibility, and development tools. AMD claims ROCm 7 offers up to 3.5x better AI inference performance compared to ROCm 6, and in internal tests, the Instinct MI355X GPU outperformed NVIDIA's B200 in DeepSeek R1 testing. While ROCm 7's open-source nature could attract developers, overcoming NVIDIA's established ecosystem will be a significant challenge.\n","title":"AMD ROCm 7 登上 GitHub，挑戰 CUDA 長期壟斷","url":"https://technews.tw/2025/09/17/amd-rocm-7-github/"},{"content":"[Skip to main content](https://rocm.docs.amd.com/rocm.docs.amd.com#main-content)\n\nBack to top`Ctrl` + `K`\n\n[ROCm Documentation](https://rocm.docs.amd.com/index.html)\n\nSearch`Ctrl` + `K`\n\n# AMD Instinct MI300X workload optimization\n\n## Contents\n\n# AMD Instinct MI300X workload optimization [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#amd-instinct-mi300x-workload-optimization)\n\n2025-09-05\n\n58 min read time\n\nApplies to Linux\n\nThis document provides guidelines for optimizing the performance of AMD\nInstinct™ MI300X accelerators, with a particular focus on GPU kernel\nprogramming, high-performance computing (HPC), and deep learning operations\nusing PyTorch. It delves into specific workloads such as\n[model inference](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-vllm-optimization), offering strategies to\nenhance efficiency.\n\nThe following topics highlight [auto-tunable configurations](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-auto-tune)\nthat streamline optimization as well as advanced techniques like\n[Triton kernel optimization](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-triton-kernel-performance-optimization) for\nmeticulous tuning.\n\n## Workload tuning strategy [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#workload-tuning-strategy)\n\nBy following a structured approach, you can systematically address\nperformance issues and enhance the efficiency of your workloads on AMD Instinct\nMI300X accelerators.\n\n### Measure the current workload [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#measure-the-current-workload)\n\nBegin by evaluating the performance of your workload in its current state. This\ninvolves running benchmarks and collecting performance data to establish a\nbaseline. Understanding how your workload behaves under different conditions\nprovides critical insights into where improvements are needed.\n\n### Identify tuning requirements [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#mi300x-profiling-start)\n\nAnalyze the collected performance data to identify areas where tuning is\nrequired. This could involve detecting bottlenecks in CPU, GPU, memory, or data\ntransfer. Understanding these requirements will help direct your optimization\nefforts more effectively.\n\nProfiling is a fundamental step in workload tuning. It allows you to gather\ndetailed information about how your workload utilizes system resources, and\nwhere potential inefficiencies lie. Profiling tools can provide insights into\nboth high-level and granular performance metrics. See [Profiling tools](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-profiling-tools).\n\n#### High-level profiling tools [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#high-level-profiling-tools)\n\nFor a broad overview, use tools like the\n[PyTorch Profiler](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-pytorch-profiler), which helps in\nunderstanding how PyTorch operations are executed and where time is spent. This\nis particularly useful for developers new to workload tuning, as it provides a\ncomprehensive view without requiring in-depth knowledge of lower-level\noperations.\n\n#### Kernel-level profiling tools [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#kernel-level-profiling-tools)\n\nWhen profiling indicates that GPUs are a performance bottleneck, delve deeper\ninto kernel-level profiling. Tools such as the\n[ROCr Debug Agent](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-rocr-debug-agent),\n[ROCProfiler](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-rocprof), and\n[ROCm Compute Profiler](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-rocprof-compute) offer detailed insights\ninto GPU kernel execution. These tools can help isolate problematic GPU\noperations and provide data needed for targeted optimizations.\n\n### Analyze and tune [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#analyze-and-tune)\n\nBased on the insights gained from profiling, focus your tuning efforts on the\nidentified bottlenecks. This might involve optimizing specific kernel\noperations, adjusting memory access patterns, or modifying computational\nalgorithms.\n\nThe following subsections discuss optimization ranging from high-level and more\nautomated strategies to more involved, hands-on optimization.\n\n#### Optimize model inference with vLLM [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#optimize-model-inference-with-vllm)\n\nvLLM provides tools and techniques specifically designed for efficient model\ninference on AMD Instinct MI300X accelerators. See [vLLM inference](https://rocm.docs.amd.com/inference/llm-inference-frameworks.html#fine-tuning-llms-vllm)\nfor installation guidance. Optimizing performance with vLLM\ninvolves configuring tensor parallelism, leveraging advanced features, and\nensuring efficient execution. Here’s how to optimize vLLM performance:\n\n- Tensor parallelism: Configure the\n[tensor-parallel-size parameter](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-vllm-multiple-gpus) to distribute\ntensor computations across multiple GPUs. Adjust parameters such as\n`batch-size`, `input-len`, and `output-len` based on your workload.\n\n- Configuration for vLLM: Set [parameters](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-vllm-optimization)\naccording to workload requirements. Benchmark performance to understand\ncharacteristics and identify bottlenecks.\n\n- Benchmarking and performance metrics: Measure latency and throughput to\nevaluate performance.\n\n\n#### Auto-tunable configurations [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#auto-tunable-configurations)\n\nAuto-tunable configurations can significantly streamline performance\noptimization by automatically adjusting parameters based on workload\ncharacteristics. For example:\n\n- PyTorch: Utilize [PyTorch’s built-in auto-tuning features](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-torchinductor-tuning),\nsuch as the [TunableOp](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-tunableop) module, which helps in\noptimizing operation performance by exploring different configurations.\n\n- MIOpen: Leverage [MIOpen’s auto-tuning capabilities](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-miopen-tuning)\nfor convolutional operations and other primitives to find optimal settings for\nyour specific hardware.\n\n- Triton: Use [Triton’s auto-tuning features](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-autotunable-kernel-config)\nto explore various kernel configurations and automatically select the\nbest-performing ones.\n\n\n#### Manual tuning [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#manual-tuning)\n\nAdvanced developers can manually adjust parameters and configurations to\noptimize performance. Both Triton and HIP involve manual tuning aspects.\n\n- ROCm libraries: Optimize GPU performance by adjusting various parameters and\nconfigurations within [ROCm libraries](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-rocm-library-tuning). This\napproach involves hands-on optimization to maximize efficiency for specific\nworkloads.\n\n- Triton: Tune Triton kernels by adjusting parameters tailored to\nyour workload to\n[optimize GPU resource utilization](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-triton-gpu-utilization) and\nbetter [leverage specific hardware features](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-assembly-analysis).\n\n- HIP: Profile and [optimize HIP kernels](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-hip-optimization) by\noptimizing parallel execution, memory access patterns, and other aspects.\n\n\n### Iterate and validate [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#iterate-and-validate)\n\nOptimization is an iterative process. After applying tuning changes, re-profile\nthe workload to validate improvements and ensure that the changes have had the\ndesired effect. Continuous iteration helps refine the performance gains and\naddress any new bottlenecks that may emerge.\n\nROCm provides a prebuilt optimized Docker image that has everything required to implement\nthe LLM inference tips in this section. It includes ROCm, PyTorch, and vLLM.\nFor more information, see [vLLM inference performance testing](https://rocm.docs.amd.com/inference/benchmark-docker/vllm.html).\n\n## Profiling tools [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#profiling-tools)\n\nAMD profiling tools provide valuable insights into how efficiently your\napplication utilizes hardware and help diagnose potential bottlenecks that\ncontribute to poor performance. Developers targeting AMD GPUs have multiple\ntools available depending on their specific profiling needs.\n\n- ROCProfiler tool collects kernel execution performance\nmetrics. For more information, see the\n[ROCProfiler](https://rocm.docs.amd.com/projects/rocprofiler/en/latest/index.html)\ndocumentation.\n\n- ROCm Compute Profiler builds upon ROCProfiler but provides more guided analysis.\nFor more information, see\n[ROCm Compute Profiler documentation](https://rocm.docs.amd.com/projects/rocprofiler-compute/en/latest/index.html).\n\n\nRefer to [Profiling and debugging](https://rocm.docs.amd.com/profiling-and-debugging.html)\nto explore commonly used profiling tools and their usage patterns.\n\nOnce performance bottlenecks are identified, you can implement an informed workload\ntuning strategy. If kernels are the bottleneck, consider:\n\n- [Auto-tuning in PyTorch with TunableOp](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-tunableop)\n\n- [Auto-tuning in MIOpen](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-miopen-tuning)\n\n- [Triton auto-tunable kernel configurations](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-autotunable-kernel-config)\n\n\nIf auto-tuning does not meet your requirements, consider\n[Triton kernel performance optimization](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-triton-kernel-performance-optimization).\n\nIf the issue is multi-GPU scale-out, try\n[RCCL tuning and configuration](https://rocm.docs.amd.com/rocm.docs.amd.com#mi300x-rccl).\n\nThis section discusses profiling and debugging tools and some of their common usage patterns with ROCm applications.","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Frocm.docs.amd.com%2Fen%2Flatest%2Fhow-to%2Frocm-for-ai%2Finference-optimization%2Fworkload.html","id":"https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/workload.html","publishedDate":null,"summary":"This document provides guidelines for optimizing the performance of AMD Instinct™ MI300X accelerators, focusing on GPU kernel programming, high-performance computing (HPC), and deep learning operations using PyTorch. It outlines a workload tuning strategy that includes measuring the current workload, identifying tuning requirements through profiling (using tools like PyTorch Profiler, ROCr Debug Agent, ROCProfiler, and ROCm Compute Profiler), and analyzing and tuning based on the profiling insights. The document also highlights auto-tunable configurations and advanced techniques like Triton kernel optimization.\n","title":"AMD Instinct MI300X workload optimization - ROCm Documentation","url":"https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/workload.html"},{"content":"[Skip to main content](https://rocm.docs.amd.com/rocm.docs.amd.com#main-content)\n\nBack to top\n\n`Ctrl` + `K`\n\n[ROCm Documentation](https://rocm.docs.amd.com/index.html)\n\n# Optimizing Triton kernels\n\n# Optimizing Triton kernels [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#optimizing-triton-kernels)\n\n2025-09-09\n\n2 min read time\n\nApplies to Linux\n\nThis section introduces the general steps for\n[Triton](https://openai.com/index/triton/) kernel optimization. Broadly,\nTriton kernel optimization is similar to [HIP](https://rocm.docs.amd.com/projects/HIP/en/latest/how-to/performance_guidelines.html)\nand CUDA kernel optimization.\n\nRefer to the\n[Triton kernel performance optimization](https://rocm.docs.amd.com/workload.html#mi300x-triton-kernel-performance-optimization)\nsection of the [AMD Instinct MI300X workload optimization](https://rocm.docs.amd.com/workload.html) guide\nfor detailed information.\n\nTriton kernel performance optimization includes the following topics.\n\n- [Auto-tunable kernel configurations](https://rocm.docs.amd.com/workload.html#mi300x-autotunable-kernel-config)\n\n- [MLIR analysis](https://rocm.docs.amd.com/workload.html#mi300x-mlir-analysis)\n\n- [ISA assembly analysis](https://rocm.docs.amd.com/workload.html#mi300x-assembly-analysis)\n\n- [PyTorch inductor max-autotune tuning knobs](https://rocm.docs.amd.com/workload.html#mi300x-torchinductor-tuning)\n\n- [Compute the occupancy of a kernel](https://rocm.docs.amd.com/workload.html#mi300x-compute-kernel-occ)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Frocm.docs.amd.com%2Fen%2Flatest%2Fhow-to%2Frocm-for-ai%2Finference-optimization%2Foptimizing-triton-kernel.html","id":"https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/optimizing-triton-kernel.html","publishedDate":"2025-03-28T00:00:00.000Z","summary":"This webpage provides an overview of optimizing Triton kernels, which is similar to optimizing HIP and CUDA kernels. It directs users to the \"Triton kernel performance optimization\" section within the \"AMD Instinct MI300X workload optimization\" guide for detailed information. The optimization process includes auto-tunable kernel configurations, MLIR and ISA assembly analysis, PyTorch inductor max-autotune tuning knobs, and computing kernel occupancy.\n","title":"Optimizing Triton kernels — ROCm Documentation","url":"https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/optimizing-triton-kernel.html"},{"content":"[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[triton-lang](https://github.com/triton-lang)/ **[triton](https://github.com/triton-lang/triton)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton) You must be signed in to change notification settings\n- [Fork\\\n2.3k](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton)\n- [Star\\\n17k](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton)\n\n\nDevelopment repository for the Triton language and compiler\n\n[triton-lang.org/](https://triton-lang.org/)\n\n### License\n\n[MIT license](https://github.com/triton-lang/triton/blob/main/LICENSE)\n\n[17k\\\nstars](https://github.com/triton-lang/triton/stargazers) [2.3k\\\nforks](https://github.com/triton-lang/triton/forks) [Branches](https://github.com/triton-lang/triton/branches) [Tags](https://github.com/triton-lang/triton/tags) [Activity](https://github.com/triton-lang/triton/activity)\n\n[Star](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton)\n\n[Notifications](https://github.com/login?return_to=%2Ftriton-lang%2Ftriton) You must be signed in to change notification settings\n\n# triton-lang/triton\n\nmain\n\n[Branches](https://github.com/triton-lang/triton/branches) [Tags](https://github.com/triton-lang/triton/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit ## History [4,897 Commits](https://github.com/triton-lang/triton/commits/main/) |\n| [.github](https://github.com/triton-lang/triton/tree/main/.github) | [.github](https://github.com/triton-lang/triton/tree/main/.github) |\n| [bin](https://github.com/triton-lang/triton/tree/main/bin) | [bin](https://github.com/triton-lang/triton/tree/main/bin) |\n| [cmake](https://github.com/triton-lang/triton/tree/main/cmake) | [cmake](https://github.com/triton-lang/triton/tree/main/cmake) |\n| [docs](https://github.com/triton-lang/triton/tree/main/docs) | [docs](https://github.com/triton-lang/triton/tree/main/docs) |\n| [include](https://github.com/triton-lang/triton/tree/main/include) | [include](https://github.com/triton-lang/triton/tree/main/include) |\n| [lib](https://github.com/triton-lang/triton/tree/main/lib) | [lib](https://github.com/triton-lang/triton/tree/main/lib) |\n| [python](https://github.com/triton-lang/triton/tree/main/python) | [python](https://github.com/triton-lang/triton/tree/main/python) |\n| [scripts](https://github.com/triton-lang/triton/tree/main/scripts) | [scripts](https://github.com/triton-lang/triton/tree/main/scripts) |\n| [test](https://github.com/triton-lang/triton/tree/main/test) | [test](https://github.com/triton-lang/triton/tree/main/test) |\n| [third\\_party](https://github.com/triton-lang/triton/tree/main/third_party) | [third\\_party](https://github.com/triton-lang/triton/tree/main/third_party) |\n| [unittest](https://github.com/triton-lang/triton/tree/main/unittest) | [unittest](https://github.com/triton-lang/triton/tree/main/unittest) |\n| [utils](https://github.com/triton-lang/triton/tree/main/utils) | [utils](https://github.com/triton-lang/triton/tree/main/utils) |\n| [.clang-format](https://github.com/triton-lang/triton/blob/main/.clang-format) | [.clang-format](https://github.com/triton-lang/triton/blob/main/.clang-format) |\n| [.editorconfig](https://github.com/triton-lang/triton/blob/main/.editorconfig) | [.editorconfig](https://github.com/triton-lang/triton/blob/main/.editorconfig) |\n| [.git-blame-ignore-revs](https://github.com/triton-lang/triton/blob/main/.git-blame-ignore-revs) | [.git-blame-ignore-revs](https://github.com/triton-lang/triton/blob/main/.git-blame-ignore-revs) |\n| [.gitignore](https://github.com/triton-lang/triton/blob/main/.gitignore) | [.gitignore](https://github.com/triton-lang/triton/blob/main/.gitignore) |\n| [.pre-commit-config.yaml](https://github.com/triton-lang/triton/blob/main/.pre-commit-config.yaml) | [.pre-commit-config.yaml](https://github.com/triton-lang/triton/blob/main/.pre-commit-config.yaml) |\n| [CMakeLists.txt](https://github.com/triton-lang/triton/blob/main/CMakeLists.txt) | [CMakeLists.txt](https://github.com/triton-lang/triton/blob/main/CMakeLists.txt) |\n| [CONTRIBUTING.md](https://github.com/triton-lang/triton/blob/main/CONTRIBUTING.md) | [CONTRIBUTING.md](https://github.com/triton-lang/triton/blob/main/CONTRIBUTING.md) |\n| [LICENSE](https://github.com/triton-lang/triton/blob/main/LICENSE) | [LICENSE](https://github.com/triton-lang/triton/blob/main/LICENSE) |\n| [MANIFEST.in](https://github.com/triton-lang/triton/blob/main/MANIFEST.in) | [MANIFEST.in](https://github.com/triton-lang/triton/blob/main/MANIFEST.in) |\n| [Makefile](https://github.com/triton-lang/triton/blob/main/Makefile) | [Makefile](https://github.com/triton-lang/triton/blob/main/Makefile) |\n| [README.md](https://github.com/triton-lang/triton/blob/main/README.md) | [README.md](https://github.com/triton-lang/triton/blob/main/README.md) |\n| [RELEASE.md](https://github.com/triton-lang/triton/blob/main/RELEASE.md) | [RELEASE.md](https://github.com/triton-lang/triton/blob/main/RELEASE.md) |\n| [pyproject.toml](https://github.com/triton-lang/triton/blob/main/pyproject.toml) | [pyproject.toml](https://github.com/triton-lang/triton/blob/main/pyproject.toml) |\n| [setup.py](https://github.com/triton-lang/triton/blob/main/setup.py) | [setup.py](https://github.com/triton-lang/triton/blob/main/setup.py) |\n| View all files |\n\n## Repository files navigation\n\n| **`Documentation`** | **`Nightly Wheels`** |\n| --- | --- |\n\n# Triton Conference 2025\n\n### Registration\n\nThe 3rd Triton conference is scheduled to take place on October 21, 2025. Click [here](https://tritonconference.eventbuilder.com/TritonDeveloperConference) to register!\n\n### Poster Submission\n\nWe invite members of the Triton community who are attending the Triton Developer Conference to present posters about their Triton-related technical work.\n\nPlease submit basic information of your poster, including author information and abstract using this [form](https://forms.gle/QfgTF8o1CWNENAnA7).\n\n**Important Dates**\n\n- Submission: 10/1/2025\n- Author notification: 10/7/2025\n- Final version (PDF): 10/14/2025\n\n# Triton\n\nThis is the development repository of Triton, a language and compiler for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.\n\nThe foundations of this project are described in the following MAPL2019 publication: [Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations](http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf). Please consider citing this work if you use Triton!\n\nThe [official documentation](https://triton-lang.org) contains installation instructions and tutorials. See also these third-party [Triton puzzles](https://github.com/srush/Triton-Puzzles), which can all be run using the Triton interpreter -- no GPU required.\n\n# Quick Installation\n\nYou can install the latest stable release of Triton from pip:\n\n```\npip install triton\n```\n\nBinary wheels are available for CPython 3.10-3.14.\n\n# Install from source\n\n```\ngit clone https://github.com/triton-lang/triton.git\ncd triton\n\npip install -r python/requirements.txt # build-time dependencies\npip install -e .\n```\n\nOr with a virtualenv:\n\n```\ngit clone https://github.com/triton-lang/triton.git\ncd triton\n\npython -m venv .venv --prompt triton\nsource .venv/bin/activate\n\npip install -r python/requirements.txt # build-time dependencies\npip install -e .\n```\n\n# Building with a custom LLVM\n\nTriton uses LLVM to generate code for GPUs and CPUs. Normally, the Triton build\ndownloads a prebuilt LLVM, but you can also build and use LLVM from source.\n\nLLVM does not have a stable API, so the Triton build will not work at an\narbitrary LLVM version.\n\nFor convenience, use the following command to build LLVM and install Triton with the custom LLVM:\n\n```\nmake dev-install-llvm\n```\n\nAlternatively, follow these steps to build LLVM from source manually.\n\n1. Find the version of LLVM that Triton builds against. Check\n`cmake/llvm-hash.txt` to see the current version. For example, if it says:\n49af6502c6dcb4a7f7520178bd14df396f78240c.\n\nThis means that the version of Triton you have builds against\n[LLVM](https://github.com/llvm/llvm-project) 49af6502.\n\n2. `git checkout` LLVM at this revision. Optionally, make additional\nmodifications to LLVM.\n\n3. [Build LLVM](https://llvm.org/docs/CMake.html). For example, you might run:\n\n\n\n```\n$ cd $HOME/llvm-project # your clone of LLVM.\n$ mkdir build\n$ cd build\n$ cmake -G Ninja -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON ../llvm -DLLVM_ENABLE_PROJECTS=\"mlir;llvm;lld\" -DLLVM_TARGETS_TO_BUILD=\"host;NVPTX;AMDGPU\"\n$ ninja\n\n```\n\n4. Grab a snack, this will take a while.\n\n5. Build Triton as above, but set the following environment variables:\n\n\n\n```\n# Modify as appropriate to point to your LLVM build.\n$ export LLVM_BUILD_DIR=$HOME/llvm-project/build\n\n$ cd \n$ LLVM_INCLUDE_DIRS=$LLVM_BUILD_DIR/include \\\n LLVM_LIBRARY_DIR=$LLVM_BUILD_DIR/lib \\\n LLVM_SYSPATH=$LLVM_BUILD_DIR \\\n pip install -e .\n\n```\n\n\n# Tips for building\n\n- Set `TRITON_BUILD_WITH_CLANG_LLD=true` as an environment variable to use clang\nand lld. lld in particular results in faster builds.\n\n- Set `TRITON_BUILD_WITH_CCACHE=true` to build with ccache.\n\n- Set `TRITON_HOME=/some/path` to change the location of the `.triton`\ndirectory where Triton's cache is located and downloads are stored\nduring the build. By default, this is the user's home directory. It\ncan be changed anytime.\n\n- If you're running out of memory when building Triton, specify the `MAX_JO","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fgithub.com%2Ftriton-lang%2Ftriton","id":"https://github.com/triton-lang/triton","publishedDate":null,"summary":"This is the development repository for the Triton language and compiler, hosted on GitHub. It has 17k stars and 2.3k forks and is licensed under the MIT license. The repository contains various folders and files, including documentation, tests, and third-party dependencies.\n","title":"Development repository for the Triton language and compiler - GitHub","url":"https://github.com/triton-lang/triton"}],"status":"completed","toolCallId":"call_BAe7QlalqLuy7yCfYNRZonGX","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Short answer: Triton works well on modern AMD GPUs via the ROCm stack, and support has matured a lot. You can write a single Triton kernel and run it on NVIDIA or AMD; on AMD it lowers through LLVM’s AMDGPU backend to HSACO. For many DL kernels (matmul/attention/epilogues), performance is competitive when you use AMD-tuned configs and MFMA paths. The main caveats are version alignment (ROCm, PyTorch, Triton versions), and sometimes needing AMD‑specific tuning to hit peak.\n\nWhat “works” today\n- Backends: Triton targets AMD via LLVM AMDGPU; kernels JIT-compile to HSACO and run on ROCm.\n- Architectures: MI200/MI300 series are well supported; ROCm 6.x/7.0 focus on MI300(X/series) and newer.\n- Instructions/cores: Triton maps tile dots to AMD MFMA/WMMA where available; layouts like amd_mfma and amd_wmma are in Triton’s AMD dialect.\n- Tooling: PyTorch 2.x Inductor emits Triton for AMD; ROCm profilers (ROCProfiler, Compute Profiler) and docs cover Triton kernel tuning.\n- Distributed: There’s active work (e.g., Triton-Distributed on AMD) to overlap comm/compute on CDNA3.\n\nPerformance picture\n- Matmul/attention-style kernels: With proper BLOCK_M/N/K, num_warps, stages (software pipelining), and MFMA usage, Triton kernels on MI300-class GPUs can reach near-library throughput on many transformer shapes.\n- Memory-bound and fused ops: Triton makes it easy to fuse epilogues or odd layouts; these often match/best HIP/CUDA-style baselines because memory traffic dominates.\n- Autotuning matters: Peak results typically require autotuning tile sizes and sometimes Split-K; AMD provides passes like OptimizeLDSUsage, BlockPingpong, and buffer ops conversion to improve coalescing and LDS usage.\n\nPractical requirements and gotchas\n- Versions:\n  - Use a Triton build that matches your ROCm (e.g., Triton 3.3 with ROCm 7.0; check release notes).\n  - PyTorch Inductor on ROCm needs compatible PyTorch/ROCm wheels.\n- Hardware/OS: Stick to ROCm-supported SKUs/OS combos from the ROCm docs (Ubuntu 24.04/Rocky 9 for ROCm 7.0).\n- Tuning:\n  - Ensure MFMA paths are chosen (tt.dot → MFMA). Check assembly or profiler counters.\n  - Balance LDS usage vs occupancy; vectorize global loads; use pipelined stages to overlap memory/compute.\n- Feature gaps:\n  - Brand-new AMD hardware features land first in ROCm libraries; Triton usually follows but may lag briefly on cutting-edge ops or datatypes.\n  - Porting highly NVIDIA-specific kernels (e.g., relying on TMA/cluster features) may need algorithmic tweaks on AMD.\n\nIf you’re deciding whether to use Triton on AMD\n- Yes, use it when you want custom/fused GPU kernels in PyTorch or standalone with competitive performance on MI300+.\n- For “classic” GEMM/conv where rocBLAS/MIOpen already shine for your shapes, start with the libraries; use Triton for fusions or nonstandard layouts.\n- Bake in an autotune step per workload and cache the best configs; profile with ROCm tools to confirm MFMA utilization and coalesced accesses.\n\nReferences to explore\n- AMD ROCm docs: Triton kernel optimization guides (autotuning, MLIR/ISA analysis, occupancy).\n- AMD blog posts on “Unlock Peak Performance on AMD GPUs with Triton” and “Triton-Distributed on AMD.”\n- Triton GitHub: third_party/amd backend, layouts (amd_mfma), and tuning examples.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d43c36bb008197965b95b831530ab30f268e38b2aea569"}},"resumableStreamId":"STREAM:68b87236-9d0a-4646-a69c-cf28ed8cd497","role":"assistant","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","timeToFirstToken":2.573,"tokens":819,"tokensPerSecond":26.284540582175296,"updated_at":1758739499424,"userId":"google:104458561188320708185","id":"j9798w9rf90hrprmsx47sgyzgn7r7vv8"},{"_creationTime":1758739574697.477,"_id":"j974h4s2xjq924knps22wgewbh7r60h5","attachmentIds":[],"content":"how is triton compatible with various different hardware backend?","created_at":1758739574697,"messageId":"db710d4b-cfbc-4acb-adb9-925f14fd6fec","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","updated_at":1758739574697,"userId":"google:104458561188320708185","id":"j974h4s2xjq924knps22wgewbh7r60h5","providerMetadata":{}},{"_creationTime":1758739574697.4773,"_id":"j97d3a56n738gnm485tfdjybv97r7ffd","attachmentIds":[],"byok":false,"content":"Triton’s portability comes from a layered design: a hardware-agnostic, tile-centric IR and APIs on top; target-specific lowerings and codegen at the bottom. You write one kernel; Triton maps it to each backend via different lowering paths.\n\nHow it’s structured\n\n- Frontend (portable)\n  - Python DSL (tl.*): You write kernels using tiles, masked loads/stores, program ids, and simple control flow. No CUDA/HIP intrinsics.\n  - Semantics are hardware-neutral: operations are defined over tiles, vectors, pointers, masks, and reductions.\n\n- Middle IR (portable, tile-aware)\n  - Triton builds a tile-first SSA IR (and/or MLIR dialects in newer versions) with explicit shapes, strides, masks, and memory scopes (global, shared/LDS, registers).\n  - Optimizations here are backend-agnostic: shape propagation, common subexpression elimination, algebraic simplification, mask/predicate simplification, loop unrolling, software pipelining decisions, and memory planning at the tile level.\n\n- Backend lowerings (target-specific)\n  - Memory hierarchy mapping:\n    - Global memory accesses are laid out to be coalesced for the target (NVIDIA: 32-thread warp patterns; AMD: wavefront 64).\n    - Shared memory vs. LDS vs. vector register tiling decisions are adjusted per backend.\n  - Threading model mapping:\n    - Triton “program instance” maps to a CUDA thread block (NVIDIA) or a workgroup (AMD). Within it, tiles are partitioned across warps/wavefronts and threads according to the target’s lane width and preferred vector widths.\n  - Matrix-core/vector ISA mapping:\n    - NVIDIA: tl.dot → WMMA/WGMMA (tensor cores) when shapes/precisions permit; otherwise vector-FMA on CUDA cores.\n    - AMD: tl.dot → MFMA/WMMA on CDNA/ROCm; otherwise VALU FMAs.\n    - CPU/other backends (experimental/third-party): map to AVX/AMX or generic vector loops when available.\n  - Sync and barriers:\n    - Inserted according to the backend’s memory model (e.g., __syncthreads for CUDA; s_barrier for AMD).\n  - Code generation:\n    - NVIDIA: Triton lowers to LLVM IR, then to NVPTX, producing PTX that the driver JITs to SASS.\n    - AMD: Lowers to LLVM AMDGPU, producing HSACO (HSA code object) for ROCm.\n    - Other targets would plug in via an LLVM backend or an MLIR-to-ISA path.\n\n- Autotuning per backend\n  - Same kernel source, different “good” parameters. The autotuner searches BLOCK_M/N/K, num_warps/wavefronts, stages (software pipelining), Split-K, and vector widths. The best configs differ across GPUs (e.g., A100 vs. H100 vs. MI300).\n  - Backend can expose additional knobs (e.g., swizzled layouts, LDS bank-friendly shapes) without changing your kernel logic.\n\nWhat makes this work across vendors\n\n- Abstractions match common GPU realities:\n  - Tiles, masks, and memory scopes map naturally to CUDA and ROCm hierarchies.\n  - Triton avoids vendor-specific intrinsics in user code; specialization happens in the backend.\n- Uniform programming model:\n  - Indexing via program_id(axis) instead of blockIdx.x or workgroup_id ensures portability.\n  - Masked operations avoid backend-specific branch patterns and map to lane predication.\n- LLVM/MLIR foundation:\n  - Leveraging LLVM means Triton can target any ISA with a maintained LLVM backend.\n  - Increasing MLIR usage allows clean separation of dialects and patterns per target.\n\nCurrent backends and status (high level)\n\n- NVIDIA CUDA (mature): PTX codegen with tensor-core support (WMMA/WGMMA), extensive autotuning recipes, integration with PyTorch Inductor.\n- AMD ROCm (mature): AMDGPU codegen with MFMA/WMMA lowering, LDS-aware tiling, supported by PyTorch Inductor ROCm builds.\n- Others (experimental/community): There have been efforts toward CPU and other accelerators via MLIR/LLVM; not production-grade compared to CUDA/ROCm.\n\nPractical portability tips\n\n- Write kernels in terms of tiles and masks; avoid assumptions about warp size or shared memory banks.\n- Keep meta-parameters abstract; let the autotuner pick per-device values.\n- Use tl.dot/tl.load/tl.store and built-ins; avoid manual per-lane shuffles unless using portable helpers.\n- Test on each backend with profilers (Nsight Compute, ROCm profilers) to confirm vectorization, coalescing, and matrix-core utilization.\n\nIn short: Triton is portable because your code lives at a tile/mask abstraction level, while backend-specific passes map that to each GPU’s threading, memory, and matrix-core instructions via LLVM-based codegen, with autotuning to pick optimal parameters per device.","created_at":1758739574698,"messageId":"13d1236f-b6aa-48e0-9da2-aa2ac67bddb8","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Triton’s portability comes from a layered design: a hardware-agnostic, tile-centric IR and APIs on top; target-specific lowerings and codegen at the bottom. You write one kernel; Triton maps it to each backend via different lowering paths.\n\nHow it’s structured\n\n- Frontend (portable)\n  - Python DSL (tl.*): You write kernels using tiles, masked loads/stores, program ids, and simple control flow. No CUDA/HIP intrinsics.\n  - Semantics are hardware-neutral: operations are defined over tiles, vectors, pointers, masks, and reductions.\n\n- Middle IR (portable, tile-aware)\n  - Triton builds a tile-first SSA IR (and/or MLIR dialects in newer versions) with explicit shapes, strides, masks, and memory scopes (global, shared/LDS, registers).\n  - Optimizations here are backend-agnostic: shape propagation, common subexpression elimination, algebraic simplification, mask/predicate simplification, loop unrolling, software pipelining decisions, and memory planning at the tile level.\n\n- Backend lowerings (target-specific)\n  - Memory hierarchy mapping:\n    - Global memory accesses are laid out to be coalesced for the target (NVIDIA: 32-thread warp patterns; AMD: wavefront 64).\n    - Shared memory vs. LDS vs. vector register tiling decisions are adjusted per backend.\n  - Threading model mapping:\n    - Triton “program instance” maps to a CUDA thread block (NVIDIA) or a workgroup (AMD). Within it, tiles are partitioned across warps/wavefronts and threads according to the target’s lane width and preferred vector widths.\n  - Matrix-core/vector ISA mapping:\n    - NVIDIA: tl.dot → WMMA/WGMMA (tensor cores) when shapes/precisions permit; otherwise vector-FMA on CUDA cores.\n    - AMD: tl.dot → MFMA/WMMA on CDNA/ROCm; otherwise VALU FMAs.\n    - CPU/other backends (experimental/third-party): map to AVX/AMX or generic vector loops when available.\n  - Sync and barriers:\n    - Inserted according to the backend’s memory model (e.g., __syncthreads for CUDA; s_barrier for AMD).\n  - Code generation:\n    - NVIDIA: Triton lowers to LLVM IR, then to NVPTX, producing PTX that the driver JITs to SASS.\n    - AMD: Lowers to LLVM AMDGPU, producing HSACO (HSA code object) for ROCm.\n    - Other targets would plug in via an LLVM backend or an MLIR-to-ISA path.\n\n- Autotuning per backend\n  - Same kernel source, different “good” parameters. The autotuner searches BLOCK_M/N/K, num_warps/wavefronts, stages (software pipelining), Split-K, and vector widths. The best configs differ across GPUs (e.g., A100 vs. H100 vs. MI300).\n  - Backend can expose additional knobs (e.g., swizzled layouts, LDS bank-friendly shapes) without changing your kernel logic.\n\nWhat makes this work across vendors\n\n- Abstractions match common GPU realities:\n  - Tiles, masks, and memory scopes map naturally to CUDA and ROCm hierarchies.\n  - Triton avoids vendor-specific intrinsics in user code; specialization happens in the backend.\n- Uniform programming model:\n  - Indexing via program_id(axis) instead of blockIdx.x or workgroup_id ensures portability.\n  - Masked operations avoid backend-specific branch patterns and map to lane predication.\n- LLVM/MLIR foundation:\n  - Leveraging LLVM means Triton can target any ISA with a maintained LLVM backend.\n  - Increasing MLIR usage allows clean separation of dialects and patterns per target.\n\nCurrent backends and status (high level)\n\n- NVIDIA CUDA (mature): PTX codegen with tensor-core support (WMMA/WGMMA), extensive autotuning recipes, integration with PyTorch Inductor.\n- AMD ROCm (mature): AMDGPU codegen with MFMA/WMMA lowering, LDS-aware tiling, supported by PyTorch Inductor ROCm builds.\n- Others (experimental/community): There have been efforts toward CPU and other accelerators via MLIR/LLVM; not production-grade compared to CUDA/ROCm.\n\nPractical portability tips\n\n- Write kernels in terms of tiles and masks; avoid assumptions about warp size or shared memory banks.\n- Keep meta-parameters abstract; let the autotuner pick per-device values.\n- Use tl.dot/tl.load/tl.store and built-ins; avoid manual per-lane shuffles unless using portable helpers.\n- Test on each backend with profilers (Nsight Compute, ROCm profilers) to confirm vectorization, coalescing, and matrix-core utilization.\n\nIn short: Triton is portable because your code lives at a tile/mask abstraction level, while backend-specific passes map that to each GPU’s threading, memory, and matrix-core instructions via LLVM-based codegen, with autotuning to pick optimal parameters per device.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d43c7766408195874058bd412f26e806509f10d3502fdf"}},"resumableStreamId":"STREAM:3899ab9a-93a8-4c5c-8f27-ba18b588e496","role":"assistant","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","timeToFirstToken":1.212,"tokens":1057,"tokensPerSecond":52.91349619543452,"updated_at":1758739574698,"userId":"google:104458561188320708185","id":"j97d3a56n738gnm485tfdjybv97r7ffd"},{"_creationTime":1758739657723.2378,"_id":"j97fa4em7crx1nhhdgzs9cjb6h7r6fwx","attachmentIds":[],"content":"how is triton compatible with various different GPU configurations","created_at":1758739657723,"messageId":"c4b06f9d-323f-449a-ad0d-7c861b2f23f3","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","updated_at":1758739657723,"userId":"google:104458561188320708185","id":"j97fa4em7crx1nhhdgzs9cjb6h7r6fwx","providerMetadata":{}},{"_creationTime":1758739657723.238,"_id":"j9778g3xf7hpkg184aypxftexn7r7ns6","attachmentIds":[],"byok":false,"content":"Triton adapts to “which GPU you have” through a mix of hardware-agnostic kernel code, backend-specific lowering, and per-device autotuning. You write one tile-centric kernel; Triton specializes it at JIT time for the exact GPU configuration.\n\nKey pieces that make it work across different GPU configurations\n\n- Device discovery and specialization at JIT time\n  - On first launch, Triton queries the GPU (compute capability/architecture, SM/CU count, warp/wave size, shared memory/LDS size, register file size, max threads per block, supported math precisions, tensor-core availability).\n  - The compiler picks legal launch parameters and lowers your kernel to ISA (PTX→SASS for NVIDIA; HSACO for AMD) tailored to those limits.\n\n- Tile-centric, parameterized kernels\n  - You express work in terms of tiles and masks, not fixed warp sizes or bank layouts.\n  - Performance-critical choices (BLOCK_M/N/K, num_warps/wavefronts, stages, vector width, Split-K) are meta-parameters that can be tuned per GPU.\n\n- Autotuning per GPU and per problem shape\n  - Triton ships an autotuner that sweeps configurations and caches the winner keyed by:\n    - GPU model/arch and driver/runtime version\n    - Kernel name and data types\n    - Problem sizes/strides\n  - This lets the same source code run near-optimally on, say, RTX 4090, A100, H100, MI250, MI300, each with different best configs.\n\n- Backend-specific lowering for each architecture\n  - NVIDIA:\n    - Maps tiles to warps of 32, handles coalescing rules, shared-memory bank conflicts, and emits WMMA/WGMMA instructions when shapes/precisions match tensor cores.\n    - Respects per-SM limits (registers, shared memory) to choose occupancy-friendly tiling.\n  - AMD:\n    - Maps tiles to 64-lane wavefronts, optimizes LDS usage, and lowers tl.dot to MFMA/WMMA.\n    - Balances VGPR/LDS pressure to keep wave occupancy high.\n  - Both:\n    - Insert appropriate barriers, predication for edges, vectorized global loads/stores (e.g., 128-bit), and software pipelining to overlap memory/compute.\n\n- Dynamic launch configuration\n  - You specify a grid in terms of problem size (e.g., grid = (ceil_div(M, BLOCK_M), ceil_div(N, BLOCK_N))). The compiler/runtime computes valid block sizes and shared memory requirements for the device.\n  - If a chosen config exceeds device limits (e.g., too much shared memory per block), Triton falls back to a different candidate during tuning.\n\n- Precision and feature negotiation\n  - The compiler checks which precisions are fast on the device (FP16/BF16/FP8, int8) and whether matrix cores are available. It picks tensor-core paths when possible, falls back to scalar/vector FMAs otherwise.\n  - On mixed fleets, you can keep one kernel and let each GPU pick its best path.\n\n- Robust masking and boundary handling\n  - Masked loads/stores and predication ensure the same kernel handles arbitrary sizes (not just multiples of the tile), without device-specific branching quirks.\n\n- Caching and reproducibility\n  - Compiled binaries are cached per GPU architecture and kernel config, avoiding repeated JIT costs and ensuring consistent performance once tuned.\n\nWhat “different GPU configurations” this covers\n\n- Different architectures and vendors\n  - NVIDIA: Turing, Ampere, Hopper, etc.\n  - AMD: CDNA2 (MI200), CDNA3 (MI300), etc.\n- Different SM/CU counts and clock/boost settings\n  - Autotuning chooses tile sizes that balance register/shared memory pressure vs. occupancy for each model.\n- Different memory hierarchies and sizes\n  - Varying L2 size, shared/LDS per block, HBM bandwidth: the backend chooses coalescing patterns, LDS tiling, and staging depth accordingly.\n- Different math units and supported types\n  - Presence/absence of tensor cores, MFMA variants, FP8 support—all negotiated by the lowering and reflected in tl.dot implementations.\n\nPractical tips to stay compatible and performant\n\n- Keep kernels parameterized; don’t hard-code warp size or shared memory assumptions.\n- Use tl.load/tl.store with masks and contiguous vectorized access patterns; let the backend coalesce.\n- Use tl.dot for matrix multiplies so the backend can pick tensor-core/MFMA paths.\n- Include an autotune decorator with a reasonable sweep of BLOCK_{M,N,K}, num_warps/wavefronts, stages, and Split-K.\n- Cache tuning results and consider per-shape tuning in production.\n- Profile on each target GPU (Nsight Compute for NVIDIA, ROCm profilers for AMD) to verify occupancy, vectorization, and matrix-core utilization.\n\nBottom line\n- Triton achieves cross-GPU compatibility by separating portable, tile-based kernel logic from target-specific code generation and by autotuning per device. The same source kernel can run efficiently across many GPU configurations, with the compiler and tuner adapting tile sizes, threading, memory staging, and instruction selection to each GPU’s characteristics.","created_at":1758739657724,"messageId":"37d8024a-803e-444f-bae2-eac619e1bffa","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Triton adapts to “which GPU you have” through a mix of hardware-agnostic kernel code, backend-specific lowering, and per-device autotuning. You write one tile-centric kernel; Triton specializes it at JIT time for the exact GPU configuration.\n\nKey pieces that make it work across different GPU configurations\n\n- Device discovery and specialization at JIT time\n  - On first launch, Triton queries the GPU (compute capability/architecture, SM/CU count, warp/wave size, shared memory/LDS size, register file size, max threads per block, supported math precisions, tensor-core availability).\n  - The compiler picks legal launch parameters and lowers your kernel to ISA (PTX→SASS for NVIDIA; HSACO for AMD) tailored to those limits.\n\n- Tile-centric, parameterized kernels\n  - You express work in terms of tiles and masks, not fixed warp sizes or bank layouts.\n  - Performance-critical choices (BLOCK_M/N/K, num_warps/wavefronts, stages, vector width, Split-K) are meta-parameters that can be tuned per GPU.\n\n- Autotuning per GPU and per problem shape\n  - Triton ships an autotuner that sweeps configurations and caches the winner keyed by:\n    - GPU model/arch and driver/runtime version\n    - Kernel name and data types\n    - Problem sizes/strides\n  - This lets the same source code run near-optimally on, say, RTX 4090, A100, H100, MI250, MI300, each with different best configs.\n\n- Backend-specific lowering for each architecture\n  - NVIDIA:\n    - Maps tiles to warps of 32, handles coalescing rules, shared-memory bank conflicts, and emits WMMA/WGMMA instructions when shapes/precisions match tensor cores.\n    - Respects per-SM limits (registers, shared memory) to choose occupancy-friendly tiling.\n  - AMD:\n    - Maps tiles to 64-lane wavefronts, optimizes LDS usage, and lowers tl.dot to MFMA/WMMA.\n    - Balances VGPR/LDS pressure to keep wave occupancy high.\n  - Both:\n    - Insert appropriate barriers, predication for edges, vectorized global loads/stores (e.g., 128-bit), and software pipelining to overlap memory/compute.\n\n- Dynamic launch configuration\n  - You specify a grid in terms of problem size (e.g., grid = (ceil_div(M, BLOCK_M), ceil_div(N, BLOCK_N))). The compiler/runtime computes valid block sizes and shared memory requirements for the device.\n  - If a chosen config exceeds device limits (e.g., too much shared memory per block), Triton falls back to a different candidate during tuning.\n\n- Precision and feature negotiation\n  - The compiler checks which precisions are fast on the device (FP16/BF16/FP8, int8) and whether matrix cores are available. It picks tensor-core paths when possible, falls back to scalar/vector FMAs otherwise.\n  - On mixed fleets, you can keep one kernel and let each GPU pick its best path.\n\n- Robust masking and boundary handling\n  - Masked loads/stores and predication ensure the same kernel handles arbitrary sizes (not just multiples of the tile), without device-specific branching quirks.\n\n- Caching and reproducibility\n  - Compiled binaries are cached per GPU architecture and kernel config, avoiding repeated JIT costs and ensuring consistent performance once tuned.\n\nWhat “different GPU configurations” this covers\n\n- Different architectures and vendors\n  - NVIDIA: Turing, Ampere, Hopper, etc.\n  - AMD: CDNA2 (MI200), CDNA3 (MI300), etc.\n- Different SM/CU counts and clock/boost settings\n  - Autotuning chooses tile sizes that balance register/shared memory pressure vs. occupancy for each model.\n- Different memory hierarchies and sizes\n  - Varying L2 size, shared/LDS per block, HBM bandwidth: the backend chooses coalescing patterns, LDS tiling, and staging depth accordingly.\n- Different math units and supported types\n  - Presence/absence of tensor cores, MFMA variants, FP8 support—all negotiated by the lowering and reflected in tl.dot implementations.\n\nPractical tips to stay compatible and performant\n\n- Keep kernels parameterized; don’t hard-code warp size or shared memory assumptions.\n- Use tl.load/tl.store with masks and contiguous vectorized access patterns; let the backend coalesce.\n- Use tl.dot for matrix multiplies so the backend can pick tensor-core/MFMA paths.\n- Include an autotune decorator with a reasonable sweep of BLOCK_{M,N,K}, num_warps/wavefronts, stages, and Split-K.\n- Cache tuning results and consider per-shape tuning in production.\n- Profile on each target GPU (Nsight Compute for NVIDIA, ROCm profilers for AMD) to verify occupancy, vectorization, and matrix-core utilization.\n\nBottom line\n- Triton achieves cross-GPU compatibility by separating portable, tile-based kernel logic from target-specific code generation and by autotuning per device. The same source kernel can run efficiently across many GPU configurations, with the compiler and tuner adapting tile sizes, threading, memory staging, and instruction selection to each GPU’s characteristics.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d43cca72e0819186f878c6461bf07c0a2e04c5cc043eae"}},"resumableStreamId":"STREAM:173f12b1-4fdb-45ee-8f7b-9207755dff40","role":"assistant","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","timeToFirstToken":0.998,"tokens":1095,"tokensPerSecond":48.751168692400164,"updated_at":1758739657724,"userId":"google:104458561188320708185","id":"j9778g3xf7hpkg184aypxftexn7r7ns6"},{"_creationTime":1758739699678.791,"_id":"j978ptzxhq7h1bxd88vdggyban7r687a","attachmentIds":[],"content":"Based on the paper, how well does Triton compare to hand-written CUDA kernels?","created_at":1758739699678,"messageId":"856a7c34-add5-46b8-ac00-9a6b0f9f5b38","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","updated_at":1758739699678,"userId":"google:104458561188320708185","id":"j978ptzxhq7h1bxd88vdggyban7r687a","providerMetadata":{}},{"_creationTime":1758739699678.7913,"_id":"j977fw8xjz3sprgjv520khth0h7r7sq3","attachmentIds":[],"byok":false,"content":"Speaking as the paper’s author, and sticking to what we measured/reported in the Triton (MAPL/PLDI’19) paper:\n\nWhat we compared against\n- We primarily compared Triton-generated kernels to:\n  - Vendor libraries (cuBLAS/cuDNN) for dense matmul/implicit-GEMM conv, since these are effectively the gold-standard hand-tuned CUDA.\n  - Contemporary DSL/auto-tuning systems (e.g., TVM/AutoTVM, Tensor Comprehensions, PlaidML).\n- For new or unusual operators (e.g., shift convolution, fused epilogues), we contrasted Triton to reasonable CUDA baselines when available; otherwise we showed absolute throughput to illustrate feasibility and speed.\n\nKey takeaways from the paper’s results\n- Matmul (GEMM):\n  - Triton kernels matched cuBLAS across a wide range of matrix shapes relevant to RNN/Transformer workloads when properly tuned (tile sizes TM/TN/TK and unroll).\n  - Against other DSLs of the time, Triton was often substantially faster, narrowing the gap to vendor-tuned CUDA to within a few percent on many cases.\n- Convolution (implicit-GEMM style):\n  - By expressing the same tiling strategy directly in Triton, we achieved performance on par with cuDNN’s implicit-GEMM for the tested conv shapes.\n- Non-standard/fused ops:\n  - Triton shined when the operator deviated from library templates. For shift convolution and fused epilogues (e.g., bias/add/ReLU folded into GEMM), Triton delivered strong performance without dropping to hand-written CUDA, whereas achieving the same in CUDA typically requires bespoke kernels.\n- Productivity vs. performance:\n  - The central result is not merely raw speed but the combination: kernels written at tile granularity in Triton were shorter, easier to reason about, and still reached near hand-optimized CUDA/library performance.\n\nWhere hand-written CUDA can still have an edge (per the paper’s scope)\n- Micro-architecture exploits beyond our 2019 backend:\n  - At the time, the paper did not target tensor-core intrinsics directly; an expert CUDA kernel that uses the latest specialized instructions can exceed our baseline FMA pipelines.\n- Extremely specialized kernels:\n  - For a very specific shape or architecture, a domain expert can still out-tune a general tile compiler by hand-picking lane mappings, double-buffering patterns, or warp-specialized tricks not yet automated.\n\nWhy Triton closes most of the gap\n- Tiles are first-class in the IR, so the compiler can:\n  - Generate coalesced global accesses and shared-memory staging “by construction.”\n  - Insert predication for edges without control-flow divergence penalties.\n  - Balance register pressure and occupancy via tile-size meta-parameters that an autotuner sweeps.\n- This captures most of what expert CUDA authors do—without manual index/lane choreography.\n\nBottom line from the paper\n- On mainstream dense kernels (matmul, implicit-GEMM conv), Triton achieves performance comparable to hand-tuned CUDA as embodied by cuBLAS/cuDNN, while greatly simplifying kernel development.\n- On custom or fused operators outside library coverage, Triton often delivers high performance with far less effort than writing bespoke CUDA, which is where it provides the biggest practical win.","created_at":1758739699679,"messageId":"98224603-cdff-4cc1-a2d4-9d32b9f7421f","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Speaking as the paper’s author, and sticking to what we measured/reported in the Triton (MAPL/PLDI’19) paper:\n\nWhat we compared against\n- We primarily compared Triton-generated kernels to:\n  - Vendor libraries (cuBLAS/cuDNN) for dense matmul/implicit-GEMM conv, since these are effectively the gold-standard hand-tuned CUDA.\n  - Contemporary DSL/auto-tuning systems (e.g., TVM/AutoTVM, Tensor Comprehensions, PlaidML).\n- For new or unusual operators (e.g., shift convolution, fused epilogues), we contrasted Triton to reasonable CUDA baselines when available; otherwise we showed absolute throughput to illustrate feasibility and speed.\n\nKey takeaways from the paper’s results\n- Matmul (GEMM):\n  - Triton kernels matched cuBLAS across a wide range of matrix shapes relevant to RNN/Transformer workloads when properly tuned (tile sizes TM/TN/TK and unroll).\n  - Against other DSLs of the time, Triton was often substantially faster, narrowing the gap to vendor-tuned CUDA to within a few percent on many cases.\n- Convolution (implicit-GEMM style):\n  - By expressing the same tiling strategy directly in Triton, we achieved performance on par with cuDNN’s implicit-GEMM for the tested conv shapes.\n- Non-standard/fused ops:\n  - Triton shined when the operator deviated from library templates. For shift convolution and fused epilogues (e.g., bias/add/ReLU folded into GEMM), Triton delivered strong performance without dropping to hand-written CUDA, whereas achieving the same in CUDA typically requires bespoke kernels.\n- Productivity vs. performance:\n  - The central result is not merely raw speed but the combination: kernels written at tile granularity in Triton were shorter, easier to reason about, and still reached near hand-optimized CUDA/library performance.\n\nWhere hand-written CUDA can still have an edge (per the paper’s scope)\n- Micro-architecture exploits beyond our 2019 backend:\n  - At the time, the paper did not target tensor-core intrinsics directly; an expert CUDA kernel that uses the latest specialized instructions can exceed our baseline FMA pipelines.\n- Extremely specialized kernels:\n  - For a very specific shape or architecture, a domain expert can still out-tune a general tile compiler by hand-picking lane mappings, double-buffering patterns, or warp-specialized tricks not yet automated.\n\nWhy Triton closes most of the gap\n- Tiles are first-class in the IR, so the compiler can:\n  - Generate coalesced global accesses and shared-memory staging “by construction.”\n  - Insert predication for edges without control-flow divergence penalties.\n  - Balance register pressure and occupancy via tile-size meta-parameters that an autotuner sweeps.\n- This captures most of what expert CUDA authors do—without manual index/lane choreography.\n\nBottom line from the paper\n- On mainstream dense kernels (matmul, implicit-GEMM conv), Triton achieves performance comparable to hand-tuned CUDA as embodied by cuBLAS/cuDNN, while greatly simplifying kernel development.\n- On custom or fused operators outside library coverage, Triton often delivers high performance with far less effort than writing bespoke CUDA, which is where it provides the biggest practical win.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d43cf4984c819699c6ff07e562f4ee0e56af967f50443f"}},"resumableStreamId":"STREAM:21d8bd9d-3585-4ef5-a19b-dbe0a395055c","role":"assistant","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","timeToFirstToken":2.652,"tokens":686,"tokensPerSecond":49.591556423046335,"updated_at":1758739699679,"userId":"google:104458561188320708185","id":"j977fw8xjz3sprgjv520khth0h7r7sq3"},{"_creationTime":1758739786463.1428,"_id":"j9748wv0cxa13e5r1x2mhkgynh7r6efb","attachmentIds":[],"content":"For modern LLMs, how well is triton integrated with the toolchain","created_at":1758739786463,"messageId":"55720036-ca8f-4d46-9a7c-a0ccf8dc4d41","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","updated_at":1758739786463,"userId":"google:104458561188320708185","id":"j9748wv0cxa13e5r1x2mhkgynh7r6efb","providerMetadata":{}},{"_creationTime":1758739786463.143,"_id":"j97bt7axk00hmr9tcbj5saned17r76cr","attachmentIds":[],"byok":false,"content":"Very well integrated. For modern LLM training and inference stacks, Triton is “in the loop” at multiple layers, and you benefit from it even if you never write a Triton kernel yourself.\n\nWhere Triton shows up today\n\n- PyTorch 2.x training/inference\n  - TorchInductor: When you call torch.compile, Inductor lowers many fused pointwise/reduction/matmul-style regions to Triton GPU kernels. This is the default path on NVIDIA and AMD, so LLM training often runs through Triton-generated kernels for epilogues, layernorms, softmax blocks, small GEMMs, etc.\n  - Custom kernels: Many projects ship Triton kernels for attention variants, normalization, dequant, rotary embeddings, sampling, etc., so they plug directly into PyTorch modules.\n\n- Attention implementations\n  - FlashAttention: Has CUDA implementations, but there are widely used Triton ports/tutorials and research forks; PyTorch Inductor also emits Triton for fused SDPA patterns when possible. Community Triton kernels exist for causal, masked, and block-sparse variants.\n  - Emerging kernels: Triton is often the first place researchers prototype new attention patterns or KV-cache layouts because iteration speed is high.\n\n- Quantization and dequantization\n  - GPTQ/AutoGPTQ: Several teams have optimized Triton dequant kernels for 4-bit/8-bit weight formats and fused dequant+GEMM epilogues; these are used in open-source LLM inference repos.\n  - FP8/BF16/FP16: Triton’s tl.dot can target tensor/matrix-core paths (WMMA/WGMMA on NVIDIA; MFMA on AMD), so mixed-precision training/inference benefits automatically when kernels are written in Triton.\n\n- Frameworks and runtimes\n  - vLLM: Core engine is CUDA/C++/PyTorch, but it interoperates fine with PyTorch modules containing Triton-generated kernels (from Inductor or custom ops).\n  - PyTorch/XFormers/Lightning: Many examples and modules rely on Triton kernels for fused ops; these are routine in LLM training scripts.\n  - AMD ROCm: Triton is a first-class backend in PyTorch ROCm builds; the same Triton kernels run on MI200/MI300 using AMD’s MFMA paths.\n  - NVIDIA Blackwell/Hopper: Triton supports new tensor-core paths (WGMMA on Hopper; Blackwell tensor cores) and new precisions (e.g., MXFP8/MXFP4) so existing tl.dot-based kernels pick up speedups without code changes.\n\n- Serving stacks (NVIDIA Triton Inference Server vs. OpenAI Triton compiler)\n  - NVIDIA Triton Inference Server (the model server) commonly hosts engines built with TensorRT-LLM or vLLM. Those engines coexist with PyTorch components that include Triton kernels, but the server itself doesn’t require you to use Triton kernels.\n  - If you serve via pure PyTorch backends or Python backends, any Inductor/Triton kernels inside your model run as usual.\n\nWhat this means for modern LLM workflows\n\n- Training\n  - Out of the box, torch.compile will fuse many transformer subgraphs into Triton kernels, improving throughput without manual CUDA.\n  - If you have a hotspot (e.g., custom attention mask, fused norm+activation), writing a small Triton kernel is often enough to reach near-library performance and keep portability across NVIDIA/AMD.\n\n- Inference\n  - If you use TensorRT-LLM, most heavy ops come from TRT kernels; Triton is still useful for pre/post ops and custom layers in PyTorch fallback paths.\n  - If you use vLLM or eager PyTorch, Triton kernels from Inductor/custom code handle many fused ops, dequant, layernorms, small matmuls, etc.\n  - Use CUDA Graphs or runtime graph capture to amortize Python/Triton JIT overhead in short kernels; cache JIT artifacts in production.\n\n- Portability and hardware coverage\n  - One kernel source runs on NVIDIA and AMD; autotuning picks the best tile sizes per GPU (A100/H100/H200/Blackwell; MI200/MI300).\n  - New hardware features land quickly: Triton already maps tl.dot to WGMMA on Hopper and new tensor cores on Blackwell; on AMD it lowers to MFMA. This matters directly for LLM matmuls and attention.\n\nCurrent limits to keep in mind\n\n- Best-in-class vendor primitives (e.g., TensorRT-LLM fused attention/GEMM) can still be faster than generic Triton for some shapes.\n- You may need autotuning of BLOCK_M/N/K, num_warps/waves, and pipelining for peak perf per model/GPU.\n- Extremely new hardware features (special copy engines, cluster scheduling) may arrive in vendor libraries first; Triton support usually follows.\n\nBottom line\n\n- Integration: Deep. PyTorch Inductor generates Triton for lots of transformer kernels by default; many popular LLM repos ship Triton kernels; AMD and NVIDIA backends are both supported.\n- Practical payoff: You get strong performance without hand-written CUDA, quick iteration for custom/fused ops, and portability across NVIDIA/AMD—ideal for modern LLM training and for inference paths that aren’t fully covered by vendor engines.","created_at":1758739786464,"messageId":"d91de1d1-a031-4a1c-9beb-cde41cb1f920","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"news","domains":null,"queries":["2025 Triton integration LLM toolchain PyTorch Inductor TorchInductor Triton 3 ROCm 7 H100 H200 Blackwell support","FlashAttention Triton 2025 integration PyTorch 2.4 2.5","TensorRT-LLM Triton integration 2025","OpenAI Triton usage GPT inference 2024 2025"]},"result":[{"content":"[Discord](https://pxl.to/ivxz41s)[Linkedin](https://www.linkedin.com/company/marktechpost/?viewAsMember=true)[Reddit](https://www.reddit.com/r/machinelearningnews/)[Twitter](https://twitter.com/Marktechpost)\n\n- [Home](https://www.marktechpost.com/)\n- [Open Source/Weights](https://www.marktechpost.com/category/technology/open-source/)\n- [Enterprise AI](https://www.marktechpost.com/category/editors-pick/enterprise-ai/)\n- [Robotics](https://www.marktechpost.com/category/robotics/)\n- [AI Agents](https://www.marktechpost.com/category/editors-pick/ai-agents/)\n- [MCP](https://www.marktechpost.com/category/editors-pick/model-context-protocol-mcp/)\n- [Tutorials](https://www.marktechpost.com/category/tutorials/)\n- [Voice AI](https://www.marktechpost.com/category/technology/artificial-intelligence/voice-ai/)\n- [Sponsorship](https://calendly.com/marktechpost/marktechpost-promotion-call)\n\nSearch\n\n[NewsHub](https://www.marktechpost.com/)\n\n[NewsHub](https://www.marktechpost.com/)\n\n[Premium Content](https://www.marktechpost.com/www.marktechpost.com)\n\n[Read our exclusive articles](https://www.marktechpost.com/www.marktechpost.com)\n\n[Facebook](https://www.marktechpost.com/www.marktechpost.com)\n\n[Instagram](https://www.marktechpost.com/www.marktechpost.com)\n\n[Twitter](https://www.marktechpost.com/www.marktechpost.com)\n\n- [Home](https://www.marktechpost.com/)\n- [Open Source/Weights](https://www.marktechpost.com/category/technology/open-source/)\n- [Enterprise AI](https://www.marktechpost.com/category/editors-pick/enterprise-ai/)\n- [Robotics](https://www.marktechpost.com/category/robotics/)\n- [AI Agents](https://www.marktechpost.com/category/editors-pick/ai-agents/)\n- [MCP](https://www.marktechpost.com/category/editors-pick/model-context-protocol-mcp/)\n- [Tutorials](https://www.marktechpost.com/category/tutorials/)\n- [Voice AI](https://www.marktechpost.com/category/technology/artificial-intelligence/voice-ai/)\n- [Sponsorship](https://calendly.com/marktechpost/marktechpost-promotion-call)\n\n[NewsHub](https://www.marktechpost.com/)\n\nSearch\n\n- [Home](https://www.marktechpost.com/)\n- [Open Source/Weights](https://www.marktechpost.com/category/technology/open-source/)\n- [Enterprise AI](https://www.marktechpost.com/category/editors-pick/enterprise-ai/)\n- [Robotics](https://www.marktechpost.com/category/robotics/)\n- [AI Agents](https://www.marktechpost.com/category/editors-pick/ai-agents/)\n- [MCP](https://www.marktechpost.com/category/editors-pick/model-context-protocol-mcp/)\n- [Tutorials](https://www.marktechpost.com/category/tutorials/)\n- [Voice AI](https://www.marktechpost.com/category/technology/artificial-intelligence/voice-ai/)\n- [Sponsorship](https://calendly.com/marktechpost/marktechpost-promotion-call)\n\n[Home](https://www.marktechpost.com/)[Artificial Intelligence](https://www.marktechpost.com/category/technology/artificial-intelligence/)[AI Infrastructure](https://www.marktechpost.com/category/technology/artificial-intelligence/ai-infrastructure/)Software Frameworks Optimized for GPUs in AI: CUDA, ROCm, Triton, TensorRT—Compiler Paths...\n\n- [Artificial Intelligence](https://www.marktechpost.com/category/technology/artificial-intelligence/)\n- [AI Infrastructure](https://www.marktechpost.com/category/technology/artificial-intelligence/ai-infrastructure/)\n- [Technology](https://www.marktechpost.com/category/technology/)\n- [AI Shorts](https://www.marktechpost.com/category/technology/ai-shorts/)\n- [Editors Pick](https://www.marktechpost.com/category/editors-pick/)\n- [Staff](https://www.marktechpost.com/category/editors-pick/staff/)\n- [Tech News](https://www.marktechpost.com/category/tech-news/)\n\n### **Table of contents**\n\n- [What actually determines performance on modern GPUs](https://www.marktechpost.com/www.marktechpost.com#h-what-actually-determines-performance-on-modern-gpus)\n- [CUDA: nvcc/ptxas, cuDNN, CUTLASS, and CUDA Graphs](https://www.marktechpost.com/www.marktechpost.com#h-cuda-nvcc-ptxas-cudnn-cutlass-and-cuda-graphs)\n- [ROCm: HIP/Clang toolchain, rocBLAS/MIOpen, and the 6.x series](https://www.marktechpost.com/www.marktechpost.com#h-rocm-hip-clang-toolchain-rocblas-miopen-and-the-6-x-series)\n- [Triton: a DSL and compiler for custom kernels](https://www.marktechpost.com/www.marktechpost.com#h-triton-a-dsl-and-compiler-for-custom-kernels)\n- [TensorRT (and TensorRT-LLM): builder-time graph optimization for inference](https://www.marktechpost.com/www.marktechpost.com#h-tensorrt-and-tensorrt-llm-builder-time-graph-optimization-for-inference)\n- [Practical guidance: choosing and tuning the stack](https://www.marktechpost.com/www.marktechpost.com#h-practical-guidance-choosing-and-tuning-the-stack)\n\nDeep-learning throughput hinges on how effectively a compiler stack maps tensor programs to GPU execution: thread/block schedules, memory movement, and instruction selection (e.g., Tensor Core MMA pipelines). In this article we will focus on four dominant stacks—CUDA, ROCm, Triton, and TensorRT—from the compiler’s perspective and explains which optimizations move the needle in practice.\n\n### **What actually determines performance on modern GPUs**\n\n**Across vendors, the same levers recur:**\n\n- **Operator scheduling & fusion:** reduce kernel launches and round-trips to HBM; expose longer producer→consumer chains for register/shared-memory reuse. TensorRT and cuDNN “runtime fusion engines” exemplify this for attention and conv blocks.\n- **Tiling & data layout:** match tile shapes to Tensor Core/WGMMA/WMMA native fragment sizes; avoid shared-memory bank conflicts and partition camping. CUTLASS documents warp-level GEMM tiling for both Tensor Cores and CUDA cores.\n- **Precision & quantization:** FP16/BF16/FP8 for training/inference; INT8/INT4 (calibrated or QAT) for inference. TensorRT automates calibration and kernel selection under these precisions.\n- **Graph capture & runtime specialization:** graph execution to amortize launch overheads; dynamic fusion of common subgraphs (e.g., attention). cuDNN 9 added graph support for attention fusion engines.\n- **Autotuning:** search tile sizes, unroll factors, and pipelining depths per arch/SKU. Triton and CUTLASS expose explicit autotune hooks; TensorRT performs builder-time tactic selection.\n\nWith that lens, here’s how each stack implements the above.\n\n### **CUDA: nvcc/ptxas, cuDNN, CUTLASS, and CUDA Graphs**\n\n**Compiler path.** CUDA code compiles through **nvcc** into PTX, then **ptxas** lowers PTX to SASS (arch-specific machine code). Controlling optimization requires feeding flags to both host and device phases; for kernels the key is `-Xptxas`. Developers often miss that `-O3` alone affects only host code.\n\n**Kernel generation & libraries.**\n\n- **CUTLASS** provides parametric templates for GEMM/conv, implementing warp-level tiling, Tensor Core MMA pipelines, and smem iterators designed for conflict-free access—canonical references for writing peak kernels, including Hopper’s **WGMMA** path.\n- **cuDNN 9** introduced runtime fusion engines (notably for attention blocks), native CUDA Graph integration for those engines, and updates for new compute capabilities—materially reducing dispatch overheads and improving memory locality in Transformer workloads.\n\n**Performance implications.**\n\n- Moving from unfused PyTorch ops to cuDNN attention fusion typically cuts kernel launches and global memory traffic; combined with **CUDA Graphs**, it reduces CPU bottlenecks in short-sequence inference.\n- On Hopper/Blackwell, aligning tile shapes to WGMMA/Tensor Core native sizes is decisive; CUTLASS tutorials quantify how mis-sized tiles waste tensor-core throughput.\n\n**When CUDA is the right tool.** You need maximum control over instruction selection, occupancy, and smem choreography; or you’re extending kernels beyond library coverage while staying on NVIDIA GPUs.\n\n### **ROCm: HIP/Clang toolchain, rocBLAS/MIOpen, and the 6.x series**\n\n**Compiler path.** ROCm uses **Clang/LLVM** to compile **HIP** (CUDA-like) into GCN/RDNA ISA. The 6.x series has focused on perf and framework coverage; release notes track component-level optimizations and HW/OS support.\n\n**Libraries and kernels.**\n\n- **rocBLAS** and **MIOpen** implement GEMM/conv primitives with arch-aware tiling and algorithm selection similar in spirit to cuBLAS/cuDNN. The consolidated changelog highlights iterative perf work across these libraries.\n- Recent ROCm workstream includes better **Triton** enablement on AMD GPUs, enabling Python-level kernel authoring while still lowering through LLVM to AMD backends.\n\n**Performance implications.**\n\n- On AMD GPUs, matching LDS (shared memory) bank widths and vectorized global loads to matrix tile shapes is as pivotal as smem bank alignment on NVIDIA. Compiler-assisted fusion in frameworks (e.g., attention) plus library autotuning in rocBLAS/MIOpen typically closes a large fraction of the gap to handwritten kernels, contingent on architecture/driver. Release documentation indicates continuous tuner improvements in 6.0–6.4.x.\n\n**When ROCm is the right tool.** You need native support and optimization on AMD accelerators, with HIP portability from existing CUDA-style kernels and a clear LLVM toolchain.\n\n### **Triton: a DSL and compiler for custom kernels**\n\n**Compiler path.** Triton is a Python-embedded DSL that lowers via **LLVM**; it handles vectorization, memory coalescing, and register allocation while giving explicit control over block sizes and program IDs. Build docs show the LLVM dependency and custom builds; NVIDIA’s developer materials discuss Triton’s tuning for newer architectures (e.g., Blackwell) with FP16/FP8 GEMM improvements.\n\n**Optimizations.**\n\n- **Autotuning** over tile sizes, `num_warps`, and pipelining stages; static **masking** for boundary conditions without scalar fallbacks; **shared-memory** staging and software pipelining to overlap global loads with compute.\n- Triton’s design aims to **automate** the error-prone parts of CUDA-level optimization while leaving block-level tiling choices to t","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.marktechpost.com%2F2025%2F09%2F14%2Fsoftware-frameworks-optimized-for-gpus-in-ai-cuda-rocm-triton-tensorrt-compiler-paths-and-performance-implications%2F","id":"https://www.marktechpost.com/2025/09/14/software-frameworks-optimized-for-gpus-in-ai-cuda-rocm-triton-tensorrt-compiler-paths-and-performance-implications/","publishedDate":"2025-09-14T00:00:00.000Z","summary":"This webpage from Marktechpost discusses software frameworks optimized for GPUs in AI, including CUDA, ROCm, Triton, and TensorRT. It covers compiler paths and performance implications of these frameworks. The article also includes links to related categories like AI Infrastructure and Technology.\n","title":"Software Frameworks Optimized for GPUs in AI: CUDA, ROCm, Triton, TensorRT—Compiler Paths and Performance Implications","url":"https://www.marktechpost.com/2025/09/14/software-frameworks-optimized-for-gpus-in-ai-cuda-rocm-triton-tensorrt-compiler-paths-and-performance-implications/"},{"content":"[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[triton-inference-server](https://github.com/triton-inference-server)/ **[server](https://github.com/triton-inference-server/server)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Ftriton-inference-server%2Fserver) You must be signed in to change notification settings\n- [Fork\\\n1.6k](https://github.com/login?return_to=%2Ftriton-inference-server%2Fserver)\n- [Star\\\n9.8k](https://github.com/login?return_to=%2Ftriton-inference-server%2Fserver)\n\n\n# Does NVIDIA Triton support the new TensorRt-LLM PyTorch workflow?\\#8388\n\n[New issue](https://github.com/login?return_to=)\n\nCopy link\n\n[New issue](https://github.com/login?return_to=)\n\nCopy link\n\nOpen\n\nOpen\n\n[Does NVIDIA Triton support the new TensorRt-LLM PyTorch workflow?](https://github.com/github.com#top)#8388\n\nCopy link\n\n## Description\n\n[JoJoLev](https://github.com/JoJoLev)\n\nopened [on Sep 11, 2025](https://github.com/triton-inference-server/server/issues/8388#issue-3407102881)\n\nIssue body actions\n\nTensorRT-LLM has shifted to a torch backend. Is this easily integrated with Triton?\nWhat is the plan for support for this?\n\n## Metadata\n\n## Metadata\n\n### Assignees\n\nNo one assigned\n\n### Labels\n\nNo labels\n\nNo labels\n\n### Type\n\nNo type\n\n### Projects\n\nNo projects\n\n### Milestone\n\nNo milestone\n\n### Relationships\n\nNone yet\n\n### Development\n\nNo branches or pull requests\n\n## Issue actions\n\nYou can’t perform that action at this time.","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fgithub.com%2Ftriton-inference-server%2Fserver%2Fissues%2F8388","id":"https://github.com/triton-inference-server/server/issues/8388","publishedDate":"2025-09-11T00:00:00.000Z","summary":"This webpage is a GitHub issue discussing whether NVIDIA Triton supports the new TensorRT-LLM PyTorch workflow. The issue, opened on September 11, 2025, asks about the integration of TensorRT-LLM's new torch backend with Triton and the plans for supporting it.\n","title":"Does NVIDIA Triton support the new TensorRt-LLM PyTorch workflow?","url":"https://github.com/triton-inference-server/server/issues/8388"},{"content":"[Skip to main content](https://rocm.docs.amd.com/rocm.docs.amd.com#main-content)\n\nBack to top\n\n`Ctrl` + `K`\n\n[Use ROCm on Radeon GPUs](https://rocm.docs.amd.com/index.html)\n\n# Install Triton for ROCm\n\n## Contents\n\n# Install Triton for ROCm [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#install-triton-for-rocm)\n\nOpenAI has developed a powerful GPU focused programming language and compiler called Triton that works seamlessly with AMD GPUs. The goal of Triton is to enable AI engineers and scientists to write high-performant GPU code with minimal expertise.\n\nTriton kernels are performant because of their blocked program representation, allowing them to be compiled into highly optimized binary code. Triton also leverages Python for kernel development, making it both familiar and accessible.\n\nThe kernels can be compiled by declaring the `triton.jit` python decorator before the kernel.\n\n## Pre-requisites [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#pre-requisites)\n\n- Compatible AMD GPU\n\n- Linux and ROCm 5.7+ is installed\n\n\n> See [Compatibility matrices](https://rocm.docs.amd.com/compatibility.html) for support information.\n\n## Install libraries [\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\#install-libraries)\n\nIf ROCm 6.0 and the latest version of PyTorch is not installed, the required libraries must first be installed.\nHowever, if you encounter issues running any of the commands, we recommend updating with the nightly wheels. This will also install the version of Triton that is compatible with PyTorch for ROCm.\n\n1. Enter the following command to install the libraries.\n\n\n\n```\npipinstallmatplotlibpandas-q\npipinstall--pretorchtorchvisiontorchaudio--index-urlhttps://download.pytorch.org/whl/nightly/rocm6.0/ -q\n```\n\n2. Enter the following command to import the libraries.\n\n\n\n```\nimporttorch\nimporttriton\nimporttriton.languageastl\n```\n\n\nNow, a Triton kernel that approximates the GELU (Gaussian Error Linear Unit) kernel using tanh can be developed.\n\nFor more information on how to develop a kernel for GELU and benchmark its performance with its PyTorch analogues, see [Developing Triton Kernels on AMD GPUs](https://rocm.blogs.amd.com/artificial-intelligence/triton/README.html).\n\nContents","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Frocm.docs.amd.com%2Fprojects%2Fradeon%2Fen%2Flatest%2Fdocs%2Finstall%2Fnative_linux%2Finstall-triton.html","id":"https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-triton.html","publishedDate":null,"summary":"This webpage provides instructions on how to install Triton for ROCm, a GPU programming language and compiler developed by OpenAI that works with AMD GPUs. It allows AI engineers and scientists to write high-performance GPU code with minimal expertise. The prerequisites include a compatible AMD GPU and Linux with ROCm 5.7+ installed. The page provides commands to install the necessary libraries, including PyTorch, and suggests using nightly wheels if encountering issues. It also links to a resource on developing Triton kernels on AMD GPUs.\n","title":"Install Triton for ROCm — Use ROCm on Radeon GPUs","url":"https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-triton.html"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdiscuss.pytorch.org%2Ft%2Fwhy-pytorch-use-triton-as-backend-of-inductor%2F221223","id":"https://discuss.pytorch.org/t/why-pytorch-use-triton-as-backend-of-inductor/221223","publishedDate":"2025-07-02T00:00:00.000Z","summary":"Failed to fetch content","title":"Why pytorch use triton as backend of inductor? - torch.compile","url":"https://discuss.pytorch.org/t/why-pytorch-use-triton-as-backend-of-inductor/221223"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.reddit.com%2Fr%2Fcomfyui%2Fcomments%2F1njjp79%2Fcomplete_rocm_70_pytorch_280_installation_guide%2F","id":"https://www.reddit.com/r/comfyui/comments/1njjp79/complete_rocm_70_pytorch_280_installation_guide/","publishedDate":"2025-09-17T18:49:52.555Z","summary":"Failed to fetch content","title":"Complete ROCm 7.0 + PyTorch 2.8.0 Installation Guide for RX 6900 ...","url":"https://www.reddit.com/r/comfyui/comments/1njjp79/complete_rocm_70_pytorch_280_installation_guide/"},{"content":"[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[Dao-AILab](https://github.com/Dao-AILab)/ **[flash-attention](https://github.com/Dao-AILab/flash-attention)** Public\n\n- [Notifications](https://github.com/login?return_to=%2FDao-AILab%2Fflash-attention) You must be signed in to change notification settings\n- [Fork\\\n2k](https://github.com/login?return_to=%2FDao-AILab%2Fflash-attention)\n- [Star\\\n19.6k](https://github.com/login?return_to=%2FDao-AILab%2Fflash-attention)\n\n\nFast and memory-efficient exact attention\n\n### License\n\n[BSD-3-Clause license](https://github.com/Dao-AILab/flash-attention/blob/main/LICENSE)\n\n[19.6k\\\nstars](https://github.com/Dao-AILab/flash-attention/stargazers) [2k\\\nforks](https://github.com/Dao-AILab/flash-attention/forks) [Branches](https://github.com/Dao-AILab/flash-attention/branches) [Tags](https://github.com/Dao-AILab/flash-attention/tags) [Activity](https://github.com/Dao-AILab/flash-attention/activity)\n\n[Star](https://github.com/login?return_to=%2FDao-AILab%2Fflash-attention)\n\n[Notifications](https://github.com/login?return_to=%2FDao-AILab%2Fflash-attention) You must be signed in to change notification settings\n\n# Dao-AILab/flash-attention\n\nmain\n\n[Branches](https://github.com/Dao-AILab/flash-attention/branches) [Tags](https://github.com/Dao-AILab/flash-attention/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit ## History [1,023 Commits](https://github.com/Dao-AILab/flash-attention/commits/main/) |\n| [.github/workflows](https://github.com/Dao-AILab/flash-attention/tree/main/.github/workflows) | [.github/workflows](https://github.com/Dao-AILab/flash-attention/tree/main/.github/workflows) |\n| [assets](https://github.com/Dao-AILab/flash-attention/tree/main/assets) | [assets](https://github.com/Dao-AILab/flash-attention/tree/main/assets) |\n| [benchmarks](https://github.com/Dao-AILab/flash-attention/tree/main/benchmarks) | [benchmarks](https://github.com/Dao-AILab/flash-attention/tree/main/benchmarks) |\n| [csrc](https://github.com/Dao-AILab/flash-attention/tree/main/csrc) | [csrc](https://github.com/Dao-AILab/flash-attention/tree/main/csrc) |\n| [examples/inference](https://github.com/Dao-AILab/flash-attention/tree/main/examples/inference) | [examples/inference](https://github.com/Dao-AILab/flash-attention/tree/main/examples/inference) |\n| [flash\\_attn](https://github.com/Dao-AILab/flash-attention/tree/main/flash_attn) | [flash\\_attn](https://github.com/Dao-AILab/flash-attention/tree/main/flash_attn) |\n| [hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper) | [hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper) |\n| [tests](https://github.com/Dao-AILab/flash-attention/tree/main/tests) | [tests](https://github.com/Dao-AILab/flash-attention/tree/main/tests) |\n| [training](https://github.com/Dao-AILab/flash-attention/tree/main/training) | [training](https://github.com/Dao-AILab/flash-attention/tree/main/training) |\n| [.gitignore](https://github.com/Dao-AILab/flash-attention/blob/main/.gitignore) | [.gitignore](https://github.com/Dao-AILab/flash-attention/blob/main/.gitignore) |\n| [.gitmodules](https://github.com/Dao-AILab/flash-attention/blob/main/.gitmodules) | [.gitmodules](https://github.com/Dao-AILab/flash-attention/blob/main/.gitmodules) |\n| [AUTHORS](https://github.com/Dao-AILab/flash-attention/blob/main/AUTHORS) | [AUTHORS](https://github.com/Dao-AILab/flash-attention/blob/main/AUTHORS) |\n| [LICENSE](https://github.com/Dao-AILab/flash-attention/blob/main/LICENSE) | [LICENSE](https://github.com/Dao-AILab/flash-attention/blob/main/LICENSE) |\n| [MANIFEST.in](https://github.com/Dao-AILab/flash-attention/blob/main/MANIFEST.in) | [MANIFEST.in](https://github.com/Dao-AILab/flash-attention/blob/main/MANIFEST.in) |\n| [Makefile](https://github.com/Dao-AILab/flash-attention/blob/main/Makefile) | [Makefile](https://github.com/Dao-AILab/flash-attention/blob/main/Makefile) |\n| [README.md](https://github.com/Dao-AILab/flash-attention/blob/main/README.md) | [README.md](https://github.com/Dao-AILab/flash-attention/blob/main/README.md) |\n| [setup.py](https://github.com/Dao-AILab/flash-attention/blob/main/setup.py) | [setup.py](https://github.com/Dao-AILab/flash-attention/blob/main/setup.py) |\n| [usage.md](https://github.com/Dao-AILab/flash-attention/blob/main/usage.md) | [usage.md](https://github.com/Dao-AILab/flash-attention/blob/main/usage.md) |\n| View all files |\n\n## Repository files navigation\n\n# FlashAttention\n\nThis repository provides the official implementation of FlashAttention and\nFlashAttention-2 from the\nfollowing papers.\n\n**FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness**\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré\nPaper: [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)\nIEEE Spectrum [article](https://spectrum.ieee.org/mlperf-rankings-2022) about our submission to the MLPerf 2.0 benchmark using FlashAttention.\n\n**FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning**\nTri Dao\n\nPaper: [https://tridao.me/publications/flash2/flash2.pdf](https://tridao.me/publications/flash2/flash2.pdf)\n\n## Usage\n\nWe've been very happy to see FlashAttention being widely adopted in such a short\ntime after its release. This [page](https://github.com/Dao-AILab/flash-attention/blob/main/usage.md)\ncontains a partial list of places where FlashAttention is being used.\n\nFlashAttention and FlashAttention-2 are free to use and modify (see LICENSE).\nPlease cite and credit FlashAttention if you use it.\n\n## FlashAttention-3 beta release\n\nFlashAttention-3 is optimized for Hopper GPUs (e.g. H100).\n\nBlogpost: [https://tridao.me/blog/2024/flash3/](https://tridao.me/blog/2024/flash3/)\n\nPaper: [https://tridao.me/publications/flash3/flash3.pdf](https://tridao.me/publications/flash3/flash3.pdf)\n\nThis is a beta release for testing / benchmarking before we integrate that with\nthe rest of the repo.\n\nCurrently released:\n\n- FP16 / BF16 forward and backward, FP8 forward\n\nRequirements: H100 / H800 GPU, CUDA >= 12.3.\n\nWe highly recommend CUDA 12.8 for best performance.\n\nTo install:\n\n```\ncd hopper\npython setup.py install\n```\n\nTo run the test:\n\n```\nexport PYTHONPATH=$PWD\npytest -q -s test_flash_attn.py\n```\n\nOnce the package is installed, you can import it as follows:\n\n```\nimport flash_attn_interface\nflash_attn_interface.flash_attn_func()\n```\n\n## Installation and features\n\n**Requirements:**\n\n- CUDA toolkit or ROCm toolkit\n- PyTorch 2.2 and above.\n- `packaging` Python package ( `pip install packaging`)\n- `ninja` Python package ( `pip install ninja`) \\*\n- Linux. Might work for Windows starting v2.3.2 (we've seen a few positive [reports](https://github.com/Dao-AILab/flash-attention/issues/595)) but Windows compilation still requires more testing. If you have ideas on how to set up prebuilt CUDA wheels for Windows, please reach out via Github issue.\n\n\\\\* Make sure that `ninja` is installed and that it works correctly (e.g. `ninja --version` then `echo $?` should return exit code 0). If not (sometimes `ninja --version` then `echo $?` returns a nonzero exit code), uninstall then reinstall\n`ninja` ( `pip uninstall -y ninja && pip install ninja`). Without `ninja`,\ncompiling can take a very long time (2h) since it does not use multiple CPU\ncores. With `ninja` compiling takes 3-5 minutes on a 64-core machine using CUDA toolkit.\n\n**To install:**\n\n```\npip install flash-attn --no-build-isolation\n```\n\nAlternatively you can compile from source:\n\n```\npython setup.py install\n```\n\nIf your machine has less than 96GB of RAM and lots of CPU cores, `ninja` might\nrun too many parallel compilation jobs that could exhaust the amount of RAM. To\nlimit the number of parallel compilation jobs, you can set the environment\nvariable `MAX_JOBS`:\n\n```\nMAX_JOBS=4 pip install flash-attn --no-build-isolation\n```\n\n**Interface:** `src/flash_attention_interface.py`\n\n### NVIDIA CUDA Support\n\n**Requirements:**\n\n- CUDA 12.0 and above.\n\nWe recommend the\n[Pytorch](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch)\ncontainer from Nvidia, which has all the required tools to install FlashAttention.\n\nFlashAttention-2 with CUDA currently supports:\n\n1. Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100). Support for Turing\nGPUs (T4, RTX 2080) is coming soon, please use FlashAttention 1.x for Turing\nGPUs for now.\n2. Datatype fp16 and bf16 (bf16 requires Ampere, Ada, or Hopper GPUs).\n3. All head dimensions up to 256. ~~Head dim > 192 backward requires A100/A800 or H100/H800~~. Head dim 256 backward now works on consumer GPUs (if there's no dropout) as of flash-attn 2.5.5.\n\n### AMD ROCm Support\n\nROCm version has two backends. There is [composable\\_kernel](https://github.com/ROCm/composable_kernel) (ck) which is the default backend and a [Triton](https://github.com/triton-lang/triton) backend. They provide an implementation of FlashAttention-2.\n\n**Requirements:**\n\n- ROCm 6.0 and above.\n\nWe recommend the\n[Pytorch](https://hub.docker.com/r/rocm/pytorch)\ncontainer from ROCm, which has all the required tools to install FlashAttention.\n\n#### Composable Kernel Backend\n\nFlashAttention-2 ROCm CK backend currently supports:\n\n1. MI200 or MI300 GPUs.\n2. Datatype fp16 and bf16\n3. Both forward's and backward's head dimensions up to 256.\n\n#### Triton Backend\n\nThe Triton implementation of the [Flash Attention v2](https://tridao.me/publications/flash2/flash2.pdf) is currently a work in progress.\n\nIt supports AMD's CDNA (MI200, MI300) and RDNA GPU's using fp16, bf16 and fp32 datatypes.\n\nThese features are supported in Fwd and Bwd\n\n1. Fwd and Bwd with causal masking\n2. Variable sequenc","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fgithub.com%2FDao-AILab%2Fflash-attention","id":"https://github.com/Dao-AILab/flash-attention","publishedDate":null,"summary":"This is the GitHub repository for \"flash-attention\" by Dao-AILab, a project focused on fast and memory-efficient exact attention mechanisms. It has 19.6k stars and 2k forks and is licensed under the BSD-3-Clause license.\n","title":"Dao-AILab/flash-attention: Fast and memory-efficient exact ... - GitHub","url":"https://github.com/Dao-AILab/flash-attention"},{"content":"[Machine Learning](https://alexdremov.me/tag/machine-learning/) [Algorithms](https://alexdremov.me/tag/algorithms/) [Code](https://alexdremov.me/tag/code/)\n\n# Understanding Flash Attention: Writing the Algorithm from Scratch in Triton\n\nWhy is Flash Attention so fast? Find out how Flash Attention works. Afterward, we'll polish our understanding by writing a GPU kernel of the algorithm in Triton.\n\n#### [Alex Dremov](https://alexdremov.me/author/alex/)\n\n[Machine Learning](https://alexdremov.me/tag/machine-learning/)•Jan 12, 2025•10 min read\n\nKernel jobs data management \\| Image by the author\n\nFlash Attention is a revolutionary technique that dramatically accelerates the attention mechanism in transformer-based models, delivering processing speeds many times faster than naive methods. By cleverly tiling data and minimizing memory transfers, it tackles the notorious GPU memory bottleneck that large language models often struggle with.\n\nIn this post, we’ll dive into how Flash Attention leverages efficient _I/O-awareness_ to reduce overhead, then take it a step further by crafting a **block-sparse attention kernel** in Triton.\n\n💥\n\nI will provide a simple explanation of how Flash Attention works. We will then implement the explained algorithm in Triton!\n\n## What is Attention?\n\nThe attention mechanism (or scaled dot-product attention) is a core element of transformer models, which is a leading architecture for solving the problem of language modeling. All popular models, like GPT, LLaMA, and BERT, rely on attention.\n\nThe formula is pretty simple:\n\n$$\\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d\\_k}}\\\\right)V,\\\\\\Q, K, V\\\\;—\\\\; \\\\text{query, key, value tensors}$$\n\nThe rest is history.\n\nEven though the formula looks simple, its computation involves multiplications of large tensors and a lot of data movement. Considering that this is a core part of the transformer architecture, optimizing the algorithm greatly improves the performance of the model in general.\n\nIn the naive implementation, attention requires \\\\(O(n^2)\\\\) additional memory and \\\\(O(n^2)\\\\) compute time complexity, where \\\\(n\\\\) is the sequence length. **That's a lot!**\n\n## **Flash Attention**\n\n### **Core Idea**\n\nThe main idea of Flash attention can be summarized in a simple quote from [the original paper](https://arxiv.org/pdf/2205.14135?ref=alexdremov.me):\n\n> We argue that a missing principle is making attention algorithms IO-aware — accounting for reads and writes between levels of GPU memory.\n\nThat is, modern GPUs have several types of memory:\n\n- **SRAM** — fast, on-chip, small\n- **HBM —** slower than SRAM, large size. That's what we usually address as GPU memory.\n\nCheck out the memory hierarchy in the image below to see the differences in bandwidth and sizes of different memory types.\n\nImage from FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness by Tri Dao et al.\n\n💡\n\nTo conduct computation, data must be transferred from HBM to SRAM, and this transfer is not overhead-free!\n\nThe Flash Attention algorithm proposes a method of **computing attention in tiles**, without explicitly materializing the attention scores tensor:\n\n$$\\\\text{AttentionScores}(Q, K) = \\\\text{Softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d\\_k}}\\\\right)$$\n\n💥\n\n**Not materializing a matrix** means that at any given time, the matrix does not exist in its full shape in memory.\n\nIt's easy to see that this matrix requires \\\\(O(n^2)\\\\) of memory to store. For large sequence lengths, **that's a lot of data!** So, if we manage to avoid explicitly materializing this matrix, we can save lots of memory.\n\nHowever, this matrix is necessary for transformer training as it is a part of backpropagation and gradient calculation. The authors propose that it's better to recalculate this matrix during the backward pass (again without explicit materialization). Not only does this saves lots of memory, but it also provides huge speedups as we don't need to transfer this enormous matrix between different GPU memory types.\n\nOverall, such an approach did not only speed up calculations by taking GPU I/O specifics into account, but also allowed processing huge sequence lengths as memory complexity drops to \\\\(O(n)\\\\).\n\n### Tiled Attention Calculation\n\nThe last thing to understand is how to compute attention **in tiles**. Basically, this means that we will calculate attention over the full sequence by processing incoming tokens in small portions.\n\nWell, it's easy to calculate \\\\(QK^T\\\\) in tiles. Considering that attention dimension is not high, we can load full matrix rows and columns and conduct multiplication in tiles.\n\n😡\n\nYes, if we want to have an enormous attention dimension, Flash Attention will not work without algorithm modifications. As dimensions are usually quite small even for enormous models, this limitation is fair.\n\nTiled QK^T \\| Image by the author\n\nSo, we have \\\\(QK^T\\\\) calculated in SRAM. All that's left is to apply softmax, multiply by \\\\(V\\\\), and that's it!\n\n$$\\\\text{Softmax}(z\\_i) = \\\\frac{e^{z\\_{i}}}{\\\\sum\\_{j=1}^T e^{z\\_{j}}} \\\\; \\\\; \\\\text{for}\\\\; i = 1, 2,\\\\ldots, T$$\n\nThat's where the trick is.\n\nThe problem is that the softmax denominator requires aggregation over the sequence length to normalize scores, and we do not have access to the whole length as we load data in tiles.\n\nTo address it, we can implement a concatenated softmax algorithm. Using it, we can calculate softmax \"in batch\" mode: by adjusting computed values with the new incoming data.\n\nTaking the algorithm from the original article, we can define rules to compute the softmax over data concatenation. Having two vectors \\\\(x^{(1)}\\\\) and \\\\(x^{(2)}\\\\), we need to calculate the softmax denominator \\\\(l(x)\\\\) over those vectors' concatenation: \\\\(x = \\\\left\\[x^{(1)}, x^{(2)}\\\\right\\]\\\\). If the vector's maximum is \\\\(m(x)\\\\), we can easily derive the softmax denominator of the concatenation:\n\n$$m(x) = m\\\\left(\\\\left\\[x^{(1)}, x^{(2)}\\\\right\\]\\\\right) = m(m(x^{(1)}), m(x^{(2)})),$$\n\n$$l(x) = l\\\\left(\\\\left\\[x^{(1)}, x^{(2)}\\\\right\\]\\\\right) = e^{m(x^{(1)}) - m(x)}l(x^{(1)}) + e^{m(x^{(2)}) - m(x)}l(x^{(2)}).$$\n\nThe last equivalence can be easily verified as \\\\(l(x)=\\\\sum\\_{j=1}^{T} e^{x\\_{j}}.\\\\)\n\nSo, now we have what we want — we can calculate softmax per-tile and then, by doing re-normalization from the formula above, compute the global softmax. The last thing to do is to incorporate the tile of the \\\\(V\\\\) tensor and keep doing the same re-normalization (as matrix multiplication is a linear operation).\n\nAnd all of this without loading the full sequence into memory or materializing \\\\(QK^T\\\\)!\n\n💥\n\nNotice that we calculate \\\\(\\\\text{Softmax}\\\\left(QK^T\\\\right)\\\\) in tiles only, without needing to have the whole matrix at any moment.\n\nAlso, in the actual algorithm for numerical stability, we will compute not \\\\(\\\\text{Softmax}(x)\\\\) but \\\\(\\\\text{Softmax}(x - \\\\max(x))\\\\). We can do that as softmax is invariant to constant shifts.\n\n## Triton Implementation\n\nNow, we can easily implement the outlined algorithm in Triton, which is a tool that allows us to write efficient GPU kernels with the ease of Python.\n\n💡\n\nTo learn more about Triton, check out their official guides.\n\n[Tutorials — Triton documentation](https://triton-lang.org/main/getting-started/tutorials/index.html?ref=alexdremov.me)Subscribe and don't miss posts!\n\n### Outlining the Algorithm\n\nThe first step is to decide how we will assign jobs and what data each job will load. By the algorithm of tiled softmax, each job must have access to \\\\(K, V\\\\) over the whole sequence length. So, each job will iterate over \\\\(K, V\\\\) in tiles. We don't have any algorithmic restriction on the number of \\\\(Q\\\\) tiles processed. Therefore, each job will load just one \\\\(Q\\\\) tile and work with it only — this way we will maximize job parallelism.\n\nKernel jobs data management \\| Image by the author\n\nIn summary, each job will load a single \\\\(Q\\\\) tile, iterate over all tiles in \\\\(K\\\\) and \\\\(V\\\\), and store one tile of result corresponding to the \\\\(Q\\\\) tile.\n\n### The Kernel\n\nWhat's left is to write the actual code. Let's focus on the core part first, and only then we'll add Triton-specific boilerplates.\n\nBelow is a Triton pseudocode with every line explained.\n\n```\ndef self_attn_fwd(...):\n # loading sample len\n seq_len = ...\n\n # running qk^T max (initialized by -inf)\n m_i = tl.zeros([TILE_Q_SIZE], dtype=tl.float32) - float(\"inf\")\n\n # current softmax denominator\n l_i = tl.zeros([TILE_Q_SIZE], dtype=tl.float32)\n\n # result tile\n # we will accumulate here (softmax numerator) @ V\n # then, we will divide it by softmax denominator in the very end\n acc = tl.zeros([TILE_Q_SIZE, HEAD_DIM], dtype=tl.float32)\n\n # notice: we accumulate all values above\n # in fp32 for higher precision\n\n # account for variable length of samples in batch\n q_tile_indices = q_token_idx + tl.arange(0, TILE_Q_SIZE)\n q_lens_mask = (\n q_tile_indices[:, None] < seq_len\n )\n\n # loading q tile into SRAM, shape (TILE_Q_SIZE, HEAD_DIM)\n q_tile = ...\n\n # softmax scale, multiplying by log_2(e)\n # to use faster exp2(...) instead of exp(...)\n softmax_scale: tl.constexpr = tl.cast(SM_SCALE * log_2(e), q_tile.dtype)\n\n # indices of tokens inside kv tile\n tile_k_arange = tl.arange(0, TILE_K_SIZE)\n\n # iterate over all tiles in k, v\n for kv_tile_idx in tl.range(\n 0, tl.cdiv(seq_len, TILE_K_SIZE), num_stages=PIPELINING\n ):\n # index of the first token in the kv tile\n kv_token_idx = kv_tile_idx * TILE_K_SIZE\n\n kt_tile = ... # load into SRAM K^T tile no. kv_tile_idx\n v_tile = ... # load into SRAM V tile no. kv_tile_idx\n\n # compute tile of QK^T\n qk = tl.dot(\n q_tile * softmax_scale,\n kt_tile,\n input_precision=INPUT_PRECISION,\n out_dtype=tl.float32\n )\n\n # masking out kv tokens after the sequence length\n kv_indices = kv_token_idx + tile_k_arange\n mask = q_lens_mask & (\n kv_indices[None, :] < seq_len\n )\n\n # set masked out values to -inf\n # for softmax to ignore them\n qk = tl.where(mask, qk, tl.cast(-float(","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Falexdremov.me%2Funderstanding-flash-attention-writing-the-algorithm-from-scratch-in-triton%2F","id":"https://alexdremov.me/understanding-flash-attention-writing-the-algorithm-from-scratch-in-triton/","publishedDate":null,"summary":"This webpage explains Flash Attention, a technique to speed up the attention mechanism in transformer models. Flash Attention uses efficient I/O-awareness to reduce overhead by computing attention in tiles and avoiding the explicit materialization of the attention scores matrix, which saves memory and provides speedups. The post will explain how Flash Attention works and then guide the reader through writing a block-sparse attention kernel in Triton.\n","title":"Understanding Flash Attention: Writing Triton Kernel Code","url":"https://alexdremov.me/understanding-flash-attention-writing-the-algorithm-from-scratch-in-triton/"},{"content":"[Skip to main content](https://pypi.org/pypi.org#content) Switch to mobile version\n\nSearch PyPISearch\n\n# jvp-flash-attention 0.0.2\n\npip install jvp-flash-attention==0.0.2Copy PIP instructions\n\n[Newer version available (0.0.6)](https://pypi.org/project/jvp-flash-attention/)\n\nReleased: Sep 6, 2025\n\nFlash Attention Triton kernel with support for second-order derivatives\n\n### Navigation\n\n### Verified details\n\n_These details have been [verified by PyPI](https://docs.pypi.org/project_metadata/#verified-details)_\n\n###### Maintainers\n\n[amorehead](https://pypi.org/user/amorehead/)\n\n### Unverified details\n\n_These details have **not** been verified by PyPI_\n\n###### Project links\n\n- [Homepage](https://pypi.org/project/jvp_flash_attention/)\n- [Repository](https://github.com/amorehead/jvp-flash-attention)\n\n###### Meta\n\n- **License Expression:** MIT\n_[SPDX](https://spdx.org/licenses/) [License Expression](https://spdx.github.io/spdx-spec/v2-draft/SPDX-license-expressions/)_\n- **Author:** [Alex Morehead](mailto:alex.morehead@gmail.edu)\n- Tagsartificial intelligence\n, deep learning\n- **Requires:** Python >=3.10\n- **Provides-Extra:** `lint`\n\n###### Classifiers\n\n- **Development Status**\n - [4 - Beta](https://pypi.org/search/?c=Development+Status+%3A%3A+4+-+Beta)\n- **Intended Audience**\n - [Developers](https://pypi.org/search/?c=Intended+Audience+%3A%3A+Developers)\n- **License**\n - [OSI Approved :: MIT License](https://pypi.org/search/?c=License+%3A%3A+OSI+Approved+%3A%3A+MIT+License)\n- **Programming Language**\n - [Python :: 3.10](https://pypi.org/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3.10)\n- **Topic**\n - [Scientific/Engineering :: Artificial Intelligence](https://pypi.org/search/?c=Topic+%3A%3A+Scientific%2FEngineering+%3A%3A+Artificial+Intelligence)\n\n[Report project as malware](https://pypi.org/project/jvp-flash-attention/submit-malware-report/)\n\n## Project description\n\n# JVP Flash Attention\n\n## Description\n\nFlash Attention Triton kernel with support for second-order derivatives, such as Jacobian-Vector Products (JVPs) and Hessian-Vector Products (HVPs)\n\n## Installation\n\nUsing `pip`, one can install `jvp_flash_attention` as follows.\n\n```\n# Install package\npipinstalljvp_flash_attention\n\n# [OPTIONAL, for development] Install package and pre-commit hooks\npipinstall-e.\npre-commitinstall\n\n```\n\n## Usage\n\nOnce installed, one can use `jvp_flash_attention` in place of PyTorch's `scaled_dot_product_attention` as follows.\n\n```\nimporttorch.nn.functionalasF\n\nfromtorch.nn.attentionimport SDPBackend, sdpa_kernel\nfromjvp_flash_attention.jvp_attentionimport attention as jvp_attention\n\nwith sdpa_kernel(SDPBackend.MATH):\n # Regular (quadratic) attention\n # x = F.scaled_dot_product_attention(\n # q,\n # k,\n # v,\n # attn_mask=attn_mask,\n # dropout_p=attn_dropout_p if self.training else 0.0,\n # )\n\n # JVP flash attention\n x = jvp_attention(\n q,\n k,\n v,\n attn_mask=attn_mask,\n dropout_p=attn_dropout_p if self.training else 0.0,\n )\n```\n\nContributions or enhancements are welcome!\n\n## Tests\n\nIf you want to run the unit tests verifying the correctness of the JVP Flash Attention Triton kernel, run the following command(s).\n\n```\npythontests/test_jvp_attention.py--dtype{float16,bfloat16,float32}\n```\n\nIn principle, the kernel should support ROCm systems as well, though it has not yet been tested on them. macOS is currently unsupported.\n\nResults for `float16`:\n\n```\n==============================================================================================================\nBENCHMARK SUMMARY\n==============================================================================================================\nSeq Len Causal Mask Method Time (ms) Mem (MB) TFLOP/s Max Error Grad Check\n--------------------------------------------------------------------------------------------------------------\n32 False additive sdpa 0.785 0.64 0.0 TFLOP/s baseline N/A\n32 False additive jvp_attn 0.475 0.23 0.0 TFLOP/s 1.95e-03 ✓\n\n32 False boolean sdpa 0.834 0.65 0.0 TFLOP/s baseline N/A\n32 False boolean jvp_attn 0.469 0.22 0.0 TFLOP/s 1.95e-03 ✓\n\n32 False none sdpa 0.546 0.64 0.0 TFLOP/s baseline N/A\n32 False none jvp_attn 0.465 0.22 0.0 TFLOP/s 1.95e-03 ✓\n\n32 True none sdpa 0.838 0.65 0.0 TFLOP/s baseline N/A\n32 True none jvp_attn 0.464 0.22 0.0 TFLOP/s 1.95e-03 ✓\n\n64 False additive sdpa 0.790 1.41 0.0 TFLOP/s baseline N/A\n64 False additive jvp_attn 0.487 0.47 0.0 TFLOP/s 9.77e-04 ✓\n\n64 False boolean sdpa 0.820 1.45 0.0 TFLOP/s baseline N/A\n64 False boolean jvp_attn 0.473 0.43 0.0 TFLOP/s 9.77e-04 ✓\n\n64 False none sdpa 0.685 1.41 0.0 TFLOP/s baseline N/A\n64 False none jvp_attn 0.474 0.43 0.0 TFLOP/s 9.77e-04 ✓\n\n64 True none sdpa 0.854 1.42 0.0 TFLOP/s baseline N/A\n64 True none jvp_attn 0.499 0.43 0.0 TFLOP/s 1.95e-03 ✓\n\n128 False additive sdpa 0.769 3.28 0.0 TFLOP/s baseline N/A\n128 False additive jvp_attn 0.494 1.02 0.1 TFLOP/s 9.77e-04 ✓\n\n128 False boolean sdpa 0.831 3.44 0.0 TFLOP/s baseline N/A\n128 False boolean jvp_attn 0.468 0.86 0.1 TFLOP/s 9.77e-04 ✓\n\n128 False none sdpa 0.533 3.28 0.0 TFLOP/s baseline N/A\n128 False none jvp_attn 0.470 0.86 0.1 TFLOP/s 9.77e-04 ✓\n\n128 True none sdpa 0.984 3.35 0.0 TFLOP/s baseline N/A\n128 True none jvp_attn 0.483 0.86 0.0 TFLOP/s 1.95e-03 ✓\n\n256 False additive sdpa 1.142 9.69 0.1 TFLOP/s baseline N/A\n256 False additive jvp_attn 0.473 2.35 0.4 TFLOP/s 9.77e-04 ✓\n\n256 False boolean sdpa 0.886 10.32 0.1 TFLOP/s baseline N/A\n256 False boolean jvp_attn 0.466 1.72 0.4 TFLOP/s 9.77e-04 ✓\n\n256 False none sdpa 0.715 9.69 0.1 TFLOP/s baseline N/A\n256 False none jvp_attn 0.472 1.72 0.4 TFLOP/s 9.77e-04 ✓\n\n256 True none sdpa 0.976 9.94 0.0 TFLOP/s baseline N/A\n256 True none jvp_attn 0.464 1.72 0.2 TFLOP/s 1.95e-03 ✓\n\n512 False additive sdpa 1.399 31.88 0.2 TFLOP/s baseline N/A\n512 False additive jvp_attn 0.481 5.95 1.4 TFLOP/s 4.88e-04 ✓\n\n512 False boolean sdpa 1.222 34.38 0.3 TFLOP/s baseline N/A\n512 False boolean jvp_attn 0.489 3.45 1.4 TFLOP/s 4.88e-04 ✓\n\n512 False none sdpa 1.106 31.88 0.3 TFLOP/s baseline N/A\n512 False none jvp_attn 0.475 3.45 1.4 TFLOP/s 4.88e-04 ✓\n\n512 True none sdpa 1.354 32.88 0.1 TFLOP/s baseline N/A\n512 True none jvp_attn 0.493 3.45 0.7 TFLOP/s 1.95e-03 ✓\n\n1024 False additive sdpa 2.430 113.77 0.6 TFLOP/s baseline N/A\n1024 False additive jvp_attn 0.480 16.89 5.7 TFLOP/s 4.88e-04 ✓\n\n1024 False boolean sdpa 2.889 123.77 0.5 TFLOP/s baseline N/A\n1024 False boolean jvp_attn 0.483 6.89 5.7 TFLOP/s 4.88e-04 ✓\n\n1024 False none sdpa 2.457 113.77 0.6 TFLOP/s baseline N/A\n1024 False none jvp_attn 0.467 6.89 5.9 TFLOP/s 4.88e-04 ✓\n\n1024 True none sdpa 2.670 117.77 0.3 TFLOP/s baseline N/A\n1024 True none jvp_attn 0.500 6.89 2.7 TFLOP/s 1.95e-03 ✓\n\n2048 False additive sdpa 7.791 427.54 0.7 TFLOP/s baseline N/A\n2048 False additive jvp_attn 0.696 53.79 15.7 TFLOP/s 2.44e-04 ✓\n\n2048 False boolean sdpa 7.673 467.54 0.7 TFLOP/s baseline N/A\n2048 False boolean jvp_attn 0.755 13.79 14.5 TFLOP/s 2.44e-04 ✓\n\n2048 False none sdpa 7.773 427.54 0.7 TFLOP/s baseline N/A\n2048 False none jvp_attn 0.614 13.79 17.8 TFLOP/s 2.44e-04 ✓\n\n2048 True none sdpa 8.609 443.54 0.3 TFLOP/s baseline N/A\n2048 True none jvp_attn 0.464 13.79 11.8 TFLOP/s 1.95e-03 ✓\n\n================================================================================\nMASK TYPE PERFORMANCE COMPARISON\n================================================================================\nSeq Len Causal Method No Mask Boolean Mask Additive Mask\n--------------------------------------------------------------------------------\n32 False jvp_attn 0.47 ms 0.47 ms (1.01x) 0.48 ms (1.02x)\n32 True jvp_attn 0.46 ms N/A N/A\n64 False jvp_attn 0.47 ms 0.47 ms (1.00x) 0.49 ms (1.03x)\n64 True jvp_attn 0.50 ms N/A N/A\n128 False jvp_attn 0.47 ms 0.47 ms (1.00x) 0.49 ms (1.05x)\n128 True jvp_attn 0.48 ms N/A N/A\n256 False jvp_attn 0.47 ms 0.47 ms (0.99x) 0.47 ms (1.00x)\n256 True jvp_attn 0.46 ms N/A N/A\n512 False jvp_attn 0.47 ms 0.49 ms (1.03x) 0.48 ms (1.01x)\n512 True jvp_attn 0.49 ms N/A N/A\n1024 False jvp_attn 0.47 ms 0.48 ms (1.03x) 0.48 ms (1.03x)\n1024 True jvp_attn 0.50 ms N/A N/A\n2048 False jvp_attn 0.61 ms 0.75 ms (1.23x) 0.70 ms (1.13x)\n2048 True jvp_attn 0.46 ms N/A N/A\n\n============================================================\nSTATISTICS\n============================================================\nAverage speedup: 4.00x\nMin speedup: 1.13x\nMax speedup: 18.54x\n\nAccuracy: 28/28 tests passed\n✓ All accuracy checks passed!\n\n```\n\nResults for `bfloat16`:\n\n```\n==============================================================================================================\nBENCHMARK SUMMARY\n==============================================================================================================\nSeq Len Causal Mask Method Time (ms) Mem (MB) TFLOP/s Max Error Grad Check\n--------------------------------------------------------------------------------------------------------------\n32 False additive sdpa 0.900 0.64 0.0 TFLOP/s baseline N/A\n32 False additive jvp_attn 0.661 0.23 0.0 TFLOP/s 1.56e-02 ✓\n\n32 False boolean sdpa 0.883 0.65 0.0 TFLOP/s baseline N/A\n32 False boolean jvp_attn 0.578 0.22 0.0 TFLOP/s 1.56e-02 ✓\n\n32 False none sdpa 0.603 0.64 0.0 TFLOP/s baseline N/A\n32 False none jvp_attn 0.513 0.22 0.0 TFLOP/s 1.56e-02 ✓\n\n32 True none sdpa 0.915 0.65 0.0 TFLOP/s baseline N/A\n32 True none jvp_attn 0.519 0.22 0.0 TFLOP/s 1.56e-02 ✓\n\n64 False additive sdpa 0.833 1.41 0.0 TFLOP/s baseline N/A\n64 False additive jvp_attn 0.531 0.47 0.0 TFLOP/s 7.81e-03 ✓\n\n64 False boolean sdpa 0.870 1.45 0.0 TFLOP/s baseline N/A\n64 False boolean jvp_attn 0.545 0.43 0.0 TFLOP/s 7.81e-03 ✓\n\n64 False none sdpa 0.565 1.41 0.0 TFLOP/s baseline N/A\n64 False none jvp_attn 0.508 0.43 0.0 TFLOP/s 7.81e-03 ✓\n\n64 True none sdpa 0.939 1.42 0.0 TFLOP/s baseline N/A\n64 True none jvp_attn 0.520 0.43 0.0 TFLOP/s 1.56e-02 ✓\n\n128 False additive sdpa 0.864 3.28 0.0 TFLOP/s baseline N/A\n128 False additive jvp_attn 0.476 1.02 0.1 TFLOP/s 7.81e-03 ✓\n\n128 False boolean sdpa 0.839 3.44 0.0 TFLOP/s baseline N/A\n128 False boolean jvp_attn 0.460 0.86 0.1","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpypi.org%2Fproject%2Fjvp-flash-attention%2F0.0.2%2F","id":"https://pypi.org/project/jvp-flash-attention/0.0.2/","publishedDate":"2025-09-06T00:00:00.000Z","summary":"jvp-flash-attention is a Python package available on PyPI that provides a Flash Attention Triton kernel with support for second-order derivatives, such as Jacobian-Vector Products (JVPs) and Hessian-Vector Products (HVPs). It can be installed using pip: `pip install jvp-flash-attention==0.0.2`. The package is maintained by amorehead and released on Sep 6, 2025. It is a beta version and requires Python 3.10 or higher. The project includes a homepage and repository links, and the license is MIT.\n","title":"jvp-flash-attention","url":"https://pypi.org/project/jvp-flash-attention/0.0.2/"},{"content":"[Skip to main content](https://pypi.org/pypi.org#content) Switch to mobile version\n\nSearch PyPISearch\n\n# flash-dmattn 1.1.8\n\npip install flash-dmattn==1.1.8Copy PIP instructions\n\n[Newer version available (1.1.9)](https://pypi.org/project/flash-dmattn/)\n\nReleased: Sep 22, 2025\n\nFlash Dynamic Mask Attention: Fast and Memory-Efficient Trainable Dynamic Mask Sparse Attention\n\n### Navigation\n\n### Verified details\n\n_These details have been [verified by PyPI](https://docs.pypi.org/project_metadata/#verified-details)_\n\n###### Maintainers\n\n[LoserCheems](https://pypi.org/user/LoserCheems/)\n\n###### Meta\n\n- **Author:** [Jingze Shi](mailto:losercheems@gmail.com)\n- **Maintainer:** [Jingze Shi](mailto:losercheems@gmail.com)\n\n### Unverified details\n\n_These details have **not** been verified by PyPI_\n\n###### Project links\n\n- [Homepage](https://github.com/SmallDoges/flash-dmattn)\n- [Issues](https://github.com/SmallDoges/flash-dmattn/issues)\n- [Source](https://github.com/SmallDoges/flash-dmattn)\n\n###### Meta\n\n- **License:** BSD License (BSD 3-Clause License)\n- **Requires:** Python >=3.9\n- **Provides-Extra:** `triton`\n, `flex`\n, `all`\n, `test`\n, `dev`\n\n###### Classifiers\n\n- **License**\n - [OSI Approved :: BSD License](https://pypi.org/search/?c=License+%3A%3A+OSI+Approved+%3A%3A+BSD+License)\n- **Operating System**\n - [Unix](https://pypi.org/search/?c=Operating+System+%3A%3A+Unix)\n- **Programming Language**\n - [Python :: 3](https://pypi.org/search/?c=Programming+Language+%3A%3A+Python+%3A%3A+3)\n\n[Report project as malware](https://pypi.org/project/flash-dmattn/submit-malware-report/)\n\n## Project description\n\n**English** \\| [简体中文](https://pypi.org/README_zh.md)\n\nFlash-DMA is a high-performance attention implementation that integrates Flash Attention's memory efficiency with Dynamic Mask Attention's sparse computation capabilities for processing extremely long sequences in transformer models.\n\n## Key Features\n\n### 🎯 Core Kernel Advantages\n\n- **Mask & Bias Support**: Native support for `(batch_size, {1|num_kv_heads|num_heads}, {0|query_len}, key_len)` shaped attention mask and attention bias tensors\n- **Intelligent Computation Skipping**: Block-level automatic skipping mechanism based on masks, completely bypassing computation and memory access for zero-mask blocks\n- **Complete Gradient Support**: Built-in full gradient computation path for attention bias, supporting end-to-end training\n\n### 🚀 Performance & Efficiency\n\n- **Dynamic Sparse Attention**: Dynamically selects the most relevant keys for each query, reducing computational complexity from $O(N^2)$ to $O(N \\\\cdot w)$ where $w \\\\ll N$, supporting trainable sparse structures\n- **Memory Efficiency**: Maintains Flash Attention's $O(N)$ memory complexity without instantiating the full attention matrix\n- **CUDA Deep Optimization**: Custom CUDA kernels with shared memory aliasing, pipelined prefetching, and block skipping for high throughput and low memory access overhead\n- **Extremely Long Context Support**: Handles 128K+ token sequences efficiently through dynamic mask windowing while preserving accuracy\n\n## Performance\n\nWe present the expected speedup of Flash-DMA over standard PyTorch SDPA under mask and bias conditions.\n\n### Forward Pass Performance\n\nThe following table shows the forward pass performance comparison between Flash-DMA and standard PyTorch SDPA on an NVIDIA A100-SXM4-80GB. Results are averaged over 3 runs after 2 warmup runs.\n\n| Mode | Q len | K len | Window W | SDPA (ms) | FDMA (ms) | Speedup |\n| --- | --- | --- | --- | --- | --- | --- |\n| Train | 256 | 256 | 1024 | 0.29 | 0.19 | 1.58x |\n| Train | 512 | 512 | 1024 | 0.35 | 0.19 | 1.86x |\n| Train | 1024 | 1024 | 1024 | 0.51 | 0.18 | 2.81x |\n| Train | 2048 | 2048 | 1024 | 1.04 | 0.18 | 5.68x |\n| Train | 4096 | 4096 | 1024 | 2.53 | 0.24 | 10.41x |\n| Train | 8192 | 8192 | 1024 | 9.38 | 0.36 | 25.93x |\n| Train | 16384 | 16384 | 1024 | 28.39 | 0.81 | 35.25x |\n| Train | 32768 | 32768 | 1024 | 111.87 | 2.25 | 49.78x |\n| Train | 32768 | 32768 | 32 | 113.19 | 2.10 | 53.97x |\n| Train | 32768 | 32768 | 64 | 113.17 | 2.12 | 53.32x |\n| Train | 32768 | 32768 | 128 | 113.14 | 2.10 | 53.78x |\n| Train | 32768 | 32768 | 256 | 113.18 | 2.13 | 53.18x |\n| Train | 32768 | 32768 | 512 | 113.19 | 2.17 | 52.17x |\n| Train | 32768 | 32768 | 1024 | 113.19 | 2.24 | 50.45x |\n| Train | 32768 | 32768 | 2048 | 113.15 | 2.39 | 47.35x |\n| Train | 32768 | 32768 | 4096 | 113.16 | 2.67 | 42.39x |\n| Train | 32768 | 32768 | 8192 | 113.11 | 3.20 | 35.29x |\n| Train | 32768 | 32768 | 16384 | 113.15 | 3.97 | 28.51x |\n| Train | 32768 | 32768 | 32768 | 113.11 | 4.90 | 23.10x |\n| Infer | 1 | 256 | 1024 | 0.25 | 0.19 | 1.28x |\n| Infer | 1 | 512 | 1024 | 0.25 | 0.19 | 1.27x |\n| Infer | 1 | 1024 | 1024 | 0.25 | 0.20 | 1.28x |\n| Infer | 1 | 2048 | 1024 | 0.25 | 0.20 | 1.24x |\n| Infer | 1 | 4096 | 1024 | 0.25 | 0.19 | 1.29x |\n| Infer | 1 | 8192 | 1024 | 0.25 | 0.20 | 1.25x |\n| Infer | 1 | 16384 | 1024 | 0.25 | 0.19 | 1.29x |\n| Infer | 1 | 32768 | 1024 | 0.27 | 0.20 | 1.33x |\n| Infer | 1 | 65536 | 1024 | 0.42 | 0.20 | 2.10x |\n| Infer | 1 | 131072 | 1024 | 0.72 | 0.20 | 3.65x |\n| Infer | 1 | 262144 | 1024 | 1.31 | 0.22 | 6.06x |\n| Infer | 1 | 524288 | 1024 | 2.49 | 0.24 | 10.45x |\n| Infer | 1 | 524288 | 32 | 2.48 | 0.21 | 11.60x |\n| Infer | 1 | 524288 | 64 | 2.44 | 0.21 | 11.66x |\n| Infer | 1 | 524288 | 128 | 2.45 | 0.21 | 11.47x |\n| Infer | 1 | 524288 | 256 | 2.43 | 0.21 | 11.47x |\n| Infer | 1 | 524288 | 512 | 2.44 | 0.22 | 10.89x |\n| Infer | 1 | 524288 | 1024 | 2.44 | 0.24 | 10.31x |\n| Infer | 1 | 524288 | 2048 | 2.44 | 0.27 | 9.07x |\n| Infer | 1 | 524288 | 4096 | 2.45 | 0.33 | 7.41x |\n| Infer | 1 | 524288 | 8192 | 2.44 | 0.35 | 6.93x |\n| Infer | 1 | 524288 | 16384 | 2.44 | 0.35 | 6.93x |\n| Infer | 1 | 524288 | 32768 | 2.45 | 0.35 | 6.96x |\n| Infer | 1 | 524288 | 65536 | 2.44 | 0.35 | 6.88x |\n\n### Backward Pass Performance\n\nThe following table shows the backward pass performance comparison between Flash-DMA and standard PyTorch SDPA on an NVIDIA A100-SXM4-80GB. Results are averaged over 3 runs after 2 warmup runs.\n\n| Mode | Q len | K len | Window W | SDPA-BWD (ms) | FDMA-BWD (ms) | Speedup |\n| --- | --- | --- | --- | --- | --- | --- |\n| Train | 256 | 256 | 1024 | 0.42 | 0.62 | 0.7x |\n| Train | 512 | 512 | 1024 | 0.56 | 0.60 | 0.9x |\n| Train | 1024 | 1024 | 1024 | 0.94 | 0.61 | 1.5x |\n| Train | 2048 | 2048 | 1024 | 1.79 | 0.69 | 2.6x |\n| Train | 4096 | 4096 | 1024 | 3.76 | 1.08 | 3.5x |\n| Train | 8192 | 8192 | 1024 | 14.39 | 2.06 | 7.0x |\n| Train | 16384 | 16384 | 1024 | 39.56 | 4.97 | 8.0x |\n| Train | 32768 | 32768 | 1024 | 142.07 | 25.63 | 5.5x |\n| Train | 32768 | 32768 | 32 | 142.70 | 21.91 | 6.5x |\n| Train | 32768 | 32768 | 64 | 142.65 | 22.29 | 6.4x |\n| Train | 32768 | 32768 | 128 | 142.69 | 23.04 | 6.2x |\n| Train | 32768 | 32768 | 256 | 142.69 | 24.27 | 5.9x |\n| Train | 32768 | 32768 | 512 | 142.67 | 25.12 | 5.7x |\n| Train | 32768 | 32768 | 1024 | 142.55 | 25.58 | 5.6x |\n| Train | 32768 | 32768 | 2048 | 142.75 | 25.64 | 5.6x |\n| Train | 32768 | 32768 | 4096 | 142.61 | 24.84 | 5.7x |\n| Train | 32768 | 32768 | 8192 | 142.33 | 25.63 | 5.6x |\n| Train | 32768 | 32768 | 16384 | 142.40 | 25.62 | 5.6x |\n| Train | 32768 | 32768 | 32768 | 142.43 | 25.63 | 5.6x |\n\n## Installation\n\n### Requirements\n\n- **Linux**: Ubuntu 22.04 or later\n- **NVIDIA GPU**: Compute Capability 8.0 or higher\n- **C++ Compiler**: GCC 7+\n- **CUDA**: 11.8 or later\n- **Python**: 3.9 or later\n- **PyTorch**: 2.5.1 or later\n\n### Install\n\nYou can install Flash-DMA via pre-compiled wheels:\n\n```\npipinstallflash-dmattn--no-build-isolation\n\n```\n\nAlternatively, you can compile and install from source:\n\n```\ngitclonehttps://github.com/SmallDoges/flash-dmattn.git\ncdflash-dmattn\npipinstall.--no-build-isolation\n\n```\n\n## Quick Start\n\n### Basic Usage\n\n```\nimporttorch\nfromflash_dmattnimport flash_dmattn_func_auto\nimportmath\n\n# Setup\nbatch_size, seq_len, num_heads, num_kv_heads, head_dim = 1, 256, 2, 1, 64\nkeep_window_size = 128\ndevice = torch.device('cuda')\ndtype = torch.bfloat16\nmin_dtype = torch.finfo(dtype).min # dtype minimum value\n\n# Input tensors\nquery = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device, dtype=dtype)\nkey = torch.randn(batch_size, seq_len, num_kv_heads, head_dim, device=device, dtype=dtype)\nvalue = torch.randn(batch_size, seq_len, num_kv_heads, head_dim, device=device, dtype=dtype)\n\n# Create mask and bias for sparse attention\nattention_mask = torch.ones(batch_size, num_kv_heads, seq_len, seq_len, device=device, dtype=dtype)\nattention_bias = torch.randn(batch_size, num_kv_heads, seq_len, seq_len, device=device, dtype=dtype)\n\n# Generate sparse mask based on bias\nif seq_len > keep_window_size:\n # Select top-k most important keys for each query\n topk_values, topk_indices = torch.topk(\n attention_bias, keep_window_size, dim=-1,\n largest=True, sorted=False\n )\n # Generate valid top-k mask\n valid_topk = (topk_values != min_dtype).to(dtype)\n attention_mask = torch.zeros_like(attention_bias, dtype=dtype, device=attention_bias.device)\n attention_mask = attention_mask.scatter(-1, topk_indices, valid_topk)\n attention_bias = attention_bias.masked_fill(attention_mask == 0.0, min_dtype)\n\n# Select FDMA kernel\nflash_dmattn_func = flash_dmattn_func_auto(backend=\"cuda\")\n\n# Run Flash Dynamic Mask Attention\noutput = flash_dmattn_func(\n query=query,\n key=key,\n value=value,\n attn_mask=attention_mask,\n attn_bias=attention_bias,\n is_causal=True,\n scale=1.0/math.sqrt(head_dim),\n)\n\nprint(f\"Output shape: {output.shape}\") # [1, 256, 2, 64]\n```\n\n### Gradient Computation Example\n\n```\n# Enable gradient computation\nquery.requires_grad_(True)\nkey.requires_grad_(True)\nvalue.requires_grad_(True)\nattention_bias.requires_grad_(True)\n\n# Forward pass\noutput = flash_dmattn_func(\n query=query, key=key, value=value,\n attn_mask=attention_mask,\n attn_bias=attention_bias,\n is_causal=True,\n scale=1.0/math.sqrt(head_dim)\n)\n\n# Backward pass\nloss = output.sum()\nloss.backward()\n\nprint(f\"Query gradient shape: {query.grad.shape}\"","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpypi.org%2Fproject%2Fflash-dmattn%2F1.1.8%2F","id":"https://pypi.org/project/flash-dmattn/1.1.8/","publishedDate":"2025-09-22T00:00:00.000Z","summary":"Flash-DMAttn (version 1.1.8) is a Python package that implements a high-performance attention mechanism, combining Flash Attention's memory efficiency with Dynamic Mask Attention's sparse computation for processing long sequences in transformer models. Key features include mask and bias support, intelligent computation skipping, and complete gradient support. It offers significant speedups over standard PyTorch SDPA, especially for longer sequences, as demonstrated in the provided performance table. The package is maintained by Jingze Shi (LoserCheems) and is available on PyPI.\n","title":"flash-dmattn","url":"https://pypi.org/project/flash-dmattn/1.1.8/"},{"content":"# Computer Science > Distributed, Parallel, and Cluster Computing\n\n**arXiv:2508.18224** (cs)\n\n\\[Submitted on 25 Aug 2025\\]\n\n# Title:Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel\n\nAuthors: [Ran Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan,+R), [Youhe Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang,+Y), [Binhang Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan,+B)\n\nView a PDF of the paper titled Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel, by Ran Yan and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2508.18224) [HTML (experimental)](https://arxiv.org/html/2508.18224v1)\n\n> Abstract:Recent progress in sparse attention mechanisms has demonstrated strong potential for reducing the computational cost of long-context training and inference in large language models (LLMs). Native Sparse Attention (NSA), a state-of-the-art approach, introduces natively trainable, hardware-aligned sparse attention that delivers substantial system-level performance gains while maintaining accuracy comparable to full attention. However, the kernel implementation of NSA relies on a query-grouping strategy that is efficient only with large Grouped Query Attention (GQA) sizes, whereas modern LLMs typically adopt much smaller GQA groups, which limits the applicability of this sparse algorithmic advance. In this work, we propose Flash Sparse Attention (FSA), which includes an alternative kernel design that enables efficient NSA computation across a wide range of popular LLMs with varied smaller GQA group sizes on modern GPUs. Compared to vanilla NSA kernel implementation, our empirical evaluation demonstrates that FSA achieves (i) up to 3.5$\\\\times$ and on average 1.6$\\\\times$ kernel-level latency reduction, (ii) up to 1.25$\\\\times$ and 1.09$\\\\times$ on average end-to-end training speedup on state-of-the-art LLMs, and (iii) up to 1.36$\\\\times$ and 1.11$\\\\times$ on average end-to-end prefill speedup on state-of-the-art LLMs. The source code is open-sourced and publicly available at [this https URL](https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention).\n\n| | |\n| --- | --- |\n| Subjects: | Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2508.18224](https://arxiv.org/abs/2508.18224) \\[cs.DC\\] |\n| (or [arXiv:2508.18224v1](https://arxiv.org/abs/2508.18224v1) \\[cs.DC\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2508.18224](https://doi.org/10.48550/arXiv.2508.18224) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Ran Yan \\[ [view email](https://arxiv.org/show-email/01daa2d6/2508.18224)\\] **\\[v1\\]**\nMon, 25 Aug 2025 17:22:15 UTC (393 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel, by Ran Yan and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2508.18224)\n- [HTML (experimental)](https://arxiv.org/html/2508.18224v1)\n- [TeX Source](https://arxiv.org/src/2508.18224)\n- [Other Formats](https://arxiv.org/format/2508.18224)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.DC\n\n[< prev](https://arxiv.org/prevnext?id=2508.18224&function=prev&context=cs.DC)  \\|  [next >](https://arxiv.org/prevnext?id=2508.18224&function=next&context=cs.DC)\n\n[new](https://arxiv.org/list/cs.DC/new) \\| [recent](https://arxiv.org/list/cs.DC/recent) \\| [2025-08](https://arxiv.org/list/cs.DC/2025-08)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2508.18224?context=cs) [cs.LG](https://arxiv.org/abs/2508.18224?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2508.18224)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2508.18224)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2508.18224)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2508.18224) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2508.18224","id":"https://arxiv.org/abs/2508.18224","publishedDate":"2025-08-25T00:00:00.000Z","summary":"This webpage is an arXiv submission for a paper titled \"Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel\" by Ran Yan and two other authors. The paper proposes Flash Sparse Attention (FSA) as an alternative kernel design for Native Sparse Attention (NSA) to improve the computational efficiency of large language models (LLMs). FSA is designed to work with smaller Grouped Query Attention (GQA) group sizes, which are common in modern LLMs. The paper reports that FSA achieves significant kernel-level latency reduction and end-to-end speedups in both training and prefill compared to the vanilla NSA kernel implementation. The source code for FSA is publicly available.\n","title":"Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel","url":"https://arxiv.org/abs/2508.18224"},{"content":"[Skip to main content](https://docs.nvidia.com/docs.nvidia.com#main-content)\n\nBack to top\n\n`Ctrl` + `K`\n\n[NVIDIA Triton Inference Server](https://docs.nvidia.com/contents.html)\n\n- [GitHub](https://github.com/triton-inference-server/server)\n\n# TensorRT-LLM Backend [\\#](https://docs.nvidia.com/docs.nvidia.com\\#tensorrt-llm-backend)\n\nThe Triton backend for [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM).\nYou can learn more about Triton backends in the [backend repo](https://github.com/triton-inference-server/backend).\nThe goal of TensorRT-LLM Backend is to let you serve [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)\nmodels with Triton Inference Server. The [inflight\\_batcher\\_llm](https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/inflight_batcher_llm)\ndirectory contains the C++ implementation of the backend supporting inflight\nbatching, paged attention and more.\n\nWhere can I ask general questions about Triton and Triton backends?\nBe sure to read all the information below as well as the [general\\\nTriton documentation](https://github.com/triton-inference-server/server#triton-inference-server)\navailable in the main [server](https://github.com/triton-inference-server/server)\nrepo. If you don’t find your answer there you can ask questions on the\n[issues page](https://github.com/triton-inference-server/tensorrtllm_backend/issues).\n\n## Table of Contents [\\#](https://docs.nvidia.com/docs.nvidia.com\\#table-of-contents)\n\n- [TensorRT-LLM Backend](https://docs.nvidia.com/docs.nvidia.com#tensorrt-llm-backend)\n\n - [Table of Contents](https://docs.nvidia.com/docs.nvidia.com#table-of-contents)\n\n - [Getting Started](https://docs.nvidia.com/docs.nvidia.com#getting-started)\n\n - [Quick Start](https://docs.nvidia.com/docs.nvidia.com#quick-start)\n\n - [Launch Triton TensorRT-LLM container](https://docs.nvidia.com/docs.nvidia.com#launch-triton-tensorrt-llm-container)\n\n - [Prepare TensorRT-LLM engines](https://docs.nvidia.com/docs.nvidia.com#prepare-tensorrt-llm-engines)\n\n - [Prepare the Model Repository](https://docs.nvidia.com/docs.nvidia.com#prepare-the-model-repository)\n\n - [Modify the Model Configuration](https://docs.nvidia.com/docs.nvidia.com#modify-the-model-configuration)\n\n - [Serving with Triton](https://docs.nvidia.com/docs.nvidia.com#serving-with-triton)\n\n - [Send an Inference Request](https://docs.nvidia.com/docs.nvidia.com#send-an-inference-request)\n\n - [Using the generate endpoint](https://docs.nvidia.com/docs.nvidia.com#using-the-generate-endpoint)\n\n - [Using the client scripts](https://docs.nvidia.com/docs.nvidia.com#using-the-client-scripts)\n\n - [Early stopping](https://docs.nvidia.com/docs.nvidia.com#early-stopping)\n\n - [Return context logits and/or generation logits](https://docs.nvidia.com/docs.nvidia.com#return-context-logits-andor-generation-logits)\n - [Requests with batch size > 1](https://docs.nvidia.com/docs.nvidia.com#requests-with-batch-size-1)\n - [Building from Source](https://docs.nvidia.com/docs.nvidia.com#building-from-source)\n\n - [Supported Models](https://docs.nvidia.com/docs.nvidia.com#supported-models)\n\n - [Model Config](https://docs.nvidia.com/docs.nvidia.com#model-config)\n\n - [Model Deployment](https://docs.nvidia.com/docs.nvidia.com#model-deployment)\n\n - [TRT-LLM Multi-instance Support](https://docs.nvidia.com/docs.nvidia.com#trt-llm-multi-instance-support)\n\n - [Leader Mode](https://docs.nvidia.com/docs.nvidia.com#leader-mode)\n\n - [Orchestrator Mode](https://docs.nvidia.com/docs.nvidia.com#orchestrator-mode)\n\n - [Running Multiple Instances of LLaMa Model](https://docs.nvidia.com/docs.nvidia.com#running-multiple-instances-of-llama-model)\n - [Multi-node Support](https://docs.nvidia.com/docs.nvidia.com#multi-node-support)\n\n - [Model Parallelism](https://docs.nvidia.com/docs.nvidia.com#model-parallelism)\n\n - [Tensor Parallelism, Pipeline Parallelism and Expert Parallelism](https://docs.nvidia.com/docs.nvidia.com#tensor-parallelism-pipeline-parallelism-and-expert-parallelism)\n - [MIG Support](https://docs.nvidia.com/docs.nvidia.com#mig-support)\n\n - [Scheduling](https://docs.nvidia.com/docs.nvidia.com#scheduling)\n\n - [Key-Value Cache](https://docs.nvidia.com/docs.nvidia.com#key-value-cache)\n\n - [Decoding](https://docs.nvidia.com/docs.nvidia.com#decoding)\n\n - [Decoding Modes - Top-k, Top-p, Top-k Top-p, Beam Search, Medusa, ReDrafter, Lookahead and Eagle](https://docs.nvidia.com/docs.nvidia.com#decoding-modes-top-k-top-p-top-k-top-p-beam-search-medusa-redrafter-lookahead-and-eagle)\n\n - [Speculative Decoding](https://docs.nvidia.com/docs.nvidia.com#speculative-decoding)\n - [Chunked Context](https://docs.nvidia.com/docs.nvidia.com#chunked-context)\n\n - [Quantization](https://docs.nvidia.com/docs.nvidia.com#quantization)\n\n - [LoRa](https://docs.nvidia.com/docs.nvidia.com#lora)\n - [Launch Triton server _within Slurm based clusters_](https://docs.nvidia.com/docs.nvidia.com#launch-triton-server-within-slurm-based-clusters)\n\n - [Prepare some scripts](https://docs.nvidia.com/docs.nvidia.com#prepare-some-scripts)\n\n - [Submit a Slurm job](https://docs.nvidia.com/docs.nvidia.com#submit-a-slurm-job)\n - [Triton Metrics](https://docs.nvidia.com/docs.nvidia.com#triton-metrics)\n\n - [Benchmarking](https://docs.nvidia.com/docs.nvidia.com#benchmarking)\n\n - [Testing the TensorRT-LLM Backend](https://docs.nvidia.com/docs.nvidia.com#testing-the-tensorrt-llm-backend)\n\n## Getting Started [\\#](https://docs.nvidia.com/docs.nvidia.com\\#getting-started)\n\n### Quick Start [\\#](https://docs.nvidia.com/docs.nvidia.com\\#quick-start)\n\nBelow is an example of how to serve a TensorRT-LLM model with the Triton\nTensorRT-LLM Backend on a 4-GPU environment. The example uses the GPT model from\nthe\n[TensorRT-LLM repository](https://github.com/NVIDIA/TensorRT-LLM/tree/v0.11.0/examples/gpt)\nwith the\n[NGC Triton TensorRT-LLM container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver).\nMake sure you are cloning the same version of TensorRT-LLM backend as the\nversion of TensorRT-LLM in the container. Please refer to the\n[support matrix](https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html)\nto see the aligned versions.\n\nIn this example, we will use Triton 24.07 with TensorRT-LLM v0.11.0.\n\n#### Launch Triton TensorRT-LLM container [\\#](https://docs.nvidia.com/docs.nvidia.com\\#launch-triton-tensorrt-llm-container)\n\nLaunch Triton docker container `nvcr.io/nvidia/tritonserver: -trtllm-python-py3`\nwith TensorRT-LLM backend.\n\nMake an `engines` folder outside docker to reuse engines for future runs. Make\nsure to replace the ` ` with the version of Triton that you want to use.\n\n```\ndockerrun--rm-it--nethost--shm-size=2g\\\n--ulimitmemlock=-1--ulimitstack=67108864--gpusall\\\n-v:/engines\\\nnvcr.io/nvidia/tritonserver:24.07-trtllm-python-py3\n\n```\n\n#### Prepare TensorRT-LLM engines [\\#](https://docs.nvidia.com/docs.nvidia.com\\#prepare-tensorrt-llm-engines)\n\nYou can skip this step if you already have the engines ready.\nFollow the [guide](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples) in\nTensorRT-LLM repository for more details on how to to prepare the engines for\nall the supported models. You can also check out the\n[tutorials](https://github.com/triton-inference-server/tutorials) to see more\nexamples with serving TensorRT-LLM models.\n\n```\ncd/app/tensorrt_llm/examples/models/core/gpt\n\n# Download weights from HuggingFace Transformers\nrm-rfgpt2&&gitclonehttps://huggingface.co/gpt2-mediumgpt2\npushdgpt2&&rmpytorch_model.binmodel.safetensors&&wget-qhttps://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin&&popd\n\n# Convert weights from HF Tranformers to TensorRT-LLM checkpoint\npython3convert_checkpoint.py--model_dirgpt2\\\n--dtypefloat16\\\n--tp_size4\\\n--output_dir./c-model/gpt2/fp16/4-gpu\n\n# Build TensorRT engines\ntrtllm-build--checkpoint_dir./c-model/gpt2/fp16/4-gpu\\\n--gpt_attention_pluginfloat16\\\n--remove_input_paddingenable\\\n--kv_cache_typepaged\\\n--gemm_pluginfloat16\\\n--output_dir/engines/gpt/fp16/4-gpu\n\n```\n\nSee [here](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/gpt) for\nmore details on the parameters.\n\n#### Prepare the Model Repository [\\#](https://docs.nvidia.com/docs.nvidia.com\\#prepare-the-model-repository)\n\nNext, create the\n[model repository](https://docs.nvidia.com/user_guide/model_repository.html)\nthat will be used by the Triton server. The models can be found in the\n[all\\_models](https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/all_models) folder. The folder contains two groups of models:\n\n- [`gpt`](https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/all_models/gpt): Using TensorRT-LLM pure Python runtime.\n\n- [`inflight_batcher_llm`](https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/all_models/inflight_batcher_llm) \\`: Using the C++\nTensorRT-LLM backend with the executor API, which includes the latest features\nincluding inflight batching.\n\n\nThere are five models in\n[all\\_models/inflight\\_batcher\\_llm](https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/all_models/inflight_batcher_llm) that will\nbe used in this example:\n\n| Model | Description |\n| --- | --- |\n| `ensemble` | This model is used to chain the preprocessing, tensorrt\\_llm and postprocessing models together. |\n| `preprocessing` | This model is used for tokenizing, meaning the conversion from prompts(string) to input\\_ids(list of ints). |\n| `tensorrt_llm` | This model is a wrapper of your TensorRT-LLM model and is used for inferencing. Input specification can be found [here](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/inference-request.md) |\n| `postprocessing` | This model is used for de-tokenizing, meaning the conversion from output\\_ids(list of ints) to outputs(string). |\n| `tensorrt_llm_bls` | This model can also be used to chain the preprocessing, tensorrt\\_llm and postprocessing models together. |\n\nTo learn more about ensemble and BLS models, please see the\n[Ensemble Models](https://","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdocs.nvidia.com%2Fdeeplearning%2Ftriton-inference-server%2Fuser-guide%2Fdocs%2Ftensorrtllm_backend%2FREADME.html","id":"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/tensorrtllm_backend/README.html","publishedDate":"2025-01-01T00:00:00.000Z","summary":"This webpage documents the TensorRT-LLM backend for NVIDIA Triton Inference Server. It allows you to serve TensorRT-LLM models with Triton. The backend supports features like inflight batching and paged attention. The document provides information on getting started, building from source, supported models, model configuration, and model deployment, including multi-instance and multi-node support. You can find more information in the general Triton documentation and ask questions on the issues page.\n","title":"TensorRT-LLM Backend — NVIDIA Triton Inference Server","url":"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/tensorrtllm_backend/README.html"},{"content":"[Skip to main content](https://docs.nvidia.com/docs.nvidia.com#main-content)\n\nBack to top\n\n`Ctrl` + `K`\n\n[NVIDIA Triton Inference Server](https://docs.nvidia.com/contents.html)\n\n- [GitHub](https://github.com/triton-inference-server/server)\n\n# TensorRT-LLM User Guide [\\#](https://docs.nvidia.com/docs.nvidia.com\\#tensorrt-llm-user-guide)\n\n## What is TensorRT-LLM [\\#](https://docs.nvidia.com/docs.nvidia.com\\#what-is-tensorrt-llm)\n\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)\n(TRT-LLM) is an open-source library designed to accelerate and optimize the\ninference performance of large language models (LLMs) on NVIDIA GPUs. TRT-LLM\noffers users an easy-to-use Python API to build TensorRT engines for LLMs,\nincorporating state-of-the-art optimizations to ensure efficient inference on\nNVIDIA GPUs.\n\n## How to run TRT-LLM models with Triton Server via TensorRT-LLM backend [\\#](https://docs.nvidia.com/docs.nvidia.com\\#how-to-run-trt-llm-models-with-triton-server-via-tensorrt-llm-backend)\n\nThe\n[TensorRT-LLM Backend](https://github.com/triton-inference-server/tensorrtllm_backend)\nlets you serve TensorRT-LLM models with Triton Inference Server. Check out the\n[Getting Started](https://github.com/triton-inference-server/tensorrtllm_backend?tab=readme-ov-file#getting-started)\nsection in the TensorRT-LLM Backend repo to learn how to utlize the\n[NGC Triton TRT-LLM container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver)\nto prepare engines for your LLM models and serve them with Triton.\n\n## How to use your custom TRT-LLM model [\\#](https://docs.nvidia.com/docs.nvidia.com\\#how-to-use-your-custom-trt-llm-model)\n\nAll the supported models can be found in the\n[examples](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples) folder in\nthe TRT-LLM repo. Follow the examples to convert your models to TensorRT\nengines.\n\nAfter the engine is built, [prepare the model repository](https://github.com/triton-inference-server/tensorrtllm_backend?tab=readme-ov-file#prepare-the-model-repository)\nfor Triton, and\n[modify the model configuration](https://github.com/triton-inference-server/tensorrtllm_backend?tab=readme-ov-file#modify-the-model-configuration).\n\nOnly the _mandatory parameters_ need to be set in the model config file. Feel free\nto modify the optional parameters as needed. To learn more about the\nparameters, model inputs, and outputs, see the\n[model config documentation](https://github.com/triton-inference-server/server/blob/main/docs/getting_started/ttps:/github.com/triton-inference-server/tensorrtllm_backend/blob/main/docs/model_config.md) for more details.\n\n## Advanced Configuration Options and Deployment Strategies [\\#](https://docs.nvidia.com/docs.nvidia.com\\#advanced-configuration-options-and-deployment-strategies)\n\nExplore advanced configuration options and deployment strategies to optimize\nand run Triton with your TRT-LLM models effectively:\n\n- [Model Deployment](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main?tab=readme-ov-file#model-deployment): Techniques for efficiently deploying and managing your models in various environments.\n\n- [Multi-Instance GPU (MIG) Support](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main?tab=readme-ov-file#mig-support): Run Triton and TRT-LLM models with MIG to optimize GPU resource management.\n\n- [Scheduling](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main?tab=readme-ov-file#scheduling): Configure scheduling policies to control how requests are managed and executed.\n\n- [Key-Value Cache](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main?tab=readme-ov-file#key-value-cache): Utlizte KV cache and KV cache reuse to optimize memory usage and improve performance.\n\n- [Decoding](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main?tab=readme-ov-file#decoding): Advanced methods for generating text, including top-k, top-p, top-k top-p, beam search, Medusa, and speculative decoding.\n\n- [Chunked Context](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main?tab=readme-ov-file#chunked-context): Splitting the context into several chunks and batching them during generation phase to increase overall throughput.\n\n- [Quantization](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main?tab=readme-ov-file#quantization): Apply quantization techniques to reduce model size and enhance inference speed.\n\n- [LoRa (Low-Rank Adaptation)](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main?tab=readme-ov-file#lora): Use LoRa for efficient model fine-tuning and adaptation.\n\n\n## Tutorials [\\#](https://docs.nvidia.com/docs.nvidia.com\\#tutorials)\n\nMake sure to check out the\n[tutorials](https://github.com/triton-inference-server/tutorials) repo to see\nmore guides on serving popular LLM models with Triton Server and TensorRT-LLM,\nas well as deploying them on Kubernetes.\n\n## Benchmark [\\#](https://docs.nvidia.com/docs.nvidia.com\\#benchmark)\n\n[GenAI-Perf](https://github.com/triton-inference-server/perf_analyzer/tree/main/genai-perf)\nis a command line tool for measuring the throughput and latency of LLMs served\nby Triton Inference Server. Check out the\n[Quick Start](https://docs.nvidia.com/perf_analyzer/genai-perf/README.html#quick-start)\nto learn how to use GenAI-Perf to benchmark your LLM models.\n\n## Performance Best Practices [\\#](https://docs.nvidia.com/docs.nvidia.com\\#performance-best-practices)\n\nCheck out the\n[Performance Best Practices guide](https://nvidia.github.io/TensorRT-LLM/performance/perf-best-practices.html)\nto learn how to optimize your TensorRT-LLM models for better performance.\n\n## Metrics [\\#](https://docs.nvidia.com/docs.nvidia.com\\#metrics)\n\nTriton Server provides\n[metrics](https://docs.nvidia.com/user_guide/metrics.html)\nindicating GPU and request statistics.\nSee the\n[Triton Metrics](https://github.com/triton-inference-server/tensorrtllm_backend?tab=readme-ov-file#triton-metrics)\nsection in the TensorRT-LLM Backend repo to learn how to query the Triton\nmetrics endpoint to obtain TRT-LLM statistics.\n\n## Ask questions or report issues [\\#](https://docs.nvidia.com/docs.nvidia.com\\#ask-questions-or-report-issues)\n\nCan’t find what you’re looking for, or have a question or issue? Feel free to\nask questions or report issues in the GitHub issues page:\n\n- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/issues)\n\n- [TensorRT-LLM Backend](https://github.com/triton-inference-server/tensorrtllm_backend/issues)\n\n- [Triton Inference Server](https://github.com/triton-inference-server/server/issues)\n\n\nOn this page","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdocs.nvidia.com%2Fdeeplearning%2Ftriton-inference-server%2Fuser-guide%2Fdocs%2Fgetting_started%2Ftrtllm_user_guide.html","id":"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/trtllm_user_guide.html","publishedDate":"2025-01-01T00:00:00.000Z","summary":"This webpage is a user guide for TensorRT-LLM, an open-source library designed to accelerate and optimize the inference performance of large language models (LLMs) on NVIDIA GPUs. It explains how to run TRT-LLM models with Triton Server via the TensorRT-LLM backend, including how to prepare the model repository and modify the model configuration. The guide also covers advanced configuration options and deployment strategies such as model deployment, Multi-Instance GPU (MIG) support, scheduling, Key-Value Cache, and decoding methods.\n","title":"TensorRT-LLM User Guide — NVIDIA Triton Inference Server","url":"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/trtllm_user_guide.html"},{"content":"[Skip to main content](https://docs.nvidia.com/docs.nvidia.com#main-content)\n\nBack to top\n\n`Ctrl` + `K`\n\n[NVIDIA Triton Inference Server](https://docs.nvidia.com/contents.html)\n\n- [GitHub](https://github.com/triton-inference-server/server)\n\n# Deploying Hugging Face Llama2-7b Model in Triton [\\#](https://docs.nvidia.com/docs.nvidia.com\\#deploying-hugging-face-llama2-7b-model-in-triton)\n\nTensorRT-LLM is Nvidia’s recommended solution of running Large Language\nModels(LLMs) on Nvidia GPUs. Read more about TensoRT-LLM [here](https://github.com/NVIDIA/TensorRT-LLM)\nand Triton’s TensorRT-LLM Backend [here](https://github.com/triton-inference-server/tensorrtllm_backend).\n\n_NOTE:_ If some parts of this tutorial doesn’t work, it is possible that there\nare some version mismatches between the `tutorials` and `tensorrtllm_backend`\nrepository. Refer to [llama.md](https://docs.nvidia.com/tensorrtllm_backend/docs/llama.html)\nfor more detailed modifications if necessary. And if you are familiar with\npython, you can also try using\n[High-level API](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/high-level-api/README.md)\nfor LLM workflow.\n\n## Acquiring Llama2-7B model [\\#](https://docs.nvidia.com/docs.nvidia.com\\#acquiring-llama2-7b-model)\n\nFor this tutorial, we are using the Llama2-7B HuggingFace model with pre-trained\nweights. Clone the repo of the model with weights and tokens\n[here](https://huggingface.co/meta-llama/Llama-2-7b-hf/tree/main).\nYou will need to get permissions for the Llama2 repository as well as get access\nto the huggingface cli. To get access to the huggingface cli,\ngo here: [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).\n\n## Deploying with Triton CLI [\\#](https://docs.nvidia.com/docs.nvidia.com\\#deploying-with-triton-cli)\n\n[Triton CLI](https://github.com/triton-inference-server/triton_cli) is\nan open source command line interface that enables users to create,\ndeploy, and profile models served by the Triton Inference Server.\n\n### Launch Triton TensorRT-LLM container [\\#](https://docs.nvidia.com/docs.nvidia.com\\#launch-triton-tensorrt-llm-container)\n\nLaunch Triton docker container with TensorRT-LLM backend.\nNote that we’re mounting the acquired Llama2-7b model to `/root/.cache/huggingface`\nin the docker container so that Triton CLI could use it and skip the download\nstep.\n\nMake an `engines` folder outside docker to reuse engines for future runs.\nPlease, make sure to replace with the version of Triton that you want\nto use.\n\n```\ndockerrun--rm-it--nethost--shm-size=2g\\\n--ulimitmemlock=-1--ulimitstack=67108864--gpusall\\\n-v:/root/.cache/huggingface\\\n-v:/engines\\\nnvcr.io/nvidia/tritonserver: -trtllm-python-py3\n\n```\n\n### Install Triton CLI [\\#](https://docs.nvidia.com/docs.nvidia.com\\#install-triton-cli)\n\nInstall [the latest release](https://github.com/triton-inference-server/triton_cli/releases)\nof Triton CLI:\n\n```\nGIT_REF= \npipinstallgit+https://github.com/triton-inference-server/triton_cli.git@${GIT_REF}\n```\n\n### Prepare Triton model repository [\\#](https://docs.nvidia.com/docs.nvidia.com\\#prepare-triton-model-repository)\n\nTriton CLI has a single command `triton import` that automatically converts HF\ncheckpoint into TensorRT-LLM checkpoint format, builds TensorRT-LLM engines,\nand prepares a Triton model repository:\n\n```\nENGINE_DEST_PATH=/enginestritonimport-mllama-2-7b--backendtensorrtllm\n\n```\n\nPlease, note that specifying `ENGINE_DEST_PATH` is optional, but recommended\nif you want to re-use compiled engines in the future.\n\nAfter successful run of `triton import`, you should see the structure of\na model repository printed in the console:\n\n```\n...\ntriton - INFO - Current repo at /root/models:\nmodels/\n├── llama-2-7b/\n│ ├── 1/\n│ │ ├── lib/\n│ │ │ ├── decode.py\n│ │ │ └── triton_decoder.py\n│ │ └── model.py\n│ └── config.pbtxt\n├── postprocessing/\n│ ├── 1/\n│ │ └── model.py\n│ └── config.pbtxt\n├── preprocessing/\n│ ├── 1/\n│ │ └── model.py\n│ └── config.pbtxt\n└── tensorrt_llm/\n ├── 1/\n └── config.pbtxt\n\n```\n\n### Start Triton Inference Server [\\#](https://docs.nvidia.com/docs.nvidia.com\\#start-triton-inference-server)\n\nStart server pointing at the default model repository:\n\n```\ntriton start\n```\n\n### Send an inference request [\\#](https://docs.nvidia.com/docs.nvidia.com\\#send-an-inference-request)\n\nUse the [generate endpoint](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/protocol/extension_generate.html).\nto send an inference request to the deployed model.\n\n```\ncurl-XPOSTlocalhost:8000/v2/models/llama-2-7b/generate-d'{\"text_input\": \"What is ML?\", \"max_tokens\": 50, \"bad_words\": \"\", \"stop_words\": \"\", \"pad_id\": 2, \"end_id\": 2}'\n```\n\n> You should expect the following response:\n>\n> ```\n> {\"context_logits\":0.0,...,\"text_output\":\"What is ML?\\nML is a branch of AI that allows computers to learn from data, identify patterns, and make predictions. It is a powerful tool that can be used in a variety of industries, including healthcare, finance, and transportation.\"}\n> ```\n\n## Deploying with Triton Inference Server [\\#](https://docs.nvidia.com/docs.nvidia.com\\#deploying-with-triton-inference-server)\n\nIf you would like to hava a better control over the deployment process,\nnext steps will guide you over the process of TensorRT-LLM engine building\nprocess and Triton model repository set up.\n\n### Prerequisite: TensorRT-LLM backend [\\#](https://docs.nvidia.com/docs.nvidia.com\\#prerequisite-tensorrt-llm-backend)\n\nThis tutorial requires TensorRT-LLM Backend repository. Please note,\nthat for best user experience we recommend using the latest\n[release tag](https://github.com/triton-inference-server/tensorrtllm_backend/tags)\nof `tensorrtllm_backend` and\nthe latest [Triton Server container.](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags)\n\nTo clone TensorRT-LLM Backend repository, make sure to run the following\nset of commands.\n\n```\ngitclonehttps://github.com/triton-inference-server/tensorrtllm_backend.git--branch \n# Update the submodules\ncdtensorrtllm_backend\n# Install git-lfs if needed\napt-getupdate&&apt-getinstallgit-lfs-y--no-install-recommends\ngitlfsinstall\ngitsubmoduleupdate--init--recursive\n\n```\n\n### Launch Triton TensorRT-LLM container [\\#](https://docs.nvidia.com/docs.nvidia.com\\#id1)\n\nLaunch Triton docker container with TensorRT-LLM backend.\nNote that we’re mounting `tensorrtllm_backend` to `/tensorrtllm_backend`\nand the Llama2 model to `/Llama-2-7b-hf` in the docker container for simplicity.\nMake an `engines` folder outside docker to reuse engines for future runs.\nPlease, make sure to replace with the version of Triton that you want\nto use.\n\n```\ndockerrun--rm-it--nethost--shm-size=2g\\\n--ulimitmemlock=-1--ulimitstack=67108864--gpusall\\\n-v:/tensorrtllm_backend\\\n-v:/Llama-2-7b-hf\\\n-v:/engines\\\nnvcr.io/nvidia/tritonserver: -trtllm-python-py3\n\n```\n\nAlternatively, you can follow instructions\n[here](https://github.com/triton-inference-server/tensorrtllm_backend?tab=readme-ov-file#build-the-docker-container)\nto build Triton Server with Tensorrt-LLM Backend if you want\nto build a specialized container.\n\nDon’t forget to allow gpu usage when you launch the container.\n\n> Optional: For simplicity, we’ve condensed all following steps into\n> a [deploy\\_trtllm\\_llama.sh](https://github.com/triton-inference-server/tutorials/blob/main/Popular_Models_Guide/Llama2/tutorials/Popular_Models_Guide/Llama2/deploy_trtllm_llama.sh).\n> Make sure to clone tutorials repo to your machine and start the docker\n> container with the tutorial repo mounted to `/tutorials` by adding\n> `-v /path/to/tutorials/:/tutorials` to docker run command, listed above.\n> Then, when container has started, simply run the script via\n>\n> ```\n> /tutorials/Popular_Models_Guide/Llama2/deploy_trtllm_llama.sh \n>\n> ```\n>\n> For how to run an inference request, refer to the [Client](https://docs.nvidia.com/docs.nvidia.com#client) section\n> of this tutorial.\n\n### Create Engines for each model \\[skip this step if you already have an engine\\] [\\#](https://docs.nvidia.com/docs.nvidia.com\\#create-engines-for-each-model-skip-this-step-if-you-already-have-an-engine)\n\nTensorRT-LLM requires each model to be compiled for the configuration\nyou need before running. To do so, before you run your model for the first time\non Triton Server you will need to create a TensorRT-LLM engine.\n\nStarting with [24.04 release](https://github.com/triton-inference-server/server/releases/tag/v2.45.0),\nTriton Server TensrRT-LLM container comes with\npre-installed TensorRT-LLM package, which allows users to build engines inside\nthe Triton container. Simply follow the next steps:\n\n```\nHF_LLAMA_MODEL=/Llama-2-7b-hf\nUNIFIED_CKPT_PATH=/tmp/ckpt/llama/7b/\nENGINE_DIR=/engines/llama-2-7b/1-gpu/\nCONVERT_CHKPT_SCRIPT=/tensorrtllm_backend/tensorrt_llm/examples/llama/convert_checkpoint.py\npython3${CONVERT_CHKPT_SCRIPT}--model_dir${HF_LLAMA_MODEL}--output_dir${UNIFIED_CKPT_PATH}--dtypefloat16\ntrtllm-build--checkpoint_dir${UNIFIED_CKPT_PATH}\\\n--remove_input_paddingenable\\\n--gpt_attention_pluginfloat16\\\n--context_fmhaenable\\\n--gemm_pluginfloat16\\\n--output_dir${ENGINE_DIR}\\\n--paged_kv_cacheenable\\\n--max_batch_size4\n```\n\n> Optional: You can check test the output of the model with `run.py`\n> located in the same llama examples folder.\n>\n> ```\n> python3/tensorrtllm_backend/tensorrt_llm/examples/run.py--engine_dir=/engines/llama-2-7b/1-gpu/--max_output_len50--tokenizer_dir/Llama-2-7b-hf--input_text\"What is ML?\"\n> ```\n>\n> You should expect the following response:\n>\n> ```\n> [TensorRT-LLM] TensorRT-LLM version: 0.17.0.post1\n> ...\n> Input [Text 0]: \" What is ML?\"\n> Output [Text 0 Beam 0]: \"\n> ML is a branch of AI that allows computers to learn fromdata, identify patterns, and make predictions. It is a powerful tool that can be used in a variety of industries, including healthcare, finance, and transportation.\"\n> ```\n\n### Serving with Triton [\\#](https://docs.nvidia.com/docs.nvidia.com\\#serving-with-triton)\n\nThe last step is to create a Triton readable model. You can\nfi","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdocs.nvidia.com%2Fdeeplearning%2Ftriton-inference-server%2Fuser-guide%2Fdocs%2Ftutorials%2FPopular_Models_Guide%2FLlama2%2Ftrtllm_guide.html","id":"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/tutorials/Popular_Models_Guide/Llama2/trtllm_guide.html","publishedDate":"2025-01-01T00:00:00.000Z","summary":"This document explains how to deploy the Hugging Face Llama2-7b model in Triton Inference Server using TensorRT-LLM. It covers acquiring the Llama2-7B model from Hugging Face, launching a Triton TensorRT-LLM container, installing Triton CLI, and preparing the Triton model repository using the `triton import` command. The document provides instructions and code snippets for each step, including setting up the environment and specifying paths for the model and engines.\n","title":"Deploying Hugging Face Llama2-7b Model in Triton — NVIDIA Triton Inference Server","url":"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/tutorials/Popular_Models_Guide/Llama2/trtllm_guide.html"},{"content":"[Skip to main content](https://docs.nvidia.com/docs.nvidia.com#main-content)\n\nBack to top\n\n`Ctrl` + `K`\n\n[NVIDIA Triton Inference Server](https://docs.nvidia.com/contents.html)\n\n- [GitHub](https://github.com/triton-inference-server/server)\n\n# Deploying Hugging Face Llava1.5-7b Model in Triton [\\#](https://docs.nvidia.com/docs.nvidia.com\\#deploying-hugging-face-llava1-5-7b-model-in-triton)\n\nTensorRT-LLM is Nvidia’s recommended solution of running Large Language\nModels(LLMs) on Nvidia GPUs. Read more about TensoRT-LLM [here](https://github.com/NVIDIA/TensorRT-LLM)\nand Triton’s TensorRT-LLM Backend [here](https://github.com/triton-inference-server/tensorrtllm_backend).\n\n_NOTE:_ If some parts of this tutorial doesn’t work, it is possible that there\nare some version mismatches between the `tutorials` and `tensorrtllm_backend`\nrepository. Refer to [llama.md](https://docs.nvidia.com/tensorrtllm_backend/docs/llama.html)\nfor more detailed modifications if necessary. And if you are familiar with\npython, you can also try using\n[High-level API](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/high-level-api/README.md)\nfor LLM workflow.\n\n## Acquiring Llava1.5-7B model [\\#](https://docs.nvidia.com/docs.nvidia.com\\#acquiring-llava1-5-7b-model)\n\nFor this tutorial, we are using the Llava1.5-7B HuggingFace model with pre-trained\nweights. Clone the repo of the model with weights and tokens\n[here](https://huggingface.co/llava-hf/llava-1.5-7b-hf/tree/main).\n\n## Deploying with Triton Inference Server [\\#](https://docs.nvidia.com/docs.nvidia.com\\#deploying-with-triton-inference-server)\n\nNext steps will guide you over the process of TensorRT and TensorRT-LLM engine\nbuilding and Triton model repository set up.\n\n### Prerequisite: TensorRT-LLM backend [\\#](https://docs.nvidia.com/docs.nvidia.com\\#prerequisite-tensorrt-llm-backend)\n\nThis tutorial requires TensorRT-LLM Backend repository. Please note,\nthat for best user experience we recommend using the latest\n[release tag](https://github.com/triton-inference-server/tensorrtllm_backend/tags)\nof `tensorrtllm_backend` and\nthe latest [Triton Server container.](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags)\n\nTo clone TensorRT-LLM Backend repository, make sure to run the following\nset of commands.\n\n```\ngitclonehttps://github.com/triton-inference-server/tensorrtllm_backend.git--branch \n# Update the submodules\ncdtensorrtllm_backend\n# Install git-lfs if needed\napt-getupdate&&apt-getinstallgit-lfs-y--no-install-recommends\ngitlfsinstall\ngitsubmoduleupdate--init--recursive\n\n```\n\n### Launch Triton TensorRT-LLM container [\\#](https://docs.nvidia.com/docs.nvidia.com\\#launch-triton-tensorrt-llm-container)\n\nLaunch Triton docker container with TensorRT-LLM backend.\nNote that we’re mounting `tensorrtllm_backend` to `/tensorrtllm_backend`\nand the Llava1.5 model to `/Llava-1.5-7b-hf` in the docker container for simplicity.\nMake an `engines` folder outside docker to reuse engines for future runs.\nPlease, make sure to replace with the version of Triton that you want\nto use.\n\n```\ndockerrun--rm-it--nethost--shm-size=2g\\\n--ulimitmemlock=-1--ulimitstack=67108864--gpusall\\\n-v:/tensorrtllm_backend\\\n-v:/llava-1.5-7b-hf\\\n-v:/engines\\\n-v:/tutorials\\\nnvcr.io/nvidia/tritonserver: -trtllm-python-py3\n\n```\n\nAlternatively, you can follow instructions\n[here](https://github.com/triton-inference-server/tensorrtllm_backend?tab=readme-ov-file#build-the-docker-container)\nto build Triton Server with Tensorrt-LLM Backend if you want\nto build a specialized container.\n\nDon’t forget to allow gpu usage when you launch the container.\n\n### Create Engines for each model \\[skip this step if you already have engines\\] [\\#](https://docs.nvidia.com/docs.nvidia.com\\#create-engines-for-each-model-skip-this-step-if-you-already-have-engines)\n\nTensorRT-LLM requires each model to be compiled for the configuration\nyou need before running. To do so, before you run your model for the first time\non Triton Server you will need to create a TensorRT-LLM engine.\n\nStarting with [24.04 release](https://github.com/triton-inference-server/server/releases/tag/v2.45.0),\nTriton Server TensrRT-LLM container comes with\npre-installed TensorRT-LLM package, which allows users to build engines inside\nthe Triton container.\n\nLlava1.5 requires 2 engines: a TensorRT engine for visual components,\nand a TRT-LLM engine for the language components. This tutorial bases on 24.05\nrelease, which corresponds to `v0.9.0` version of TensorRT-LLM and\nTensorRT-LLM backend and follows [this](https://github.com/NVIDIA/TensorRT-LLM/tree/v0.9.0/examples/multimodal#llava-and-vila)\nTensorRT-LLM multi-modal guide.\n\nTo generate engines, simply follow the next steps:\n\n```\nHF_LLAVA_MODEL=/llava-1.5-7b-hf\nUNIFIED_CKPT_PATH=/tmp/ckpt/llava/7b/\nENGINE_DIR=/engines/llava1.5\nCONVERT_CHKPT_SCRIPT=/tensorrtllm_backend/tensorrt_llm/examples/llama/convert_checkpoint.py\npython3${CONVERT_CHKPT_SCRIPT}--model_dir${HF_LLAVA_MODEL}--output_dir${UNIFIED_CKPT_PATH}--dtypefloat16\ntrtllm-build--checkpoint_dir${UNIFIED_CKPT_PATH}\\\n--output_dir${ENGINE_DIR}\\\n--gemm_pluginfloat16\\\n--use_fused_mlp\\\n--max_batch_size1\\\n--max_input_len2048\\\n--max_output_len512\\\n--max_multimodal_len576# 1 (max_batch_size) * 576 (num_visual_features)\n\npython/tensorrtllm_backend/tensorrt_llm/examples/multimodal/build_visual_engine.py--model_path${HF_LLAVA_MODEL}--model_typellava--output_dir${ENGINE_DIR}\n```\n\n> Optional: You can check test the output of the model with `run.py`\n> located in the same llama examples folder.\n>\n> ```\n> python3/tensorrtllm_backend/tensorrt_llm/examples/multimodal/run.py--max_new_tokens30--hf_model_dir${HF_LLAVA_MODEL}--visual_engine_dir${ENGINE_DIR}--llm_engine_dir${ENGINE_DIR}--decoder_llm--input_text\"Question: which city is this? Answer:\"\n> ```\n>\n> You should expect the following response:\n>\n> ```\n> [TensorRT-LLM] TensorRT-LLM version: 0.9.0\n> ...\n> [06/18/2024-01:02:24] [TRT-LLM] [I] ---------------------------------------------------------\n> [06/18/2024-01:02:24] [TRT-LLM] [I]\n> [Q] Question: which city is this? Answer:\n> [06/18/2024-01:02:24] [TRT-LLM] [I]\n> [A] ['Singapore']\n> [06/18/2024-01:02:24] [TRT-LLM] [I] Generated 1 tokens\n> [06/18/2024-01:02:24] [TRT-LLM] [I] ---------------------------------------------------------\n>\n> ```\n\n### Serving with Triton [\\#](https://docs.nvidia.com/docs.nvidia.com\\#serving-with-triton)\n\nThe last step is to set up a Triton model repository. For this tutorial,\nwe provide all necessary Triton related files under `model_repository/`.\nYou simply need to provide TensorRT-LLM engine location in its `config.pbtxt`:\n\n```\nFILL_TEMPLATE_SCRIPT=/tensorrtllm_backend/tools/fill_template.py\npython3${FILL_TEMPLATE_SCRIPT}-i/tutorials/Popular_Models_Guide/Llava1.5/model_repository/tensorrt_llm/config.pbtxtengine_dir:${ENGINE_DIR}\n```\n\n1. Launch Tritonserver\n\n\nUse the [launch\\_triton\\_server.py](https://github.com/triton-inference-server/tensorrtllm_backend/blob/release/0.5.0/scripts/launch_triton_server.py) script. This launches multiple instances of `tritonserver` with MPI.\n\n```\nexportTRT_ENGINE_LOCATION=\"/engines/llava1.5/visual_encoder.engine\"\nexportHF_LOCATION=\"/llava-1.5-7b-hf\"\npython3/tensorrtllm_backend/scripts/launch_triton_server.py--world_size= --model_repo=/tutorials/Popular_Models_Guide/Llava1.5/model_repository\n\n```\n\n> You should expect the following response:\n>\n> ```\n> ...\n> I0503 22:01:25.210518 1175 grpc_server.cc:2463] Started GRPCInferenceService at 0.0.0.0:8001\n> I0503 22:01:25.211612 1175 http_server.cc:4692] Started HTTPService at 0.0.0.0:8000\n> I0503 22:01:25.254914 1175 http_server.cc:362] Started Metrics Service at 0.0.0.0:8002\n> ```\n\nTo stop Triton Server inside the container, run:\n\n```\npkilltritonserver\n\n```\n\n### Send an inference request [\\#](https://docs.nvidia.com/docs.nvidia.com\\#send-an-inference-request)\n\nYou can test the results of the run with:\n\n1. The [multi\\_modal\\_client.py](https://github.com/triton-inference-server/tutorials/blob/main/Popular_Models_Guide/Llava1.5/tutorials/Popular_Models_Guide/Llava1.5/multi_modal_client.py) script.\n\n\n```\n# Using the SDK container as an example\ndockerrun--rm-it--nethost--shm-size=2g\\\n--ulimitmemlock=-1--ulimitstack=67108864--gpusall\\\n-v/path/to/tutorials:/tutorials\nnvcr.io/nvidia/tritonserver: -py3-sdk\n\nCLIENT_SCRIPT=/tutorials/Popular_Models_Guide/Llava1.5/multi_modal_client.py\npython3${CLIENT_SCRIPT}--prompt\"Describe the picture.\"--image_url\"http://images.cocodataset.org/test2017/000000155781.jpg\"--max-tokens=15\n```\n\n> You should expect the following response:\n>\n> ```\n> Got completed request\n> The image features a city bus parked on the side of a street.\n> ```\n\n1. The [generate endpoint](https://github.com/triton-inference-server/tensorrtllm_backend/tree/release/0.5.0#query-the-server-with-the-triton-generate-endpoint).\n\n\n```\ncurl-XPOSTlocalhost:8000/v2/models/llava-1.5/generate-d'{\"prompt\":\"USER: \\nQuestion:Describe the picture. Answer:\", \"image\":\"http://images.cocodataset.org/test2017/000000155781.jpg\", \"max_tokens\":100}'\n```\n\n> You should expect the following response:\n>\n> ```\n> data: {\"completion_tokens\":77,\"finish_reason\":\"stop\",\"model_name\":\"llava-1.5\",\"model_version\":\"1\",\"prompt_tokens\":592,\"text\":\"The image features a city bus parked on the side of a street. The bus is positioned near a railroad crossing, and there is a stop sign visible in the scene. The bus is also displaying an \\\"Out of Service\\\" sign, indicating that it is not currently in operation. The street appears to be foggy, adding a sense of atmosphere to the scene. \",\"total_tokens\":669}\n> ```\n\n## References [\\#](https://docs.nvidia.com/docs.nvidia.com\\#references)\n\nFor more examples feel free to refer to [End to end workflow to run multi-modal models.](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/multimodal/README.md)\n\nOn this page","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdocs.nvidia.com%2Fdeeplearning%2Ftriton-inference-server%2Fuser-guide%2Fdocs%2Ftutorials%2FPopular_Models_Guide%2FLlava1.5%2Fllava_trtllm_guide.html","id":"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/tutorials/Popular_Models_Guide/Llava1.5/llava_trtllm_guide.html","publishedDate":"2025-01-01T00:00:00.000Z","summary":"This webpage provides a guide on deploying the Hugging Face Llava1.5-7b model in Triton Inference Server. It recommends using TensorRT-LLM for running Large Language Models (LLMs) on NVIDIA GPUs. The tutorial covers acquiring the Llava1.5-7B model, setting up the TensorRT-LLM backend, launching a Triton Docker container, and creating engines for the model. It also provides links to relevant resources like the TensorRT-LLM repository and the Triton Server container.\n","title":"Deploying Hugging Face Llava1.5-7b Model in Triton — NVIDIA Triton Inference Server","url":"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/tutorials/Popular_Models_Guide/Llava1.5/llava_trtllm_guide.html"},{"content":"[Skip to main content](https://docs.nvidia.com/docs.nvidia.com#main-content)\n\nBack to top\n\n`Ctrl` + `K`\n\n[NVIDIA Triton Inference Server](https://docs.nvidia.com/contents.html)\n\n- [GitHub](https://github.com/triton-inference-server/server)\n\n# vLLM Backend [\\#](https://docs.nvidia.com/docs.nvidia.com\\#vllm-backend)\n\nThe Triton backend for [vLLM](https://github.com/vllm-project/vllm)\nis designed to run\n[supported models](https://vllm.readthedocs.io/en/latest/models/supported_models.html)\non a\n[vLLM engine](https://github.com/vllm-project/vllm/blob/main/vllm/engine/async_llm_engine.py).\nYou can learn more about Triton backends in the [backend\\\nrepo](https://github.com/triton-inference-server/backend).\n\nThis is a [Python-based backend](https://docs.nvidia.com/backend/docs/python_based_backends.html#python-based-backends).\nWhen using this backend, all requests are placed on the\nvLLM AsyncEngine as soon as they are received. Inflight batching and paged attention is handled\nby the vLLM engine.\n\nWhere can I ask general questions about Triton and Triton backends?\nBe sure to read all the information below as well as the [general\\\nTriton documentation](https://github.com/triton-inference-server/server#triton-inference-server)\navailable in the main [server](https://github.com/triton-inference-server/server)\nrepo. If you don’t find your answer there you can ask questions on the\nmain Triton [issues page](https://github.com/triton-inference-server/server/issues).\n\n## Installing the vLLM Backend [\\#](https://docs.nvidia.com/docs.nvidia.com\\#installing-the-vllm-backend)\n\nThere are several ways to install and deploy the vLLM backend.\n\n### Option 1. Use the Pre-Built Docker Container. [\\#](https://docs.nvidia.com/docs.nvidia.com\\#option-1-use-the-pre-built-docker-container)\n\nPull a `tritonserver: -vllm-python-py3` container with vLLM backend from the\n[NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver)\nregistry. is the version of Triton that you want to use. Please note,\nthat Triton’s vLLM container has been introduced starting from 23.10 release.\n\n```\ndocker pull nvcr.io/nvidia/tritonserver: -vllm-python-py3\n```\n\n### Option 2. Build a Custom Container From Source [\\#](https://docs.nvidia.com/docs.nvidia.com\\#option-2-build-a-custom-container-from-source)\n\nYou can follow steps described in the\n[Building With Docker](https://docs.nvidia.com/customization_guide/build.html#building-with-docker)\nguide and use the\n[build.py](https://github.com/triton-inference-server/server/blob/main/build.py)\nscript.\n\nA sample command to build a Triton Server container with all options enabled is shown below. Feel free to customize flags according to your needs.\n\nPlease use [NGC registry](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver/tags)\nto get the latest version of the Triton vLLM container, which corresponds to the\nlatest YY.MM (year.month) of [Triton release](https://github.com/triton-inference-server/server/releases).\n\n```\n# YY.MM is the version of Triton.\nexport TRITON_CONTAINER_VERSION= \n./build.py -v --enable-logging\n --enable-stats\n --enable-tracing\n --enable-metrics\n --enable-gpu-metrics\n --enable-cpu-metrics\n --enable-gpu\n --filesystem=gcs\n --filesystem=s3\n --filesystem=azure_storage\n --endpoint=http\n --endpoint=grpc\n --endpoint=sagemaker\n --endpoint=vertex-ai\n --upstream-container-version=${TRITON_CONTAINER_VERSION}\n --backend=python:r${TRITON_CONTAINER_VERSION}\n --backend=vllm:r${TRITON_CONTAINER_VERSION}\n --backend=ensemble\n\n```\n\n### Option 3. Add the vLLM Backend to the Default Triton Container [\\#](https://docs.nvidia.com/docs.nvidia.com\\#option-3-add-the-vllm-backend-to-the-default-triton-container)\n\nYou can install the vLLM backend directly into the NGC Triton container.\nIn this case, please install vLLM first. You can do so by running\n`pip install vllm== `. Then, set up the vLLM backend in the\ncontainer with the following commands:\n\n```\nmkdir -p /opt/tritonserver/backends/vllm\ngit clone https://github.com/triton-inference-server/vllm_backend.git /tmp/vllm_backend\ncp -r /tmp/vllm_backend/src/* /opt/tritonserver/backends/vllm\n```\n\n## Using the vLLM Backend [\\#](https://docs.nvidia.com/docs.nvidia.com\\#using-the-vllm-backend)\n\nYou can see an example\n[model\\_repository](https://github.com/triton-inference-server/vllm_backend/blob/main/samples/model_repository)\nin the [samples](https://github.com/triton-inference-server/vllm_backend/blob/main/samples) folder.\nYou can use this as is and change the model by changing the `model` value in `model.json`.\n`model.json` represents a key-value dictionary that is fed to vLLM’s AsyncLLMEngine when initializing the model.\nYou can see supported arguments in vLLM’s\n[arg\\_utils.py](https://github.com/vllm-project/vllm/blob/main/vllm/engine/arg_utils.py).\nSpecifically,\n[here](https://github.com/vllm-project/vllm/blob/ee8217e5bee5860469204ee57077a91138c9af02/vllm/engine/arg_utils.py#L11)\nand\n[here](https://github.com/vllm-project/vllm/blob/ee8217e5bee5860469204ee57077a91138c9af02/vllm/engine/arg_utils.py#L201).\n\nFor multi-GPU support, EngineArgs like tensor\\_parallel\\_size can be specified in\n[model.json](https://github.com/triton-inference-server/vllm_backend/blob/main/samples/model_repository/vllm_model/1/model.json).\n\nNote: vLLM greedily consume up to 90% of the GPU’s memory under default settings.\nThe sample model updates this behavior by setting gpu\\_memory\\_utilization to 50%.\nYou can tweak this behavior using fields like gpu\\_memory\\_utilization and other settings in\n[model.json](https://github.com/triton-inference-server/vllm_backend/blob/main/samples/model_repository/vllm_model/1/model.json).\n\n### Launching Triton Inference Server [\\#](https://docs.nvidia.com/docs.nvidia.com\\#launching-triton-inference-server)\n\nOnce you have the model repository set up, it is time to launch the Triton server.\nWe will use the [pre-built Triton container with vLLM backend](https://docs.nvidia.com/docs.nvidia.com#option-1-use-the-pre-built-docker-container) from\n[NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver) in this example.\n\nRun the following command inside the `vllm_backend` directory:\n\n```\ndocker run --gpus all -it --net=host --rm -p 8001:8001 --shm-size=1G --ulimit memlock=-1 --ulimit stack=67108864 -v ${PWD}:/work -w /work nvcr.io/nvidia/tritonserver: -vllm-python-py3 tritonserver --model-repository ./samples/model_repository\n\n```\n\nReplace with the version of Triton that you want to use.\nNote that Triton’s vLLM container was first published starting from\n23.10 release.\n\nAfter you start Triton you will see output on the console showing\nthe server starting up and loading the model. When you see output\nlike the following, Triton is ready to accept inference requests.\n\n```\nI1030 22:33:28.291908 1 grpc_server.cc:2513] Started GRPCInferenceService at 0.0.0.0:8001\nI1030 22:33:28.292879 1 http_server.cc:4497] Started HTTPService at 0.0.0.0:8000\nI1030 22:33:28.335154 1 http_server.cc:270] Started Metrics Service at 0.0.0.0:8002\n```\n\n### Sending Your First Inference [\\#](https://docs.nvidia.com/docs.nvidia.com\\#sending-your-first-inference)\n\nAfter you\n[start Triton](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html)\nwith the\n[sample model\\_repository](https://github.com/triton-inference-server/vllm_backend/blob/main/samples/model_repository),\nyou can quickly run your first inference request with the\n[generate endpoint](https://docs.nvidia.com/protocol/extension_generate.html).\n\nTry out the command below from another terminal:\n\n```\ncurl -X POST localhost:8000/v2/models/vllm_model/generate -d '{\"text_input\": \"What is Triton Inference Server?\", \"parameters\": {\"stream\": false, \"temperature\": 0}}'\n```\n\nUpon success, you should see a response from the server like this one:\n\n```\n{\"model_name\":\"vllm_model\",\"model_version\":\"1\",\"text_output\":\"What is Triton Inference Server?\\n\\nTriton Inference Server is a server that is used by many\"}\n```\n\nIn the [samples](https://github.com/triton-inference-server/vllm_backend/blob/main/samples) folder, you can also find a sample client,\n[client.py](https://github.com/triton-inference-server/vllm_backend/blob/main/samples/client.py) which uses Triton’s\n[asyncio gRPC client library](https://docs.nvidia.com/client/README.html#id3)\nto run inference on Triton.\n\n### Running the Latest vLLM Version [\\#](https://docs.nvidia.com/docs.nvidia.com\\#running-the-latest-vllm-version)\n\nYou can check the vLLM version included in Triton Inference Server from\n[Framework Containers Support Matrix](https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html).\n_Note:_ The vLLM Triton Inference Server container has been introduced\nstarting from 23.10 release.\n\nYou can use `pip install ...` within the container to upgrade vLLM version.\n\n## Running Multiple Instances of Triton Server [\\#](https://docs.nvidia.com/docs.nvidia.com\\#running-multiple-instances-of-triton-server)\n\nIf you are running multiple instances of Triton server with a Python-based backend,\nyou need to specify a different `shm-region-prefix-name` for each server. See\n[here](https://docs.nvidia.com/python_backend/README.html#running-multiple-instances-of-triton-server)\nfor more information.\n\n## Additional vLLM outputs [\\#](https://docs.nvidia.com/docs.nvidia.com\\#additional-vllm-outputs)\n\nAdditional vLLM outputs may be requested optionally on a per-request basis. See\n[this docs](https://docs.nvidia.com/docs/additional_outputs.html) for more information.\n\n## Triton Metrics [\\#](https://docs.nvidia.com/docs.nvidia.com\\#triton-metrics)\n\nStarting with the 24.08 release of Triton, users can now obtain specific\nvLLM metrics by querying the Triton metrics endpoint (see complete vLLM metrics\n[here](https://docs.vllm.ai/en/latest/serving/metrics.html)). This can be\naccomplished by launching a Triton server in any of the ways described above\n(ensuring the build code / container is 24.08 or later) and querying the server.","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdocs.nvidia.com%2Fdeeplearning%2Ftriton-inference-server%2Fuser-guide%2Fdocs%2Fvllm_backend%2FREADME.html","id":"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/vllm_backend/README.html","publishedDate":"2025-01-01T00:00:00.000Z","summary":"The vLLM backend for NVIDIA Triton Inference Server allows you to run supported models on a vLLM engine. It's a Python-based backend that places all requests on the vLLM AsyncEngine. You can install the vLLM backend using a pre-built Docker container from NGC, building a custom container from source, or adding it to the default Triton container.\n","title":"vLLM Backend — NVIDIA Triton Inference Server","url":"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/vllm_backend/README.html"},{"content":"[Technical Blog](https://developer.nvidia.com/blog)\n\n[Subscribe](https://developer.nvidia.com/email-signup)\n\n[Related Resources](https://developer.nvidia.com/developer.nvidia.com#main-content-end)\n\n[Generative AI](https://developer.nvidia.com/blog/category/generative-ai/)\nEnglish한국어中文\n\n# OpenAI Triton on NVIDIA Blackwell Boosts AI Performance and Programmability\n\nFeb 05, 2025\n\nBy [Pradeep Ramani](https://developer.nvidia.com/blog/author/prramani/), [Jason Knight](https://developer.nvidia.com/blog/author/jaknight/), [Philippe Tillet](https://developer.nvidia.com/blog/author/philopenai/), [Thomas Raoux](https://developer.nvidia.com/blog/author/thomasraoux/), [Paweł Szczerbuk](https://developer.nvidia.com/blog/author/pawelszczerbuk/) and [Peter Bell](https://developer.nvidia.com/blog/author/peterbell10/)\n\nLike\n\n[Discuss (0)](https://developer.nvidia.com/developer.nvidia.com#entry-content-comments)\n\n- [L](https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fopenai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability%2F)\n- [T](https://twitter.com/intent/tweet?text=OpenAI+Triton+on+NVIDIA+Blackwell+Boosts+AI+Performance+and+Programmability+%7C+NVIDIA+Technical+Blog+https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fopenai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability%2F)\n- [F](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fopenai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability%2F)\n- [R](https://www.reddit.com/submit?url=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fopenai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability%2F&title=OpenAI+Triton+on+NVIDIA+Blackwell+Boosts+AI+Performance+and+Programmability+%7C+NVIDIA+Technical+Blog)\n- [E](mailto://developer.nvidia.com?subject=I'd like to share a link with you&body=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fopenai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability%2F)\n\nAI-Generated Summary\n\nLike\n\nDislike\n\n- The Triton compiler now supports NVIDIA BLACKWELL architecture, enabling developers to leverage its advanced features through a Python-based compiler, thanks to NVIDIA's collaboration with OpenAI.\n- The NVIDIA BLACKWELL architecture introduces a new Tensor Core that improves throughput and energy efficiency for matrix multiplications, with Triton's compiler infrastructure automatically exploiting these capabilities for FP8 and FP16 GEMM operations.\n- Triton's support for new precision formats, such as the Open Computing Project's microscaling formats, provides higher average precision at higher performance, with MXFP8 and MXFP4 formats offering exceptional performance and new operating points in the precision-performance trade-off space.\n\nAI-generated content may summarize information incompletely. Verify important information. [Learn more](https://www.nvidia.com/en-us/agreements/trustworthy-ai/terms/)\n\nMatrix multiplication and attention mechanisms are the computational backbone of modern AI workloads. While libraries like [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) provide highly optimized implementations, and frameworks such as [CUTLASS](https://github.com/NVIDIA/cutlass) offer deep customization, many developers and researchers need a middle ground that combines performance with programmability.\n\nThe [open-source Triton compiler](https://openai.com/index/triton/) on the [NVIDIA Blackwell architecture](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/) addresses this need by exposing Blackwell’s advanced features through an intuitive programming model.\n\nAs a result of NVIDIA’s ongoing collaboration with OpenAI, the Triton compiler now supports the NVIDIA Blackwell architecture. This ensures that developers and researchers can use the latest and greatest features from Blackwell architecture easily from the comfort of a Python-based compiler such as Triton.\n\n## Performance advances on NVIDIA Blackwell\n\nThe NVIDIA Blackwell architecture introduces substantial improvements in both raw computing power and architectural innovations. NVIDIA’s collaboration with OpenAI has focused on leveraging these capabilities transparently through Triton’s compiler infrastructure, particularly in two key areas:\n\n- Matrix multiplications including flash attention\n- New precision formats\n\n### Matrix multiplications\n\nThe NVIDIA Blackwell architecture adds a brand-new Tensor Core designed from the ground up for improved throughput and [energy efficiency](https://www.nvidia.com/en-us/glossary/energy-efficiency/).\n\nBy extending Triton’s Matrix Multiply-Accumulate (MMA) pipelining machinery, we’ve enabled automatic exploitation of NVIDIA Blackwell’s new Tensor Cores. This required careful analysis of memory access patterns and sophisticated compiler transformations to ensure correct and efficient compute / data-movement overlap.\n\nThe result is exceptional performance for both FP8 and FP16 GEMM operations out of the box, with these optimizations automatically applying to any kernel using Triton’s \\`tl.dot\\` primitive. Overall, Triton manages to achieve near-optimal performance, comparable to library implementations across several critical use cases.\n\n_Figure 1.  Performance improvements with Triton on NVIDIA Blackwell_\n\nFigure 1 shows that Triton optimizations on NVIDIA Blackwell architecture bring hardware performance improvements to users in both FP16 and FP8 in this _K_ sweep analysis for a typical generative AI size of GEMM kernel, as provided in the Triton tutorials.\n\n#### Flash attention\n\nFlash attention, a crucial primitive in modern transformer architectures, sees significant speedups on NVIDIA Blackwell through Triton, with up to 1.5x for FP16 attention over the NVIDIA Hopper GPU architecture.\n\nWhile we continue to optimize absolute performance through ongoing compiler enhancements on FP8 and other precisions, the current work helps customers readily transition to NVIDIA Blackwell on Day 0 for existing products.\n\nAnother important aspect to note here is the ability to deliver this performance gain “for free” with existing Triton flash attention implementations, requiring no code changes.\n\n_Figure 2. Large performance gains for more complex workloads_\n\nFigure 2 shows that more complex workloads, such as the flash attention kernel as provided in the Triton tutorials, again show the large performance gains of the NVIDIA Blackwell architecture when unlocked with Triton compiler improvements. Some improvements from this work have improved NVIDIA Hopper attention performance as well and don’t show up in this data.\n\n### New precision formats\n\nNVIDIA Blackwell introduces revolutionary block-scaled floating point formats, including the [Open Computing Project’s microscaling formats](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf), which Triton now unlocks forNVIDIABlackwell-powered hardware acceleration.\n\nThese formats provide higher average precision at higher performance than the non-native block-scaling techniques emulated frequently in LLM inference projects today.\n\nFor OCP format support, MXFP8 GEMMs on Triton showcase exceptional performance similar to the FP8 GEMMs performance accelerated and shown earlier in this post, while natively allowing for scaling in the Tensor Core.\n\nSimilarly, MXFP4 provides a new operating point in the precision-performance trade-off space but while offering double the hardware-accelerated performance of FP8 and MXFP8 GEMMs.\n\nTo learn more about the new block-scaled floating point support, take a look at the new [Triton tutorial](https://github.com/triton-lang/triton/pull/5813) dedicated to this functionality.\n\n## Areas of improvement going forward\n\nThe layout and packing of sub-byte datatype formats like MXFP4 still require care by the end user. We look forward to working with the community to improve the ergonomics for kernel authors and seamless framework integrations.\n\nThe earlier-referenced matrix multiplication kernels across all data types still achieve relatively low utilization when `GEMM_K` is small. This can be mitigated through manual sub-tiling in the kernel itself and has been implemented in the GEMM tutorials as an example. It will eventually be addressed transparently in the compiler through automatic warp-specialization.\n\n## More information\n\nPhillippe Tillet, the creator of Triton, and NVIDIA will be diving into the details of this NVIDIA Blackwell work and the resulting performance at the NVIDIA GTC conference on March 17.\n\n[Register to attend GTC 2025 virtually or or attend live](https://www.nvidia.com/gtc/pricing/?nvid=nv-int-bnr-346123).\n\nThis release establishes a powerful foundation for NVIDIA Blackwell support in Triton—but it’s just the beginning. Here’s how you can help shape what’s next:\n\n- Explore the complete implementation in the [/triton-lang/triton](https://github.com/triton-lang/triton) GitHub repo.\n- Join the vibrant [GPU MODE Discord](https://discord.com/invite/gpumode) community.\n- Dive into our growing [collection of examples and tutorials](https://developer.nvidia.com/blog/tag/ai-inference-microservices/).\n\nStart building with Triton on NVIDIA Blackwell today and unlock the full potential of NVIDIA’s latest architecture while maintaining complete control over your development.\n\nHave ideas or encountered issues? Contact our NVIDIA product manager [Matthew Nicely](https://github.com/manicely) by tagging him on GitHub.\n\n[Discuss (0)](https://developer.nvidia.com/developer.nvidia.com#entry-content-comments)\n\nLike\n\n## Tags\n\n[Data Center / Cloud](https://developer.nvidia.com/blog/category/data-center-cloud/) \\| [Generative AI](https://developer.nvidia.com/blog/category/generative-ai/) \\| [Cloud Services](https://developer.nvidia.com/blog/recent-posts/?industry=Cloud+Services) \\| [Blackwell](https://developer.nvidia.com/blog/recent-posts/?products=Blackwell) \\| [Hopper](https://dev","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdeveloper.nvidia.com%2Fblog%2Fopenai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability%2F","id":"https://developer.nvidia.com/blog/openai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability/","publishedDate":"2025-02-05T00:00:00.000Z","summary":"This NVIDIA technical blog post announces that the Triton compiler now supports the NVIDIA Blackwell architecture, a result of NVIDIA's collaboration with OpenAI. This allows developers to leverage Blackwell's advanced features through a Python-based compiler. The Blackwell architecture includes a new Tensor Core that improves throughput and energy efficiency for matrix multiplications. Triton's support for new precision formats, like MXFP8 and MXFP4, offers higher average precision at higher performance.\n","title":"OpenAI Triton on NVIDIA Blackwell Boosts AI Performance and Programmability | NVIDIA Technical Blog","url":"https://developer.nvidia.com/blog/openai-triton-on-nvidia-blackwell-boosts-ai-performance-and-programmability/"},{"content":"[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fba2babce8cc5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fflymy-ai%2Fhow-to-outperform-pytorch-kernels-with-openais-triton-compiler-ba2babce8cc5&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fflymy-ai%2Fhow-to-outperform-pytorch-kernels-with-openais-triton-compiler-ba2babce8cc5&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[**FlyMy.AI**](https://medium.com/flymy-ai?source=post_page---publication_nav-9343f2050356-ba2babce8cc5---------------------------------------)\n\n·\n\nThe Agentic Cloud. We make your agents fly — slashing costs and latency for AI at scale.\n\nFeatured\n\n# How to Outperform PyTorch Kernels with OpenAI’s Triton Compiler\n\n[Denis Timonin](https://medium.com/@denistimonin?source=post_page---byline--ba2babce8cc5---------------------------------------)\n\n10 min read\n\n·\n\nNov 13, 2024\n\n--\n\nListen\n\nShare\n\n> By FlyMy.AI team: Mateus, Denis\\| 01/11/2024\n\nAt FlyMy.AI, we’re building the fastest Compound AI platform, delivering unparalleled inference speed for all types of neural networks and AI applications. We take pride in our in-depth system implementation, optimizing every component from SIMD kernels to distillation techniques and protocols. This relentless focus on optimization is how we achieved the [world’s fastest real-time demo of Diffusion](http://flymy.ai) [NN](http://flymy.ai) , maintaining high image quality without compromising on FPS\n\nFor some parts in our system we use Triton, a language and compiler developed by OpenAI, aims to provide better performance than CUDA while using a higher-level language. This allows us to rewrite certain neural network algorithms in Triton, outperforming those in popular Python libraries like PyTorch — while still writing in Python.\n\nIn this post, we’ll demonstrate how well-known kernels can be implemented in Triton, showcasing its significant performance improvements. Specifically, we’ll focus on a kernel related to input normalization — **Group Normalization**.\n\n## 1\\. Types of Normalization Techniques\n\nNormalization techniques are essential in AI because they stabilize and accelerate training by ensuring that inputs to each layer are on a consistent scale, making learning more efficient. In Generative AI (GenAI), these techniques are critical for producing high-quality, realistic outputs. They maintain balance in the data, preventing issues like exploding or vanishing gradients, which can hinder model performance.\n\n**Batch Normalization** has been the most used algorithm for normalizing layers in deep learning models for quite a while now. It consists of normalizing the channels by using the mean and variance over all batches, however efficiency is diminished if the amount of batches isn’t largeenough (usually at least 32). After the mean and variance of a specific channel are computed, the channel is normalized, then scaled and shifted accordingly..\n\n**Layer Normalization** does the same thing, however over all channels of a specific batch. **Instance Normalization**, on the other hand, performs the calculations only over a channel in a batch. **Group Normalization** is a mid-term between these methods, in which the calculations are performed over a group of channels of a specific batch; this amount of channels in which the algorithm runs through has to be divisible by the amount of channels in total. The following image describes the relations between each method.\n\nPress enter or click to view image in full size\n\nImage 1: the most popular normalization techniques in AI\n\n## **2\\. Group Normalization in GenerativeAI**\n\nBatch Normalization is a widely used technique known for delivering high-quality results, but Group Normalization (GN) is particularly important in Generative AI and diffusion models. GN enhances the stability and quality of neural networks during training and overcomes the limitations of BatchNorm, which can be problematic in image generation processes.\n\nSpecifically:\n\n- **_Batch Size Independence_**: GroupNorm works well with any batch size, making it ideal for tasks with small or variable batches, such as diffusion models.\n- **_Consistent Stability_**: GroupNorm offers more reliable stability across different tasks and architectures, and it generalizes better in non-i.i.d. settings, such as when data distribution varies across batches that may not be from the same source.\n- **_No Running Statistics_**: Unlike BatchNorm, GroupNorm doesn’t require running statistics, simplifying the inference process.\n\n## 3\\. Step by Step of the Algorithm\n\nWe’re going to follow using an example of a tensor, whose batch size is 2, with each batch consisting of 4 channels, height 2 and width 2. As such, the example has the shape (N, C, H, W) = (2, 4, 2, 2)And also, let’s define the following tensors as the weights (also known as gamma) and the bias (also known as beta). These two tensors must have the same length as the amount of channels in the input tensor, as they are applied independently for each channel.\n\n```\nγ = [0.2963, −0.9232, 1.4572, −1.0557]β = [0.0984, 1.2018, −1.0154, 0.0261]\n```\n\nFinally, we define what’s the amount of groups that we’re going to work with. This value needs to be a divisor of the number of channels of the input, and so for this case we must choose between 1, 2 and 4. If we choose 1, we’re pretty much doing the same as layer normalization, and if wechoose 4, it’s going to be the same as instance normalization. To demonstrate the middle-ground that is group normalization, I’m going to choose 2 for this example.Therefore, the code using the builtin group normalization function from PyTorch should look something like this:\n\n```\nimport torcha = torch.tensor( [[[[0.0127, 0.6262], [-0.3884, -0.9077]], [[-0.5615, -0.4755], [-1.2070, 0.2371]], [[1.2504, 0.0138], [-0.5010, 0.3860]], [[-0.2113, 0.8218], [-0.4039, 1.7963]]], [[[0.6527, 1.4151], [-0.3092, -0.4007]], [[0.4586, -1.1754], [-0.3117, -0.9210]], [[-2.5762, 0.6758], [0.4550, 0.1805]], [[-0.3143, -0.8534], [0.8388, 0.7529]]]], device='cuda')groups = 2gamma = torch.tensor([0.2963, -0.9232, 1.4572, -1.0557], device='cuda')beta = torch.tensor([0.0984, 1.2018, -1.0154, 0.0261], device='cuda')res = torch.nn.functional.group_norm(a, groups, gamma, beta, 1e-5)\n```\n\nAs such, let’s calculate the value of the element (0, 0, 0, 0) (remembering that the tensor is 0-indexed), by doing the following:\n\n- First of all, let’s calculate the mean. Since we’re working with element (0, 0, 0, 0), then we must calculate the mean and variance of the first group of the first channel of the first batch. By looking at the example, we can see that we’ll be working with the following values: _0.0127, 0.6262, −0.3884, −0.9077, −0.5615, −0.4755, −1.2070, 0.2371_ In the formula below _xi_ are the elements and _n_ is the amount of elements. In this case, the mean is _µ_ = −0.3330125\n\n- Then we need to calculate the variance, which is simply the average of the square difference to the mean. We find that _σ2_ = 0.31679136\n\n- Now, we’re able to find out the new value of _xi. U_ sually we use that ϵ = 10−5, and so for our first element, we find out that _ˆxi_ = 0.6142166:\n\n- Now we can’t forget about the gammas and the betas. _γ_ is the gamma value associated to that channel, and β is also the one associated to that channel. Since we’re working with the first channel, we see that γ = 0.2963 and β = 0.0984. Plugging back in, we find that yi = 0.280392:\n\n## 4\\. Triton Implementation\n\nFirst of all, the implementation of the algorithm will vary depending on how the tensor is configured in memory. This depends on whether or not the format is in channels last or not; for a better understanding [check out here](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html). For simplicity reasons, we’re going to work as if the input tensor isin classic contiguous storage, however the difference to the other one is how we will iterate through the memory.\n\n### 4.1 Welford Combine\n\nLoading everything into GPU memory for calculations isn’t efficient, especially when working with large datasets. Some algorithms calculate variance in an online manner, meaning you don’t need the entire dataset at once. This is useful because, for each group where we calculate the mean and variance, dividing it into smaller blocks is faster than processing everything at once.\n\nThe Welford Combine algorithm is straightforward. It works by storing the running mean and the running squared difference from the mean, similar to the naive method. Here’s how it can be understood:\n\nPress enter or click to view image in full size\n\nOf course, implementing this linearly would be a bit costly, as you’d have to apply the algorithm having a separate function call for every element, so what we can do is do a bunch at the same time (size being the block size that we’re going to work with) and at the end combine them all by using the function triton.language.reduce. The Welford Combine should then look something like this:\n\n```\n@triton.jitdef welford_combine(mean1, m21, weight1, mean2, m22, weight2): delta = mean2 - mean1 new_weight = weight1 + weight2 w2_over_w = tl.where(new_weight == 0.0, 0.0, weight2/new_weight) return ( mean1 +","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fmedium.com%2Fflymy-ai%2Fhow-to-outperform-pytorch-kernels-with-openais-triton-compiler-ba2babce8cc5","id":"https://medium.com/flymy-ai/how-to-outperform-pytorch-kernels-with-openais-triton-compiler-ba2babce8cc5","publishedDate":"2025-02-12T00:00:00.000Z","summary":"This article from FlyMy.AI discusses how they use OpenAI's Triton compiler to optimize neural network algorithms, specifically focusing on Group Normalization, to achieve better performance than PyTorch kernels. The article highlights the importance of normalization techniques in AI for stabilizing and accelerating training, particularly in Generative AI.\n","title":"How to Outperform PyTorch Kernels with OpenAI’s Triton Compiler","url":"https://medium.com/flymy-ai/how-to-outperform-pytorch-kernels-with-openais-triton-compiler-ba2babce8cc5"},{"content":"[Skip to main content](https://pytorch.org/pytorch.org#ajax-content-wrap)\n\nHit enter to search or ESC to closeSearch\n\n[Close Search](https://pytorch.org/pytorch.org)\n\n[Blog](https://pytorch.org/blog/category/blog/)\n\n# Accelerating Triton Dequantization Kernels for GPTQ\n\nBy [Less Wright, Adnan Hoque (IBM)](https://pytorch.org/pytorch.org)January 16, 2024November 14th, 2024[No Comments](https://pytorch.org/blog/accelerating-triton/#respond)\n\n## TL;DR\n\nLeveraging a first principles approach, we showcase a step by step process undertaken to accelerate the current Triton GPTQ kernels by 3x (core GPTQ) and 6x (AutoGPTQ). Example: 275us to 47us on a typical Llama style inference input. The goal is to provide a helpful template for accelerating any given Triton kernel. We provide a background on Triton and GPTQ quantization and dequantization process, showcase the impact of coalesced memory access to improve shared and global memory throughput, highlight changes made to reduce warp stalling to improve total throughput, and an overview on integrating Triton kernels into PyTorch code. Longer term, we hope to surpass the existing CUDA native GPTQ kernel with our Triton kernel.\n\n_Fig 1: Performance benchmarking the optimized AutoGTPQ kernel vs the current AutoGPTQ kernel on H100_\n\n_Fig 2: Performance benchmarking the newly optimized AutoGTPQ kernel vs the current AutoGPTQ kernel on A100_\n\n_Fig 3: Even with these improvements, there remains a gap between our optimized Triton kernel and the CUDA native AutoGTPQ kernel on A100. More to come…_\n\n## 1.0 Introduction to Triton\n\nThe [Triton framework](https://openai.com/research/triton) provides a hardware agnostic way of programming and targeting GPUs, currently supporting both NVIDIA and AMD, with support for additional hardware vendors in progress. Triton is now a mainstay for PyTorch 2.0 as torch.compile decomposes eager PyTorch and re-assembles it into a high percentage of Triton kernels with PyTorch connecting code.\n\nAs Triton becomes more widely adopted, it will be essential that programmers understand how to systematically step through the Triton stack (from the high level Python down to the low-level SASS) to address performance bottlenecks in order to optimize GPU efficiency for algorithms that go beyond torch.compile generated kernels.\n\nIn this post, we will introduce some core concepts of the Triton programming language, how to identify common performance limiters in GPU kernels, and in parallel, tune a quantization kernel used in AutoGPTQ that can be used for high throughput inference applications.\n\n### Intro to GPTQ Quantization and Dequantization\n\n[GPTQ](https://arxiv.org/abs/2210.17323) is a quantization algorithm that is able to compress ultra-large (175B+) LLMs efficiently to int4 bit representation, via approximate second order information (Hessian inverse). [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) is a framework built on GPTQ, allowing for rapid dequantization and inference/serving of LLMs that have been quantized with GPTQ.\n\nAs part of the AutoGPTQ stack, they provide a Triton GPTQ kernel to handle the dequantization of a model for inference.\n\nThe basic process for INT quantization is shown below and involves determining the scale and zero point, and then computing the quantized 4bit Weight using the Scale and Zero point:\n\nWe thus store the 4 Bit weights along with the meta information of Scale and ZeroPoint for each group of weights.\n\nTo ‘dequant’ these weights, we do the following:\n\nAnd then proceed to **Matrix Multiply** the dequantized weights with the dense input feature matrix for this linear layer.\n\n## 2.0 Identify the Bottlenecks – Optimizing Matrix Multiplication\n\nAs it turns out, making a fast matrix multiplication kernel is not trivial. A naively implemented matrix multiply will rarely reach peak throughput performance on highly parallel machines like GPUs. So – we need to tackle our compute and memory subsystems in our GPU in an hierarchical fashion to make sure we are maximally utilizing each resource.\n\nWe start our optimization process, by running the unoptimized Triton Kernel, through the Nvidia Nsight Compute tool and taking a note of some important metrics and warnings:\n\nFig xy (todo)\n\nWe notice first that both compute and memory throughput are low, 7.40% and 21.19% respectively (fig xy) . Knowing that for typical inference matrix problem sizes, we are in the memory bound regime, we will attempt to optimize the kernel by applying code changes that target the memory subsystem of our A100 GPU.\n\nThe three topics this post will cover are:\n\n1. L2 Optimization\n2. Vectorized Load\n3. Warp Stalling\n\nLet’s walk through each topic, make the appropriate changes, and see its corresponding impact on our Triton Kernel. This Triton kernel is a fused dequantization kernel that dequantizes a packed int32 weight (we will refer to this as the B Matrix) Tensor into int4 weights, performs matrix multiplication with the activation tensor (refer to as the A matrix) in FP16 mode, and then stores the results back to a matrix C.\n\nThe above is referred to as W4A16 quantization. Keep in mind that the process we describe can and should be used for the development of any GPU kernel, as these are common bottlenecks in any unoptimized kernel.\n\n## 3.0 L2 Optimization\n\nThis optimization already exists in the AutoGPTQ kernel, but we’d like to dedicate a section to this to help readers better understand how mapping and execution order of thread blocks is handled in Triton. Thus, we will step through a naive mapping and then a more optimal mapping to see its corresponding impact.\n\nLet’s build up our kernel naively, starting with a “linear” load from global memory and then compare it to a more optimized “swizzled” load. Linear vs Swizzled determines the execution order of our grid of work on the GPU. Let’s take a look at the hints that the [Nvidia Nsight Compute Tool](https://developer.nvidia.com/nsight-compute) provides regarding our kernels shared memory access pattern in the naive case:\n\nTo tackle this issue we can use an approach referred to as “tile-swizzling.” The idea of this method is to launch our thread blocks in a more L2 cache friendly order.\n\nLet’s take a step back and familiarize ourselves with some Triton semantics and make a simple CUDA analogy to understand the concept better. Triton kernels launch “programs”. These so-called programs map to the concept of a Thread Block in CUDA and it is the basic unit of parallelism in a Triton Kernel. Every program has with it associated a “pid” and all the threads in a program are guaranteed to be executing the same instruction.\n\nThe Triton programs will be distributed onto your SMs in a naive-way if you do a simple linear mapping of “pid” to a 2D grid location of your output matrix C.\n\nThis 2D grid location is determined by pid\\_m and pid\\_n in Triton. We would like to exploit data and cache locality in the L2 cache of our GPU, when we distribute our grid of work. To do this in Triton we can make the following changes:\n\nThe code highlighted in red would be the naive “linear” tile ordering, and the code highlighted in green is the “swizzled” tile ordering. This type of launch promotes a sense of locality. Here is a visual to help understand this better.\n\nAfter incorporating this change, the profiler no longer complains about uncoalesced memory accesses. Let’s take a look at how our memory throughput has changed:\n\nThis change was tested on a simple load store kernel. Looking at the GPU speed of light statistics section in the profiler we also see a 112.07% increase in the memory throughput of the simple load kernel, which is what we were after with this optimization. Again, this optimization already exists in the AutoGPTQ kernel, but is the boilerplate logic that every Triton Kernel programmer will have to write in the beginning of their kernel, before any of the exciting dequantization or matrix multiply logic. It is thus important to understand that:\n\n1. This mapping is not unique\n2. Triton does not automatically handle this kind of optimization for the programmer, and careful thought must be taken to ensure your kernel is optimally handling shared memory accesses\n\nThese are not obvious for those new to Triton, as much of the shared memory access optimization is handled by the Triton compiler. However, in the cases where these are not handled by the compiler, it is important to be able to understand what tools and methods are available to us to be able to influence memory behavior.\n\n## 4.0 Vectorized Load\n\nNow, back to the original complaints of our unoptimized kernel. We want to optimize the global memory access pattern of our kernel. From the details page of the Nvidia Nsight compute tool, we see the following note, where the profiler is complaining about uncoalesced global memory accesses.\n\nLet’s dig deeper and take a look at the SASS (Assembly) Code load for an unoptimized memory read:\n\nThis load operation resulted in 32 global load operations that are 16 bit wide. This is not optimal.\n\nWe would like to do our global memory loads in a vectorized way so that it results in the least amount of load instructions. To combat this we can give the Triton Compiler some help.\n\nThe green highlighted lines above act as a compiler hint. It tells the compiler that these elements are contiguous in memory and that this load operation can be coalesced.\n\nLet’s see the effect in assembly after adding these lines.\n\nThe load is now performed in 4 global load operations that are each 128 bit wide, instead of 32 16 bit global load operations. This means 28 fewer memory fetch instructions, and importantly a coalesced memory access. This can be seen from the fact that a single thread is not accessing consecutive memory addresses, which without the compiler hint, was the behavior.\n\nThe resulting effect is 73x speedup in an isolated load operation, and after incorporating it in the full dequantization kernel we were able to see another 6% ","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpytorch.org%2Fblog%2Faccelerating-triton%2F","id":"https://pytorch.org/blog/accelerating-triton/","publishedDate":"2024-01-16T00:00:00.000Z","summary":"This blog post details the process of accelerating Triton GPTQ kernels for GPTQ, achieving a 3x speedup for core GPTQ and a 6x speedup for AutoGPTQ. The authors provide a step-by-step guide to optimizing Triton kernels, focusing on improving memory access and reducing warp stalling. They also introduce Triton and GPTQ quantization/dequantization, and benchmark the optimized kernels against existing ones.\n","title":"Accelerating Triton Dequantization Kernels for GPTQ","url":"https://pytorch.org/blog/accelerating-triton/"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.05687","id":"https://arxiv.org/abs/2507.05687","publishedDate":"2025-07-08T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs","url":"https://arxiv.org/abs/2507.05687"},{"content":"[Faculty & Staff](https://blink.ucsd.edu)\n\n2. [Home](https://blink.ucsd.edu/index.html)\n3. [Technology](https://blink.ucsd.edu/index.html)\n4. [Artificial Intelligence](https://blink.ucsd.edu/index.html)\n5. [TritonGPT](https://blink.ucsd.edu/index.html)\n6. Release Notes\n\n# TritonGPT Release Notes\n\nLast Updated: August 28, 2025 2:46:45 PM PDT\n\nGive [feedback](https://eforms.ucsd.edu/view.php?id=252329/)\n\nProvides an overview of changes to TritonGPT.\n\n## Change Log\n\nChanges to TritonGPT are documented below — they are searchable, listed chronologically, and can be sorted by each column.\n\n### TritonGPT Updates\n\n| Year | Month | What We Updated | Description |\n| --- | --- | --- | --- |\n| 2025 | August 28 | Expert Notetaker | Paste a meeting transcript and get a detailed summary with action items. |\n| 2025 | July 7 | AI Chatbot Widget | The UC San Diego Assistant is now available as a Chatbot Widget on Blink. [Find out more about the AI Chatbot Widget](https://blink.ucsd.edu/chatbot-widget.html). |\n| 2025 | June 16 | TritonGPT now available to students | Students now have access to TritonGPT. [More information is available on TritonLink](https://students.ucsd.edu/campus-services/technology/tritongpt/index.html). |\n| 2025 | June 13 | OpenAI GPT 4.1 | [GPT-4.1](https://blink.ucsd.edu/models.html) is now available for all AI Assistants. It supports text and image inputs and delivers faster, more cost-effective performance with enhanced coding, reasoning, and instruction-following capabilities. This release also includes bug fixes for the My Documents feature. |\n| 2025 | May 23 | Default LLM | Llama 4 Scout is now the default model, replacing Llama 3.3. This upgrade delivers significantly improved output quality. |\n| 2025 | May 9 | My Documents Feature | Allows users to [create folders and upload documents](https://blink.ucsd.edu/_images/ai/Documents%20TGPT.gif), making it easy to organize and combine context from multiple files to generate new, cohesive content. |\n| 2025 | Apri 25 | UC San Diego Assistant | The UC San Diego Assistant now intelligently evaluates prompts to determine when UC San Diego-specific context should be applied. This enhancement improves response accuracy in scenarios where the General AI Assistant would provide a better fit, reducing irrelevant context injection and streamlining the user experience. |\n| 2025 | April 2 | UI overhaul, multi-LLM support, enhanced context, and user customization. | TritonGPT introduces a redesigned sidebar with grouped chat history, assistant-specific threads, and a new “Explore Assistants” panel for enhanced discoverability. A streamlined assistant selection interface and prompt shortcuts improve usability and workflow. Users can now choose between multiple LLMs—GPT-4o and LLaMA 3.3 70B—based on task needs. Backend upgrades include a 128,000-token context window. The new User Settings panel enables theme toggling, auto-scroll, keyboard shortcuts, and full chat deletion. Legacy folders and dropdowns have been removed in favor of a cleaner, persistent navigation model. [More details can be found here](https://blink.ucsd.edu/4-2-2025-release.html). |\n| 2025 | February 14 | Platform updates and UX enhancements | Upgraded to the latest version of Onyx and improved the **Related Questions** feature. Users can now click on related questions to automatically populate them in the prompt window for a seamless experience. |\n| 2025 | January 14 | Platform updates | Upgraded to the latest version of Onyx (formerly Danswer), integrating chat and search functionalities into a unified experience. The update also includes an upgrade to Llama 3.3 and the expansion of indexed websites—now totaling 63—to enrich the UC San Diego Assistant's contextual knowledge and improve its ability to answer a broader range of questions. |\n| 2024 | December 13 | JD Helper Tool Overhaul, Internet Search Assistant, \"Related Questions\" feature | The JD Helper tool has been revamped to improve its functionality and reliability. Once you've developed your Position Overview, ask it to create a job posting, interview questions and other collatoral related to the hiring process.  The Internet Search Assistant has been introduced, providing real-time informaton and answers. A \"Related Questions\" feature has been added to the Fund Manager Coach, UC San Diego and Internet Assistants, providing thought-provoking topics and suggestions at the end of each response, tailored to the context of the original question. |\n| 2024 | November 26 | UX enhancements and system updates | Cleaner interface that allows for easier toggling between AI Assistants. |\n| 2024 | November 8 | Added Email Phishing Analyzer and general platform improvements | Upload or paste emails, including key details like sender, subject, and content. This assistant will analyze them, provide a risk rating, and explain the ranking. |\n| 2024 | August 13 | Feature additions and updated LLM | TritonGPT updated with a refreshed UI design and the ability to save and organize chats. Users can now upload and interact with documents using an AI Assistant. Additionally, users can search across all documents and websites, switch between Assistants within a chat flow, and access a Prompt Gallery to reuse previously created prompts. Other new features include the ability to see the context behind an Assistant's response, share chats with others, and interact with specific sources within the chat flow. A new AI Assistant, Fund Manager Coach, is now broadly available. [View more detailed release notes](https://blink.ucsd.edu/8-13-24-release.html). |\n| 2024 | May 14 | Campus rollout complete | TritonGPT is now available to all employees that are part of campus and health sciences, totalling close to 37,000 faculty, staff and student staff. If you are part of those units and are having difficulty accessing TritonGPT, [contact the Service Desk](https://blink.ucsd.edu/help-desk/index.html). |\n| 2024 | April 29 | Large Language Model (LLM) | TritonGPT now uses the Llama 3 LLM to power the UC San Diego Assistant and the General AI Assistant. Llama 3 was trained on a much larger dataset (15T tokens) compared to Llama 2 (2.2T tokens), resulting in improved performance across various benchmarks. Llama 3 offers better reasoning abilities, code generation, and instruction-following capabilities compared to Llama 2. As part of this enhancement, an upgrade was made to the Virtual Large Language Model (VLLM) environment. VLLM is a powerful and versatile open-source library that makes it easier and more efficient to use large language models (LLMs) for inference and serving. |\n| 2024 | April 15 | Authentication | TritonGPT now uses campus Single Sign-on for authentication. This replaces the previous authentication method which was the Google Authenticator. This deployment also included some minor user interface improvements. |\n| 2024 | March 28 | JD Helper | Enhancements and bug fixes to JD Helper: updated wording in the JD Helper flow to clarify what is needed in order to produce better outcomes in retrieving job cards. Enhanced JD Helper system prompt to provide more robust output. |\n| 2024 | March 20 | TritonGPT | Launched production candidate that will support use by employees of campus and health sciences. Danswer enterprise code base and larger embedding model. User experience improvements. |\n| 2024 | February | Hardware | Added additional hardware into the environment in perparation for campus launch. |\n| 2023 | October | TritonGPT | Initial proof of concept version deployed and made available to over 400 pilot users. |\n\nContact [wts@ucsd.edu](mailto:wts@ucsd.edu).\n\n{{ chatWidgetName }}\n\n\\[{{ citation.num }}\\]\n\nPowered by [TritonGPT](https://blink.ucsd.edu/technology/ai/tritongpt/index.html)\n\n- [Download Transcript](https://blink.ucsd.edu/blink.ucsd.edu)\n- [Clear Chat](https://blink.ucsd.edu/blink.ucsd.edu)\n\n{{chatWidgetBubble}}","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fblink.ucsd.edu%2Ftechnology%2Fai%2Ftritongpt%2Frelease-notes%2Findex.html","id":"https://blink.ucsd.edu/technology/ai/tritongpt/release-notes/index.html","publishedDate":null,"summary":"This webpage details the release notes for TritonGPT, a tool developed at UC San Diego. Key updates include an Expert Notetaker, an AI Chatbot Widget, student access, OpenAI GPT 4.1 integration, and improvements to the default LLM (Llama 4 Scout). Other features include a My Documents feature, enhancements to the UC San Diego Assistant, a UI overhaul, platform updates, and UX enhancements. The updates are listed chronologically in a searchable table.\n","title":"TritonGPT Release Notes - UCSD Blink","url":"https://blink.ucsd.edu/technology/ai/tritongpt/release-notes/index.html"}],"status":"completed","toolCallId":"call_REWecfEJ6FIogEGk5UYqdof7","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Very well integrated. For modern LLM training and inference stacks, Triton is “in the loop” at multiple layers, and you benefit from it even if you never write a Triton kernel yourself.\n\nWhere Triton shows up today\n\n- PyTorch 2.x training/inference\n  - TorchInductor: When you call torch.compile, Inductor lowers many fused pointwise/reduction/matmul-style regions to Triton GPU kernels. This is the default path on NVIDIA and AMD, so LLM training often runs through Triton-generated kernels for epilogues, layernorms, softmax blocks, small GEMMs, etc.\n  - Custom kernels: Many projects ship Triton kernels for attention variants, normalization, dequant, rotary embeddings, sampling, etc., so they plug directly into PyTorch modules.\n\n- Attention implementations\n  - FlashAttention: Has CUDA implementations, but there are widely used Triton ports/tutorials and research forks; PyTorch Inductor also emits Triton for fused SDPA patterns when possible. Community Triton kernels exist for causal, masked, and block-sparse variants.\n  - Emerging kernels: Triton is often the first place researchers prototype new attention patterns or KV-cache layouts because iteration speed is high.\n\n- Quantization and dequantization\n  - GPTQ/AutoGPTQ: Several teams have optimized Triton dequant kernels for 4-bit/8-bit weight formats and fused dequant+GEMM epilogues; these are used in open-source LLM inference repos.\n  - FP8/BF16/FP16: Triton’s tl.dot can target tensor/matrix-core paths (WMMA/WGMMA on NVIDIA; MFMA on AMD), so mixed-precision training/inference benefits automatically when kernels are written in Triton.\n\n- Frameworks and runtimes\n  - vLLM: Core engine is CUDA/C++/PyTorch, but it interoperates fine with PyTorch modules containing Triton-generated kernels (from Inductor or custom ops).\n  - PyTorch/XFormers/Lightning: Many examples and modules rely on Triton kernels for fused ops; these are routine in LLM training scripts.\n  - AMD ROCm: Triton is a first-class backend in PyTorch ROCm builds; the same Triton kernels run on MI200/MI300 using AMD’s MFMA paths.\n  - NVIDIA Blackwell/Hopper: Triton supports new tensor-core paths (WGMMA on Hopper; Blackwell tensor cores) and new precisions (e.g., MXFP8/MXFP4) so existing tl.dot-based kernels pick up speedups without code changes.\n\n- Serving stacks (NVIDIA Triton Inference Server vs. OpenAI Triton compiler)\n  - NVIDIA Triton Inference Server (the model server) commonly hosts engines built with TensorRT-LLM or vLLM. Those engines coexist with PyTorch components that include Triton kernels, but the server itself doesn’t require you to use Triton kernels.\n  - If you serve via pure PyTorch backends or Python backends, any Inductor/Triton kernels inside your model run as usual.\n\nWhat this means for modern LLM workflows\n\n- Training\n  - Out of the box, torch.compile will fuse many transformer subgraphs into Triton kernels, improving throughput without manual CUDA.\n  - If you have a hotspot (e.g., custom attention mask, fused norm+activation), writing a small Triton kernel is often enough to reach near-library performance and keep portability across NVIDIA/AMD.\n\n- Inference\n  - If you use TensorRT-LLM, most heavy ops come from TRT kernels; Triton is still useful for pre/post ops and custom layers in PyTorch fallback paths.\n  - If you use vLLM or eager PyTorch, Triton kernels from Inductor/custom code handle many fused ops, dequant, layernorms, small matmuls, etc.\n  - Use CUDA Graphs or runtime graph capture to amortize Python/Triton JIT overhead in short kernels; cache JIT artifacts in production.\n\n- Portability and hardware coverage\n  - One kernel source runs on NVIDIA and AMD; autotuning picks the best tile sizes per GPU (A100/H100/H200/Blackwell; MI200/MI300).\n  - New hardware features land quickly: Triton already maps tl.dot to WGMMA on Hopper and new tensor cores on Blackwell; on AMD it lowers to MFMA. This matters directly for LLM matmuls and attention.\n\nCurrent limits to keep in mind\n\n- Best-in-class vendor primitives (e.g., TensorRT-LLM fused attention/GEMM) can still be faster than generic Triton for some shapes.\n- You may need autotuning of BLOCK_M/N/K, num_warps/waves, and pipelining for peak perf per model/GPU.\n- Extremely new hardware features (special copy engines, cluster scheduling) may arrive in vendor libraries first; Triton support usually follows.\n\nBottom line\n\n- Integration: Deep. PyTorch Inductor generates Triton for lots of transformer kernels by default; many popular LLM repos ship Triton kernels; AMD and NVIDIA backends are both supported.\n- Practical payoff: You get strong performance without hand-written CUDA, quick iteration for custom/fused ops, and portability across NVIDIA/AMD—ideal for modern LLM training and for inference paths that aren’t fully covered by vendor engines.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d43d56336481a1871cbadde1538e010087fd27ec7772de"}},"resumableStreamId":"STREAM:910219fe-65a2-4688-a155-8f415da9cf0c","role":"assistant","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","timeToFirstToken":1.303,"tokens":1117,"tokensPerSecond":29.127985814123292,"updated_at":1758739786464,"userId":"google:104458561188320708185","id":"j97bt7axk00hmr9tcbj5saned17r76cr"},{"_creationTime":1758739843269.04,"_id":"j97fd7sea97f12rdch3eqnnyr57r71y2","attachmentIds":[],"content":"How well is triton implemented for Diffusion LLM (dllm) toolchains","created_at":1758739856480,"messageId":"b32bfff1-105d-409d-bf8e-df689037e1b1","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","updated_at":1758739856480,"userId":"google:104458561188320708185","id":"j97fd7sea97f12rdch3eqnnyr57r71y2","providerMetadata":{}},{"_creationTime":1758739856480.66,"_id":"j979h5vcva6wvkndhepg66rzds7r7qz4","attachmentIds":[],"byok":false,"content":"Short answer: Triton is a solid fit for diffusion + LLM-style toolchains, but “DLLM” isn’t a single standardized stack. In practice, Triton is widely used underneath PyTorch 2.x (Inductor) for both transformer and diffusion workloads, and many diffusion/LLM repos include custom Triton kernels for hotspots (attention, norm, dequant, up/downsampling). You’ll get good coverage and performance for fused pointwise/reduction blocks, attention variants, and many matmul-like pieces, with portability across NVIDIA and AMD. For highly engineered vendor kernels (e.g., TensorRT-LLM, custom CUDA in diffusion engines), those can still edge Triton on some paths.\n\nWhat “implemented” typically means in DLLM-style pipelines\n- Model authoring and training (PyTorch 2.x)\n  - torch.compile → TorchInductor lowers many fused subgraphs into Triton kernels automatically on GPU. This applies to transformer stacks and U-Net backbones used in diffusion.\n  - Commonly accelerated via Triton:\n    - LayerNorm/GroupNorm/RMSNorm\n    - Fused bias+activation+dropout epilogues\n    - Attention blocks (SDPA, causal/non-causal, masking variants); FlashAttention-style Triton kernels are popular for research/prototyping\n    - Small/medium GEMMs and epilogues\n    - Convolution-adjacent kernels in U-Net (im2col-like tiles, up/downsample ops), though very large convs still often use cuDNN/MIOpen\n\n- Inference and serving\n  - If you run eager/torch.compile or engines like vLLM-compatible PyTorch modules, Triton kernels will execute for fused ops and custom layers.\n  - If you export to ONNX → TensorRT (or TensorRT-LLM for the transformer part), those engines use vendor kernels for the heavy ops; Triton then covers custom PyTorch fallbacks or pre/post-processing.\n  - For diffusion pipelines (e.g., Stable Diffusion, SDXL), open-source repos frequently merge Triton kernels for:\n    - Fused attention in U-Net and text encoder\n    - Efficient upsample/downsample (nearest/bilinear with fused cast/pack)\n    - Memory-bound steps like normalization and elementwise schedules\n\nPerformance expectations\n- Very good for:\n  - Fused elementwise epilogues and reductions (saturates memory BW; Triton is competitive with hand CUDA)\n  - Attention blocks and matmul-ish pieces when tuned (BLOCK_M/N/K, num_warps, stages, Split-K)\n  - Custom data layouts (e.g., interleaved channels, packed low-bit weights) and dequantization fused with compute\n- Depends on vendor libraries for:\n  - Huge convolutions/FFT-heavy steps where cuDNN/MIOpen have deep specialization\n  - Ultra-optimized GEMMs covered by cuBLAS/rocBLAS or TensorRT-LLM kernels\n- Overheads to manage:\n  - JIT and Python launch overhead: mitigate with caching and CUDA Graphs/ROCm graph capture for short kernels\n  - Per-GPU autotuning: cache best configs per shape/device in production\n\nHardware coverage\n- NVIDIA: Mature backend with tensor-core paths (WMMA/WGMMA). Many diffusion/LLM kernels in the wild rely on Triton on Ampere/Hopper.\n- AMD ROCm: Good support on MI200/MI300; tl.dot lowers to MFMA. PyTorch Inductor on ROCm emits Triton by default, which benefits diffusion/LLM training/inference on AMD clusters.\n\nPractical guidance for a DLLM toolchain\n- Keep kernels tile-parameterized and add an autotune sweep for BLOCK_{M,N,K}, num_warps, and stages.\n- Use tl.dot for matmul-like compute to pick up tensor/MFMA cores automatically.\n- Fuse cheap ops aggressively (bias, activation, normalization steps, casts) to cut memory traffic.\n- Profile with Nsight Compute (NVIDIA) or ROCm profilers (AMD) to confirm:\n  - Coalesced 128-bit global accesses\n  - Matrix-core utilization where applicable\n  - Healthy occupancy without register/LDS spills\n- For diffusion U-Net conv-heavy regions, start with cuDNN/MIOpen; layer on Triton fusions around them (e.g., norm+activation, attention blocks). For transformer parts (text encoder, guidance models), Triton is often first-class.\n\nBottom line\n- There isn’t a single “DLLM implementation,” but across modern diffusion+LLM stacks built on PyTorch 2.x, Triton is well implemented and commonly used: it powers many auto-fused kernels via Inductor and enables high-performance custom kernels where libraries don’t fit. Expect near hand-tuned CUDA performance for many fused and matmul/attention kernels, with vendor libraries still leading for the biggest GEMM/conv primitives in fully optimized inference engines.","created_at":1758739856481,"messageId":"c6661c41-e037-42a6-9030-dd3d23b68c87","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Short answer: Triton is a solid fit for diffusion + LLM-style toolchains, but “DLLM” isn’t a single standardized stack. In practice, Triton is widely used underneath PyTorch 2.x (Inductor) for both transformer and diffusion workloads, and many diffusion/LLM repos include custom Triton kernels for hotspots (attention, norm, dequant, up/downsampling). You’ll get good coverage and performance for fused pointwise/reduction blocks, attention variants, and many matmul-like pieces, with portability across NVIDIA and AMD. For highly engineered vendor kernels (e.g., TensorRT-LLM, custom CUDA in diffusion engines), those can still edge Triton on some paths.\n\nWhat “implemented” typically means in DLLM-style pipelines\n- Model authoring and training (PyTorch 2.x)\n  - torch.compile → TorchInductor lowers many fused subgraphs into Triton kernels automatically on GPU. This applies to transformer stacks and U-Net backbones used in diffusion.\n  - Commonly accelerated via Triton:\n    - LayerNorm/GroupNorm/RMSNorm\n    - Fused bias+activation+dropout epilogues\n    - Attention blocks (SDPA, causal/non-causal, masking variants); FlashAttention-style Triton kernels are popular for research/prototyping\n    - Small/medium GEMMs and epilogues\n    - Convolution-adjacent kernels in U-Net (im2col-like tiles, up/downsample ops), though very large convs still often use cuDNN/MIOpen\n\n- Inference and serving\n  - If you run eager/torch.compile or engines like vLLM-compatible PyTorch modules, Triton kernels will execute for fused ops and custom layers.\n  - If you export to ONNX → TensorRT (or TensorRT-LLM for the transformer part), those engines use vendor kernels for the heavy ops; Triton then covers custom PyTorch fallbacks or pre/post-processing.\n  - For diffusion pipelines (e.g., Stable Diffusion, SDXL), open-source repos frequently merge Triton kernels for:\n    - Fused attention in U-Net and text encoder\n    - Efficient upsample/downsample (nearest/bilinear with fused cast/pack)\n    - Memory-bound steps like normalization and elementwise schedules\n\nPerformance expectations\n- Very good for:\n  - Fused elementwise epilogues and reductions (saturates memory BW; Triton is competitive with hand CUDA)\n  - Attention blocks and matmul-ish pieces when tuned (BLOCK_M/N/K, num_warps, stages, Split-K)\n  - Custom data layouts (e.g., interleaved channels, packed low-bit weights) and dequantization fused with compute\n- Depends on vendor libraries for:\n  - Huge convolutions/FFT-heavy steps where cuDNN/MIOpen have deep specialization\n  - Ultra-optimized GEMMs covered by cuBLAS/rocBLAS or TensorRT-LLM kernels\n- Overheads to manage:\n  - JIT and Python launch overhead: mitigate with caching and CUDA Graphs/ROCm graph capture for short kernels\n  - Per-GPU autotuning: cache best configs per shape/device in production\n\nHardware coverage\n- NVIDIA: Mature backend with tensor-core paths (WMMA/WGMMA). Many diffusion/LLM kernels in the wild rely on Triton on Ampere/Hopper.\n- AMD ROCm: Good support on MI200/MI300; tl.dot lowers to MFMA. PyTorch Inductor on ROCm emits Triton by default, which benefits diffusion/LLM training/inference on AMD clusters.\n\nPractical guidance for a DLLM toolchain\n- Keep kernels tile-parameterized and add an autotune sweep for BLOCK_{M,N,K}, num_warps, and stages.\n- Use tl.dot for matmul-like compute to pick up tensor/MFMA cores automatically.\n- Fuse cheap ops aggressively (bias, activation, normalization steps, casts) to cut memory traffic.\n- Profile with Nsight Compute (NVIDIA) or ROCm profilers (AMD) to confirm:\n  - Coalesced 128-bit global accesses\n  - Matrix-core utilization where applicable\n  - Healthy occupancy without register/LDS spills\n- For diffusion U-Net conv-heavy regions, start with cuDNN/MIOpen; layer on Triton fusions around them (e.g., norm+activation, attention blocks). For transformer parts (text encoder, guidance models), Triton is often first-class.\n\nBottom line\n- There isn’t a single “DLLM implementation,” but across modern diffusion+LLM stacks built on PyTorch 2.x, Triton is well implemented and commonly used: it powers many auto-fused kernels via Inductor and enables high-performance custom kernels where libraries don’t fit. Expect near hand-tuned CUDA performance for many fused and matmul/attention kernels, with vendor libraries still leading for the biggest GEMM/conv primitives in fully optimized inference engines.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d43d9433a08197a9d0730e0eb37233053ed6c18d16f8a9"}},"resumableStreamId":"STREAM:b4afeb2d-69f0-42e1-8f32-1098b4937d37","role":"assistant","status":"done","threadId":"7b782838-0ad5-46c8-8d73-1edafa252319","timeToFirstToken":1.049,"tokens":1028,"tokensPerSecond":62.03983101991551,"updated_at":1758739856481,"userId":"google:104458561188320708185","id":"j979h5vcva6wvkndhepg66rzds7r7qz4"}],"version":"6.0.2"}
