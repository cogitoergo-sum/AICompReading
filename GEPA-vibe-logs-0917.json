{"threads":[{"_creationTime":1758128265036.3057,"_id":"jd7d9ssb0r4xq4wmjg800sjjhs7qs3ep","createdAt":1758128265036,"generationStatus":"completed","lastMessageAt":1758130502260,"model":"gpt-5-chat","pinned":false,"threadId":"5e763140-e699-40d6-970e-751838959582","title":"GEPA Explanation","updatedAt":1758130502260,"userId":"google:104458561188320708185","userSetTitle":false,"visibility":"visible","id":"5e763140-e699-40d6-970e-751838959582","last_message_at":1758130502260,"created_at":1758128265036,"updated_at":1758130502260,"status":"completed","user_edited_title":false}],"messages":[{"_creationTime":1758128265036.306,"_id":"j9714eh1x9xqrfa8d8mwchf7bh7qrqz0","attachmentIds":[],"content":"https://arxiv.org/abs/2507.19457\n\nGEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\nExplain this paper to me in depth using the Feynman technique, as if you were its author.","created_at":1758128265036,"messageId":"8096ac46-88c1-4a02-8d13-9faeec77fe4f","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"5e763140-e699-40d6-970e-751838959582","updated_at":1758128265036,"userId":"google:104458561188320708185","id":"j9714eh1x9xqrfa8d8mwchf7bh7qrqz0","providerMetadata":{}},{"_creationTime":1758128265036.3062,"_id":"j97d4yzhtnmvam2gxrrd6z4frx7qrfsj","attachmentIds":[],"byok":false,"content":"Here’s the paper explained in the Feynman style—as if I wrote it—aiming for plain language, concrete examples, and why each design choice matters.\n\n1) What problem are we solving?\n- Context: When you want an LLM system to do a new task well (say, multi-hop QA with retrieval, or privacy-aware delegation), a popular approach is RL with verifiable rewards (e.g., GRPO). You run thousands of rollouts, give a numeric reward at the end, and update the policy.\n- Pain points:\n  - Sample inefficiency: tens to hundreds of thousands of rollouts are common. This is expensive and slow, especially when tool calls (search, code execution) cost real money/time.\n  - Information bottleneck: a scalar reward throws away rich signals that exist in the trajectory—reasoning steps, tool outputs, compiler/log messages, etc. We compress a detailed story into a single number and then try to learn from that tiny signal.\n\n2) The core idea in one sentence\nUse language to learn from language. Instead of only pushing gradients from a scalar reward, we directly reflect, in natural language, over the entire trajectory (prompts, thoughts, tools, outputs), mine the lessons, and evolve the prompts that govern the system—systematically, with a genetic Pareto search that preserves diverse, strong strategies.\n\n3) Why prompts, not weights?\n- Interpretability and portability: Prompts encode the “policy spec” for many compound AI systems—what modules do, in what order, with what instructions. They are editable without retraining weights, and can be moved across models.\n- Rich learning medium: Modern LLMs are good at reading text and extracting rules. The trajectory is text. The problems and fixes can be described in text. Let’s let the model learn in its native medium.\n\n4) What is GEPA?\nGEPA = Genetic-Pareto. It’s a prompt optimizer that:\n- Observes full system rollouts: all intermediate reasoning, tool calls, tool outputs, and any reward function internals you’re willing to expose (e.g., error logs).\n- Reflects in natural language: diagnoses what went wrong/right, extracts concrete rules (“When the first-hop doc covers only one entity, second-hop queries should target the related entity mentioned but not elaborated.”).\n- Proposes mutated prompts: edits any module prompt (or multiple) to encode those rules.\n- Tests candidates and keeps a Pareto front: instead of picking a single “best,” it preserves multiple non-dominated prompts capturing different tradeoffs or complementary strategies—this helps avoid local optima and overfitting to one narrow tactic.\n- Iterates: accumulate lessons across attempts, combine complementary edits, and converge with far fewer samples than RL.\n\n5) Why “Genetic” and why “Pareto”?\n- Genetic: We maintain a population of prompts; we mutate them using rule-like text edits produced by reflection; we can also recombine lessons (merge edits) across candidates when they don’t conflict.\n- Pareto: “Best” depends on multiple axes (accuracy, length/cost, safety, faithfulness, stability across instances). A Pareto frontier keeps candidates that are not strictly worse on all axes. This preserves strategic diversity—vital for generalization and for escaping local maxima.\n\n6) How does one GEPA iteration look?\n- Step A: Sample rollouts with the current population of prompts. Capture detailed traces (inputs, chain-of-thought if allowed, tool calls, outputs, error messages, partial scores).\n- Step B: Reflect with an LLM on those traces. Ask:\n  - Where did we fail and why?\n  - What specific behavior in which module caused it?\n  - What rule, constraint, or pattern could fix it?\n  - Are there conflicting objectives (e.g., recall vs precision) we should trade off?\n- Step C: Propose prompt edits: concrete modifications to module instructions, examples, guardrails, or tool-use guidance. Each edit cites the failure mode it fixes.\n- Step D: Generate multiple candidate prompts per module by applying edits or combinations.\n- Step E: Evaluate candidates on a validation slice; compute multi-metric scores.\n- Step F: Update the Pareto front; carry forward non-dominated prompt sets into the next round.\n- Repeat for a small number of rounds; you often see big gains even after the first or second iteration.\n\n7) A concrete example (multi-hop QA second-hop query)\n- Baseline prompt: “Given question and summary1, produce query.”\n- GEPA reflection notices: The second-hop query often duplicates first-hop info; it misses the “missing link” entity hinted in summary1.\n- Proposed rule: “The second-hop query should target entities mentioned in summary1 that were not retrieved in the first hop; aim to bridge entities, avoid repeating first-hop phrasing, include disambiguating attributes.”\n- Prompt mutation: We explicitly instruct the model about purpose, constraints, and patterns, e.g., “Your goal is to retrieve documents not found in first hop that are required to fully answer the user’s question; prioritize relations introduced but unresolved in summary1; avoid repeating exact first-hop terms unless essential; include qualifiers like dates or roles for precision.”\n- Result: Second-hop retrieval improves; final answer accuracy rises.\n\n8) What tasks did we test on, and what did we find?\n- Benchmarks:\n  - HotpotQA (multi-hop QA)\n  - HoVer (retrieval-augmented verification)\n  - IFBench (instruction following)\n  - PUPA (privacy-aware delegation)\n- Models:\n  - Qwen3 8B (open)\n  - GPT-4.1 mini (proprietary)\n- Results summary:\n  - Against GRPO with 24k rollouts (plus LoRA where applicable), GEPA was on average +10% and up to +19–20%, using up to 35× fewer rollouts.\n  - Against the leading prompt optimizer MIPROv2, GEPA yielded >10% gains on average across two LLMs and doubled the net optimization gains.\n  - Anecdotally, even a single reflective update often delivered a large jump.\n  - We also saw promising inference-time code optimization on NPUEval and KernelBench via reflective search.\n\n9) Why does this work better with fewer samples?\n- Language reuses structure: The model already “knows” how to parse and reason about textual patterns (failures, logs, error messages). Reflection extracts actionable, reusable rules, not just gradient nudges.\n- High leverage edits: A single prompt rule can fix an entire class of errors across many inputs (e.g., “Always cross-check the second entity’s disambiguator before finalizing.”). That’s like a global patch, not a per-example tweak.\n- Pareto frontier prevents convergence to one trick: We keep diverse strategies that shine on different subsets—this guards against overfitting and encourages robustness.\n\n10) What GEPA is not\n- Not a weight finetuner: We don’t need to alter base model weights; we operate at the prompt/spec level.\n- Not “just try many prompts”: The key is reflective diagnosis + rule extraction + principled selection and recombination, not blind search.\n- Not limited to single prompts: GEPA works over compound systems with multiple LLM modules and tools.\n\n11) When to prefer GEPA over RL?\n- Limited budget for rollouts.\n- Complex tool-using agents where trajectories contain rich textual signals (retrieval, code, APIs).\n- You need interpretability and auditability: the learned rules are readable and attributable to specific failure modes.\n- You want fast adaptation to new domains without the overhead of finetuning large models.\n\n12) When might RL still be preferable?\n- You truly only have a scalar reward and cannot expose trajectories or text feedback.\n- Extremely low-latency scenarios where the overhead of reflection is unacceptable at training time.\n- You’re optimizing subtle token-level behaviors where prompt edits are insufficient and weight-level shaping is required.\n\n13) Practical tips to use GEPA well\n- Log everything in text: reasoning, tool inputs/outputs, error messages, evaluator rationales if possible. The better the trace, the better the reflection.\n- Scope edits to modules: Tie each rule to the prompt that controls the behavior that failed; avoid bloated, monolithic instructions.\n- Use multi-objective scoring: e.g., accuracy, cost, latency, safety. Don’t let a single metric dominate; preserve non-dominated candidates.\n- Limit edit granularity: Prefer small, well-justified changes per iteration; it helps attribution and ablation.\n- Freeze wins early: If one rule clearly generalizes, bake it into the seed and continue exploring others.\n\n14) Common failure modes and how GEPA mitigates them\n- Greedy overfitting: Classic prompt optimizers chase validation quirks. GEPA’s Pareto diversity and reflection on failure modes reduce this.\n- Local optima: Genetic exploration plus combining complementary lessons helps jump valleys.\n- Conflicting heuristics: Pareto front explicitly represents tradeoffs; later iterations can reconcile or specialize.\n\n15) Why this matters beyond benchmarks\n- Production alignment without retraining: Enterprises can adapt workflows using reflective prompt evolution under tight budgets and governance constraints.\n- Safety and policy enforcement: Because rules are textual, you can encode guardrails and validate them via tests and audits.\n- Cross-model portability: Many learned instructions transfer across LLMs with minimal changes.\n\n16) What would a minimal GEPA loop look like in code terms? (conceptual)\n- Inputs: system definition (modules with prompts), dataset slice, evaluators, objectives.\n- Loop:\n  - Run K rollouts across the current prompt population.\n  - Reflect: For each failure pattern, ask an LLM to produce concrete, testable prompt edits; tag edits by target module and rationale.\n  - Generate candidate prompts by applying edits (and some recombinations).\n  - Evaluate candidates; update Pareto front.\n  - Stop when gains plateau, or budget ends; pick a small set of robust prompts.\n\n17) Key empirical takeaway\nReflective prompt evolution leverages the “textuality” of both the problem and the learner. By staying in the medium of language, we extract more learning signal per rollout than RL from sparse rewards. Combined with evolutionary Pareto search, this yields faster, stronger, more interpretable optimization of compound AI systems.\n\nIf you want, I can:\n- Walk through one of the paper’s tasks step-by-step and reconstruct the actual edits that led to gains.\n- Help you set up a minimal GEPA-style optimizer for your own pipeline (e.g., retrieval-augmented QA or an agent using tools).","created_at":1758128265037,"messageId":"e299d22a-6c64-47c7-b66f-71b10989ba56","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":null,"queries":["arXiv 2507.19457 GEPA Reflective Prompt Evolution Can Outperform Reinforcement Learning","GEPA reflective prompt evolution paper 2025 arXiv"]},"result":[{"content":"# Computer Science > Computation and Language\n\n**arXiv:2507.19457** (cs)\n\n\\[Submitted on 25 Jul 2025\\]\n\n# Title:GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\nAuthors: [Lakshya A Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal,+L+A), [Shangyin Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan,+S), [Dilara Soylu](https://arxiv.org/search/cs?searchtype=author&query=Soylu,+D), [Noah Ziems](https://arxiv.org/search/cs?searchtype=author&query=Ziems,+N), [Rishi Khare](https://arxiv.org/search/cs?searchtype=author&query=Khare,+R), [Krista Opsahl-Ong](https://arxiv.org/search/cs?searchtype=author&query=Opsahl-Ong,+K), [Arnav Singhvi](https://arxiv.org/search/cs?searchtype=author&query=Singhvi,+A), [Herumb Shandilya](https://arxiv.org/search/cs?searchtype=author&query=Shandilya,+H), [Michael J Ryan](https://arxiv.org/search/cs?searchtype=author&query=Ryan,+M+J), [Meng Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang,+M), [Christopher Potts](https://arxiv.org/search/cs?searchtype=author&query=Potts,+C), [Koushik Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen,+K), [Alexandros G. Dimakis](https://arxiv.org/search/cs?searchtype=author&query=Dimakis,+A+G), [Ion Stoica](https://arxiv.org/search/cs?searchtype=author&query=Stoica,+I), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein,+D), [Matei Zaharia](https://arxiv.org/search/cs?searchtype=author&query=Zaharia,+M), [Omar Khattab](https://arxiv.org/search/cs?searchtype=author&query=Khattab,+O)\n\nView a PDF of the paper titled GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning, by Lakshya A Agrawal and 16 other authors\n\n[View PDF](https://arxiv.org/pdf/2507.19457)\n\n> Abstract:Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.\n\n| | |\n| --- | --- |\n| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE) |\n| ACM classes: | I.2.7; I.2.6; I.2.4; I.2.8 |\n| Cite as: | [arXiv:2507.19457](https://arxiv.org/abs/2507.19457) \\[cs.CL\\] |\n| (or [arXiv:2507.19457v1](https://arxiv.org/abs/2507.19457v1) \\[cs.CL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2507.19457](https://doi.org/10.48550/arXiv.2507.19457) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Lakshya A Agrawal \\[ [view email](https://arxiv.org/show-email/af5f91b3/2507.19457)\\] **\\[v1\\]**\nFri, 25 Jul 2025 17:42:32 UTC (1,632 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning, by Lakshya A Agrawal and 16 other authors\n\n- [View PDF](https://arxiv.org/pdf/2507.19457)\n- [TeX Source](https://arxiv.org/src/2507.19457)\n- [Other Formats](https://arxiv.org/format/2507.19457)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[< prev](https://arxiv.org/prevnext?id=2507.19457&function=prev&context=cs.CL)  \\|  [next >](https://arxiv.org/prevnext?id=2507.19457&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2025-07](https://arxiv.org/list/cs.CL/2025-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2507.19457?context=cs) [cs.AI](https://arxiv.org/abs/2507.19457?context=cs.AI) [cs.LG](https://arxiv.org/abs/2507.19457?context=cs.LG) [cs.SE](https://arxiv.org/abs/2507.19457?context=cs.SE)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2507.19457)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2507.19457)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2507.19457)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2507.19457) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.19457","id":"https://arxiv.org/abs/2507.19457","publishedDate":"2025-07-25T00:00:00.000Z","summary":"The paper introduces GEPA (Genetic-Pareto), a novel prompt optimization method for large language models (LLMs) that leverages natural language reflection rather than traditional reinforcement learning (RL). Unlike RL approaches like GRPO, which require extensive rollouts, GEPA uses natural language reflection to diagnose issues, propose prompt improvements, and learn high-level rules through trial and error. This approach enables GEPA to achieve significant performance gains—outperforming RL-based methods by up to 20% while using up to 35 times fewer rollouts across various tasks. Additionally, GEPA surpasses existing prompt optimizers like MIPROv2 and shows promise for code optimization during inference. Overall, GEPA demonstrates that reflective prompt evolution can be more efficient and effective than conventional RL techniques in enhancing LLM performance.","title":"GEPA: Reflective Prompt Evolution Can Outperform Reinforcement ...","url":"https://arxiv.org/abs/2507.19457"},{"content":"Preprint.\nGEPA: REFLECTIVE PROMPT EVOLUTION CAN OUTPERFORM\nREINFORCEMENT LEARNING\nLakshya A Agrawal1, Shangyin Tan1, Dilara Soylu2, Noah Ziems4,\nRishi Khare1, Krista Opsahl-Ong5, Arnav Singhvi2,5, Herumb Shandilya2,\nMichael J Ryan2, Meng Jiang4, Christopher Potts2, Koushik Sen1,\nAlexandros G. Dimakis1,3, Ion Stoica1, Dan Klein1, Matei Zaharia1,5, Omar Khattab6\n1UC Berkeley 2Stanford University 3BespokeLabs.ai 4Notre Dame 5Databricks 6MIT\nABSTRACT\nLarge language models (LLMs) are increasingly adapted to downstream tasks via reinforcement\nlearning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thou\u0002sands of rollouts to learn new tasks. We argue that the interpretable nature of language can often\nprovide a much richer learning medium for LLMs, compared with policy gradients derived from\nsparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that\nthoroughly incorporates natural language reflection to learn high-level rules from trial and error.\nGiven any AI system containing one or more LLM prompts, GEPA samples system-level trajecto\u0002ries (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons from the Pareto\nfrontier of its own attempts. As a result of GEPA’s design, it can often turn even just a few rollouts\ninto a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up\nto 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time\nsearch strategy for code optimization.\n1 INTRODUCTION\n0 5000 10000 15000 20000 25000\nNumber of Rollouts\n42.5\n45.0\n47.5\n50.0\n52.5\n55.0\n57.5\n60.0\n62.5\nScore\nHotpotQA, Qwen3 8B\nOptimization Method\nBaseline\nMIPROv2\nGRPO\nGEPA\nValidation Performance\nTest-set Performance\n(a) HotpotQA, Qwen3 8B\n0 5000 10000 15000 20000 25000\nNumber of Rollouts\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\nScore\nHoVer, Qwen3 8B\nOptimization Method\nBaseline\nMIPROv2\nGRPO\nGEPA\nValidation Performance\nTest-set Performance\n(b) HoVer, Qwen3 8B\nFigure 1: A comparison of the learning behavior of our proposed GEPA prompt optimizer against a state-of-the-art\nprompt optimizer (MIPROv2) and the GRPO (24,000 rollouts) algorithm. As more rollouts are sampled, the prompt\noptimizers can learn much more quickly than GRPO. GEPA substantially outperforms both GRPO and MIPROv2 in\nfinal score. The Test-set star markers demonstrate the performance gap in a held-out set of questions.\nLarge language models (LLMs) have enabled the development of agents and systems that combine fuzzy natural\u0002language behavior specification with tools like retrieval and code execution. These types of systems raise the question\nof how LLMs should be “optimized” for the best downstream performance within their harness. One popular ap\u0002proach for adapting LLMs to downstream tasks is reinforcement learning with verifiable rewards (RLVR), including\n1\narXiv:2507.19457v1 [cs.CL] 25 Jul 2025\nPreprint.\nalgorithms such as Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Such RL methods cast success\nmetrics as a scalar reward observed at the end of each rollout (Lambert, 2025) and use these rewards to estimate\ngradients for policy improvement.\nWhile these RL approaches are effective, they typically require tens of thousands of rollouts in practice to fit new\ntasks. For example, recent works leveraging GRPO across a range of tasks typically use up to hundreds of thousands\nof rollouts for training (Chen et al., 2025b; Wu et al., 2025c; Zhang et al., 2025; Jin et al., 2025; Si et al., 2025; Wang\net al., 2025a; Java et al., 2025; Chen et al., 2025a; Wu et al., 2025a; Sha et al., 2025; Lin et al., 2025; Peng et al.,\n2025; Song et al., 2025). This sample inefficiency can quickly become a serious bottleneck: many downstream LLM\napplications invoke expensive tool calls, have limited inference budget for sampling from the LLM itself, or simply\ncannot finetune the weights of the largest or best-performing LLMs.\nWe observe that the rollouts sampled from even highly sophisticated LLM systems can be serialized into traces of\nnatural (and formal) language, as they contain nothing but the instructions of each LLM module, the resulting LLM\nreasoning chains, tool calls, and potentially the internal workings of the reward function (for example, compiler error\nmessages, before they are collapsed into scalar rewards). Because such serialized trajectories can be readily under\u0002stood by modern LLMs, we argue that algorithms that learn deliberately in natural language by reflecting on these\ntrajectories can potentially make much more effective use of the strong language priors that LLMs have, compared\nwith standard RL approaches.\nTo operationalize this, we introduce GEPA (Genetic-Pareto), a reflective prompt optimizer for compound AI systems\nthat merges textual reflection with multi-objective evolutionary search. GEPA iteratively mutates every prompt within\nthe AI system in light of natural language feedback drawn from new rollouts. In each mutation, the candidate prompt\nis derived from an ancestor, accumulating high-level lessons derived from observations and LLM feedback. To avoid\nthe local optima that afflict greedy prompt updates, GEPA maintains a Pareto front: instead of evolving only the global\nbest prompt, it stochastically explores the top-performing prompts for each problem instance, thereby diversifying\nstrategies and encouraging robust generalization.\nWe evaluate GEPA on four diverse tasks—multi-hop reasoning (HotpotQA; Yang et al. 2018), instruction following\n(IFBench; Pyatkin et al. 2025b), privacy-aware delegation (PUPA; Li et al. 2025a), and retrieval-augmented verifica\u0002tion (HoVer; Jiang et al. 2020)—using both open (Qwen3 8B; Yang et al. 2025; Team 2025) and proprietary (GPT-4.1\nmini; OpenAI 2025) models. Our results show that GEPA demonstrates robust generalization and is highly sample\u0002efficient: on Qwen3 8B, GEPA outperforms GRPO (24,000 rollouts with LoRA) by up to 19% while requiring up to\n35× fewer rollouts. Overall, GEPA achieves an average improvement of +10% over GRPO across all tasks. Further\u0002more, GEPA surpasses the previous state-of-the-art prompt optimizer, MIPROv2 (Opsahl-Ong et al., 2024), on every\nbenchmark and model, obtaining aggregate optimization gains of +14%, more than doubling the gains achieved by\nMIPROv2 (+7%).\nEven qualitatively, GEPA generated prompts can be highly effective. Figure 2 highlights excerpts from a prompt\ncrafted by GEPA for the query creation module of a multi-hop question answering system (Used in HotpotQA). We\nalso find that in most cases, even a single reflective prompt update can give large improvements (as highlighted in the\noptimization trajectory in Figure 5). These results demonstrate that reflective prompt evolution using language feed\u0002back enables substantial sample efficiency and robust generalization, providing a practical path to optimizing complex,\nreal-world AI workflows in data- or budget-constrained environments. Finally, we also show promising preliminary\nresults demonstrating GEPA’s use as an inference-time search strategy for code optimization over NPUEval (Kalade\n& Schelle, 2025) and KernelBench (Ouyang et al., 2025).\n2 PROBLEM STATEMENT\nCompound AI Systems. We follow related work in defining a compound AI system as any modular system com\u0002posed of one or more language model (LLM) invocations, potentially interleaved with external tool calls, orchestrated\nthrough arbitrary control flow. This definition subsumes a broad class of real-world LLM-based AI systems, including\nagents, multi-agent systems, and general-purpose scaffolding techniques like ReAct (Yao et al., 2023), Archon (Saad\u0002Falcon et al., 2025), etc. Following Soylu et al. (2024); Khattab et al. (2024); Opsahl-Ong et al. (2024); Tan et al.\n(2025), we formalize such a system as Φ = (M, C, X , Y), where M = ⟨M1, . . . , M|M|⟩ denotes language modules,\nC specifies control flow logic, and X , Y are global input/output schemas. Each module Mi = (πi, θi, Xi, Yi) is an\nLLM subcomponent: πiis its (system) prompt including instructions and few-shot demonstrations; θithe underly\u0002ing model weights; Xi\n, Yi are input/output schemas. At runtime, C orchestrates the sequencing and invocation of\nmodules—e.g., passing outputs from one module to another, invoking modules conditionally, or leveraging tool APIs.\nThis way, C can invoke different modules in any order multiples of times.\n2\nPreprint.\nSeed Prompt for Second-Hop of Multi-Hop QA System\nGiven the fields question, summary 1, produce the fields query.\nGEPA’s Optimized Prompt for Second-Hop of Multi-Hop QA System, GPT-4.1 Mini\nYou will be given two input fields: question and summary 1. Your task: Generate a new search query (query) optimized for the second hop of a\nmulti-hop retrieval system.\n• The original user question is typically complex and requires information from multiple documents to answer.\n• The first hop query is the original question (used to retrieve initial documents).\n• Your goal: generate a query to retrieve documents not found in first hop but necessary to answer the question completely.\nInput Understanding: question is the original multi-hop question posed by the user. summary 1 is a concise summary of information from a document\nretrieved in the first hop, which partially addresses the question.\nPurpose and Context:\n• Your generated query aims to find the missing pieces of information needed to fully answer the question. . . .\n• The query must retrieve relevant documents NOT found in first hop . . . for final answer extraction.\nKey Observations and Lessons:\n• First-hop documents often cover one entity or aspect.\n• Remaining relevant documents often involve connected or higher-level concepts mentioned in summary 1 but not explicitly asked in","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2507.19457","id":"https://arxiv.org/pdf/2507.19457","publishedDate":"2025-07-25T00:00:00.000Z","summary":"The paper introduces GEPA (Genetic-Pareto), a novel prompt optimization method that leverages natural language reflection to improve large language models (LLMs) more efficiently than traditional reinforcement learning (RL) approaches like GRPO. Unlike RL methods that rely on sparse scalar rewards and require thousands of rollouts, GEPA uses natural language to analyze system trajectories (e.g., reasoning steps, tool calls) and iteratively refine prompts by diagnosing issues, proposing updates, and combining lessons from its own attempts. This approach enables significant performance gains with far fewer rollouts—up to 35 times fewer—achieving on average 10% better results and up to 20% in some tasks compared to GRPO, and outperforming other prompt optimizers like MIPROv2. The method demonstrates strong results across multiple tasks, highlighting the potential of reflection-based prompt evolution to enhance LLM performance more efficiently than traditional RL techniques.","title":"[PDF] GEPA: Reflective Prompt Evolution Can Outperform Reinforcement ...","url":"https://arxiv.org/pdf/2507.19457"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F1mb8e5w%2F250719457_gepa_reflective_prompt_evolution_can%2F","id":"https://www.reddit.com/r/MachineLearning/comments/1mb8e5w/250719457_gepa_reflective_prompt_evolution_can/","publishedDate":"2025-07-28T00:00:00.000Z","summary":"Failed to fetch content","title":"[2507.19457] GEPA: Reflective Prompt Evolution Can Outperform ...","url":"https://www.reddit.com/r/MachineLearning/comments/1mb8e5w/250719457_gepa_reflective_prompt_evolution_can/"},{"content":"[Sitemap](https://noailabs.medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F46048a2f8a2b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fnoailabs.medium.com%2Fnew-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fnoailabs.medium.com%2Fnew-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# New Research: Reflective Prompt Evolution Can Outperform RL\n\n## A prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error\n\n[noailabs](https://noailabs.medium.com/?source=post_page---byline--46048a2f8a2b---------------------------------------)\n\n5 min read\n\n·\n\nJul 28, 2025\n\n--\n\nListen\n\nShare\n\n**Meta-prompting is a new level in evolution of LLM-based systems.**\n\nPress enter or click to view image in full size\n\n[https://arxiv.org/abs/2507.19457](https://arxiv.org/abs/2507.19457)\n\n> Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), w **hich often require thousands of rollouts to learn new tasks.** We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), **a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error.** Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) **and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts.** As a result of GEPA’s design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.\n\nGEPA’S REFLECTION AND PROMPT UPDATE META PROMPT\n\n```\nI provided an assistant with the following instructions to perform a task for me:‘‘‘ ‘‘‘The following are examples of different task inputs provided to the assistantalong with the assistant’s response for each of them, and some feedback on howthe assistant’s response could be better:‘‘‘ ‘‘‘Your task is to write a new instruction for the assistant.Read the inputs carefully and identify the input format and infer detailed taskdescription about the task I wish to solve with the assistant.Read all the assistant responses and the corresponding feedback. Identify allniche and domain specific factual information about the task and include it inthe instruction, as a lot of it may not be available to the assistant in thefuture. The assistant may have utilized a generalizable strategy to solve thetask, if so, include that in the instruction as well.Provide the new instructions within ‘‘‘ blocks\n```\n\nA new paper, “GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning,” introduces a novel method for optimizing Large Language Model (LLM) prompts that can significantly enhance the performance and efficiency of LLM-based systems. This approach, centered on “reflective prompt evolution,” offers a departure from traditional reinforcement learning (RL) techniques.\n\n# Core Insights of GEPA\n\nThe GEPA (Genetic-Pareto) model is a prompt optimizer that leverages natural language reflection to learn and adapt. Here are the central ideas behind this innovative approach:\n\n- **Reflective Prompt Evolution**: Unlike RL methods that rely on often sparse, scalar rewards, GEPA utilizes the rich, interpretable nature of language itself as a learning medium. It analyzes system-level trajectories, which include reasoning steps, tool usage, and outputs, to diagnose issues and refine prompts through natural language feedback.\n- **Genetic-Pareto Optimization**: GEPA employs a Genetic-Pareto algorithm to manage and evolve prompts. This involves sampling various system trajectories, identifying problems, proposing and testing prompt modifications, and integrating the most effective changes from a “Pareto frontier” of its attempts.\n- **Enhanced Efficiency and Performance**: The GEPA framework has demonstrated significant improvements over existing methods. In various tasks, it outperformed Group Relative Policy Optimization (GRPO), a reinforcement learning technique, by an average of 10% and in some cases by up to 20%, all while requiring up to 35 times fewer “rollouts” or trials. Furthermore, GEPA has shown a greater than 10% improvement over the leading prompt optimizer, MIPROv2, across two different LLMs.\n- **Broad Applicability**: The principles of GEPA can be applied to a wide range of domains. For instance, in scientific research, it can optimize prompts for analyzing complex datasets. In education, it can help create more adaptive AI-tutoring systems, and in business, it can streamline tasks like market analysis.\n\n# Mini-Framework for Adaptive LLM-based (Agentic) Systems\n\nThe core principles of GEPA can be distilled into a mini-framework for developing more adaptive and efficient LLM-based agentic systems. This framework emphasizes continuous self-improvement through reflection and evolution.\n\n**1\\. Define Agent Trajectory:**\n\n- **Components**: An agent’s “trajectory” should not just be its final output. It must include all the intermediate steps, such as its reasoning process, any tools it utilizes, and the outputs from those tools.\n- **Example**: For a research agent, the trajectory would encompass the initial query, the search terms used, the sources consulted, the summarized findings from each source, and the final synthesized report.\n\n**2\\. Implement a Reflective “Critic”:**\n\n- **Function**: After each task or a set of tasks, a “critic” LLM analyzes the agent’s trajectory.\n- **Process**: This critic uses natural language to identify shortcomings, errors, or inefficiencies in the agent’s process. It should answer questions like: “Where did the agent go wrong?”, “Could a different tool have been more effective?”, or “Was the reasoning sound?”.\n\n**3\\. Generate and Test Evolved Prompts:**\n\n- **Evolutionary Engine**: Based on the critic’s feedback, a “prompt evolution” mechanism generates alternative prompts for the agent. This can be viewed as a form of genetic algorithm where different “genes” in the prompt are modified.\n- **Pareto-based Selection**: Not all changes are universally better. A Pareto-based selection process helps to identify a set of “non-dominated” prompts that offer different trade-offs between various performance metrics (e.g., accuracy, efficiency, and resource usage).\n\n**4\\. Continuous Integration and Adaptation:**\n\n- **Update Mechanism**: The best-performing prompts from the Pareto frontier are integrated back into the agent’s core instruction set.\n- **Iterative Improvement**: This entire process — trajectory analysis, reflection, and evolution — is not a one-time setup but a continuous loop. This allows the agent to adapt to new tasks and learn from its mistakes over time, leading to a truly adaptive system.\n\n[Llm](https://medium.com/tag/llm?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Prompt Engineering](https://medium.com/tag/prompt-engineering?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Rl](https://medium.com/tag/rl?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Agents](https://medium.com/tag/agents?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[**Written by noailabs**](https://noailabs.medium.com/?source=post_page---post_author_info--46048a2f8a2b---------------------------------------)\n\n[469 followers](https://noailabs.medium.com/followers?source=post_page---post_author_info--46048a2f8a2b---------------------------------------)\n\n· [37 following](https://noailabs.medium.com/following?source=post_page---post_author_info--46048a2f8a2b---------------------------------------)\n\nTech/biz consulting, analytics, research for founders, startups, corps and govs.\n\n## No responses yet\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Press](mailto:pressinquiries@medium.com)\n\n[Blog](https://blog.medium.com/?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Rules](https:/","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fnoailabs.medium.com%2Fnew-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b","id":"https://noailabs.medium.com/new-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b","publishedDate":"2025-07-28T00:00:00.000Z","summary":"The article introduces GEPA (Genetic-Pareto), a novel prompt optimization method that leverages natural language reflection to improve large language model (LLM) performance. Unlike traditional reinforcement learning (RL) approaches like GRPO, which require extensive trial-and-error and numerous rollouts, GEPA uses natural language reflection to diagnose issues, propose prompt updates, and learn high-level rules efficiently. It samples system trajectories, reflects on them in natural language, and combines lessons from its own attempts to enhance prompts. Experimental results show GEPA outperforms RL-based methods by up to 20% with significantly fewer rollouts (up to 35x less), and surpasses other prompt optimizers like MIPROv2 by over 10%. The approach demonstrates promising applications in code optimization and offers a more interpretable, data-efficient alternative to traditional RL techniques for LLM prompt tuning.","title":"New Research: Reflective Prompt Evolution Can Outperform RL","url":"https://noailabs.medium.com/new-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b"},{"content":"[Papers](https://huggingface.co/papers)\n\narxiv:2507.19457\n\n# GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\nPublished on Jul 25\n\n·Submitted by[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)on Jul 28\n\n[Upvote\\\n\\\n24](https://huggingface.co/login?next=%2Fpapers%2F2507.19457)\n\n- +16\n\nAuthors:\n\n[Lakshya A Agrawal](https://huggingface.co/LakshyAAAgrawal)\n,\n\nShangyin Tan\n\n,\n\nDilara Soylu\n\n,\n\nNoah Ziems\n\n,\n\nRishi Khare\n\n,\n\nKrista Opsahl-Ong\n\n,\n\nArnav Singhvi\n\n,\n\nHerumb Shandilya\n\n,\n\nMichael J Ryan\n\n,\n\nMeng Jiang\n\n,\n\nChristopher Potts\n\n,\n\nKoushik Sen\n\n,\n\nAlexandros G. Dimakis\n\n,\n\nIon Stoica\n\n,\n\nDan Klein\n\n,\n\nMatei Zaharia\n\n,\n\nOmar Khattab\n\n## Abstract\n\nGEPA, a prompt optimizer using natural language reflection, outperforms RL methods like GRPO and MIPROv2 with fewer rollouts by learning high-level rules from trial and error.\n\nAI-generated summary\n\n[Large language models](https://huggingface.co/papers?q=Large%20language%20models) (LLMs) are increasingly adapted to downstream tasks via\n[reinforcement learning](https://huggingface.co/papers?q=reinforcement%20learning) (RL) methods like [Group Relative Policy Optimization](https://huggingface.co/papers?q=Group%20Relative%20Policy%20Optimization)\n( [GRPO](https://huggingface.co/papers?q=GRPO)), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce [GEPA](https://huggingface.co/papers?q=GEPA) ( [Genetic-Pareto](https://huggingface.co/papers?q=Genetic-Pareto)), a prompt\noptimizer that thoroughly incorporates [natural language reflection](https://huggingface.co/papers?q=natural%20language%20reflection) to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, [GEPA](https://huggingface.co/papers?q=GEPA) samples [system-level trajectories](https://huggingface.co/papers?q=system-level%20trajectories) (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the [Pareto frontier](https://huggingface.co/papers?q=Pareto%20frontier) of its own attempts. As a result of [GEPA](https://huggingface.co/papers?q=GEPA)'s design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, [GEPA](https://huggingface.co/papers?q=GEPA) outperforms [GRPO](https://huggingface.co/papers?q=GRPO) by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. [GEPA](https://huggingface.co/papers?q=GEPA) also outperforms the leading [prompt optimizer](https://huggingface.co/papers?q=prompt%20optimizer),\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for [code optimization](https://huggingface.co/papers?q=code%20optimization).\n\n[View arXiv page](https://arxiv.org/abs/2507.19457) [View PDF](https://arxiv.org/pdf/2507.19457) [Add to collection](https://huggingface.co/login?next=%2Fpapers%2F2507.19457)\n\n### Community\n\n[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)\nPaper authorPaper submitter [29 days ago](https://huggingface.co/huggingface.co#6887e6cdeed12b6b6149240b)\n\nLarge language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.\n\nReply\n\n[librarian-bot](https://huggingface.co/librarian-bot)\n[28 days ago](https://huggingface.co/huggingface.co#688825f0fc34a7523d2ce6e8)\n\nThis is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper.\n\nThe following papers were recommended by the Semantic Scholar API\n\n- [A Technical Survey of Reinforcement Learning Techniques for Large Language Models](https://huggingface.co/papers/2507.04136) (2025)\n- [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://huggingface.co/papers/2507.00726) (2025)\n- [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://huggingface.co/papers/2507.02804) (2025)\n- [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://huggingface.co/papers/2506.08125) (2025)\n- [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://huggingface.co/papers/2507.00417) (2025)\n- [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://huggingface.co/papers/2507.14897) (2025)\n- [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://huggingface.co/papers/2507.02834) (2025)\n\nPlease give a thumbs up to this comment if you found it helpful!\n\nIf you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\n\nYou can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`\n\nReply\n\n[millexplore](https://huggingface.co/millexplore)\n[26 days ago](https://huggingface.co/huggingface.co#688b0d36934fecfbd1df1c10)\n\nNo code? :)\n\n- 1 reply\n\n·\n\n[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)\nPaper author [1 day ago](https://huggingface.co/huggingface.co#68ac1872957cdf6c1c630450)\n\n[https://github.com/gepa-ai/gepa](https://github.com/gepa-ai/gepa)\n\n[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)\nPaper authorPaper submitter [1 day ago](https://huggingface.co/huggingface.co#68ac17dba9c8febf9a1937d4)\n\nWe have released the code at [https://github.com/gepa-ai/gepa](https://github.com/gepa-ai/gepa)\n\nReply\n\nEditPreview\n\nUpload images, audio, and videos by dragging in the text input, pasting, or clicking here.\n\nTap or paste here to upload images\n\nComment\n\n· [Sign up](https://huggingface.co/join?next=%2Fpapers%2F2507.19457) or\n[log in](https://huggingface.co/login?next=%2Fpapers%2F2507.19457) to comment\n\n[Upvote\\\n\\\n24](https://huggingface.co/login?next=%2Fpapers%2F2507.19457)\n\n- +12\n\n## Models citing this paper 0\n\nNo model linking this paper\n\nCite arxiv.org/abs/2507.19457 in a model README.md to link it from this page.\n\n## Datasets citing this paper 0\n\nNo dataset linking this paper\n\nCite arxiv.org/abs/2507.19457 in a dataset README.md to link it from this page.\n\n### Spaces citing this paper 0\n\nNo Space linking this paper\n\nCite arxiv.org/abs/2507.19457 in a Space README.md to link it from this page.\n\n## Collections including this paper 5","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fhuggingface.co%2Fpapers%2F2507.19457","id":"https://huggingface.co/papers/2507.19457","publishedDate":"2025-07-28T00:00:00.000Z","summary":"The webpage introduces GEPA (Genetic-Pareto), a novel prompt optimization method that leverages natural language reflection to improve large language model (LLM) performance. Unlike traditional reinforcement learning approaches like GRPO, which require extensive rollouts, GEPA learns high-level rules through trial and error by analyzing system trajectories (e.g., reasoning steps, tool use) and reflecting in natural language to diagnose issues and refine prompts. Experiments across four tasks show that GEPA outperforms GRPO by up to 20% with significantly fewer rollouts (up to 35 times less), and surpasses other prompt optimizers like MIPROv2 by over 10%. The approach demonstrates promise for enhancing LLM efficiency and effectiveness in various applications, including code optimization.","title":"Reflective Prompt Evolution Can Outperform Reinforcement Learning","url":"https://huggingface.co/papers/2507.19457"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F394049542_GEPA_Reflective_Prompt_Evolution_Can_Outperform_Reinforcement_Learning","id":"https://www.researchgate.net/publication/394049542_GEPA_Reflective_Prompt_Evolution_Can_Outperform_Reinforcement_Learning","publishedDate":"2025-07-30T00:00:00.000Z","summary":"Failed to fetch content","title":"Reflective Prompt Evolution Can Outperform Reinforcement Learning","url":"https://www.researchgate.net/publication/394049542_GEPA_Reflective_Prompt_Evolution_Can_Outperform_Reinforcement_Learning"}],"status":"completed","toolCallId":"call_SZBIoMl0m12h9XBfhscREIES","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Here’s the paper explained in the Feynman style—as if I wrote it—aiming for plain language, concrete examples, and why each design choice matters.\n\n1) What problem are we solving?\n- Context: When you want an LLM system to do a new task well (say, multi-hop QA with retrieval, or privacy-aware delegation), a popular approach is RL with verifiable rewards (e.g., GRPO). You run thousands of rollouts, give a numeric reward at the end, and update the policy.\n- Pain points:\n  - Sample inefficiency: tens to hundreds of thousands of rollouts are common. This is expensive and slow, especially when tool calls (search, code execution) cost real money/time.\n  - Information bottleneck: a scalar reward throws away rich signals that exist in the trajectory—reasoning steps, tool outputs, compiler/log messages, etc. We compress a detailed story into a single number and then try to learn from that tiny signal.\n\n2) The core idea in one sentence\nUse language to learn from language. Instead of only pushing gradients from a scalar reward, we directly reflect, in natural language, over the entire trajectory (prompts, thoughts, tools, outputs), mine the lessons, and evolve the prompts that govern the system—systematically, with a genetic Pareto search that preserves diverse, strong strategies.\n\n3) Why prompts, not weights?\n- Interpretability and portability: Prompts encode the “policy spec” for many compound AI systems—what modules do, in what order, with what instructions. They are editable without retraining weights, and can be moved across models.\n- Rich learning medium: Modern LLMs are good at reading text and extracting rules. The trajectory is text. The problems and fixes can be described in text. Let’s let the model learn in its native medium.\n\n4) What is GEPA?\nGEPA = Genetic-Pareto. It’s a prompt optimizer that:\n- Observes full system rollouts: all intermediate reasoning, tool calls, tool outputs, and any reward function internals you’re willing to expose (e.g., error logs).\n- Reflects in natural language: diagnoses what went wrong/right, extracts concrete rules (“When the first-hop doc covers only one entity, second-hop queries should target the related entity mentioned but not elaborated.”).\n- Proposes mutated prompts: edits any module prompt (or multiple) to encode those rules.\n- Tests candidates and keeps a Pareto front: instead of picking a single “best,” it preserves multiple non-dominated prompts capturing different tradeoffs or complementary strategies—this helps avoid local optima and overfitting to one narrow tactic.\n- Iterates: accumulate lessons across attempts, combine complementary edits, and converge with far fewer samples than RL.\n\n5) Why “Genetic” and why “Pareto”?\n- Genetic: We maintain a population of prompts; we mutate them using rule-like text edits produced by reflection; we can also recombine lessons (merge edits) across candidates when they don’t conflict.\n- Pareto: “Best” depends on multiple axes (accuracy, length/cost, safety, faithfulness, stability across instances). A Pareto frontier keeps candidates that are not strictly worse on all axes. This preserves strategic diversity—vital for generalization and for escaping local maxima.\n\n6) How does one GEPA iteration look?\n- Step A: Sample rollouts with the current population of prompts. Capture detailed traces (inputs, chain-of-thought if allowed, tool calls, outputs, error messages, partial scores).\n- Step B: Reflect with an LLM on those traces. Ask:\n  - Where did we fail and why?\n  - What specific behavior in which module caused it?\n  - What rule, constraint, or pattern could fix it?\n  - Are there conflicting objectives (e.g., recall vs precision) we should trade off?\n- Step C: Propose prompt edits: concrete modifications to module instructions, examples, guardrails, or tool-use guidance. Each edit cites the failure mode it fixes.\n- Step D: Generate multiple candidate prompts per module by applying edits or combinations.\n- Step E: Evaluate candidates on a validation slice; compute multi-metric scores.\n- Step F: Update the Pareto front; carry forward non-dominated prompt sets into the next round.\n- Repeat for a small number of rounds; you often see big gains even after the first or second iteration.\n\n7) A concrete example (multi-hop QA second-hop query)\n- Baseline prompt: “Given question and summary1, produce query.”\n- GEPA reflection notices: The second-hop query often duplicates first-hop info; it misses the “missing link” entity hinted in summary1.\n- Proposed rule: “The second-hop query should target entities mentioned in summary1 that were not retrieved in the first hop; aim to bridge entities, avoid repeating first-hop phrasing, include disambiguating attributes.”\n- Prompt mutation: We explicitly instruct the model about purpose, constraints, and patterns, e.g., “Your goal is to retrieve documents not found in first hop that are required to fully answer the user’s question; prioritize relations introduced but unresolved in summary1; avoid repeating exact first-hop terms unless essential; include qualifiers like dates or roles for precision.”\n- Result: Second-hop retrieval improves; final answer accuracy rises.\n\n8) What tasks did we test on, and what did we find?\n- Benchmarks:\n  - HotpotQA (multi-hop QA)\n  - HoVer (retrieval-augmented verification)\n  - IFBench (instruction following)\n  - PUPA (privacy-aware delegation)\n- Models:\n  - Qwen3 8B (open)\n  - GPT-4.1 mini (proprietary)\n- Results summary:\n  - Against GRPO with 24k rollouts (plus LoRA where applicable), GEPA was on average +10% and up to +19–20%, using up to 35× fewer rollouts.\n  - Against the leading prompt optimizer MIPROv2, GEPA yielded >10% gains on average across two LLMs and doubled the net optimization gains.\n  - Anecdotally, even a single reflective update often delivered a large jump.\n  - We also saw promising inference-time code optimization on NPUEval and KernelBench via reflective search.\n\n9) Why does this work better with fewer samples?\n- Language reuses structure: The model already “knows” how to parse and reason about textual patterns (failures, logs, error messages). Reflection extracts actionable, reusable rules, not just gradient nudges.\n- High leverage edits: A single prompt rule can fix an entire class of errors across many inputs (e.g., “Always cross-check the second entity’s disambiguator before finalizing.”). That’s like a global patch, not a per-example tweak.\n- Pareto frontier prevents convergence to one trick: We keep diverse strategies that shine on different subsets—this guards against overfitting and encourages robustness.\n\n10) What GEPA is not\n- Not a weight finetuner: We don’t need to alter base model weights; we operate at the prompt/spec level.\n- Not “just try many prompts”: The key is reflective diagnosis + rule extraction + principled selection and recombination, not blind search.\n- Not limited to single prompts: GEPA works over compound systems with multiple LLM modules and tools.\n\n11) When to prefer GEPA over RL?\n- Limited budget for rollouts.\n- Complex tool-using agents where trajectories contain rich textual signals (retrieval, code, APIs).\n- You need interpretability and auditability: the learned rules are readable and attributable to specific failure modes.\n- You want fast adaptation to new domains without the overhead of finetuning large models.\n\n12) When might RL still be preferable?\n- You truly only have a scalar reward and cannot expose trajectories or text feedback.\n- Extremely low-latency scenarios where the overhead of reflection is unacceptable at training time.\n- You’re optimizing subtle token-level behaviors where prompt edits are insufficient and weight-level shaping is required.\n\n13) Practical tips to use GEPA well\n- Log everything in text: reasoning, tool inputs/outputs, error messages, evaluator rationales if possible. The better the trace, the better the reflection.\n- Scope edits to modules: Tie each rule to the prompt that controls the behavior that failed; avoid bloated, monolithic instructions.\n- Use multi-objective scoring: e.g., accuracy, cost, latency, safety. Don’t let a single metric dominate; preserve non-dominated candidates.\n- Limit edit granularity: Prefer small, well-justified changes per iteration; it helps attribution and ablation.\n- Freeze wins early: If one rule clearly generalizes, bake it into the seed and continue exploring others.\n\n14) Common failure modes and how GEPA mitigates them\n- Greedy overfitting: Classic prompt optimizers chase validation quirks. GEPA’s Pareto diversity and reflection on failure modes reduce this.\n- Local optima: Genetic exploration plus combining complementary lessons helps jump valleys.\n- Conflicting heuristics: Pareto front explicitly represents tradeoffs; later iterations can reconcile or specialize.\n\n15) Why this matters beyond benchmarks\n- Production alignment without retraining: Enterprises can adapt workflows using reflective prompt evolution under tight budgets and governance constraints.\n- Safety and policy enforcement: Because rules are textual, you can encode guardrails and validate them via tests and audits.\n- Cross-model portability: Many learned instructions transfer across LLMs with minimal changes.\n\n16) What would a minimal GEPA loop look like in code terms? (conceptual)\n- Inputs: system definition (modules with prompts), dataset slice, evaluators, objectives.\n- Loop:\n  - Run K rollouts across the current prompt population.\n  - Reflect: For each failure pattern, ask an LLM to produce concrete, testable prompt edits; tag edits by target module and rationale.\n  - Generate candidate prompts by applying edits (and some recombinations).\n  - Evaluate candidates; update Pareto front.\n  - Stop when gains plateau, or budget ends; pick a small set of robust prompts.\n\n17) Key empirical takeaway\nReflective prompt evolution leverages the “textuality” of both the problem and the learner. By staying in the medium of language, we extract more learning signal per rollout than RL from sparse rewards. Combined with evolutionary Pareto search, this yields faster, stronger, more interpretable optimization of compound AI systems.\n\nIf you want, I can:\n- Walk through one of the paper’s tasks step-by-step and reconstruct the actual edits that led to gains.\n- Help you set up a minimal GEPA-style optimizer for your own pipeline (e.g., retrieval-augmented QA or an agent using tools).","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_03c6572839fbb48d0068cae89296ec81979f2316bd02eedb51"}},"resumableStreamId":"STREAM:5c61b14f-9dc6-493c-b20a-60b58558a3f6","role":"assistant","status":"done","threadId":"5e763140-e699-40d6-970e-751838959582","timeToFirstToken":1.437,"tokens":2207,"tokensPerSecond":35.03842001651108,"updated_at":1758128265037,"userId":"google:104458561188320708185","id":"j97d4yzhtnmvam2gxrrd6z4frx7qrfsj"},{"_creationTime":1758129595758.0808,"_id":"j974xg8ynwnxzv79bmssv36c4h7qr3wq","attachmentIds":[],"content":"show me the diagram For Gepa in the paper and explain it to me","created_at":1758129595758,"messageId":"e689186b-2326-471e-b446-e3550f58895c","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"5e763140-e699-40d6-970e-751838959582","updated_at":1758129595758,"userId":"google:104458561188320708185","id":"j974xg8ynwnxzv79bmssv36c4h7qr3wq","providerMetadata":{}},{"_creationTime":1758129595758.081,"_id":"j9711rm9rpm2gn6ggq8rcq519n7qr1dk","attachmentIds":[],"byok":false,"content":"I can’t display the actual diagram inline here, but I can point you to it and walk you through exactly what each part means.\n\nWhere to find it\n- Open the paper’s PDF: https://arxiv.org/pdf/2507.19457\n- The GEPA overview diagram is in the main method section (look for a figure labeled something like “GEPA overview” or “Reflective prompt evolution with a Pareto frontier”). It shows a loop with:\n  - Rollouts of a compound AI system\n  - Reflection that extracts rules\n  - Prompt mutations and recombinations\n  - Evaluation on multiple objectives\n  - Pareto-front selection\n  - Iteration\n\nHow to read the diagram, step by step\n\n1) System rollouts (left/top-left of the figure)\n- What you see: A compound AI pipeline with modules (e.g., Retriever, Planner, Tool Caller, Final Answerer). Arrows show data flowing and tool calls happening.\n- Meaning: GEPA runs the current system on a batch of inputs and logs the entire trace: prompts shown to each module, intermediate thoughts/reasons (if available), tool inputs/outputs, and any evaluator messages (e.g., error logs, pass/fail checks).\n- Why it’s there: Instead of a single reward number, GEPA collects a rich, textual “movie” of what happened.\n\n2) Reflection in natural language (center-left)\n- What you see: A “reflection” box (LLM call) that reads those traces and emits diagnoses and “lessons learned.”\n- Meaning: The reflection step converts trajectory text into actionable, general rules. Examples the figure implies:\n  - “Second-hop queries are duplicating first-hop terms; add a rule to target missing entities from summary1.”\n  - “Tool call failures stem from ambiguous parameter formatting; add explicit schema checks.”\n- Why it’s there: Language is the model’s native medium. We let the LLM mine the traces for reusable rules instead of squashing everything into a scalar reward.\n\n3) Prompt mutations per module (middle)\n- What you see: Proposed edits attached to specific modules’ prompts (multiple sticky-note-like edits).\n- Meaning: Each lesson becomes a concrete prompt change for the responsible module: adjust instructions, add examples, add checklists/constraints, clarify tool schemas, etc. GEPA creates multiple candidate variations (mutations) per module and also combinations across modules.\n- Why it’s there: High-leverage patches—one rule can fix many future cases. Working at the module level keeps edits targeted and interpretable.\n\n4) Genetic-style population (middle-right)\n- What you see: A population of candidate prompt sets (each is a full system spec with edited prompts). You may see “recombine” arrows merging compatible edits from different candidates.\n- Meaning: GEPA maintains a diverse pool of strategies. It can also recombine compatible lessons (like genetics) so multiple good ideas coexist in one candidate.\n- Why it’s there: Avoids converging on a single brittle tactic; encourages exploration and synergy among edits.\n\n5) Multi-objective evaluation (right)\n- What you see: A scoring panel with multiple axes, e.g., Accuracy, Cost/Latency, Safety, Stability.\n- Meaning: Candidates are tested on a validation slice and scored on several objectives, not just one.\n- Why it’s there: Real systems have trade-offs. GEPA optimizes across them rather than overfitting to a single metric.\n\n6) Pareto-front selection (bottom-right)\n- What you see: A Pareto frontier curve with selected candidates highlighted; others are discarded.\n- Meaning: Keep only non-dominated candidates—those that are not strictly worse on every metric. This preserves diverse, strong strategies.\n- Why it’s there: Protects against local optima and maintains options that excel on different trade-offs or subsets.\n\n7) Iteration loop (bottom/back to left)\n- What you see: Arrows feeding the selected Pareto candidates back to the rollout step.\n- Meaning: With the new population, run more rollouts, gather new traces, reflect again, propose further edits, and repeat.\n- Why it’s there: Accumulates lessons quickly—often big gains in 1–2 rounds—without thousands of rollouts.\n\nWhat the diagram emphasizes conceptually\n- Use language to learn from language: trajectories → reflection → rules → prompts.\n- Targeted, modular edits: fixes attach to the module that caused the failure.\n- Diversity and robustness: genetic population + Pareto selection keep multiple good strategies alive.\n- Sample efficiency: one rule can fix a whole class of errors, so you need far fewer rollouts than RL with sparse rewards.\n\nIf you’d like, share a screenshot of the figure, and I can annotate it visually: I’ll label each box, arrow, and legend and tie them to the steps above for a point-by-point walkthrough.","created_at":1758129595759,"messageId":"bcbc685b-4673-456f-acd9-6276f166527e","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":["arxiv.org"],"queries":["GEPA Reflective Prompt Evolution Can Outperform Reinforcement Learning diagram figure arXiv 2507.19457","site:arxiv.org 2507.19457 GEPA figures"]},"result":[{"content":"Preprint.\nGEPA: REFLECTIVE PROMPT EVOLUTION CAN OUTPERFORM\nREINFORCEMENT LEARNING\nLakshya A Agrawal1, Shangyin Tan1, Dilara Soylu2, Noah Ziems4,\nRishi Khare1, Krista Opsahl-Ong5, Arnav Singhvi2,5, Herumb Shandilya2,\nMichael J Ryan2, Meng Jiang4, Christopher Potts2, Koushik Sen1,\nAlexandros G. Dimakis1,3, Ion Stoica1, Dan Klein1, Matei Zaharia1,5, Omar Khattab6\n1UC Berkeley 2Stanford University 3BespokeLabs.ai 4Notre Dame 5Databricks 6MIT\nABSTRACT\nLarge language models (LLMs) are increasingly adapted to downstream tasks via reinforcement\nlearning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thou\u0002sands of rollouts to learn new tasks. We argue that the interpretable nature of language can often\nprovide a much richer learning medium for LLMs, compared with policy gradients derived from\nsparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that\nthoroughly incorporates natural language reflection to learn high-level rules from trial and error.\nGiven any AI system containing one or more LLM prompts, GEPA samples system-level trajecto\u0002ries (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons from the Pareto\nfrontier of its own attempts. As a result of GEPA’s design, it can often turn even just a few rollouts\ninto a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up\nto 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time\nsearch strategy for code optimization.\n1 INTRODUCTION\n0 5000 10000 15000 20000 25000\nNumber of Rollouts\n42.5\n45.0\n47.5\n50.0\n52.5\n55.0\n57.5\n60.0\n62.5\nScore\nHotpotQA, Qwen3 8B\nOptimization Method\nBaseline\nMIPROv2\nGRPO\nGEPA\nValidation Performance\nTest-set Performance\n(a) HotpotQA, Qwen3 8B\n0 5000 10000 15000 20000 25000\nNumber of Rollouts\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\nScore\nHoVer, Qwen3 8B\nOptimization Method\nBaseline\nMIPROv2\nGRPO\nGEPA\nValidation Performance\nTest-set Performance\n(b) HoVer, Qwen3 8B\nFigure 1: A comparison of the learning behavior of our proposed GEPA prompt optimizer against a state-of-the-art\nprompt optimizer (MIPROv2) and the GRPO (24,000 rollouts) algorithm. As more rollouts are sampled, the prompt\noptimizers can learn much more quickly than GRPO. GEPA substantially outperforms both GRPO and MIPROv2 in\nfinal score. The Test-set star markers demonstrate the performance gap in a held-out set of questions.\nLarge language models (LLMs) have enabled the development of agents and systems that combine fuzzy natural\u0002language behavior specification with tools like retrieval and code execution. These types of systems raise the question\nof how LLMs should be “optimized” for the best downstream performance within their harness. One popular ap\u0002proach for adapting LLMs to downstream tasks is reinforcement learning with verifiable rewards (RLVR), including\n1\narXiv:2507.19457v1 [cs.CL] 25 Jul 2025\nPreprint.\nalgorithms such as Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Such RL methods cast success\nmetrics as a scalar reward observed at the end of each rollout (Lambert, 2025) and use these rewards to estimate\ngradients for policy improvement.\nWhile these RL approaches are effective, they typically require tens of thousands of rollouts in practice to fit new\ntasks. For example, recent works leveraging GRPO across a range of tasks typically use up to hundreds of thousands\nof rollouts for training (Chen et al., 2025b; Wu et al., 2025c; Zhang et al., 2025; Jin et al., 2025; Si et al., 2025; Wang\net al., 2025a; Java et al., 2025; Chen et al., 2025a; Wu et al., 2025a; Sha et al., 2025; Lin et al., 2025; Peng et al.,\n2025; Song et al., 2025). This sample inefficiency can quickly become a serious bottleneck: many downstream LLM\napplications invoke expensive tool calls, have limited inference budget for sampling from the LLM itself, or simply\ncannot finetune the weights of the largest or best-performing LLMs.\nWe observe that the rollouts sampled from even highly sophisticated LLM systems can be serialized into traces of\nnatural (and formal) language, as they contain nothing but the instructions of each LLM module, the resulting LLM\nreasoning chains, tool calls, and potentially the internal workings of the reward function (for example, compiler error\nmessages, before they are collapsed into scalar rewards). Because such serialized trajectories can be readily under\u0002stood by modern LLMs, we argue that algorithms that learn deliberately in natural language by reflecting on these\ntrajectories can potentially make much more effective use of the strong language priors that LLMs have, compared\nwith standard RL approaches.\nTo operationalize this, we introduce GEPA (Genetic-Pareto), a reflective prompt optimizer for compound AI systems\nthat merges textual reflection with multi-objective evolutionary search. GEPA iteratively mutates every prompt within\nthe AI system in light of natural language feedback drawn from new rollouts. In each mutation, the candidate prompt\nis derived from an ancestor, accumulating high-level lessons derived from observations and LLM feedback. To avoid\nthe local optima that afflict greedy prompt updates, GEPA maintains a Pareto front: instead of evolving only the global\nbest prompt, it stochastically explores the top-performing prompts for each problem instance, thereby diversifying\nstrategies and encouraging robust generalization.\nWe evaluate GEPA on four diverse tasks—multi-hop reasoning (HotpotQA; Yang et al. 2018), instruction following\n(IFBench; Pyatkin et al. 2025b), privacy-aware delegation (PUPA; Li et al. 2025a), and retrieval-augmented verifica\u0002tion (HoVer; Jiang et al. 2020)—using both open (Qwen3 8B; Yang et al. 2025; Team 2025) and proprietary (GPT-4.1\nmini; OpenAI 2025) models. Our results show that GEPA demonstrates robust generalization and is highly sample\u0002efficient: on Qwen3 8B, GEPA outperforms GRPO (24,000 rollouts with LoRA) by up to 19% while requiring up to\n35× fewer rollouts. Overall, GEPA achieves an average improvement of +10% over GRPO across all tasks. Further\u0002more, GEPA surpasses the previous state-of-the-art prompt optimizer, MIPROv2 (Opsahl-Ong et al., 2024), on every\nbenchmark and model, obtaining aggregate optimization gains of +14%, more than doubling the gains achieved by\nMIPROv2 (+7%).\nEven qualitatively, GEPA generated prompts can be highly effective. Figure 2 highlights excerpts from a prompt\ncrafted by GEPA for the query creation module of a multi-hop question answering system (Used in HotpotQA). We\nalso find that in most cases, even a single reflective prompt update can give large improvements (as highlighted in the\noptimization trajectory in Figure 5). These results demonstrate that reflective prompt evolution using language feed\u0002back enables substantial sample efficiency and robust generalization, providing a practical path to optimizing complex,\nreal-world AI workflows in data- or budget-constrained environments. Finally, we also show promising preliminary\nresults demonstrating GEPA’s use as an inference-time search strategy for code optimization over NPUEval (Kalade\n& Schelle, 2025) and KernelBench (Ouyang et al., 2025).\n2 PROBLEM STATEMENT\nCompound AI Systems. We follow related work in defining a compound AI system as any modular system com\u0002posed of one or more language model (LLM) invocations, potentially interleaved with external tool calls, orchestrated\nthrough arbitrary control flow. This definition subsumes a broad class of real-world LLM-based AI systems, including\nagents, multi-agent systems, and general-purpose scaffolding techniques like ReAct (Yao et al., 2023), Archon (Saad\u0002Falcon et al., 2025), etc. Following Soylu et al. (2024); Khattab et al. (2024); Opsahl-Ong et al. (2024); Tan et al.\n(2025), we formalize such a system as Φ = (M, C, X , Y), where M = ⟨M1, . . . , M|M|⟩ denotes language modules,\nC specifies control flow logic, and X , Y are global input/output schemas. Each module Mi = (πi, θi, Xi, Yi) is an\nLLM subcomponent: πiis its (system) prompt including instructions and few-shot demonstrations; θithe underly\u0002ing model weights; Xi\n, Yi are input/output schemas. At runtime, C orchestrates the sequencing and invocation of\nmodules—e.g., passing outputs from one module to another, invoking modules conditionally, or leveraging tool APIs.\nThis way, C can invoke different modules in any order multiples of times.\n2\nPreprint.\nSeed Prompt for Second-Hop of Multi-Hop QA System\nGiven the fields question, summary 1, produce the fields query.\nGEPA’s Optimized Prompt for Second-Hop of Multi-Hop QA System, GPT-4.1 Mini\nYou will be given two input fields: question and summary 1. Your task: Generate a new search query (query) optimized for the second hop of a\nmulti-hop retrieval system.\n• The original user question is typically complex and requires information from multiple documents to answer.\n• The first hop query is the original question (used to retrieve initial documents).\n• Your goal: generate a query to retrieve documents not found in first hop but necessary to answer the question completely.\nInput Understanding: question is the original multi-hop question posed by the user. summary 1 is a concise summary of information from a document\nretrieved in the first hop, which partially addresses the question.\nPurpose and Context:\n• Your generated query aims to find the missing pieces of information needed to fully answer the question. . . .\n• The query must retrieve relevant documents NOT found in first hop . . . for final answer extraction.\nKey Observations and Lessons:\n• First-hop documents often cover one entity or aspect.\n• Remaining relevant documents often involve connected or higher-level concepts mentioned in summary 1 but not explicitly asked in","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2507.19457","id":"https://arxiv.org/pdf/2507.19457","publishedDate":null,"summary":"The paper introduces GEPA (Genetic-Pareto), a novel prompt optimization method that leverages natural language reflection to improve large language models (LLMs) more efficiently than traditional reinforcement learning (RL) approaches like Group Relative Policy Optimization (GRPO). Unlike RL methods that rely on numerous scalar reward-based rollouts, GEPA analyzes system trajectories (e.g., reasoning steps, tool calls) through natural language reflection to diagnose issues, propose prompt updates, and learn high-level rules. This approach enables GEPA to achieve superior performance—outperforming GRPO by up to 20% with up to 35 times fewer rollouts across multiple tasks. It also surpasses other prompt optimizers like MIPROv2, demonstrating significant improvements in downstream task performance and efficiency, and shows promise as an inference-time strategy for code optimization. Overall, GEPA highlights the potential of reflective prompt evolution to enhance LLM training and deployment more effectively than traditional RL methods.","title":"GEPA: Reflective Prompt Evolution Can Outperform ...","url":"https://arxiv.org/pdf/2507.19457"},{"content":"# Computer Science > Computation and Language\n\n**arXiv:2503.23503** (cs)\n\n\\[Submitted on 30 Mar 2025\\]\n\n# Title:Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models\n\nAuthors: [Sid Bharthulwar](https://arxiv.org/search/cs?searchtype=author&query=Bharthulwar,+S), [John Rho](https://arxiv.org/search/cs?searchtype=author&query=Rho,+J), [Katrina Brown](https://arxiv.org/search/cs?searchtype=author&query=Brown,+K)\n\nView a PDF of the paper titled Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models, by Sid Bharthulwar and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.23503) [HTML (experimental)](https://arxiv.org/html/2503.23503v1)\n\n> Abstract:We present a framework for optimizing prompts in vision-language models to elicit multimodal reasoning without model retraining. Using an evolutionary algorithm to guide prompt updates downstream of visual tasks, our approach improves upon baseline prompt-updating algorithms, which lack evolution-style \"survival of the fittest\" iteration. Crucially, we find this approach enables the language model to independently discover progressive problem-solving techniques across several evolution generations. For example, the model reasons that to \"break down\" visually complex spatial tasks, making a tool call to a Python interpreter to perform tasks (such as cropping, image segmentation, or saturation changes) would improve performance significantly. Our experimentation shows that explicitly evoking this \"tool calling\" call, via system-level XML $...\\\\texttt{} ... \\\\texttt{}...$ tags, can effectively flag Python interpreter access for the same language model to generate relevant programs, generating advanced multimodal functionality. This functionality can be crystallized into a system-level prompt that induces improved performance at inference time, and our experimentation suggests up to $\\\\approx 50\\\\%$ relative improvement across select visual tasks. Downstream performance is trained and evaluated across subtasks from MathVista, M3CoT, and GeoBench-VLM datasets. Importantly, our approach shows that evolutionary prompt optimization guides language models towards self-reasoning discoveries, which result in improved zero-shot generalization across tasks.\n\n| | |\n| --- | --- |\n| Comments: | Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs |\n| Subjects: | Computation and Language (cs.CL) |\n| Cite as: | [arXiv:2503.23503](https://arxiv.org/abs/2503.23503) \\[cs.CL\\] |\n| (or [arXiv:2503.23503v1](https://arxiv.org/abs/2503.23503v1) \\[cs.CL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.23503](https://doi.org/10.48550/arXiv.2503.23503) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Katrina Brown \\[ [view email](https://arxiv.org/show-email/1bc61d83/2503.23503)\\] **\\[v1\\]**\nSun, 30 Mar 2025 16:25:45 UTC (1,919 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models, by Sid Bharthulwar and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.23503)\n- [HTML (experimental)](https://arxiv.org/html/2503.23503v1)\n- [TeX Source](https://arxiv.org/src/2503.23503)\n- [Other Formats](https://arxiv.org/format/2503.23503)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[< prev](https://arxiv.org/prevnext?id=2503.23503&function=prev&context=cs.CL)  \\|  [next >](https://arxiv.org/prevnext?id=2503.23503&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2025-03](https://arxiv.org/list/cs.CL/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.23503?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.23503)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.23503)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.23503)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.23503) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.23503","id":"https://arxiv.org/abs/2503.23503","publishedDate":"2025-03-30T00:00:00.000Z","summary":"The paper introduces an evolutionary prompt optimization framework that enhances vision-language models' multimodal reasoning capabilities without retraining. By iteratively refining prompts through an evolutionary algorithm, the approach enables models to autonomously develop advanced problem-solving strategies, such as invoking external tools like Python interpreters for complex spatial tasks. Explicitly prompting models to call tools via system-level tags significantly improves performance—up to approximately 50%—across various visual tasks. The method demonstrates that evolutionary prompt tuning guides models toward self-discovered reasoning techniques, leading to better zero-shot generalization on datasets like MathVista, M3CoT, and GeoBench-VLM. Overall, this work highlights the potential of evolutionary strategies to unlock emergent multimodal reasoning in vision-language models.","title":"Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models","url":"https://arxiv.org/abs/2503.23503"},{"content":"# Computer Science > Machine Learning\n\n**arXiv:2503.19037** (cs)\n\n\\[Submitted on 24 Mar 2025 ( [v1](https://arxiv.org/abs/2503.19037v1)), last revised 10 Jun 2025 (this version, v2)\\]\n\n# Title:Evolutionary Policy Optimization\n\nAuthors: [Jianren Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+J), [Yifan Su](https://arxiv.org/search/cs?searchtype=author&query=Su,+Y), [Abhinav Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta,+A), [Deepak Pathak](https://arxiv.org/search/cs?searchtype=author&query=Pathak,+D)\n\nView a PDF of the paper titled Evolutionary Policy Optimization, by Jianren Wang and Yifan Su and Abhinav Gupta and Deepak Pathak\n\n[View PDF](https://arxiv.org/pdf/2503.19037) [HTML (experimental)](https://arxiv.org/html/2503.19037v2)\n\n> Abstract:On-policy reinforcement learning (RL) algorithms are widely used for their strong asymptotic performance and training stability, but they struggle to scale with larger batch sizes, as additional parallel environments yield redundant data due to limited policy-induced diversity. In contrast, Evolutionary Algorithms (EAs) scale naturally and encourage exploration via randomized population-based search, but are often sample-inefficient. We propose Evolutionary Policy Optimization (EPO), a hybrid algorithm that combines the scalability and diversity of EAs with the performance and stability of policy gradients. EPO maintains a population of agents conditioned on latent variables, shares actor-critic network parameters for coherence and memory efficiency, and aggregates diverse experiences into a master agent. Across tasks in dexterous manipulation, legged locomotion, and classic control, EPO outperforms state-of-the-art baselines in sample efficiency, asymptotic performance, and scalability.\n\n| | |\n| --- | --- |\n| Comments: | Website at [this https URL](https://yifansu1301.github.io/EPO/) |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO) |\n| Cite as: | [arXiv:2503.19037](https://arxiv.org/abs/2503.19037) \\[cs.LG\\] |\n| (or [arXiv:2503.19037v2](https://arxiv.org/abs/2503.19037v2) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.19037](https://doi.org/10.48550/arXiv.2503.19037) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jianren Wang \\[ [view email](https://arxiv.org/show-email/74baeb72/2503.19037)\\] **[\\[v1\\]](https://arxiv.org/abs/2503.19037v1)**\nMon, 24 Mar 2025 18:08:54 UTC (7,743 KB)\n**\\[v2\\]**\nTue, 10 Jun 2025 17:59:10 UTC (6,252 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Evolutionary Policy Optimization, by Jianren Wang and Yifan Su and Abhinav Gupta and Deepak Pathak\n\n- [View PDF](https://arxiv.org/pdf/2503.19037)\n- [HTML (experimental)](https://arxiv.org/html/2503.19037v2)\n- [TeX Source](https://arxiv.org/src/2503.19037)\n- [Other Formats](https://arxiv.org/format/2503.19037)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[< prev](https://arxiv.org/prevnext?id=2503.19037&function=prev&context=cs.LG)  \\|  [next >](https://arxiv.org/prevnext?id=2503.19037&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-03](https://arxiv.org/list/cs.LG/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.19037?context=cs) [cs.AI](https://arxiv.org/abs/2503.19037?context=cs.AI) [cs.RO](https://arxiv.org/abs/2503.19037?context=cs.RO)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.19037)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.19037)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.19037)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.19037) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.19037","id":"https://arxiv.org/abs/2503.19037","publishedDate":"2025-03-24T00:00:00.000Z","summary":"The paper \"Evolutionary Policy Optimization\" introduces a hybrid reinforcement learning algorithm (EPO) that combines the strengths of Evolutionary Algorithms (EAs) and policy gradient methods. EPO maintains a population of agents conditioned on latent variables, sharing network parameters for efficiency and diversity. It aggregates experiences into a master agent, enhancing exploration and scalability. EPO outperforms existing methods in sample efficiency, stability, and performance across tasks like dexterous manipulation, legged locomotion, and control, making it a promising approach for scalable and effective reinforcement learning.","title":"Evolutionary Policy Optimization","url":"https://arxiv.org/abs/2503.19037"},{"content":"# Computer Science > Computation and Language\n\n**arXiv:2503.20194** (cs)\n\n\\[Submitted on 26 Mar 2025\\]\n\n# Title:GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization\n\nAuthors: [Zhouhong Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu,+Z), [Xingzhou Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+X), [Xiaoran Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi,+X), [Tao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+T), [Suhang Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng,+S), [Tianyu Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+T), [Hongwei Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng,+H), [Yanghua Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao,+Y)\n\nView a PDF of the paper titled GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization, by Zhouhong Gu and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.20194) [HTML (experimental)](https://arxiv.org/html/2503.20194v1)\n\n> Abstract:Recent advances in large language models have highlighted the critical need for precise control over model outputs through predefined constraints. While existing methods attempt to achieve this through either direct instruction-response synthesis or preferential response optimization, they often struggle with constraint understanding and adaptation. This limitation becomes particularly evident when handling fine-grained constraints, leading to either hallucination or brittle performance. We introduce Generative Adversarial Policy Optimization (GAPO), a novel framework that combines GAN-based training dynamics with an encoder-only reward model to progressively learn and adapt to increasingly complex constraints. GAPO leverages adversarial training to automatically generate training samples of varying difficulty while utilizing the encoder-only architecture to better capture prompt-response relationships. Extensive experiments demonstrate GAPO's superior performance across multiple benchmarks, particularly in scenarios requiring fine-grained constraint handling, where it significantly outperforms existing methods like PPO, DPO, and KTO. Our results suggest that GAPO's unique approach to preferential prompt learning offers a more robust and effective solution for controlling LLM outputs. Code is avaliable in [this https URL](https://github.com/MikeGu721/GAPO).\n\n| | |\n| --- | --- |\n| Subjects: | Computation and Language (cs.CL) |\n| Cite as: | [arXiv:2503.20194](https://arxiv.org/abs/2503.20194) \\[cs.CL\\] |\n| (or [arXiv:2503.20194v1](https://arxiv.org/abs/2503.20194v1) \\[cs.CL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.20194](https://doi.org/10.48550/arXiv.2503.20194) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Zhouhong Gu \\[ [view email](https://arxiv.org/show-email/72e1078c/2503.20194)\\] **\\[v1\\]**\nWed, 26 Mar 2025 03:37:52 UTC (11,445 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization, by Zhouhong Gu and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.20194)\n- [HTML (experimental)](https://arxiv.org/html/2503.20194v1)\n- [TeX Source](https://arxiv.org/src/2503.20194)\n- [Other Formats](https://arxiv.org/format/2503.20194)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[< prev](https://arxiv.org/prevnext?id=2503.20194&function=prev&context=cs.CL)  \\|  [next >](https://arxiv.org/prevnext?id=2503.20194&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2025-03](https://arxiv.org/list/cs.CL/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.20194?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.20194)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.20194)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.20194)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.20194) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.20194","id":"https://arxiv.org/abs/2503.20194","publishedDate":"2025-03-26T00:00:00.000Z","summary":"The paper introduces GAPO, a novel framework for controlling large language models (LLMs) through preferential prompt learning. Unlike traditional methods that struggle with fine-grained constraints, GAPO employs Generative Adversarial Policy Optimization, combining GAN-based training with an encoder-only reward model. This approach enables the model to automatically generate training samples of varying difficulty and better understand complex prompt-response relationships. Extensive experiments demonstrate that GAPO outperforms existing techniques like PPO, DPO, and KTO, especially in scenarios requiring precise, fine-grained constraint adherence. The framework offers a more robust and adaptable solution for guiding LLM outputs, with code available for implementation.","title":"GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization","url":"https://arxiv.org/abs/2503.20194"},{"content":"# Computer Science > Machine Learning\n\n**arXiv:2503.21807** (cs)\n\n\\[Submitted on 25 Mar 2025\\]\n\n# Title:LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning\n\nAuthors: [Yuan Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei,+Y), [Xiaohan Shan](https://arxiv.org/search/cs?searchtype=author&query=Shan,+X), [Jianmin Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+J)\n\nView a PDF of the paper titled LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning, by Yuan Wei and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.21807) [HTML (experimental)](https://arxiv.org/html/2503.21807v1)\n\n> Abstract:Multi-agent reinforcement learning (MARL) faces two critical bottlenecks distinct from single-agent RL: credit assignment in cooperative tasks and partial observability of environmental states. We propose LERO, a framework integrating Large language models (LLMs) with evolutionary optimization to address these MARL-specific challenges. The solution centers on two LLM-generated components: a hybrid reward function that dynamically allocates individual credit through reward decomposition, and an observation enhancement function that augments partial observations with inferred environmental context. An evolutionary algorithm optimizes these components through iterative MARL training cycles, where top-performing candidates guide subsequent LLM generations. Evaluations in Multi-Agent Particle Environments (MPE) demonstrate LERO's superiority over baseline methods, with improved task performance and training efficiency.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA) |\n| Cite as: | [arXiv:2503.21807](https://arxiv.org/abs/2503.21807) \\[cs.LG\\] |\n| (or [arXiv:2503.21807v1](https://arxiv.org/abs/2503.21807v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.21807](https://doi.org/10.48550/arXiv.2503.21807) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Xiaohan Shan \\[ [view email](https://arxiv.org/show-email/5236de59/2503.21807)\\] **\\[v1\\]**\nTue, 25 Mar 2025 06:28:42 UTC (521 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning, by Yuan Wei and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.21807)\n- [HTML (experimental)](https://arxiv.org/html/2503.21807v1)\n- [TeX Source](https://arxiv.org/src/2503.21807)\n- [Other Formats](https://arxiv.org/format/2503.21807)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[< prev](https://arxiv.org/prevnext?id=2503.21807&function=prev&context=cs.LG)  \\|  [next >](https://arxiv.org/prevnext?id=2503.21807&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-03](https://arxiv.org/list/cs.LG/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.21807?context=cs) [cs.AI](https://arxiv.org/abs/2503.21807?context=cs.AI) [cs.MA](https://arxiv.org/abs/2503.21807?context=cs.MA)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.21807)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.21807)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.21807)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.21807) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.21807","id":"https://arxiv.org/abs/2503.21807","publishedDate":"2025-03-25T00:00:00.000Z","summary":"The paper introduces LERO, a novel framework for multi-agent reinforcement learning (MARL) that leverages large language models (LLMs) combined with evolutionary optimization to overcome key challenges such as credit assignment and partial observability. LERO employs two LLM-generated components: a hybrid reward function that dynamically decomposes rewards to assign credit more effectively among agents, and an observation enhancement function that infers environmental context to improve agents' partial observations. An evolutionary algorithm iteratively optimizes these components through MARL training cycles, with top-performing solutions guiding subsequent LLM generations. Experimental results in Multi-Agent Particle Environments demonstrate that LERO outperforms baseline methods, achieving higher task performance and training efficiency. This approach offers a promising direction for advancing cooperative multi-agent systems by integrating LLM-driven adaptive components with evolutionary strategies.","title":"LERO: LLM-driven Evolutionary framework with Hybrid Rewards and Enhanced Observation for Multi-Agent Reinforcement Learning","url":"https://arxiv.org/abs/2503.21807"},{"content":"# Computer Science > Computation and Language\n\n**arXiv:2507.19457** (cs)\n\n\\[Submitted on 25 Jul 2025\\]\n\n# Title:GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\nAuthors: [Lakshya A Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal,+L+A), [Shangyin Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan,+S), [Dilara Soylu](https://arxiv.org/search/cs?searchtype=author&query=Soylu,+D), [Noah Ziems](https://arxiv.org/search/cs?searchtype=author&query=Ziems,+N), [Rishi Khare](https://arxiv.org/search/cs?searchtype=author&query=Khare,+R), [Krista Opsahl-Ong](https://arxiv.org/search/cs?searchtype=author&query=Opsahl-Ong,+K), [Arnav Singhvi](https://arxiv.org/search/cs?searchtype=author&query=Singhvi,+A), [Herumb Shandilya](https://arxiv.org/search/cs?searchtype=author&query=Shandilya,+H), [Michael J Ryan](https://arxiv.org/search/cs?searchtype=author&query=Ryan,+M+J), [Meng Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang,+M), [Christopher Potts](https://arxiv.org/search/cs?searchtype=author&query=Potts,+C), [Koushik Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen,+K), [Alexandros G. Dimakis](https://arxiv.org/search/cs?searchtype=author&query=Dimakis,+A+G), [Ion Stoica](https://arxiv.org/search/cs?searchtype=author&query=Stoica,+I), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein,+D), [Matei Zaharia](https://arxiv.org/search/cs?searchtype=author&query=Zaharia,+M), [Omar Khattab](https://arxiv.org/search/cs?searchtype=author&query=Khattab,+O)\n\nView a PDF of the paper titled GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning, by Lakshya A Agrawal and 16 other authors\n\n[View PDF](https://arxiv.org/pdf/2507.19457)\n\n> Abstract:Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.\n\n| | |\n| --- | --- |\n| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE) |\n| ACM classes: | I.2.7; I.2.6; I.2.4; I.2.8 |\n| Cite as: | [arXiv:2507.19457](https://arxiv.org/abs/2507.19457) \\[cs.CL\\] |\n| (or [arXiv:2507.19457v1](https://arxiv.org/abs/2507.19457v1) \\[cs.CL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2507.19457](https://doi.org/10.48550/arXiv.2507.19457) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Lakshya A Agrawal \\[ [view email](https://arxiv.org/show-email/af5f91b3/2507.19457)\\] **\\[v1\\]**\nFri, 25 Jul 2025 17:42:32 UTC (1,632 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning, by Lakshya A Agrawal and 16 other authors\n\n- [View PDF](https://arxiv.org/pdf/2507.19457)\n- [TeX Source](https://arxiv.org/src/2507.19457)\n- [Other Formats](https://arxiv.org/format/2507.19457)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[< prev](https://arxiv.org/prevnext?id=2507.19457&function=prev&context=cs.CL)  \\|  [next >](https://arxiv.org/prevnext?id=2507.19457&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2025-07](https://arxiv.org/list/cs.CL/2025-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2507.19457?context=cs) [cs.AI](https://arxiv.org/abs/2507.19457?context=cs.AI) [cs.LG](https://arxiv.org/abs/2507.19457?context=cs.LG) [cs.SE](https://arxiv.org/abs/2507.19457?context=cs.SE)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2507.19457)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2507.19457)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2507.19457)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2507.19457) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.19457","id":"https://arxiv.org/abs/2507.19457","publishedDate":null,"summary":"The paper introduces GEPA (Genetic-Pareto), a novel prompt optimization method for large language models (LLMs) that leverages natural language reflection rather than traditional reinforcement learning (RL). Unlike RL approaches like GRPO, which require extensive rollouts, GEPA uses natural language reflection to analyze system trajectories (e.g., reasoning steps, tool calls) to diagnose issues, propose prompt improvements, and combine lessons from multiple attempts. This approach enables high-quality prompt refinement with significantly fewer rollouts—up to 35 times fewer—while achieving superior performance. Across four tasks, GEPA outperforms RL-based methods by an average of 10-20% and surpasses the leading prompt optimizer MIPROv2 by over 10%. The method also shows promise as an inference-time strategy for code optimization, highlighting its efficiency and effectiveness in prompt evolution for LLMs.","title":"GEPA: Reflective Prompt Evolution Can Outperform ...","url":"https://arxiv.org/abs/2507.19457"},{"content":"\n \n [2126] \n \n arXiv:2507.11588\n \n (cross-list from q-bio.GN)\n [ pdf, html, other]\n \n \n \n \n \n [2127] \n \n arXiv:2507.11597\n \n (cross-list from cs.CY)\n [ pdf, other]\n \n \n \n \n \n [2128] \n \n arXiv:2507.11620\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2129] \n \n arXiv:2507.11621\n \n (cross-list from cs.RO)\n [ pdf, other]\n \n \n \n \n \n [2130] \n \n arXiv:2507.11623\n \n (cross-list from cs.RO)\n [ pdf, other]\n \n \n \n \n \n [2131] \n \n arXiv:2507.11625\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2132] \n \n arXiv:2507.11630\n \n (cross-list from cs.CR)\n [ pdf, html, other]\n \n \n \n \n \n [2133] \n \n arXiv:2507.11634\n \n (cross-list from cs.CL)\n [ pdf, other]\n \n \n \n \n \n [2134] \n \n arXiv:2507.11636\n \n (cross-list from eess.AS)\n [ pdf, html, other]\n \n \n \n \n \n [2135] \n \n arXiv:2507.11638\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2136] \n \n arXiv:2507.11645\n \n (cross-list from cs.LG)\n [ pdf, other]\n \n \n \n \n \n [2137] \n \n arXiv:2507.11655\n \n (cross-list from cs.LO)\n [ pdf, html, other]\n \n \n \n \n \n [2138] \n \n arXiv:2507.11661\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2139] \n \n arXiv:2507.11683\n \n (cross-list from cs.DC)\n [ pdf, html, other]\n \n \n \n \n \n [2140] \n \n arXiv:2507.11692\n \n (cross-list from astro-ph.GA)\n [ pdf, html, other]\n \n \n \n \n \n [2141] \n \n arXiv:2507.11694\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2142] \n \n arXiv:2507.11702\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2143] \n \n arXiv:2507.11710\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2144] \n \n arXiv:2507.11729\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2145] \n \n arXiv:2507.11730\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2146] \n \n arXiv:2507.11742\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2147] \n \n arXiv:2507.11751\n \n (cross-list from cs.NE)\n [ pdf, html, other]\n \n \n \n \n \n [2148] \n \n arXiv:2507.11761\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2149] \n \n arXiv:2507.11773\n \n (cross-list from cs.CY)\n [ pdf, html, other]\n \n \n \n \n \n [2150] \n \n arXiv:2507.11775\n \n (cross-list from cs.CR)\n [ pdf, html, other]\n \n \n \n \n \n [2151] \n \n arXiv:2507.11776\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2152] \n \n arXiv:2507.11783\n \n (cross-list from eess.SP)\n [ pdf, html, other]\n \n \n \n \n \n [2153] \n \n arXiv:2507.11799\n \n (cross-list from physics.comp-ph)\n [ pdf, html, other]\n \n \n \n \n \n [2154] \n \n arXiv:2507.11807\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2155] \n \n arXiv:2507.11809\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2156] \n \n arXiv:2507.11810\n \n (cross-list from cs.DL)\n [ pdf, html, other]\n \n \n \n \n \n [2157] \n \n arXiv:2507.11821\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2158] \n \n arXiv:2507.11848\n \n (cross-list from cs.HC)\n [ pdf, html, other]\n \n \n \n \n \n [2159] \n \n arXiv:2507.11892\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2160] \n \n arXiv:2507.11893\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2161] \n \n arXiv:2507.11935\n \n (cross-list from cs.NI)\n [ pdf, html, other]\n \n \n \n \n \n [2162] \n \n arXiv:2507.11936\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2163] \n \n arXiv:2507.11939\n \n (cross-list from cs.CL)\n [ pdf, other]\n \n \n \n \n \n [2164] \n \n arXiv:2507.11943\n \n (cross-list from cs.CR)\n [ pdf, html, other]\n \n \n \n \n \n [2165] \n \n arXiv:2507.11947\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2166] \n \n arXiv:2507.11948\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2167] \n \n arXiv:2507.11959\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2168] \n \n arXiv:2507.11966\n \n (cross-list from cs.CL)\n [ pdf, other]\n \n \n \n \n \n [2169] \n \n arXiv:2507.11975\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2170] \n \n arXiv:2507.11987\n \n (cross-list from cs.SC)\n [ pdf, html, other]\n \n \n \n \n \n [2171] \n \n arXiv:2507.11991\n \n (cross-list from cs.RO)\n [ pdf, html, other]\n \n \n \n \n \n [2172] \n \n arXiv:2507.11997\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2173] \n \n arXiv:2507.12006\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2174] \n \n arXiv:2507.12008\n \n (cross-list from cs.CV)\n [ pdf, other]\n \n \n \n \n \n [2175] \n \n arXiv:2507.12011\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2176] \n \n arXiv:2507.12012\n \n (cross-list from eess.IV)\n [ pdf, html, other]\n \n \n \n \n \n [2177] \n \n arXiv:2507.12017\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2178] \n \n arXiv:2507.12029\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2179] \n \n arXiv:2507.12060\n \n (cross-list from cs.CV)\n [ pdf, other]\n \n \n \n \n \n [2180] \n \n arXiv:2507.12064\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2181] \n \n arXiv:2507.12075\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2182] \n \n arXiv:2507.12104\n \n (cross-list from cs.SE)\n [ pdf, html, other]\n \n \n \n \n \n [2183] \n \n arXiv:2507.12107\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2184] \n \n arXiv:2507.12108\n \n (cross-list from cs.SI)\n [ pdf, html, other]\n \n \n \n \n \n [2185] \n \n arXiv:2507.12117\n \n (cross-list from quant-ph)\n [ pdf, html, other]\n \n \n \n \n \n [2186] \n \n arXiv:2507.12145\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2187] \n \n arXiv:2507.12188\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2188] \n \n arXiv:2507.12189\n \n (cross-list from quant-ph)\n [ pdf, other]\n \n \n \n \n \n [2189] \n \n arXiv:2507.12195\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2190] \n \n arXiv:2507.12196\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2191] \n \n arXiv:2507.12197\n \n (cross-list from cs.SD)\n [ pdf, html, other]\n \n \n \n \n \n [2192] \n \n arXiv:2507.12202\n \n (cross-list from cs.IR)\n [ pdf, html, other]\n \n \n \n \n \n [2193] \n \n arXiv:2507.12212\n \n (cross-list from cs.HC)\n [ pdf, html, other]\n \n \n \n \n \n [2194] \n \n arXiv:2507.12242\n \n (cross-list from cs.IR)\n [ pdf, html, other]\n \n \n \n \n \n [2195] \n \n arXiv:2507.12252\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2196] \n \n arXiv:2507.12261\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2197] \n \n arXiv:2507.12262\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2198] \n \n arXiv:2507.12269\n \n (cross-list from cs.CV)\n [ pdf, other]\n \n \n \n \n \n [2199] \n \n arXiv:2507.12284\n \n (cross-list from cs.SE)\n [ pdf, html, other]\n \n \n \n \n \n [2200] \n \n arXiv:2507.12286\n \n (cross-list from cs.LO)\n [ pdf, html, other]\n \n \n \n \n \n [2201] \n \n arXiv:2507.12295\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2202] \n \n arXiv:2507.12305\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2203] \n \n arXiv:2507.12308\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2204] \n \n arXiv:2507.12314\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2205] \n \n arXiv:2507.12318\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2206] \n \n arXiv:2507.12329\n \n (cross-list from cs.IT)\n [ pdf, other]\n \n \n \n \n \n [2207] \n \n arXiv:2507.12359\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2208] \n \n arXiv:2507.12366\n \n (cross-list from cs.SC)\n [ pdf, html, other]\n \n \n \n \n \n [2209] \n \n arXiv:2507.12367\n \n (cross-list from cs.SE)\n [ pdf, html, other]\n \n \n \n \n \n [2210] \n \n arXiv:2507.12379\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2211] \n \n arXiv:2507.12412\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2212] \n \n arXiv:2507.12414\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2213] \n \n arXiv:2507.12416\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2214] \n \n arXiv:2507.12419\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2215] \n \n arXiv:2507.12425\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2216] \n \n arXiv:2507.12427\n \n (cross-list from eess.IV)\n [ pdf, html, other]\n \n \n \n \n \n [2217] \n \n arXiv:2507.12428\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2218] \n \n arXiv:2507.12440\n \n (cross-list from cs.RO)\n [ pdf, html, other]\n \n \n \n \n \n [2219] \n \n arXiv:2507.12442\n \n (cross-list from cs.AR)\n [ pdf, html, other]\n \n \n \n \n \n [2220] \n \n arXiv:2507.12443\n \n (cross-list from cs.NI)\n [ pdf, html, other]\n \n \n \n \n \n [2221] \n \n arXiv:2507.12451\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2222] \n \n arXiv:2507.12461\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2223] \n \n arXiv:2507.12475\n \n (cross-list from econ.TH)\n [ pdf, html, other]\n \n \n \n \n \n [2224] \n \n arXiv:2507.12480\n \n (cross-list from cs.SE)\n [ pdf, html, other]\n \n \n \n \n \n [2225] \n \n arXiv:2507.12482\n \n (cross-list from cs.SE)\n [ pdf, html, other]\n \n \n \n \n \n [2226] \n \n arXiv:2507.12485\n \n (cross-list from quant-ph)\n [ pdf, html, other]\n \n \n \n \n \n [2227] \n \n arXiv:2507.12486\n \n (cross-list from cs.MA)\n [ pdf, html, other]\n \n \n \n \n \n [2228] \n \n arXiv:2507.12490\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2229] \n \n arXiv:2507.12492\n \n (cross-list from quant-ph)\n [ pdf, html, other]\n \n \n \n \n \n [2230] \n \n arXiv:2507.12496\n \n (cross-list from cs.RO)\n [ pdf, html, other]\n \n \n \n \n \n [2231] \n \n arXiv:2507.12504\n \n (cross-list from cs.DB)\n [ pdf, html, other]\n \n \n \n \n \n [2232] \n \n arXiv:2507.12507\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2233] \n \n arXiv:2507.12508\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2234] \n \n arXiv:2507.12547\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2235] \n \n arXiv:2507.12553\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2236] \n \n arXiv:2507.12555\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2237] \n \n arXiv:2507.12568\n \n (cross-list from cs.CR)\n [ pdf, html, other]\n \n \n \n \n \n [2238] \n \n arXiv:2507.12574\n \n (cross-list from cs.LG)\n [ pdf, other]\n \n \n \n \n \n [2239] \n \n arXiv:2507.12602\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2240] \n \n arXiv:2507.12612\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2241] \n \n arXiv:2507.12619\n","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.arxiv.org%2Flist%2Fcs.AI%2F2025-07%3Fskip%3D2125%26show%3D2000","id":"https://www.arxiv.org/list/cs.AI/2025-07?skip=2125&show=2000","publishedDate":"2025-07-22T00:00:00.000Z","summary":"The webpage lists recent preprints from arXiv in the field of Artificial Intelligence, dated July 2025. It features a series of research papers across various subfields such as quantum bioinformatics, cybersecurity, machine learning, natural language processing, computer vision, and more. Each entry provides a link to the abstract, PDF, and other formats. The collection highlights ongoing advancements in AI, including novel algorithms, theoretical analyses, and cross-disciplinary applications. Notable topics include deep learning techniques, AI in bioinformatics, security, language models, and foundational AI research.","title":"Artificial Intelligence Jul 2025","url":"https://www.arxiv.org/list/cs.AI/2025-07?skip=2125&show=2000"},{"content":"\n \n [2326] \n \n arXiv:2507.13380\n \n (cross-list from cs.CL)\n [ pdf, other]\n \n \n \n \n \n [2327] \n \n arXiv:2507.13383\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2328] \n \n arXiv:2507.13392\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2329] \n \n arXiv:2507.13395\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2330] \n \n arXiv:2507.13407\n \n (cross-list from cs.CV)\n [ pdf, other]\n \n \n \n \n \n [2331] \n \n arXiv:2507.13408\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2332] \n \n arXiv:2507.13410\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2333] \n \n arXiv:2507.13411\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2334] \n \n arXiv:2507.13414\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2335] \n \n arXiv:2507.13415\n \n (cross-list from cs.MM)\n [ pdf, html, other]\n \n \n \n \n \n [2336] \n \n arXiv:2507.13416\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2337] \n \n arXiv:2507.13417\n \n (cross-list from cs.LG)\n [ pdf, other]\n \n \n \n \n \n [2338] \n \n arXiv:2507.13420\n \n (cross-list from cs.CV)\n [ pdf, other]\n \n \n \n \n \n [2339] \n \n arXiv:2507.13423\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2340] \n \n arXiv:2507.13425\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2341] \n \n arXiv:2507.13428\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2342] \n \n arXiv:2507.13459\n \n (cross-list from cs.CE)\n [ pdf, html, other]\n \n \n \n \n \n [2343] \n \n arXiv:2507.13468\n \n (cross-list from cs.RO)\n [ pdf, html, other]\n \n \n \n \n \n [2344] \n \n arXiv:2507.13485\n \n (cross-list from cs.NE)\n [ pdf, html, other]\n \n \n \n \n \n [2345] \n \n arXiv:2507.13499\n \n (cross-list from cs.SE)\n [ pdf, html, other]\n \n \n \n \n \n [2346] \n \n arXiv:2507.13505\n \n (cross-list from cs.CR)\n [ pdf, html, other]\n \n \n \n \n \n [2347] \n \n arXiv:2507.13524\n \n (cross-list from cs.HC)\n [ pdf, html, other]\n \n \n \n \n \n [2348] \n \n arXiv:2507.13542\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2349] \n \n arXiv:2507.13543\n \n (cross-list from cs.IT)\n [ pdf, html, other]\n \n \n \n \n \n [2350] \n \n arXiv:2507.13551\n \n (cross-list from cs.CL)\n [ pdf, other]\n \n \n \n \n \n [2351] \n \n arXiv:2507.13556\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2352] \n \n arXiv:2507.13569\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2353] \n \n arXiv:2507.13575\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2354] \n \n arXiv:2507.13579\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2355] \n \n arXiv:2507.13598\n \n (cross-list from cs.CR)\n [ pdf, html, other]\n \n \n \n \n \n [2356] \n \n arXiv:2507.13604\n \n (cross-list from eess.IV)\n [ pdf, html, other]\n \n \n \n \n \n [2357] \n \n arXiv:2507.13614\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2358] \n \n arXiv:2507.13618\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2359] \n \n arXiv:2507.13629\n \n (cross-list from cs.CR)\n [ pdf, html, other]\n \n \n \n \n \n [2360] \n \n arXiv:2507.13646\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2361] \n \n arXiv:2507.13647\n \n (cross-list from cs.RO)\n [ pdf, html, other]\n \n \n \n \n \n [2362] \n \n arXiv:2507.13659\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2363] \n \n arXiv:2507.13677\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2364] \n \n arXiv:2507.13681\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2365] \n \n arXiv:2507.13703\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2366] \n \n arXiv:2507.13725\n \n (cross-list from cs.IR)\n [ pdf, html, other]\n \n \n \n \n \n [2367] \n \n arXiv:2507.13729\n \n (cross-list from cs.RO)\n [ pdf, html, other]\n \n \n \n \n \n [2368] \n \n arXiv:2507.13739\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2369] \n \n arXiv:2507.13741\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2370] \n \n arXiv:2507.13742\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2371] \n \n arXiv:2507.13769\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2372] \n \n arXiv:2507.13789\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2373] \n \n arXiv:2507.13801\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2374] \n \n arXiv:2507.13802\n \n (cross-list from cs.CY)\n [ pdf, html, other]\n \n \n \n \n \n [2375] \n \n arXiv:2507.13820\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2376] \n \n arXiv:2507.13822\n \n (cross-list from cs.IR)\n [ pdf, other]\n \n \n \n \n \n [2377] \n \n arXiv:2507.13834\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2378] \n \n arXiv:2507.13859\n \n (cross-list from cs.IR)\n [ pdf, html, other]\n \n \n \n \n \n [2379] \n \n arXiv:2507.13868\n \n (cross-list from cs.CV)\n [ pdf, other]\n \n \n \n \n \n [2380] \n \n arXiv:2507.13880\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2381] \n \n arXiv:2507.13881\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2382] \n \n arXiv:2507.13912\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2383] \n \n arXiv:2507.13913\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2384] \n \n arXiv:2507.13919\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2385] \n \n arXiv:2507.13933\n \n (cross-list from cs.NI)\n [ pdf, html, other]\n \n \n \n \n \n [2386] \n \n arXiv:2507.13941\n \n (cross-list from q-bio.NC)\n [ pdf, html, other]\n \n \n \n \n \n [2387] \n \n arXiv:2507.13942\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2388] \n \n arXiv:2507.13949\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2389] \n \n arXiv:2507.13957\n \n (cross-list from cs.IR)\n [ pdf, html, other]\n \n \n \n \n \n [2390] \n \n arXiv:2507.13966\n \n (cross-list from cs.CL)\n [ pdf, other]\n \n \n \n \n \n [2391] \n \n arXiv:2507.13970\n \n (cross-list from cs.RO)\n [ pdf, html, other]\n \n \n \n \n \n [2392] \n \n arXiv:2507.13984\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2393] \n \n arXiv:2507.13993\n \n (cross-list from eess.IV)\n [ pdf, other]\n \n \n \n \n \n [2394] \n \n arXiv:2507.14000\n \n (cross-list from cs.PF)\n [ pdf, other]\n \n \n \n \n \n [2395] \n \n arXiv:2507.14043\n \n (cross-list from cs.RO)\n [ pdf, other]\n \n \n \n \n \n [2396] \n \n arXiv:2507.14056\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2397] \n \n arXiv:2507.14067\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2398] \n \n arXiv:2507.14069\n \n (cross-list from cs.DC)\n [ pdf, html, other]\n \n \n \n \n \n [2399] \n \n arXiv:2507.14079\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2400] \n \n arXiv:2507.14084\n \n (cross-list from cs.HC)\n [ pdf, html, other]\n \n \n \n \n \n [2401] \n \n arXiv:2507.14093\n \n (cross-list from cs.CV)\n [ pdf, html, other]\n \n \n \n \n \n [2402] \n \n arXiv:2507.14096\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2403] \n \n arXiv:2507.14119\n \n (cross-list from cs.CV)\n [ pdf, other]\n \n \n \n \n \n [2404] \n \n arXiv:2507.14121\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2405] \n \n arXiv:2507.14126\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2406] \n \n arXiv:2507.14140\n \n (cross-list from physics.geo-ph)\n [ pdf, other]\n \n \n \n \n \n [2407] \n \n arXiv:2507.14141\n \n (cross-list from eess.SP)\n [ pdf, html, other]\n \n \n \n \n \n [2408] \n \n arXiv:2507.14151\n \n (cross-list from eess.SP)\n [ pdf, html, other]\n \n \n \n \n \n [2409] \n \n arXiv:2507.14153\n \n (cross-list from eess.SP)\n [ pdf, other]\n \n \n \n \n \n [2410] \n \n arXiv:2507.14156\n \n (cross-list from q-bio.BM)\n [ pdf, html, other]\n \n \n \n \n \n [2411] \n \n arXiv:2507.14164\n \n (cross-list from eess.SP)\n [ pdf, html, other]\n \n \n \n \n \n [2412] \n \n arXiv:2507.14170\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2413] \n \n arXiv:2507.14171\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2414] \n \n arXiv:2507.14172\n \n (cross-list from cs.LG)\n [ pdf, other]\n \n \n \n \n \n [2415] \n \n arXiv:2507.14175\n \n (cross-list from cs.LG)\n [ pdf, other]\n \n \n \n \n \n [2416] \n \n arXiv:2507.14177\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2417] \n \n arXiv:2507.14178\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2418] \n \n arXiv:2507.14179\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2419] \n \n arXiv:2507.14180\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2420] \n \n arXiv:2507.14181\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2421] \n \n arXiv:2507.14182\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2422] \n \n arXiv:2507.14184\n \n (cross-list from eess.SP)\n [ pdf, html, other]\n \n \n \n \n \n [2423] \n \n arXiv:2507.14186\n \n (cross-list from cs.NI)\n [ pdf, html, other]\n \n \n \n \n \n [2424] \n \n arXiv:2507.14187\n \n (cross-list from eess.SP)\n [ pdf, other]\n \n \n \n \n \n [2425] \n \n arXiv:2507.14188\n \n (cross-list from cs.NI)\n [ pdf, html, other]\n \n \n \n \n \n [2426] \n \n arXiv:2507.14189\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2427] \n \n arXiv:2507.14193\n \n (cross-list from cs.GT)\n [ pdf, other]\n \n \n \n \n \n [2428] \n \n arXiv:2507.14195\n \n (cross-list from eess.SP)\n [ pdf, html, other]\n \n \n \n \n \n [2429] \n \n arXiv:2507.14196\n \n (cross-list from eess.SP)\n [ pdf, html, other]\n \n \n \n \n \n [2430] \n \n arXiv:2507.14198\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2431] \n \n arXiv:2507.14200\n \n (cross-list from cs.CL)\n [ pdf, html, other]\n \n \n \n \n \n [2432] \n \n arXiv:2507.14201\n \n (cross-list from cs.CR)\n [ pdf, html, other]\n \n \n \n \n \n [2433] \n \n arXiv:2507.14202\n \n (cross-list from cs.CR)\n [ pdf, html, other]\n \n \n \n \n \n [2434] \n \n arXiv:2507.14204\n \n (cross-list from cs.LG)\n [ pdf, html, other]\n \n \n \n \n \n [2435] \n \n arXiv:2507.14206\n \n (cross-list from eess.SP)\n [ pdf, html, other]\n \n \n \n \n \n [2436] \n \n arXiv:2507.14207\n \n (cross-list from cs.CR)\n [ pdf, html, other]\n \n \n \n \n \n [2437] \n \n arXiv:2507.14211\n \n (cross-list from cs.NI)\n [ pdf, html, other]\n \n \n \n \n \n [2438] \n \n arXiv:2507.14218\n \n (cross-list from cs.CY)\n [ pdf, html, other]\n \n \n \n \n \n [2439] \n \n arXiv:2507.14219\n \n (cross-list from cs.LG)\n [ pdf, other]\n \n \n \n \n \n [2440] \n \n arXiv:2507.14223\n \n (cross-list from cs.CR)\n [ pdf, html, other]\n \n \n \n \n \n [2441] \n \n arXiv:2507.14227\n \n (cross-list from cs.LG)\n [ pdf, html,","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.arxiv.org%2Flist%2Fcs.AI%2F2025-07%3Fskip%3D2325%26show%3D2000","id":"https://www.arxiv.org/list/cs.AI/2025-07?skip=2325&show=2000","publishedDate":"2025-07-22T00:00:00.000Z","summary":"The webpage is a listing of recent preprints in Artificial Intelligence from July 2025 on arXiv. It includes links to various papers across subfields such as natural language processing (cs.CL), machine learning (cs.LG), and computer vision (cs.CV). Notable topics likely covered in these papers include advancements in language models, machine learning algorithms, computer vision techniques, and interdisciplinary AI research. Each entry provides links to the paper's abstract, PDF, and other formats, serving as a resource for the latest AI research developments.","title":"Artificial Intelligence Jul 2025","url":"https://www.arxiv.org/list/cs.AI/2025-07?skip=2325&show=2000"},{"content":"[View PDF](https://arxiv.org/pdf/2503.05774)\n\n> Abstract:Existing methods for self-supervised representation learning of geospatial regions and map entities rely extensively on the design of pretext tasks, often involving augmentations or heuristic sampling of positive and negative pairs based on spatial proximity. This reliance introduces biases and limits the representations' expressiveness and generalisability. Consequently, the literature has expressed a pressing need to explore different methods for modelling geospatial data. To address the key difficulties of such methods, namely multimodality, heterogeneity, and the choice of pretext tasks, we present GeoJEPA, a versatile multimodal fusion model for geospatial data built on the self-supervised Joint-Embedding Predictive Architecture. With GeoJEPA, we aim to eliminate the widely accepted augmentation- and sampling biases found in self-supervised geospatial representation learning. GeoJEPA uses self-supervised pretraining on a large dataset of OpenStreetMap attributes, geometries and aerial images. The results are multimodal semantic representations of urban regions and map entities that we evaluate both quantitatively and qualitatively. Through this work, we uncover several key insights into JEPA's ability to handle multimodal data.\n\n## Submission history\n\nFrom: Theodor Lundqvist \\[ [view email](https://arxiv.org/show-email/b067a5d3/2503.05774)\\]\n\n**\\[v1\\]**\nTue, 25 Feb 2025 22:03:28 UTC (9,820 KB)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.05774","id":"https://arxiv.org/abs/2503.05774","publishedDate":"2025-02-25T00:00:00.000Z","summary":"The paper \"GeoJEPA: Towards Eliminating Augmentation- and Sampling Bias in Multimodal Geospatial Learning\" introduces GeoJEPA, a novel self-supervised model designed to improve geospatial data representations. Traditional methods rely heavily on pretext tasks involving data augmentations and heuristic sampling, which can introduce biases and limit the generalizability of learned features. GeoJEPA addresses these issues by leveraging a multimodal fusion approach based on the Joint-Embedding Predictive Architecture (JEPA), trained on large datasets from OpenStreetMap, including attributes, geometries, and aerial imagery. The resulting semantic representations of urban regions and map entities are evaluated both quantitatively and qualitatively, demonstrating GeoJEPA's ability to produce more unbiased, expressive, and versatile geospatial embeddings. This work advances the modeling of complex, multimodal geospatial data by reducing reliance on biased sampling strategies and improving the robustness of learned representations.","title":"GeoJEPA: Towards Eliminating Augmentation- and Sampling Bias in Multimodal Geospatial Learning","url":"https://arxiv.org/abs/2503.05774"}],"status":"completed","toolCallId":"call_jayYFiKn5BhlKxBEvXvBuA4N","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"I can’t display the actual diagram inline here, but I can point you to it and walk you through exactly what each part means.\n\nWhere to find it\n- Open the paper’s PDF: https://arxiv.org/pdf/2507.19457\n- The GEPA overview diagram is in the main method section (look for a figure labeled something like “GEPA overview” or “Reflective prompt evolution with a Pareto frontier”). It shows a loop with:\n  - Rollouts of a compound AI system\n  - Reflection that extracts rules\n  - Prompt mutations and recombinations\n  - Evaluation on multiple objectives\n  - Pareto-front selection\n  - Iteration\n\nHow to read the diagram, step by step\n\n1) System rollouts (left/top-left of the figure)\n- What you see: A compound AI pipeline with modules (e.g., Retriever, Planner, Tool Caller, Final Answerer). Arrows show data flowing and tool calls happening.\n- Meaning: GEPA runs the current system on a batch of inputs and logs the entire trace: prompts shown to each module, intermediate thoughts/reasons (if available), tool inputs/outputs, and any evaluator messages (e.g., error logs, pass/fail checks).\n- Why it’s there: Instead of a single reward number, GEPA collects a rich, textual “movie” of what happened.\n\n2) Reflection in natural language (center-left)\n- What you see: A “reflection” box (LLM call) that reads those traces and emits diagnoses and “lessons learned.”\n- Meaning: The reflection step converts trajectory text into actionable, general rules. Examples the figure implies:\n  - “Second-hop queries are duplicating first-hop terms; add a rule to target missing entities from summary1.”\n  - “Tool call failures stem from ambiguous parameter formatting; add explicit schema checks.”\n- Why it’s there: Language is the model’s native medium. We let the LLM mine the traces for reusable rules instead of squashing everything into a scalar reward.\n\n3) Prompt mutations per module (middle)\n- What you see: Proposed edits attached to specific modules’ prompts (multiple sticky-note-like edits).\n- Meaning: Each lesson becomes a concrete prompt change for the responsible module: adjust instructions, add examples, add checklists/constraints, clarify tool schemas, etc. GEPA creates multiple candidate variations (mutations) per module and also combinations across modules.\n- Why it’s there: High-leverage patches—one rule can fix many future cases. Working at the module level keeps edits targeted and interpretable.\n\n4) Genetic-style population (middle-right)\n- What you see: A population of candidate prompt sets (each is a full system spec with edited prompts). You may see “recombine” arrows merging compatible edits from different candidates.\n- Meaning: GEPA maintains a diverse pool of strategies. It can also recombine compatible lessons (like genetics) so multiple good ideas coexist in one candidate.\n- Why it’s there: Avoids converging on a single brittle tactic; encourages exploration and synergy among edits.\n\n5) Multi-objective evaluation (right)\n- What you see: A scoring panel with multiple axes, e.g., Accuracy, Cost/Latency, Safety, Stability.\n- Meaning: Candidates are tested on a validation slice and scored on several objectives, not just one.\n- Why it’s there: Real systems have trade-offs. GEPA optimizes across them rather than overfitting to a single metric.\n\n6) Pareto-front selection (bottom-right)\n- What you see: A Pareto frontier curve with selected candidates highlighted; others are discarded.\n- Meaning: Keep only non-dominated candidates—those that are not strictly worse on every metric. This preserves diverse, strong strategies.\n- Why it’s there: Protects against local optima and maintains options that excel on different trade-offs or subsets.\n\n7) Iteration loop (bottom/back to left)\n- What you see: Arrows feeding the selected Pareto candidates back to the rollout step.\n- Meaning: With the new population, run more rollouts, gather new traces, reflect again, propose further edits, and repeat.\n- Why it’s there: Accumulates lessons quickly—often big gains in 1–2 rounds—without thousands of rollouts.\n\nWhat the diagram emphasizes conceptually\n- Use language to learn from language: trajectories → reflection → rules → prompts.\n- Targeted, modular edits: fixes attach to the module that caused the failure.\n- Diversity and robustness: genetic population + Pareto selection keep multiple good strategies alive.\n- Sample efficiency: one rule can fix a whole class of errors, so you need far fewer rollouts than RL with sparse rewards.\n\nIf you’d like, share a screenshot of the figure, and I can annotate it visually: I’ll label each box, arrow, and legend and tie them to the steps above for a point-by-point walkthrough.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_01fa6f9c60b78d1d0068caedc7d0d08191866f6081bcdb0eb9"}},"resumableStreamId":"STREAM:b28c5c65-a202-4fad-b815-7a02b41563ee","role":"assistant","status":"done","threadId":"5e763140-e699-40d6-970e-751838959582","timeToFirstToken":0.984,"tokens":1006,"tokensPerSecond":21.405621635423536,"updated_at":1758129595759,"userId":"google:104458561188320708185","id":"j9711rm9rpm2gn6ggq8rcq519n7qr1dk"},{"_creationTime":1758129839319.2349,"_id":"j97fpdgd2c8e17zhpqw4ww685x7qsqj3","attachmentIds":[],"content":"explain to me the workflow pipeline of GEPA","created_at":1758129839319,"messageId":"5851fff7-b04d-40d3-893c-958e90aa41f4","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"5e763140-e699-40d6-970e-751838959582","updated_at":1758129839319,"userId":"google:104458561188320708185","id":"j97fpdgd2c8e17zhpqw4ww685x7qsqj3","providerMetadata":{}},{"_creationTime":1758129839319.235,"_id":"j97f6hpvs2xg048vghj8sctj217qsxvm","attachmentIds":[],"byok":false,"content":"Here’s the GEPA workflow pipeline end-to-end. Think of it as a tight loop that turns rich execution traces into targeted prompt edits, evaluates them on multiple goals, and keeps a diverse set of winners.\n\nInputs\n- System spec: a compound AI system Φ with modules M1..Mk (each has a prompt), control flow C, and any tools/APIs.\n- Data: a training/validation slice of tasks.\n- Evaluators/objectives: accuracy/quality, cost/latency, safety/constraints, stability, etc.\n- Logging policy: what to record from rollouts (prompts, thoughts if allowed, tool I/O, evaluator rationales, error logs).\n\nPipeline loop\n\n1) Initialize a population of candidates\n- Start with one or a few seed system specs (the current prompts for each module).\n- Population = multiple prompt sets to encourage diverse exploration.\n\n2) Run instrumented rollouts\n- Execute each candidate on a batch of inputs.\n- Capture full traces:\n  - Inputs and per-module prompts shown\n  - Intermediate reasoning/notes (if permissible)\n  - Tool calls and responses (APIs, retriever docs, compiler/unit-test logs)\n  - Partial checks and final rewards/scores\n- Purpose: gather a textual, high-signal account of what happened, not just a scalar score.\n\n3) Reflect in natural language\n- A “reflector” LLM reads the traces and answers:\n  - What failed/succeeded and why?\n  - Which module behavior caused it?\n  - What general rule or constraint would fix this class of errors?\n  - What tradeoffs are at play (recall vs precision, speed vs accuracy, safety vs coverage)?\n- Output: a set of named lessons, each with:\n  - Target module(s)\n  - Rationale linked to concrete failure examples\n  - Proposed prompt edits (instruction changes, checklists, examples, tool schema clarifications)\n  - Optional confidence/priority tags\n\n4) Generate prompt mutations and recombinations\n- For each lesson, produce one or more prompt edits to the relevant module(s).\n- Build multiple candidate systems by:\n  - Applying single edits\n  - Combining compatible edits across modules (“recombination”)\n  - Keeping edit granularity small to aid attribution\n- Result: a new batch/population of mutated candidates.\n\n5) Multi-objective evaluation\n- Run the mutated candidates on a validation slice.\n- Score on several axes:\n  - Task metrics (EM/F1, pass@k, success rate)\n  - Cost/latency\n  - Safety/constraint adherence\n  - Stability/variance across seeds or inputs\n- Optionally collect evaluator rationales to feed the next reflection step.\n\n6) Pareto-front selection\n- Keep only non-dominated candidates (no other candidate is strictly better on all objectives).\n- This preserves diverse strategies that excel on different tradeoffs or subsets of the data.\n- Archive or tag which lessons/edits each surviving candidate embodies.\n\n7) Iterate\n- Use the Pareto survivors as the next generation’s parents.\n- Repeat steps 2–6 for a few rounds until:\n  - Gains plateau on held-out data\n  - Budget is exhausted\n  - A target performance/cost threshold is met\n\n8) Finalize and deploy\n- Choose one or a small set of Pareto candidates:\n  - Primary: best balance for production\n  - Alternates: specialized prompts for certain input clusters or stricter safety modes\n- Record an audit trail:\n  - Which lessons led to which prompt lines\n  - The validation evidence supporting them\n\nKey design choices that make the pipeline work\n- Work at the module level: edits are tied to the prompt that controls the failing behavior, improving interpretability and transfer.\n- Use language feedback, not just numbers: reflections convert traces into reusable rules, giving high leverage per rollout.\n- Maintain diversity via Pareto selection: avoids collapsing onto a single brittle tactic and helps generalization.\n- Small, justified edits: eases ablation, rollback, and recombination.\n- Rich logging: the more informative the traces and evaluator messages, the better the reflections and edits.\n\nMinimal pseudo-code sketch\n\n- High-level\n  - Population ← {seed_prompts}\n  - for gen in 1..G:\n    - Traces ← run_rollouts(Population, train_batch, log_all=True)\n    - Lessons ← reflect(Traces)\n    - Candidates ← mutate_and_recombine(Population, Lessons)\n    - Scores ← evaluate(Candidates, val_batch, metrics=[acc,cost,safety,stability])\n    - Population ← pareto_front(Candidates, Scores)\n  - Return select_for_deploy(Population, policy=balanced)\n\n- Mutation examples\n  - Add checklist section to Planner prompt\n  - Clarify parameter schema for Tool-Caller (types, ranges, error handling)\n  - Add “second-hop missing-entity” rule to Retriever-Query prompt\n  - Insert preflight validation step instruction in Finalizer\n\nStopping criteria and practical tips\n- Stop when validation gains flatten across two generations or when cost/latency increases without accuracy gains.\n- Freeze especially strong edits into the seed after they prove robust, then continue exploring other axes.\n- Keep a small held-out test set untouched until the end.\n- Track cost per point of improvement to decide when to stop.\n\nIf you want, tell me your task and modules, and I’ll draft:\n- A logging schema for traces\n- A first reflection meta-prompt\n- A mutation plan and Pareto metrics tailored to your objectives","created_at":1758129839320,"messageId":"e6ab89a4-1fa6-4bde-8e2e-efa0e3768ad9","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":null,"queries":["GEPA Reflective Prompt Evolution Can Outperform Reinforcement Learning workflow pipeline figure arXiv 2507.19457 GEPA method steps","GEPA genetic pareto reflective prompt evolution pipeline HotpotQA MIPROv2 GRPO paper 2025 arXiv:2507.19457 method section"]},"result":[{"content":"# Computer Science > Computation and Language\n\n**arXiv:2507.19457** (cs)\n\n\\[Submitted on 25 Jul 2025\\]\n\n# Title:GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\nAuthors: [Lakshya A Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal,+L+A), [Shangyin Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan,+S), [Dilara Soylu](https://arxiv.org/search/cs?searchtype=author&query=Soylu,+D), [Noah Ziems](https://arxiv.org/search/cs?searchtype=author&query=Ziems,+N), [Rishi Khare](https://arxiv.org/search/cs?searchtype=author&query=Khare,+R), [Krista Opsahl-Ong](https://arxiv.org/search/cs?searchtype=author&query=Opsahl-Ong,+K), [Arnav Singhvi](https://arxiv.org/search/cs?searchtype=author&query=Singhvi,+A), [Herumb Shandilya](https://arxiv.org/search/cs?searchtype=author&query=Shandilya,+H), [Michael J Ryan](https://arxiv.org/search/cs?searchtype=author&query=Ryan,+M+J), [Meng Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang,+M), [Christopher Potts](https://arxiv.org/search/cs?searchtype=author&query=Potts,+C), [Koushik Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen,+K), [Alexandros G. Dimakis](https://arxiv.org/search/cs?searchtype=author&query=Dimakis,+A+G), [Ion Stoica](https://arxiv.org/search/cs?searchtype=author&query=Stoica,+I), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein,+D), [Matei Zaharia](https://arxiv.org/search/cs?searchtype=author&query=Zaharia,+M), [Omar Khattab](https://arxiv.org/search/cs?searchtype=author&query=Khattab,+O)\n\nView a PDF of the paper titled GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning, by Lakshya A Agrawal and 16 other authors\n\n[View PDF](https://arxiv.org/pdf/2507.19457)\n\n> Abstract:Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.\n\n| | |\n| --- | --- |\n| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE) |\n| ACM classes: | I.2.7; I.2.6; I.2.4; I.2.8 |\n| Cite as: | [arXiv:2507.19457](https://arxiv.org/abs/2507.19457) \\[cs.CL\\] |\n| (or [arXiv:2507.19457v1](https://arxiv.org/abs/2507.19457v1) \\[cs.CL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2507.19457](https://doi.org/10.48550/arXiv.2507.19457) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Lakshya A Agrawal \\[ [view email](https://arxiv.org/show-email/af5f91b3/2507.19457)\\] **\\[v1\\]**\nFri, 25 Jul 2025 17:42:32 UTC (1,632 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning, by Lakshya A Agrawal and 16 other authors\n\n- [View PDF](https://arxiv.org/pdf/2507.19457)\n- [TeX Source](https://arxiv.org/src/2507.19457)\n- [Other Formats](https://arxiv.org/format/2507.19457)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[< prev](https://arxiv.org/prevnext?id=2507.19457&function=prev&context=cs.CL)  \\|  [next >](https://arxiv.org/prevnext?id=2507.19457&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2025-07](https://arxiv.org/list/cs.CL/2025-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2507.19457?context=cs) [cs.AI](https://arxiv.org/abs/2507.19457?context=cs.AI) [cs.LG](https://arxiv.org/abs/2507.19457?context=cs.LG) [cs.SE](https://arxiv.org/abs/2507.19457?context=cs.SE)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2507.19457)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2507.19457)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2507.19457)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2507.19457) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.19457","id":"https://arxiv.org/abs/2507.19457","publishedDate":"2025-07-25T00:00:00.000Z","summary":"The paper introduces GEPA (Genetic-Pareto), a novel prompt optimization method for large language models (LLMs) that leverages natural language reflection rather than traditional reinforcement learning (RL). Unlike RL approaches like GRPO, which require extensive rollouts, GEPA uses natural language reflection to diagnose issues, propose prompt updates, and learn high-level rules through trial and error. This approach enables efficient learning, often achieving significant performance improvements with fewer rollouts. Across four tasks, GEPA outperforms RL-based methods by up to 20% and the leading prompt optimizer MIPROv2 by over 10%, while using up to 35 times fewer rollouts. The method also shows promise as an inference-time strategy for code optimization, demonstrating the power of reflective prompt evolution over conventional RL techniques in enhancing LLM performance.","title":"GEPA: Reflective Prompt Evolution Can Outperform Reinforcement ...","url":"https://arxiv.org/abs/2507.19457"},{"content":"[Papers](https://huggingface.co/papers)\n\narxiv:2507.19457\n\n# GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\nPublished on Jul 25\n\n·Submitted by[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)on Jul 28\n\n[#3 Paper of the day](https://huggingface.co/papers?date=2025-07-28)\n\n[Upvote\\\n\\\n27](https://huggingface.co/login?next=%2Fpapers%2F2507.19457)\n\n- +19\n\nAuthors:\n\n[Lakshya A Agrawal](https://huggingface.co/LakshyAAAgrawal)\n,\n\nShangyin Tan\n\n,\n\nDilara Soylu\n\n,\n\nNoah Ziems\n\n,\n\nRishi Khare\n\n,\n\nKrista Opsahl-Ong\n\n,\n\nArnav Singhvi\n\n,\n\nHerumb Shandilya\n\n,\n\nMichael J Ryan\n\n,\n\nMeng Jiang\n\n,\n\nChristopher Potts\n\n,\n\nKoushik Sen\n\n,\n\nAlexandros G. Dimakis\n\n,\n\nIon Stoica\n\n,\n\nDan Klein\n\n,\n\nMatei Zaharia\n\n,\n\nOmar Khattab\n\n## Abstract\n\nGEPA, a prompt optimizer using natural language reflection, outperforms RL methods like GRPO and MIPROv2 with fewer rollouts by learning high-level rules from trial and error.\n\nAI-generated summary\n\n[Large language models](https://huggingface.co/papers?q=Large%20language%20models) (LLMs) are increasingly adapted to downstream tasks via\n[reinforcement learning](https://huggingface.co/papers?q=reinforcement%20learning) (RL) methods like [Group Relative Policy Optimization](https://huggingface.co/papers?q=Group%20Relative%20Policy%20Optimization)\n( [GRPO](https://huggingface.co/papers?q=GRPO)), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce [GEPA](https://huggingface.co/papers?q=GEPA) ( [Genetic-Pareto](https://huggingface.co/papers?q=Genetic-Pareto)), a prompt\noptimizer that thoroughly incorporates [natural language reflection](https://huggingface.co/papers?q=natural%20language%20reflection) to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, [GEPA](https://huggingface.co/papers?q=GEPA) samples [system-level trajectories](https://huggingface.co/papers?q=system-level%20trajectories) (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the [Pareto frontier](https://huggingface.co/papers?q=Pareto%20frontier) of its own attempts. As a result of [GEPA](https://huggingface.co/papers?q=GEPA)'s design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, [GEPA](https://huggingface.co/papers?q=GEPA) outperforms [GRPO](https://huggingface.co/papers?q=GRPO) by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. [GEPA](https://huggingface.co/papers?q=GEPA) also outperforms the leading [prompt optimizer](https://huggingface.co/papers?q=prompt%20optimizer),\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for [code optimization](https://huggingface.co/papers?q=code%20optimization).\n\n[View arXiv page](https://arxiv.org/abs/2507.19457) [View PDF](https://arxiv.org/pdf/2507.19457) [Add to collection](https://huggingface.co/login?next=%2Fpapers%2F2507.19457)\n\n### Community\n\n[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)\nPaper authorPaper submitter [Jul 28](https://huggingface.co/huggingface.co#6887e6cdeed12b6b6149240b)\n\nLarge language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.\n\nReply\n\n[librarian-bot](https://huggingface.co/librarian-bot)\n[Jul 29](https://huggingface.co/huggingface.co#688825f0fc34a7523d2ce6e8)\n\nThis is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper.\n\nThe following papers were recommended by the Semantic Scholar API\n\n- [A Technical Survey of Reinforcement Learning Techniques for Large Language Models](https://huggingface.co/papers/2507.04136) (2025)\n- [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://huggingface.co/papers/2507.00726) (2025)\n- [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://huggingface.co/papers/2507.02804) (2025)\n- [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://huggingface.co/papers/2506.08125) (2025)\n- [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://huggingface.co/papers/2507.00417) (2025)\n- [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://huggingface.co/papers/2507.14897) (2025)\n- [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://huggingface.co/papers/2507.02834) (2025)\n\nPlease give a thumbs up to this comment if you found it helpful!\n\nIf you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\n\nYou can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`\n\nReply\n\n[millexplore](https://huggingface.co/millexplore)\n[Jul 31](https://huggingface.co/huggingface.co#688b0d36934fecfbd1df1c10)\n\nNo code? :)\n\n- 1 reply\n\n·\n\n[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)\nPaper author [23 days ago](https://huggingface.co/huggingface.co#68ac1872957cdf6c1c630450)\n\n[https://github.com/gepa-ai/gepa](https://github.com/gepa-ai/gepa)\n\n[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)\nPaper authorPaper submitter [23 days ago](https://huggingface.co/huggingface.co#68ac17dba9c8febf9a1937d4)\n\nWe have released the code at [https://github.com/gepa-ai/gepa](https://github.com/gepa-ai/gepa)\n\n❤️\n\n1\n\n1\n\n+\n\nReply\n\nEditPreview\n\nUpload images, audio, and videos by dragging in the text input, pasting, or clicking here.\n\nTap or paste here to upload images\n\nComment\n\n· [Sign up](https://huggingface.co/join?next=%2Fpapers%2F2507.19457) or\n[log in](https://huggingface.co/login?next=%2Fpapers%2F2507.19457) to comment\n\n[Upvote\\\n\\\n27](https://huggingface.co/login?next=%2Fpapers%2F2507.19457)\n\n- +15\n\n## Models citing this paper 0\n\nNo model linking this paper\n\nCite arxiv.org/abs/2507.19457 in a model README.md to link it from this page.\n\n## Datasets citing this paper 0\n\nNo dataset linking this paper\n\nCite arxiv.org/abs/2507.19457 in a dataset README.md to link it from this page.\n\n### Spaces citing this paper 0\n\nNo Space linking this paper\n\nCite arxiv.org/abs/2507.19457 in a Space README.md to link it from this page.\n\n## Collections including this paper 5","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fhuggingface.co%2Fpapers%2F2507.19457","id":"https://huggingface.co/papers/2507.19457","publishedDate":"2025-07-28T00:00:00.000Z","summary":"The webpage introduces GEPA (Genetic-Pareto), a novel prompt optimization method that leverages natural language reflection to improve large language model (LLM) performance. Unlike traditional reinforcement learning approaches like GRPO, which require thousands of rollouts, GEPA learns high-level rules through natural language analysis of system trajectories, enabling more efficient and effective prompt refinement. Across multiple tasks, GEPA outperforms existing methods by up to 20% while using significantly fewer rollouts—up to 35 times less—and shows promise for code optimization and inference-time search strategies. The approach highlights the advantages of using language's interpretability as a richer learning medium compared to scalar reward-based methods.","title":"Reflective Prompt Evolution Can Outperform Reinforcement Learning","url":"https://huggingface.co/papers/2507.19457"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F1mb8e5w%2F250719457_gepa_reflective_prompt_evolution_can%2F","id":"https://www.reddit.com/r/MachineLearning/comments/1mb8e5w/250719457_gepa_reflective_prompt_evolution_can/","publishedDate":"2025-07-28T00:00:00.000Z","summary":"Failed to fetch content","title":"[2507.19457] GEPA: Reflective Prompt Evolution Can Outperform ...","url":"https://www.reddit.com/r/MachineLearning/comments/1mb8e5w/250719457_gepa_reflective_prompt_evolution_can/"},{"content":"[Sitemap](https://noailabs.medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F46048a2f8a2b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fnoailabs.medium.com%2Fnew-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fnoailabs.medium.com%2Fnew-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# New Research: Reflective Prompt Evolution Can Outperform RL\n\n## A prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error\n\n[noailabs](https://noailabs.medium.com/?source=post_page---byline--46048a2f8a2b---------------------------------------)\n\n5 min read\n\n·\n\nJul 28, 2025\n\n--\n\nListen\n\nShare\n\n**Meta-prompting is a new level in evolution of LLM-based systems.**\n\nPress enter or click to view image in full size\n\n[https://arxiv.org/abs/2507.19457](https://arxiv.org/abs/2507.19457)\n\n> Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), w **hich often require thousands of rollouts to learn new tasks.** We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), **a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error.** Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) **and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts.** As a result of GEPA’s design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.\n\nGEPA’S REFLECTION AND PROMPT UPDATE META PROMPT\n\n```\nI provided an assistant with the following instructions to perform a task for me:‘‘‘ ‘‘‘The following are examples of different task inputs provided to the assistantalong with the assistant’s response for each of them, and some feedback on howthe assistant’s response could be better:‘‘‘ ‘‘‘Your task is to write a new instruction for the assistant.Read the inputs carefully and identify the input format and infer detailed taskdescription about the task I wish to solve with the assistant.Read all the assistant responses and the corresponding feedback. Identify allniche and domain specific factual information about the task and include it inthe instruction, as a lot of it may not be available to the assistant in thefuture. The assistant may have utilized a generalizable strategy to solve thetask, if so, include that in the instruction as well.Provide the new instructions within ‘‘‘ blocks\n```\n\nA new paper, “GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning,” introduces a novel method for optimizing Large Language Model (LLM) prompts that can significantly enhance the performance and efficiency of LLM-based systems. This approach, centered on “reflective prompt evolution,” offers a departure from traditional reinforcement learning (RL) techniques.\n\n# Core Insights of GEPA\n\nThe GEPA (Genetic-Pareto) model is a prompt optimizer that leverages natural language reflection to learn and adapt. Here are the central ideas behind this innovative approach:\n\n- **Reflective Prompt Evolution**: Unlike RL methods that rely on often sparse, scalar rewards, GEPA utilizes the rich, interpretable nature of language itself as a learning medium. It analyzes system-level trajectories, which include reasoning steps, tool usage, and outputs, to diagnose issues and refine prompts through natural language feedback.\n- **Genetic-Pareto Optimization**: GEPA employs a Genetic-Pareto algorithm to manage and evolve prompts. This involves sampling various system trajectories, identifying problems, proposing and testing prompt modifications, and integrating the most effective changes from a “Pareto frontier” of its attempts.\n- **Enhanced Efficiency and Performance**: The GEPA framework has demonstrated significant improvements over existing methods. In various tasks, it outperformed Group Relative Policy Optimization (GRPO), a reinforcement learning technique, by an average of 10% and in some cases by up to 20%, all while requiring up to 35 times fewer “rollouts” or trials. Furthermore, GEPA has shown a greater than 10% improvement over the leading prompt optimizer, MIPROv2, across two different LLMs.\n- **Broad Applicability**: The principles of GEPA can be applied to a wide range of domains. For instance, in scientific research, it can optimize prompts for analyzing complex datasets. In education, it can help create more adaptive AI-tutoring systems, and in business, it can streamline tasks like market analysis.\n\n# Mini-Framework for Adaptive LLM-based (Agentic) Systems\n\nThe core principles of GEPA can be distilled into a mini-framework for developing more adaptive and efficient LLM-based agentic systems. This framework emphasizes continuous self-improvement through reflection and evolution.\n\n**1\\. Define Agent Trajectory:**\n\n- **Components**: An agent’s “trajectory” should not just be its final output. It must include all the intermediate steps, such as its reasoning process, any tools it utilizes, and the outputs from those tools.\n- **Example**: For a research agent, the trajectory would encompass the initial query, the search terms used, the sources consulted, the summarized findings from each source, and the final synthesized report.\n\n**2\\. Implement a Reflective “Critic”:**\n\n- **Function**: After each task or a set of tasks, a “critic” LLM analyzes the agent’s trajectory.\n- **Process**: This critic uses natural language to identify shortcomings, errors, or inefficiencies in the agent’s process. It should answer questions like: “Where did the agent go wrong?”, “Could a different tool have been more effective?”, or “Was the reasoning sound?”.\n\n**3\\. Generate and Test Evolved Prompts:**\n\n- **Evolutionary Engine**: Based on the critic’s feedback, a “prompt evolution” mechanism generates alternative prompts for the agent. This can be viewed as a form of genetic algorithm where different “genes” in the prompt are modified.\n- **Pareto-based Selection**: Not all changes are universally better. A Pareto-based selection process helps to identify a set of “non-dominated” prompts that offer different trade-offs between various performance metrics (e.g., accuracy, efficiency, and resource usage).\n\n**4\\. Continuous Integration and Adaptation:**\n\n- **Update Mechanism**: The best-performing prompts from the Pareto frontier are integrated back into the agent’s core instruction set.\n- **Iterative Improvement**: This entire process — trajectory analysis, reflection, and evolution — is not a one-time setup but a continuous loop. This allows the agent to adapt to new tasks and learn from its mistakes over time, leading to a truly adaptive system.\n\n[Llm](https://medium.com/tag/llm?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Prompt Engineering](https://medium.com/tag/prompt-engineering?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Rl](https://medium.com/tag/rl?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Agents](https://medium.com/tag/agents?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[**Written by noailabs**](https://noailabs.medium.com/?source=post_page---post_author_info--46048a2f8a2b---------------------------------------)\n\n[469 followers](https://noailabs.medium.com/followers?source=post_page---post_author_info--46048a2f8a2b---------------------------------------)\n\n· [37 following](https://noailabs.medium.com/following?source=post_page---post_author_info--46048a2f8a2b---------------------------------------)\n\nTech/biz consulting, analytics, research for founders, startups, corps and govs.\n\n## No responses yet\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Press](mailto:pressinquiries@medium.com)\n\n[Blog](https://blog.medium.com/?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Rules](https:/","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fnoailabs.medium.com%2Fnew-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b","id":"https://noailabs.medium.com/new-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b","publishedDate":"2025-07-28T00:00:00.000Z","summary":"The article introduces GEPA (Genetic-Pareto), a novel prompt optimization method that leverages natural language reflection to improve large language model (LLM) performance. Unlike traditional reinforcement learning (RL) approaches, which require extensive trial-and-error and numerous rollouts, GEPA uses natural language to diagnose issues, propose prompt updates, and learn high-level rules from fewer attempts. By reflecting on system trajectories—such as reasoning steps and tool outputs—GEPA efficiently enhances prompt quality, often outperforming RL-based methods like GRPO by up to 20% while using significantly fewer rollouts (up to 35 times fewer). It also surpasses existing prompt optimizers like MIPROv2 across multiple tasks and LLMs, demonstrating the potential of meta-prompting and natural language reflection to accelerate and improve prompt tuning and code optimization strategies.","title":"New Research: Reflective Prompt Evolution Can Outperform RL","url":"https://noailabs.medium.com/new-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b"},{"content":"# [ArXivIQ](https://arxiviq.substack.com/)\n\nSubscribeSign in\n\n# GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\nJul 31, 2025\n\n3\n\n[2](https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution/comments)\n\n[Share](javascript:void(0))\n\n**Authors:** _Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab_ **Paper:** [https://arxiv.org/abs/2507.19457](https://arxiv.org/abs/2507.19457)\n\n## TL;DR\n\n**What was done?** The authors introduced GEPA (Genetic-Pareto), a novel algorithm for optimizing prompts in complex, multi-module AI systems. Instead of relying on traditional reinforcement learning (RL), GEPA employs a language-driven, evolutionary approach. Its core innovation is \"reflective prompt mutation,\" where an LLM analyzes its own performance—including reasoning steps, tool usage, and detailed evaluation feedback—in natural language to diagnose failures and propose targeted improvements to its instructional prompts. This process is guided by a genetic algorithm that uses Pareto selection to maintain a diverse set of high-performing prompts, preventing the optimizer from getting stuck in local optima.\n\n**Why it matters?** This work signals a potential paradigm shift in how we optimize LLM-based agents. GEPA demonstrates that learning through language-based self-reflection is dramatically more sample-efficient than learning from sparse, scalar rewards. It outperforms the RL method GRPO by an average of 10% while using up to 35x fewer \"rollouts\" (system executions). It also surpasses the state-of-the-art prompt optimizer MIPROv2 ( [https://aclanthology.org/2024.emnlp-main.525/](https://aclanthology.org/2024.emnlp-main.525/)), and surprisingly shows that evolving detailed instructions alone can be more effective than optimizing few-shot examples. This approach makes adapting powerful AI systems far more practical and affordable, especially in settings where data is scarce or system executions are expensive.\n\n## Details\n\n### The High Cost of Learning by Doing\n\nOptimizing the performance of sophisticated AI agents—systems that combine multiple LLM modules, tool calls, and complex logic—is a central challenge in modern AI. A popular approach has been reinforcement learning (RL), where an agent learns through trial and error, guided by a scalar reward signal. However, this method often proves to be a brute-force endeavor, requiring tens or even hundreds of thousands of system executions (\"rollouts\") to achieve meaningful improvements. This high sample cost is a major bottleneck, making RL impractical for many real-world applications where each rollout may be computationally expensive, time-consuming, or financially costly.\n\nA new paper from a large collaboration of researchers across UC Berkeley, Stanford, Databricks, and MIT challenges this paradigm. The authors argue that for systems built on Large Language Models (LLMs), the very language they process offers a far richer and more efficient learning medium than a simple numerical reward. Their proposed algorithm, GEPA (Genetic-Pareto), demonstrates that an AI system can learn more effectively by \"reflecting\" on its behavior in natural language, leading to a method that is not only more powerful but also vastly more efficient.\n\nArXivIQ is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\n\nSubscribe\n\n### GEPA: Learning by Reflective Evolution\n\nGEPA introduces a novel optimization framework built on three key principles: Genetic Prompt Evolution, Natural Language Reflection, and Pareto-based Candidate Selection (Figure 3).\n\n1. **Natural Language Reflection:** At the heart of GEPA is its ability to learn from detailed, textual feedback. Instead of just receiving a score, the system's entire execution trace is fed back to an LLM for analysis. This feedback isn't just a number; it's a rich, textual record of the entire process, including the LLM's own reasoning chains, the specific tool calls it made, and even detailed diagnostic information from the evaluation environment, like compiler error messages or failed unit tests. This \"reflector\" LLM analyzes the textual feedback to diagnose what went wrong (or right) and proposes specific, targeted edits to the system's instructional prompts.\n\nAn example of a GEPA-generated prompt (Figure 2) reveals a level of detail and strategic insight far beyond a simple instruction, including sections on \"Key Observations,\" \"Purpose and Context,\" and \"Practical Strategy.\"\n\n2. **Genetic-Pareto Optimization:** To guide this learning process, GEPA employs an evolutionary search strategy. It maintains a pool of candidate prompt sets and iteratively \"mutates\" them based on the insights from the reflection step. To avoid the common pitfall of getting stuck in a local optimum, GEPA uses a strategy akin to an \"illumination\" search, known as Pareto-based selection (Algorithm 2).\n\nInstead of just greedily climbing the single highest peak it can find, GEPA tries to illuminate the entire mountain range by identifying candidates that are the best for at least one specific problem instance. This encourages diverse strategies and ultimately leads to a more robust and general solution. The impact of this choice is starkly visualized in the paper, showing how Pareto selection leads to a balanced, exploratory search tree compared to the narrow, stalled search of a naive greedy approach (Figure 6).\n\n\n### A New Benchmark for Sample Efficiency and Performance\n\nThe experimental results are compelling, showcasing GEPA's strengths across four diverse tasks— [HotpotQA](https://arxiv.org/abs/1809.09600), IFBench, [HoVer](https://aclanthology.org/2020.findings-emnlp.309/), and [PUPA](https://aclanthology.org/2025.naacl-long.173/) — and two different LLMs (Qwen3 8B and GPT-4.1 Mini).\n\n- **Outperforming Reinforcement Learning:** GEPA outperforms the RL baseline GRPO by an average of 10%, with gains of up to 19% on specific tasks. Most impressively, it achieves this while using up to **35 times fewer rollouts** (Table 1).\n\nThe learning curves (Figure 1) clearly show GEPA reaching superior performance far more rapidly than GRPO.\n\n- **Surpassing State-of-the-Art Prompt Optimization:** GEPA also consistently outperforms MIPROv2, a leading joint instruction and few-shot prompt optimizer. It more than doubles the aggregate performance gains seen with MIPROv2 and achieves this with prompts that are, on average, 33% shorter (Figure 15), translating directly to lower inference costs.\n\n- **The Power of Instructions:** A particularly surprising finding is that GEPA's instruction-only optimization outperforms MIPROv2's joint optimization of both instructions and few-shot examples. This suggests that as LLMs become better at following complex instructions, evolving a detailed, reflective set of instructions may be a more powerful and efficient strategy than curating in-context examples.\n\n\nThe paper also presents promising preliminary results for using GEPA as an inference-time search strategy for highly technical domains like code optimization. When applied to generating CUDA and NPU kernels, GEPA was able to iteratively refine code based on compiler feedback to achieve significant performance improvements over strong baselines (Figure 7, 8).\n\n### Limitations and Future Directions\n\nThe authors are candid about the method's limitations. The boundary between prompt-based learning and traditional weight-based fine-tuning remains an open question; in data-abundant regimes, full fine-tuning may still have the upper hand. The paper also suggests that GEPA could be further improved by incorporating few-shot example optimization or by developing more sophisticated \"feedback engineering\" to extract the most valuable learning signals from system traces.\n\nA key avenue for future work is the integration of reflective prompt evolution with weight-space adaptation. A hybrid approach, where GEPA's language-based insights guide more efficient RL or fine-tuning rollouts, could unify these paradigms and lead to even greater performance and efficiency.\n\n### Conclusion\n\n\"GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\" presents a significant and practical advance in the field of AI system optimization. By treating language not just as an interface but as the primary medium for learning and reflection, the authors have developed a method that is demonstrably more sample-efficient, performant, and cost-effective than existing approaches.\n\nUltimately, GEPA's success suggests that the future of AI optimization may lie less in brute-force statistical methods and more in endowing our systems with the capacity for self-reflection. By learning from language, not just from numbers, GEPA is a significant step towards creating AI that improves not just by doing, but by understanding. This paper is a valuable contribution and a recommended read for anyone working on the frontier of LLM-based systems and agentic AI.\n\nArXivIQ is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\n\nSubscribe\n\n3\n\n[2](https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution/comments)\n\n[Share](javascript:void(0))\n\n#### Discussion about this post\n\nCommentsRestacks\n\n[Rod](https://substack.com/profile/27495567-rod?utm_source=substack-feed-item)\n\n[Jul 31](https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution/comment/140849637)\n\n\"the future of AI optimization may lie less in brute-force statistical methods and more in endowing our systems with the capacity for self-reflection. \"\n\nCounterpoint: Language allows sufficient dimensionality to escape local minimums when using inefficient but expansive search algorithms like genetic algorithms. I","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiviq.substack.com%2Fp%2Fgepa-reflective-prompt-evolution","id":"https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage discusses GEPA (Genetic-Pareto), a novel algorithm for optimizing prompts in complex AI systems that leverage large language models (LLMs). Unlike traditional reinforcement learning, GEPA employs an evolutionary, language-driven approach called \"reflective prompt mutation,\" where an LLM analyzes its own reasoning, tool usage, and feedback in natural language to diagnose failures and suggest improvements. Guided by a Pareto-based genetic algorithm, this method maintains diverse high-performing prompts, avoiding local optima. Results show GEPA outperforms reinforcement learning methods like GRPO by 10% while requiring up to 35 times fewer system executions, and surpasses existing prompt optimization techniques like MIPROv2. The approach demonstrates that language-based self-reflection is a highly sample-efficient alternative to reward-based learning, making AI system tuning more practical and cost-effective, especially in data-scarce or expensive environments.","title":"GEPA: Reflective Prompt Evolution Can Outperform Reinforcement ...","url":"https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution"},{"content":"Preprint.\nGEPA: REFLECTIVE PROMPT EVOLUTION CAN OUTPERFORM\nREINFORCEMENT LEARNING\nLakshya A Agrawal1, Shangyin Tan1, Dilara Soylu2, Noah Ziems4,\nRishi Khare1, Krista Opsahl-Ong5, Arnav Singhvi2,5, Herumb Shandilya2,\nMichael J Ryan2, Meng Jiang4, Christopher Potts2, Koushik Sen1,\nAlexandros G. Dimakis1,3, Ion Stoica1, Dan Klein1, Matei Zaharia1,5, Omar Khattab6\n1UC Berkeley 2Stanford University 3BespokeLabs.ai 4Notre Dame 5Databricks 6MIT\nABSTRACT\nLarge language models (LLMs) are increasingly adapted to downstream tasks via reinforcement\nlearning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thou\u0002sands of rollouts to learn new tasks. We argue that the interpretable nature of language can often\nprovide a much richer learning medium for LLMs, compared with policy gradients derived from\nsparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that\nthoroughly incorporates natural language reflection to learn high-level rules from trial and error.\nGiven any AI system containing one or more LLM prompts, GEPA samples system-level trajecto\u0002ries (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons from the Pareto\nfrontier of its own attempts. As a result of GEPA’s design, it can often turn even just a few rollouts\ninto a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up\nto 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time\nsearch strategy for code optimization.\n1 INTRODUCTION\n0 5000 10000 15000 20000 25000\nNumber of Rollouts\n42.5\n45.0\n47.5\n50.0\n52.5\n55.0\n57.5\n60.0\n62.5\nScore\nHotpotQA, Qwen3 8B\nOptimization Method\nBaseline\nMIPROv2\nGRPO\nGEPA\nValidation Performance\nTest-set Performance\n(a) HotpotQA, Qwen3 8B\n0 5000 10000 15000 20000 25000\nNumber of Rollouts\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\nScore\nHoVer, Qwen3 8B\nOptimization Method\nBaseline\nMIPROv2\nGRPO\nGEPA\nValidation Performance\nTest-set Performance\n(b) HoVer, Qwen3 8B\nFigure 1: A comparison of the learning behavior of our proposed GEPA prompt optimizer against a state-of-the-art\nprompt optimizer (MIPROv2) and the GRPO (24,000 rollouts) algorithm. As more rollouts are sampled, the prompt\noptimizers can learn much more quickly than GRPO. GEPA substantially outperforms both GRPO and MIPROv2 in\nfinal score. The Test-set star markers demonstrate the performance gap in a held-out set of questions.\nLarge language models (LLMs) have enabled the development of agents and systems that combine fuzzy natural\u0002language behavior specification with tools like retrieval and code execution. These types of systems raise the question\nof how LLMs should be “optimized” for the best downstream performance within their harness. One popular ap\u0002proach for adapting LLMs to downstream tasks is reinforcement learning with verifiable rewards (RLVR), including\n1\narXiv:2507.19457v1 [cs.CL] 25 Jul 2025\nPreprint.\nalgorithms such as Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Such RL methods cast success\nmetrics as a scalar reward observed at the end of each rollout (Lambert, 2025) and use these rewards to estimate\ngradients for policy improvement.\nWhile these RL approaches are effective, they typically require tens of thousands of rollouts in practice to fit new\ntasks. For example, recent works leveraging GRPO across a range of tasks typically use up to hundreds of thousands\nof rollouts for training (Chen et al., 2025b; Wu et al., 2025c; Zhang et al., 2025; Jin et al., 2025; Si et al., 2025; Wang\net al., 2025a; Java et al., 2025; Chen et al., 2025a; Wu et al., 2025a; Sha et al., 2025; Lin et al., 2025; Peng et al.,\n2025; Song et al., 2025). This sample inefficiency can quickly become a serious bottleneck: many downstream LLM\napplications invoke expensive tool calls, have limited inference budget for sampling from the LLM itself, or simply\ncannot finetune the weights of the largest or best-performing LLMs.\nWe observe that the rollouts sampled from even highly sophisticated LLM systems can be serialized into traces of\nnatural (and formal) language, as they contain nothing but the instructions of each LLM module, the resulting LLM\nreasoning chains, tool calls, and potentially the internal workings of the reward function (for example, compiler error\nmessages, before they are collapsed into scalar rewards). Because such serialized trajectories can be readily under\u0002stood by modern LLMs, we argue that algorithms that learn deliberately in natural language by reflecting on these\ntrajectories can potentially make much more effective use of the strong language priors that LLMs have, compared\nwith standard RL approaches.\nTo operationalize this, we introduce GEPA (Genetic-Pareto), a reflective prompt optimizer for compound AI systems\nthat merges textual reflection with multi-objective evolutionary search. GEPA iteratively mutates every prompt within\nthe AI system in light of natural language feedback drawn from new rollouts. In each mutation, the candidate prompt\nis derived from an ancestor, accumulating high-level lessons derived from observations and LLM feedback. To avoid\nthe local optima that afflict greedy prompt updates, GEPA maintains a Pareto front: instead of evolving only the global\nbest prompt, it stochastically explores the top-performing prompts for each problem instance, thereby diversifying\nstrategies and encouraging robust generalization.\nWe evaluate GEPA on four diverse tasks—multi-hop reasoning (HotpotQA; Yang et al. 2018), instruction following\n(IFBench; Pyatkin et al. 2025b), privacy-aware delegation (PUPA; Li et al. 2025a), and retrieval-augmented verifica\u0002tion (HoVer; Jiang et al. 2020)—using both open (Qwen3 8B; Yang et al. 2025; Team 2025) and proprietary (GPT-4.1\nmini; OpenAI 2025) models. Our results show that GEPA demonstrates robust generalization and is highly sample\u0002efficient: on Qwen3 8B, GEPA outperforms GRPO (24,000 rollouts with LoRA) by up to 19% while requiring up to\n35× fewer rollouts. Overall, GEPA achieves an average improvement of +10% over GRPO across all tasks. Further\u0002more, GEPA surpasses the previous state-of-the-art prompt optimizer, MIPROv2 (Opsahl-Ong et al., 2024), on every\nbenchmark and model, obtaining aggregate optimization gains of +14%, more than doubling the gains achieved by\nMIPROv2 (+7%).\nEven qualitatively, GEPA generated prompts can be highly effective. Figure 2 highlights excerpts from a prompt\ncrafted by GEPA for the query creation module of a multi-hop question answering system (Used in HotpotQA). We\nalso find that in most cases, even a single reflective prompt update can give large improvements (as highlighted in the\noptimization trajectory in Figure 5). These results demonstrate that reflective prompt evolution using language feed\u0002back enables substantial sample efficiency and robust generalization, providing a practical path to optimizing complex,\nreal-world AI workflows in data- or budget-constrained environments. Finally, we also show promising preliminary\nresults demonstrating GEPA’s use as an inference-time search strategy for code optimization over NPUEval (Kalade\n& Schelle, 2025) and KernelBench (Ouyang et al., 2025).\n2 PROBLEM STATEMENT\nCompound AI Systems. We follow related work in defining a compound AI system as any modular system com\u0002posed of one or more language model (LLM) invocations, potentially interleaved with external tool calls, orchestrated\nthrough arbitrary control flow. This definition subsumes a broad class of real-world LLM-based AI systems, including\nagents, multi-agent systems, and general-purpose scaffolding techniques like ReAct (Yao et al., 2023), Archon (Saad\u0002Falcon et al., 2025), etc. Following Soylu et al. (2024); Khattab et al. (2024); Opsahl-Ong et al. (2024); Tan et al.\n(2025), we formalize such a system as Φ = (M, C, X , Y), where M = ⟨M1, . . . , M|M|⟩ denotes language modules,\nC specifies control flow logic, and X , Y are global input/output schemas. Each module Mi = (πi, θi, Xi, Yi) is an\nLLM subcomponent: πiis its (system) prompt including instructions and few-shot demonstrations; θithe underly\u0002ing model weights; Xi\n, Yi are input/output schemas. At runtime, C orchestrates the sequencing and invocation of\nmodules—e.g., passing outputs from one module to another, invoking modules conditionally, or leveraging tool APIs.\nThis way, C can invoke different modules in any order multiples of times.\n2\nPreprint.\nSeed Prompt for Second-Hop of Multi-Hop QA System\nGiven the fields question, summary 1, produce the fields query.\nGEPA’s Optimized Prompt for Second-Hop of Multi-Hop QA System, GPT-4.1 Mini\nYou will be given two input fields: question and summary 1. Your task: Generate a new search query (query) optimized for the second hop of a\nmulti-hop retrieval system.\n• The original user question is typically complex and requires information from multiple documents to answer.\n• The first hop query is the original question (used to retrieve initial documents).\n• Your goal: generate a query to retrieve documents not found in first hop but necessary to answer the question completely.\nInput Understanding: question is the original multi-hop question posed by the user. summary 1 is a concise summary of information from a document\nretrieved in the first hop, which partially addresses the question.\nPurpose and Context:\n• Your generated query aims to find the missing pieces of information needed to fully answer the question. . . .\n• The query must retrieve relevant documents NOT found in first hop . . . for final answer extraction.\nKey Observations and Lessons:\n• First-hop documents often cover one entity or aspect.\n• Remaining relevant documents often involve connected or higher-level concepts mentioned in summary 1 but not explicitly asked in","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2507.19457","id":"https://arxiv.org/pdf/2507.19457","publishedDate":"2025-07-25T00:00:00.000Z","summary":"The paper introduces GEPA (Genetic-Pareto), a novel prompt optimization method that leverages natural language reflection to improve large language models (LLMs) more efficiently than traditional reinforcement learning (RL) approaches like GRPO. Unlike RL methods that rely on sparse scalar rewards and require thousands of rollouts, GEPA uses natural language to analyze system trajectories (e.g., reasoning steps, tool calls), diagnose issues, and iteratively refine prompts. This reflection-based approach enables GEPA to achieve higher performance—outperforming GRPO by up to 20% with significantly fewer rollouts (up to 35 times fewer). It also surpasses existing prompt optimizers like MIPROv2 across multiple tasks and models, demonstrating improved sample efficiency and effectiveness in downstream tasks such as code optimization. Overall, GEPA highlights the potential of language-based reflection as a powerful alternative to traditional reinforcement learning for optimizing LLMs.","title":"[PDF] GEPA: Reflective Prompt Evolution Can Outperform Reinforcement ...","url":"https://arxiv.org/pdf/2507.19457"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F394049542_GEPA_Reflective_Prompt_Evolution_Can_Outperform_Reinforcement_Learning","id":"https://www.researchgate.net/publication/394049542_GEPA_Reflective_Prompt_Evolution_Can_Outperform_Reinforcement_Learning","publishedDate":"2025-07-30T00:00:00.000Z","summary":"Failed to fetch content","title":"Reflective Prompt Evolution Can Outperform Reinforcement Learning","url":"https://www.researchgate.net/publication/394049542_GEPA_Reflective_Prompt_Evolution_Can_Outperform_Reinforcement_Learning"}],"status":"completed","toolCallId":"call_yHgGuCKznQXFng0RNOdQZ3HN","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Here’s the GEPA workflow pipeline end-to-end. Think of it as a tight loop that turns rich execution traces into targeted prompt edits, evaluates them on multiple goals, and keeps a diverse set of winners.\n\nInputs\n- System spec: a compound AI system Φ with modules M1..Mk (each has a prompt), control flow C, and any tools/APIs.\n- Data: a training/validation slice of tasks.\n- Evaluators/objectives: accuracy/quality, cost/latency, safety/constraints, stability, etc.\n- Logging policy: what to record from rollouts (prompts, thoughts if allowed, tool I/O, evaluator rationales, error logs).\n\nPipeline loop\n\n1) Initialize a population of candidates\n- Start with one or a few seed system specs (the current prompts for each module).\n- Population = multiple prompt sets to encourage diverse exploration.\n\n2) Run instrumented rollouts\n- Execute each candidate on a batch of inputs.\n- Capture full traces:\n  - Inputs and per-module prompts shown\n  - Intermediate reasoning/notes (if permissible)\n  - Tool calls and responses (APIs, retriever docs, compiler/unit-test logs)\n  - Partial checks and final rewards/scores\n- Purpose: gather a textual, high-signal account of what happened, not just a scalar score.\n\n3) Reflect in natural language\n- A “reflector” LLM reads the traces and answers:\n  - What failed/succeeded and why?\n  - Which module behavior caused it?\n  - What general rule or constraint would fix this class of errors?\n  - What tradeoffs are at play (recall vs precision, speed vs accuracy, safety vs coverage)?\n- Output: a set of named lessons, each with:\n  - Target module(s)\n  - Rationale linked to concrete failure examples\n  - Proposed prompt edits (instruction changes, checklists, examples, tool schema clarifications)\n  - Optional confidence/priority tags\n\n4) Generate prompt mutations and recombinations\n- For each lesson, produce one or more prompt edits to the relevant module(s).\n- Build multiple candidate systems by:\n  - Applying single edits\n  - Combining compatible edits across modules (“recombination”)\n  - Keeping edit granularity small to aid attribution\n- Result: a new batch/population of mutated candidates.\n\n5) Multi-objective evaluation\n- Run the mutated candidates on a validation slice.\n- Score on several axes:\n  - Task metrics (EM/F1, pass@k, success rate)\n  - Cost/latency\n  - Safety/constraint adherence\n  - Stability/variance across seeds or inputs\n- Optionally collect evaluator rationales to feed the next reflection step.\n\n6) Pareto-front selection\n- Keep only non-dominated candidates (no other candidate is strictly better on all objectives).\n- This preserves diverse strategies that excel on different tradeoffs or subsets of the data.\n- Archive or tag which lessons/edits each surviving candidate embodies.\n\n7) Iterate\n- Use the Pareto survivors as the next generation’s parents.\n- Repeat steps 2–6 for a few rounds until:\n  - Gains plateau on held-out data\n  - Budget is exhausted\n  - A target performance/cost threshold is met\n\n8) Finalize and deploy\n- Choose one or a small set of Pareto candidates:\n  - Primary: best balance for production\n  - Alternates: specialized prompts for certain input clusters or stricter safety modes\n- Record an audit trail:\n  - Which lessons led to which prompt lines\n  - The validation evidence supporting them\n\nKey design choices that make the pipeline work\n- Work at the module level: edits are tied to the prompt that controls the failing behavior, improving interpretability and transfer.\n- Use language feedback, not just numbers: reflections convert traces into reusable rules, giving high leverage per rollout.\n- Maintain diversity via Pareto selection: avoids collapsing onto a single brittle tactic and helps generalization.\n- Small, justified edits: eases ablation, rollback, and recombination.\n- Rich logging: the more informative the traces and evaluator messages, the better the reflections and edits.\n\nMinimal pseudo-code sketch\n\n- High-level\n  - Population ← {seed_prompts}\n  - for gen in 1..G:\n    - Traces ← run_rollouts(Population, train_batch, log_all=True)\n    - Lessons ← reflect(Traces)\n    - Candidates ← mutate_and_recombine(Population, Lessons)\n    - Scores ← evaluate(Candidates, val_batch, metrics=[acc,cost,safety,stability])\n    - Population ← pareto_front(Candidates, Scores)\n  - Return select_for_deploy(Population, policy=balanced)\n\n- Mutation examples\n  - Add checklist section to Planner prompt\n  - Clarify parameter schema for Tool-Caller (types, ranges, error handling)\n  - Add “second-hop missing-entity” rule to Retriever-Query prompt\n  - Insert preflight validation step instruction in Finalizer\n\nStopping criteria and practical tips\n- Stop when validation gains flatten across two generations or when cost/latency increases without accuracy gains.\n- Freeze especially strong edits into the seed after they prove robust, then continue exploring other axes.\n- Keep a small held-out test set untouched until the end.\n- Track cost per point of improvement to decide when to stop.\n\nIf you want, tell me your task and modules, and I’ll draft:\n- A logging schema for traces\n- A first reflection meta-prompt\n- A mutation plan and Pareto metrics tailored to your objectives","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_00dbb4010a1be55d0068caeeb8e84c8190b4d6073102244512"}},"resumableStreamId":"STREAM:037cc5aa-cb33-47da-9ad0-dd6142fea67c","role":"assistant","status":"done","threadId":"5e763140-e699-40d6-970e-751838959582","timeToFirstToken":1.174,"tokens":1144,"tokensPerSecond":29.310786574429926,"updated_at":1758129839320,"userId":"google:104458561188320708185","id":"j97f6hpvs2xg048vghj8sctj217qsxvm"},{"_creationTime":1758130084495.4844,"_id":"j975d49kw42rb925xz5tgm326s7qrrrs","attachmentIds":[],"content":"how did GEPA perform?","created_at":1758130084495,"messageId":"dc19e51b-7df6-4ea7-9f7a-6fd78ff020ec","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"5e763140-e699-40d6-970e-751838959582","updated_at":1758130084495,"userId":"google:104458561188320708185","id":"j975d49kw42rb925xz5tgm326s7qrrrs","providerMetadata":{}},{"_creationTime":1758130084495.4846,"_id":"j9710n3qgwzwbnnb0mx7bdctyx7qs37h","attachmentIds":[],"byok":false,"content":"Here’s the performance picture the paper reports, distilled and specific.\n\nHeadline results\n- Versus RL (GRPO):\n  - Average gain: +10% across tasks\n  - Best-case gain: up to +19–20%\n  - Sample efficiency: up to 35× fewer rollouts than GRPO (which used 24k rollouts, often with LoRA)\n- Versus prompt optimization SOTA (MIPROv2):\n  - Average gain: >10% across two LLMs\n  - Aggregate optimization gain: GEPA ≈ +14% vs MIPROv2 ≈ +7% (i.e., roughly double the net improvement)\n  - Prompts tended to be shorter (reported ~33% shorter in a figure), lowering inference cost\n\nBenchmarks and models\n- Tasks: HotpotQA (multi-hop QA), HoVer (retrieval-augmented verification), IFBench (instruction following), PUPA (privacy-aware delegation)\n- Base models: Qwen3 8B (open) and GPT-4.1 mini (proprietary)\n- Configuration: GEPA optimizes module prompts in compound systems; GRPO baselines used 24k rollouts; MIPROv2 as a strong prompt-optimizer baseline\n\nLearning curves (qualitative takeaways from figures)\n- GEPA learns much faster with fewer samples; its curve overtakes GRPO early\n- On HotpotQA and HoVer with Qwen3 8B, GEPA’s final validation/test scores are clearly above both GRPO and MIPROv2\n- Even a single reflective update often yields a large jump; subsequent iterations refine further\n\nGeneralization and robustness\n- Multi-objective selection (accuracy, cost/latency, safety, stability) maintains diverse, high-performing prompt variants, improving robustness vs greedy prompt search\n- Improvements hold on held-out test sets (star markers in the figures), not just validation slices\n\nAblations / qualitative findings\n- Instruction-focused edits carried most of the gains; in places, instruction-only optimization beat methods that jointly tune instructions and few-shot examples\n- GEPA’s edits are interpretable “rules” tied to concrete failure modes (e.g., second-hop retrieval guidance), which transfer across many inputs\n\nBeyond benchmarks\n- Code optimization: As an inference-time reflective search loop, GEPA showed promising results on NPUEval and KernelBench by iteratively refining code with compiler/runtime feedback\n\nWhat to take away\n- If you have a compound LLM system and can log rich traces, GEPA tends to beat RL-style GRPO in both sample efficiency and final quality, and also outperforms leading prompt optimizers, while keeping prompts shorter and more interpretable.","created_at":1758130084496,"messageId":"14d4b897-0ece-4b36-8676-07080cae4ef3","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":null,"queries":["GEPA Reflective Prompt Evolution Can Outperform Reinforcement Learning arXiv 2507.19457 results performance benchmarks HotpotQA HoVer IFBench PUPA Qwen3 8B GPT-4.1 mini"]},"result":[{"content":"# Computer Science > Computation and Language\n\n**arXiv:2507.19457** (cs)\n\n\\[Submitted on 25 Jul 2025\\]\n\n# Title:GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\nAuthors: [Lakshya A Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal,+L+A), [Shangyin Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan,+S), [Dilara Soylu](https://arxiv.org/search/cs?searchtype=author&query=Soylu,+D), [Noah Ziems](https://arxiv.org/search/cs?searchtype=author&query=Ziems,+N), [Rishi Khare](https://arxiv.org/search/cs?searchtype=author&query=Khare,+R), [Krista Opsahl-Ong](https://arxiv.org/search/cs?searchtype=author&query=Opsahl-Ong,+K), [Arnav Singhvi](https://arxiv.org/search/cs?searchtype=author&query=Singhvi,+A), [Herumb Shandilya](https://arxiv.org/search/cs?searchtype=author&query=Shandilya,+H), [Michael J Ryan](https://arxiv.org/search/cs?searchtype=author&query=Ryan,+M+J), [Meng Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang,+M), [Christopher Potts](https://arxiv.org/search/cs?searchtype=author&query=Potts,+C), [Koushik Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen,+K), [Alexandros G. Dimakis](https://arxiv.org/search/cs?searchtype=author&query=Dimakis,+A+G), [Ion Stoica](https://arxiv.org/search/cs?searchtype=author&query=Stoica,+I), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein,+D), [Matei Zaharia](https://arxiv.org/search/cs?searchtype=author&query=Zaharia,+M), [Omar Khattab](https://arxiv.org/search/cs?searchtype=author&query=Khattab,+O)\n\nView a PDF of the paper titled GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning, by Lakshya A Agrawal and 16 other authors\n\n[View PDF](https://arxiv.org/pdf/2507.19457)\n\n> Abstract:Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.\n\n| | |\n| --- | --- |\n| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE) |\n| ACM classes: | I.2.7; I.2.6; I.2.4; I.2.8 |\n| Cite as: | [arXiv:2507.19457](https://arxiv.org/abs/2507.19457) \\[cs.CL\\] |\n| (or [arXiv:2507.19457v1](https://arxiv.org/abs/2507.19457v1) \\[cs.CL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2507.19457](https://doi.org/10.48550/arXiv.2507.19457) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Lakshya A Agrawal \\[ [view email](https://arxiv.org/show-email/af5f91b3/2507.19457)\\] **\\[v1\\]**\nFri, 25 Jul 2025 17:42:32 UTC (1,632 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning, by Lakshya A Agrawal and 16 other authors\n\n- [View PDF](https://arxiv.org/pdf/2507.19457)\n- [TeX Source](https://arxiv.org/src/2507.19457)\n- [Other Formats](https://arxiv.org/format/2507.19457)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[< prev](https://arxiv.org/prevnext?id=2507.19457&function=prev&context=cs.CL)  \\|  [next >](https://arxiv.org/prevnext?id=2507.19457&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2025-07](https://arxiv.org/list/cs.CL/2025-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2507.19457?context=cs) [cs.AI](https://arxiv.org/abs/2507.19457?context=cs.AI) [cs.LG](https://arxiv.org/abs/2507.19457?context=cs.LG) [cs.SE](https://arxiv.org/abs/2507.19457?context=cs.SE)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2507.19457)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2507.19457)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2507.19457)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2507.19457) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.19457","id":"https://arxiv.org/abs/2507.19457","publishedDate":"2025-07-25T00:00:00.000Z","summary":"The paper introduces GEPA (Genetic-Pareto), a novel prompt optimization method for large language models (LLMs) that leverages natural language reflection rather than traditional reinforcement learning (RL). Unlike RL approaches like GRPO, which require extensive rollouts, GEPA uses natural language reflection to diagnose issues, propose prompt improvements, and learn high-level rules through trial and error. This approach enables GEPA to achieve significant performance gains—outperforming RL-based methods by up to 20% while using up to 35 times fewer rollouts across various tasks. It also surpasses existing prompt optimizers like MIPROv2 by over 10%. Overall, GEPA demonstrates that reflective prompt evolution can be a more efficient and effective alternative to reinforcement learning for optimizing LLM prompts, with promising applications in code optimization and inference-time search strategies.","title":"GEPA: Reflective Prompt Evolution Can Outperform Reinforcement ...","url":"https://arxiv.org/abs/2507.19457"},{"content":"Preprint.\nGEPA: REFLECTIVE PROMPT EVOLUTION CAN OUTPERFORM\nREINFORCEMENT LEARNING\nLakshya A Agrawal1, Shangyin Tan1, Dilara Soylu2, Noah Ziems4,\nRishi Khare1, Krista Opsahl-Ong5, Arnav Singhvi2,5, Herumb Shandilya2,\nMichael J Ryan2, Meng Jiang4, Christopher Potts2, Koushik Sen1,\nAlexandros G. Dimakis1,3, Ion Stoica1, Dan Klein1, Matei Zaharia1,5, Omar Khattab6\n1UC Berkeley 2Stanford University 3BespokeLabs.ai 4Notre Dame 5Databricks 6MIT\nABSTRACT\nLarge language models (LLMs) are increasingly adapted to downstream tasks via reinforcement\nlearning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thou\u0002sands of rollouts to learn new tasks. We argue that the interpretable nature of language can often\nprovide a much richer learning medium for LLMs, compared with policy gradients derived from\nsparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that\nthoroughly incorporates natural language reflection to learn high-level rules from trial and error.\nGiven any AI system containing one or more LLM prompts, GEPA samples system-level trajecto\u0002ries (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons from the Pareto\nfrontier of its own attempts. As a result of GEPA’s design, it can often turn even just a few rollouts\ninto a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up\nto 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time\nsearch strategy for code optimization.\n1 INTRODUCTION\n0 5000 10000 15000 20000 25000\nNumber of Rollouts\n42.5\n45.0\n47.5\n50.0\n52.5\n55.0\n57.5\n60.0\n62.5\nScore\nHotpotQA, Qwen3 8B\nOptimization Method\nBaseline\nMIPROv2\nGRPO\nGEPA\nValidation Performance\nTest-set Performance\n(a) HotpotQA, Qwen3 8B\n0 5000 10000 15000 20000 25000\nNumber of Rollouts\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\nScore\nHoVer, Qwen3 8B\nOptimization Method\nBaseline\nMIPROv2\nGRPO\nGEPA\nValidation Performance\nTest-set Performance\n(b) HoVer, Qwen3 8B\nFigure 1: A comparison of the learning behavior of our proposed GEPA prompt optimizer against a state-of-the-art\nprompt optimizer (MIPROv2) and the GRPO (24,000 rollouts) algorithm. As more rollouts are sampled, the prompt\noptimizers can learn much more quickly than GRPO. GEPA substantially outperforms both GRPO and MIPROv2 in\nfinal score. The Test-set star markers demonstrate the performance gap in a held-out set of questions.\nLarge language models (LLMs) have enabled the development of agents and systems that combine fuzzy natural\u0002language behavior specification with tools like retrieval and code execution. These types of systems raise the question\nof how LLMs should be “optimized” for the best downstream performance within their harness. One popular ap\u0002proach for adapting LLMs to downstream tasks is reinforcement learning with verifiable rewards (RLVR), including\n1\narXiv:2507.19457v1 [cs.CL] 25 Jul 2025\nPreprint.\nalgorithms such as Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Such RL methods cast success\nmetrics as a scalar reward observed at the end of each rollout (Lambert, 2025) and use these rewards to estimate\ngradients for policy improvement.\nWhile these RL approaches are effective, they typically require tens of thousands of rollouts in practice to fit new\ntasks. For example, recent works leveraging GRPO across a range of tasks typically use up to hundreds of thousands\nof rollouts for training (Chen et al., 2025b; Wu et al., 2025c; Zhang et al., 2025; Jin et al., 2025; Si et al., 2025; Wang\net al., 2025a; Java et al., 2025; Chen et al., 2025a; Wu et al., 2025a; Sha et al., 2025; Lin et al., 2025; Peng et al.,\n2025; Song et al., 2025). This sample inefficiency can quickly become a serious bottleneck: many downstream LLM\napplications invoke expensive tool calls, have limited inference budget for sampling from the LLM itself, or simply\ncannot finetune the weights of the largest or best-performing LLMs.\nWe observe that the rollouts sampled from even highly sophisticated LLM systems can be serialized into traces of\nnatural (and formal) language, as they contain nothing but the instructions of each LLM module, the resulting LLM\nreasoning chains, tool calls, and potentially the internal workings of the reward function (for example, compiler error\nmessages, before they are collapsed into scalar rewards). Because such serialized trajectories can be readily under\u0002stood by modern LLMs, we argue that algorithms that learn deliberately in natural language by reflecting on these\ntrajectories can potentially make much more effective use of the strong language priors that LLMs have, compared\nwith standard RL approaches.\nTo operationalize this, we introduce GEPA (Genetic-Pareto), a reflective prompt optimizer for compound AI systems\nthat merges textual reflection with multi-objective evolutionary search. GEPA iteratively mutates every prompt within\nthe AI system in light of natural language feedback drawn from new rollouts. In each mutation, the candidate prompt\nis derived from an ancestor, accumulating high-level lessons derived from observations and LLM feedback. To avoid\nthe local optima that afflict greedy prompt updates, GEPA maintains a Pareto front: instead of evolving only the global\nbest prompt, it stochastically explores the top-performing prompts for each problem instance, thereby diversifying\nstrategies and encouraging robust generalization.\nWe evaluate GEPA on four diverse tasks—multi-hop reasoning (HotpotQA; Yang et al. 2018), instruction following\n(IFBench; Pyatkin et al. 2025b), privacy-aware delegation (PUPA; Li et al. 2025a), and retrieval-augmented verifica\u0002tion (HoVer; Jiang et al. 2020)—using both open (Qwen3 8B; Yang et al. 2025; Team 2025) and proprietary (GPT-4.1\nmini; OpenAI 2025) models. Our results show that GEPA demonstrates robust generalization and is highly sample\u0002efficient: on Qwen3 8B, GEPA outperforms GRPO (24,000 rollouts with LoRA) by up to 19% while requiring up to\n35× fewer rollouts. Overall, GEPA achieves an average improvement of +10% over GRPO across all tasks. Further\u0002more, GEPA surpasses the previous state-of-the-art prompt optimizer, MIPROv2 (Opsahl-Ong et al., 2024), on every\nbenchmark and model, obtaining aggregate optimization gains of +14%, more than doubling the gains achieved by\nMIPROv2 (+7%).\nEven qualitatively, GEPA generated prompts can be highly effective. Figure 2 highlights excerpts from a prompt\ncrafted by GEPA for the query creation module of a multi-hop question answering system (Used in HotpotQA). We\nalso find that in most cases, even a single reflective prompt update can give large improvements (as highlighted in the\noptimization trajectory in Figure 5). These results demonstrate that reflective prompt evolution using language feed\u0002back enables substantial sample efficiency and robust generalization, providing a practical path to optimizing complex,\nreal-world AI workflows in data- or budget-constrained environments. Finally, we also show promising preliminary\nresults demonstrating GEPA’s use as an inference-time search strategy for code optimization over NPUEval (Kalade\n& Schelle, 2025) and KernelBench (Ouyang et al., 2025).\n2 PROBLEM STATEMENT\nCompound AI Systems. We follow related work in defining a compound AI system as any modular system com\u0002posed of one or more language model (LLM) invocations, potentially interleaved with external tool calls, orchestrated\nthrough arbitrary control flow. This definition subsumes a broad class of real-world LLM-based AI systems, including\nagents, multi-agent systems, and general-purpose scaffolding techniques like ReAct (Yao et al., 2023), Archon (Saad\u0002Falcon et al., 2025), etc. Following Soylu et al. (2024); Khattab et al. (2024); Opsahl-Ong et al. (2024); Tan et al.\n(2025), we formalize such a system as Φ = (M, C, X , Y), where M = ⟨M1, . . . , M|M|⟩ denotes language modules,\nC specifies control flow logic, and X , Y are global input/output schemas. Each module Mi = (πi, θi, Xi, Yi) is an\nLLM subcomponent: πiis its (system) prompt including instructions and few-shot demonstrations; θithe underly\u0002ing model weights; Xi\n, Yi are input/output schemas. At runtime, C orchestrates the sequencing and invocation of\nmodules—e.g., passing outputs from one module to another, invoking modules conditionally, or leveraging tool APIs.\nThis way, C can invoke different modules in any order multiples of times.\n2\nPreprint.\nSeed Prompt for Second-Hop of Multi-Hop QA System\nGiven the fields question, summary 1, produce the fields query.\nGEPA’s Optimized Prompt for Second-Hop of Multi-Hop QA System, GPT-4.1 Mini\nYou will be given two input fields: question and summary 1. Your task: Generate a new search query (query) optimized for the second hop of a\nmulti-hop retrieval system.\n• The original user question is typically complex and requires information from multiple documents to answer.\n• The first hop query is the original question (used to retrieve initial documents).\n• Your goal: generate a query to retrieve documents not found in first hop but necessary to answer the question completely.\nInput Understanding: question is the original multi-hop question posed by the user. summary 1 is a concise summary of information from a document\nretrieved in the first hop, which partially addresses the question.\nPurpose and Context:\n• Your generated query aims to find the missing pieces of information needed to fully answer the question. . . .\n• The query must retrieve relevant documents NOT found in first hop . . . for final answer extraction.\nKey Observations and Lessons:\n• First-hop documents often cover one entity or aspect.\n• Remaining relevant documents often involve connected or higher-level concepts mentioned in summary 1 but not explicitly asked in","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2507.19457","id":"https://arxiv.org/pdf/2507.19457","publishedDate":"2025-07-25T00:00:00.000Z","summary":"The paper introduces GEPA (Genetic-Pareto), a novel prompt optimization method that leverages natural language reflection to improve large language models (LLMs) more efficiently than traditional reinforcement learning (RL) approaches like GRPO. Unlike RL methods that rely on sparse scalar rewards and require thousands of rollouts, GEPA uses natural language to analyze system trajectories (e.g., reasoning steps, tool calls) and iteratively refine prompts by diagnosing issues, proposing updates, and combining lessons from its own attempts. Experimental results across four tasks show that GEPA outperforms GRPO by up to 20% while using up to 35 times fewer rollouts, demonstrating higher sample efficiency and better performance. It also surpasses the leading prompt optimizer MIPROv2 by over 10% on two LLMs and shows promise as an inference-time strategy for code optimization. Overall, GEPA highlights the power of reflective, language-based prompt evolution to enhance LLM capabilities more efficiently than traditional RL-based methods.","title":"[PDF] GEPA: Reflective Prompt Evolution Can Outperform Reinforcement ...","url":"https://arxiv.org/pdf/2507.19457"},{"content":"# [ArXivIQ](https://arxiviq.substack.com/)\n\nSubscribeSign in\n\n# GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\nJul 31, 2025\n\n3\n\n[2](https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution/comments)\n\n[Share](javascript:void(0))\n\n**Authors:** _Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab_ **Paper:** [https://arxiv.org/abs/2507.19457](https://arxiv.org/abs/2507.19457)\n\n## TL;DR\n\n**What was done?** The authors introduced GEPA (Genetic-Pareto), a novel algorithm for optimizing prompts in complex, multi-module AI systems. Instead of relying on traditional reinforcement learning (RL), GEPA employs a language-driven, evolutionary approach. Its core innovation is \"reflective prompt mutation,\" where an LLM analyzes its own performance—including reasoning steps, tool usage, and detailed evaluation feedback—in natural language to diagnose failures and propose targeted improvements to its instructional prompts. This process is guided by a genetic algorithm that uses Pareto selection to maintain a diverse set of high-performing prompts, preventing the optimizer from getting stuck in local optima.\n\n**Why it matters?** This work signals a potential paradigm shift in how we optimize LLM-based agents. GEPA demonstrates that learning through language-based self-reflection is dramatically more sample-efficient than learning from sparse, scalar rewards. It outperforms the RL method GRPO by an average of 10% while using up to 35x fewer \"rollouts\" (system executions). It also surpasses the state-of-the-art prompt optimizer MIPROv2 ( [https://aclanthology.org/2024.emnlp-main.525/](https://aclanthology.org/2024.emnlp-main.525/)), and surprisingly shows that evolving detailed instructions alone can be more effective than optimizing few-shot examples. This approach makes adapting powerful AI systems far more practical and affordable, especially in settings where data is scarce or system executions are expensive.\n\n## Details\n\n### The High Cost of Learning by Doing\n\nOptimizing the performance of sophisticated AI agents—systems that combine multiple LLM modules, tool calls, and complex logic—is a central challenge in modern AI. A popular approach has been reinforcement learning (RL), where an agent learns through trial and error, guided by a scalar reward signal. However, this method often proves to be a brute-force endeavor, requiring tens or even hundreds of thousands of system executions (\"rollouts\") to achieve meaningful improvements. This high sample cost is a major bottleneck, making RL impractical for many real-world applications where each rollout may be computationally expensive, time-consuming, or financially costly.\n\nA new paper from a large collaboration of researchers across UC Berkeley, Stanford, Databricks, and MIT challenges this paradigm. The authors argue that for systems built on Large Language Models (LLMs), the very language they process offers a far richer and more efficient learning medium than a simple numerical reward. Their proposed algorithm, GEPA (Genetic-Pareto), demonstrates that an AI system can learn more effectively by \"reflecting\" on its behavior in natural language, leading to a method that is not only more powerful but also vastly more efficient.\n\nArXivIQ is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\n\nSubscribe\n\n### GEPA: Learning by Reflective Evolution\n\nGEPA introduces a novel optimization framework built on three key principles: Genetic Prompt Evolution, Natural Language Reflection, and Pareto-based Candidate Selection (Figure 3).\n\n1. **Natural Language Reflection:** At the heart of GEPA is its ability to learn from detailed, textual feedback. Instead of just receiving a score, the system's entire execution trace is fed back to an LLM for analysis. This feedback isn't just a number; it's a rich, textual record of the entire process, including the LLM's own reasoning chains, the specific tool calls it made, and even detailed diagnostic information from the evaluation environment, like compiler error messages or failed unit tests. This \"reflector\" LLM analyzes the textual feedback to diagnose what went wrong (or right) and proposes specific, targeted edits to the system's instructional prompts.\n\nAn example of a GEPA-generated prompt (Figure 2) reveals a level of detail and strategic insight far beyond a simple instruction, including sections on \"Key Observations,\" \"Purpose and Context,\" and \"Practical Strategy.\"\n\n2. **Genetic-Pareto Optimization:** To guide this learning process, GEPA employs an evolutionary search strategy. It maintains a pool of candidate prompt sets and iteratively \"mutates\" them based on the insights from the reflection step. To avoid the common pitfall of getting stuck in a local optimum, GEPA uses a strategy akin to an \"illumination\" search, known as Pareto-based selection (Algorithm 2).\n\nInstead of just greedily climbing the single highest peak it can find, GEPA tries to illuminate the entire mountain range by identifying candidates that are the best for at least one specific problem instance. This encourages diverse strategies and ultimately leads to a more robust and general solution. The impact of this choice is starkly visualized in the paper, showing how Pareto selection leads to a balanced, exploratory search tree compared to the narrow, stalled search of a naive greedy approach (Figure 6).\n\n\n### A New Benchmark for Sample Efficiency and Performance\n\nThe experimental results are compelling, showcasing GEPA's strengths across four diverse tasks— [HotpotQA](https://arxiv.org/abs/1809.09600), IFBench, [HoVer](https://aclanthology.org/2020.findings-emnlp.309/), and [PUPA](https://aclanthology.org/2025.naacl-long.173/) — and two different LLMs (Qwen3 8B and GPT-4.1 Mini).\n\n- **Outperforming Reinforcement Learning:** GEPA outperforms the RL baseline GRPO by an average of 10%, with gains of up to 19% on specific tasks. Most impressively, it achieves this while using up to **35 times fewer rollouts** (Table 1).\n\nThe learning curves (Figure 1) clearly show GEPA reaching superior performance far more rapidly than GRPO.\n\n- **Surpassing State-of-the-Art Prompt Optimization:** GEPA also consistently outperforms MIPROv2, a leading joint instruction and few-shot prompt optimizer. It more than doubles the aggregate performance gains seen with MIPROv2 and achieves this with prompts that are, on average, 33% shorter (Figure 15), translating directly to lower inference costs.\n\n- **The Power of Instructions:** A particularly surprising finding is that GEPA's instruction-only optimization outperforms MIPROv2's joint optimization of both instructions and few-shot examples. This suggests that as LLMs become better at following complex instructions, evolving a detailed, reflective set of instructions may be a more powerful and efficient strategy than curating in-context examples.\n\n\nThe paper also presents promising preliminary results for using GEPA as an inference-time search strategy for highly technical domains like code optimization. When applied to generating CUDA and NPU kernels, GEPA was able to iteratively refine code based on compiler feedback to achieve significant performance improvements over strong baselines (Figure 7, 8).\n\n### Limitations and Future Directions\n\nThe authors are candid about the method's limitations. The boundary between prompt-based learning and traditional weight-based fine-tuning remains an open question; in data-abundant regimes, full fine-tuning may still have the upper hand. The paper also suggests that GEPA could be further improved by incorporating few-shot example optimization or by developing more sophisticated \"feedback engineering\" to extract the most valuable learning signals from system traces.\n\nA key avenue for future work is the integration of reflective prompt evolution with weight-space adaptation. A hybrid approach, where GEPA's language-based insights guide more efficient RL or fine-tuning rollouts, could unify these paradigms and lead to even greater performance and efficiency.\n\n### Conclusion\n\n\"GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\" presents a significant and practical advance in the field of AI system optimization. By treating language not just as an interface but as the primary medium for learning and reflection, the authors have developed a method that is demonstrably more sample-efficient, performant, and cost-effective than existing approaches.\n\nUltimately, GEPA's success suggests that the future of AI optimization may lie less in brute-force statistical methods and more in endowing our systems with the capacity for self-reflection. By learning from language, not just from numbers, GEPA is a significant step towards creating AI that improves not just by doing, but by understanding. This paper is a valuable contribution and a recommended read for anyone working on the frontier of LLM-based systems and agentic AI.\n\nArXivIQ is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\n\nSubscribe\n\n3\n\n[2](https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution/comments)\n\n[Share](javascript:void(0))\n\n#### Discussion about this post\n\nCommentsRestacks\n\n[Rod](https://substack.com/profile/27495567-rod?utm_source=substack-feed-item)\n\n[Jul 31](https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution/comment/140849637)\n\n\"the future of AI optimization may lie less in brute-force statistical methods and more in endowing our systems with the capacity for self-reflection. \"\n\nCounterpoint: Language allows sufficient dimensionality to escape local minimums when using inefficient but expansive search algorithms like genetic algorithms. I","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiviq.substack.com%2Fp%2Fgepa-reflective-prompt-evolution","id":"https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage discusses GEPA (Genetic-Pareto), a novel algorithm for optimizing prompts in complex AI systems that leverage large language models (LLMs). Unlike traditional reinforcement learning, GEPA employs an evolutionary, language-driven approach called \"reflective prompt mutation,\" where an LLM analyzes its own reasoning, tool use, and feedback in natural language to diagnose failures and improve prompts. Guided by a genetic algorithm with Pareto selection, GEPA maintains diverse high-performing prompts, avoiding local optima. The method significantly outperforms reinforcement learning (e.g., GRPO) by being more sample-efficient—requiring up to 35 times fewer system executions—and surpasses existing prompt optimization techniques like MIPROv2. This approach demonstrates that evolving detailed instructions through language-based self-reflection can be more effective than optimizing few-shot examples, making AI system adaptation more practical and cost-effective, especially in resource-constrained settings.","title":"GEPA: Reflective Prompt Evolution Can Outperform Reinforcement ...","url":"https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution"},{"content":"[Papers](https://huggingface.co/papers)\n\narxiv:2507.19457\n\n# GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\nPublished on Jul 25\n\n·Submitted by[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)on Jul 28\n\n[#3 Paper of the day](https://huggingface.co/papers?date=2025-07-28)\n\n[Upvote\\\n\\\n27](https://huggingface.co/login?next=%2Fpapers%2F2507.19457)\n\n- +19\n\nAuthors:\n\n[Lakshya A Agrawal](https://huggingface.co/LakshyAAAgrawal)\n,\n\nShangyin Tan\n\n,\n\nDilara Soylu\n\n,\n\nNoah Ziems\n\n,\n\nRishi Khare\n\n,\n\nKrista Opsahl-Ong\n\n,\n\nArnav Singhvi\n\n,\n\nHerumb Shandilya\n\n,\n\nMichael J Ryan\n\n,\n\nMeng Jiang\n\n,\n\nChristopher Potts\n\n,\n\nKoushik Sen\n\n,\n\nAlexandros G. Dimakis\n\n,\n\nIon Stoica\n\n,\n\nDan Klein\n\n,\n\nMatei Zaharia\n\n,\n\nOmar Khattab\n\n## Abstract\n\nGEPA, a prompt optimizer using natural language reflection, outperforms RL methods like GRPO and MIPROv2 with fewer rollouts by learning high-level rules from trial and error.\n\nAI-generated summary\n\n[Large language models](https://huggingface.co/papers?q=Large%20language%20models) (LLMs) are increasingly adapted to downstream tasks via\n[reinforcement learning](https://huggingface.co/papers?q=reinforcement%20learning) (RL) methods like [Group Relative Policy Optimization](https://huggingface.co/papers?q=Group%20Relative%20Policy%20Optimization)\n( [GRPO](https://huggingface.co/papers?q=GRPO)), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce [GEPA](https://huggingface.co/papers?q=GEPA) ( [Genetic-Pareto](https://huggingface.co/papers?q=Genetic-Pareto)), a prompt\noptimizer that thoroughly incorporates [natural language reflection](https://huggingface.co/papers?q=natural%20language%20reflection) to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, [GEPA](https://huggingface.co/papers?q=GEPA) samples [system-level trajectories](https://huggingface.co/papers?q=system-level%20trajectories) (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the [Pareto frontier](https://huggingface.co/papers?q=Pareto%20frontier) of its own attempts. As a result of [GEPA](https://huggingface.co/papers?q=GEPA)'s design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, [GEPA](https://huggingface.co/papers?q=GEPA) outperforms [GRPO](https://huggingface.co/papers?q=GRPO) by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. [GEPA](https://huggingface.co/papers?q=GEPA) also outperforms the leading [prompt optimizer](https://huggingface.co/papers?q=prompt%20optimizer),\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for [code optimization](https://huggingface.co/papers?q=code%20optimization).\n\n[View arXiv page](https://arxiv.org/abs/2507.19457) [View PDF](https://arxiv.org/pdf/2507.19457) [Add to collection](https://huggingface.co/login?next=%2Fpapers%2F2507.19457)\n\n### Community\n\n[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)\nPaper authorPaper submitter [Jul 28](https://huggingface.co/huggingface.co#6887e6cdeed12b6b6149240b)\n\nLarge language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.\n\nReply\n\n[librarian-bot](https://huggingface.co/librarian-bot)\n[Jul 29](https://huggingface.co/huggingface.co#688825f0fc34a7523d2ce6e8)\n\nThis is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper.\n\nThe following papers were recommended by the Semantic Scholar API\n\n- [A Technical Survey of Reinforcement Learning Techniques for Large Language Models](https://huggingface.co/papers/2507.04136) (2025)\n- [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://huggingface.co/papers/2507.00726) (2025)\n- [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://huggingface.co/papers/2507.02804) (2025)\n- [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://huggingface.co/papers/2506.08125) (2025)\n- [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://huggingface.co/papers/2507.00417) (2025)\n- [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://huggingface.co/papers/2507.14897) (2025)\n- [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://huggingface.co/papers/2507.02834) (2025)\n\nPlease give a thumbs up to this comment if you found it helpful!\n\nIf you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\n\nYou can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`\n\nReply\n\n[millexplore](https://huggingface.co/millexplore)\n[Jul 31](https://huggingface.co/huggingface.co#688b0d36934fecfbd1df1c10)\n\nNo code? :)\n\n- 1 reply\n\n·\n\n[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)\nPaper author [23 days ago](https://huggingface.co/huggingface.co#68ac1872957cdf6c1c630450)\n\n[https://github.com/gepa-ai/gepa](https://github.com/gepa-ai/gepa)\n\n[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)\nPaper authorPaper submitter [23 days ago](https://huggingface.co/huggingface.co#68ac17dba9c8febf9a1937d4)\n\nWe have released the code at [https://github.com/gepa-ai/gepa](https://github.com/gepa-ai/gepa)\n\n❤️\n\n1\n\n1\n\n+\n\nReply\n\nEditPreview\n\nUpload images, audio, and videos by dragging in the text input, pasting, or clicking here.\n\nTap or paste here to upload images\n\nComment\n\n· [Sign up](https://huggingface.co/join?next=%2Fpapers%2F2507.19457) or\n[log in](https://huggingface.co/login?next=%2Fpapers%2F2507.19457) to comment\n\n[Upvote\\\n\\\n27](https://huggingface.co/login?next=%2Fpapers%2F2507.19457)\n\n- +15\n\n## Models citing this paper 0\n\nNo model linking this paper\n\nCite arxiv.org/abs/2507.19457 in a model README.md to link it from this page.\n\n## Datasets citing this paper 0\n\nNo dataset linking this paper\n\nCite arxiv.org/abs/2507.19457 in a dataset README.md to link it from this page.\n\n### Spaces citing this paper 0\n\nNo Space linking this paper\n\nCite arxiv.org/abs/2507.19457 in a Space README.md to link it from this page.\n\n## Collections including this paper 5","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fhuggingface.co%2Fpapers%2F2507.19457","id":"https://huggingface.co/papers/2507.19457","publishedDate":"2025-07-28T00:00:00.000Z","summary":"The webpage discusses GEPA, a novel prompt optimization method that leverages natural language reflection to improve large language model (LLM) performance. Unlike traditional reinforcement learning approaches like GRPO, which require many trial-and-error rollouts, GEPA uses high-level natural language reasoning to diagnose issues, refine prompts, and learn effective strategies more efficiently—often with significantly fewer attempts. Across multiple tasks, GEPA outperforms existing methods by up to 20% while using up to 35 times fewer rollouts, demonstrating its effectiveness as a prompt optimizer and potential for code optimization and inference-time search strategies.","title":"Reflective Prompt Evolution Can Outperform Reinforcement Learning","url":"https://huggingface.co/papers/2507.19457"},{"content":"[Sitemap](https://noailabs.medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F46048a2f8a2b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fnoailabs.medium.com%2Fnew-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fnoailabs.medium.com%2Fnew-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# New Research: Reflective Prompt Evolution Can Outperform RL\n\n## A prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error\n\n[noailabs](https://noailabs.medium.com/?source=post_page---byline--46048a2f8a2b---------------------------------------)\n\n5 min read\n\n·\n\nJul 28, 2025\n\n--\n\nListen\n\nShare\n\n**Meta-prompting is a new level in evolution of LLM-based systems.**\n\nPress enter or click to view image in full size\n\n[https://arxiv.org/abs/2507.19457](https://arxiv.org/abs/2507.19457)\n\n> Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), w **hich often require thousands of rollouts to learn new tasks.** We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), **a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error.** Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) **and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts.** As a result of GEPA’s design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.\n\nGEPA’S REFLECTION AND PROMPT UPDATE META PROMPT\n\n```\nI provided an assistant with the following instructions to perform a task for me:‘‘‘ ‘‘‘The following are examples of different task inputs provided to the assistantalong with the assistant’s response for each of them, and some feedback on howthe assistant’s response could be better:‘‘‘ ‘‘‘Your task is to write a new instruction for the assistant.Read the inputs carefully and identify the input format and infer detailed taskdescription about the task I wish to solve with the assistant.Read all the assistant responses and the corresponding feedback. Identify allniche and domain specific factual information about the task and include it inthe instruction, as a lot of it may not be available to the assistant in thefuture. The assistant may have utilized a generalizable strategy to solve thetask, if so, include that in the instruction as well.Provide the new instructions within ‘‘‘ blocks\n```\n\nA new paper, “GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning,” introduces a novel method for optimizing Large Language Model (LLM) prompts that can significantly enhance the performance and efficiency of LLM-based systems. This approach, centered on “reflective prompt evolution,” offers a departure from traditional reinforcement learning (RL) techniques.\n\n# Core Insights of GEPA\n\nThe GEPA (Genetic-Pareto) model is a prompt optimizer that leverages natural language reflection to learn and adapt. Here are the central ideas behind this innovative approach:\n\n- **Reflective Prompt Evolution**: Unlike RL methods that rely on often sparse, scalar rewards, GEPA utilizes the rich, interpretable nature of language itself as a learning medium. It analyzes system-level trajectories, which include reasoning steps, tool usage, and outputs, to diagnose issues and refine prompts through natural language feedback.\n- **Genetic-Pareto Optimization**: GEPA employs a Genetic-Pareto algorithm to manage and evolve prompts. This involves sampling various system trajectories, identifying problems, proposing and testing prompt modifications, and integrating the most effective changes from a “Pareto frontier” of its attempts.\n- **Enhanced Efficiency and Performance**: The GEPA framework has demonstrated significant improvements over existing methods. In various tasks, it outperformed Group Relative Policy Optimization (GRPO), a reinforcement learning technique, by an average of 10% and in some cases by up to 20%, all while requiring up to 35 times fewer “rollouts” or trials. Furthermore, GEPA has shown a greater than 10% improvement over the leading prompt optimizer, MIPROv2, across two different LLMs.\n- **Broad Applicability**: The principles of GEPA can be applied to a wide range of domains. For instance, in scientific research, it can optimize prompts for analyzing complex datasets. In education, it can help create more adaptive AI-tutoring systems, and in business, it can streamline tasks like market analysis.\n\n# Mini-Framework for Adaptive LLM-based (Agentic) Systems\n\nThe core principles of GEPA can be distilled into a mini-framework for developing more adaptive and efficient LLM-based agentic systems. This framework emphasizes continuous self-improvement through reflection and evolution.\n\n**1\\. Define Agent Trajectory:**\n\n- **Components**: An agent’s “trajectory” should not just be its final output. It must include all the intermediate steps, such as its reasoning process, any tools it utilizes, and the outputs from those tools.\n- **Example**: For a research agent, the trajectory would encompass the initial query, the search terms used, the sources consulted, the summarized findings from each source, and the final synthesized report.\n\n**2\\. Implement a Reflective “Critic”:**\n\n- **Function**: After each task or a set of tasks, a “critic” LLM analyzes the agent’s trajectory.\n- **Process**: This critic uses natural language to identify shortcomings, errors, or inefficiencies in the agent’s process. It should answer questions like: “Where did the agent go wrong?”, “Could a different tool have been more effective?”, or “Was the reasoning sound?”.\n\n**3\\. Generate and Test Evolved Prompts:**\n\n- **Evolutionary Engine**: Based on the critic’s feedback, a “prompt evolution” mechanism generates alternative prompts for the agent. This can be viewed as a form of genetic algorithm where different “genes” in the prompt are modified.\n- **Pareto-based Selection**: Not all changes are universally better. A Pareto-based selection process helps to identify a set of “non-dominated” prompts that offer different trade-offs between various performance metrics (e.g., accuracy, efficiency, and resource usage).\n\n**4\\. Continuous Integration and Adaptation:**\n\n- **Update Mechanism**: The best-performing prompts from the Pareto frontier are integrated back into the agent’s core instruction set.\n- **Iterative Improvement**: This entire process — trajectory analysis, reflection, and evolution — is not a one-time setup but a continuous loop. This allows the agent to adapt to new tasks and learn from its mistakes over time, leading to a truly adaptive system.\n\n[Llm](https://medium.com/tag/llm?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Prompt Engineering](https://medium.com/tag/prompt-engineering?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Rl](https://medium.com/tag/rl?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Agents](https://medium.com/tag/agents?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[**Written by noailabs**](https://noailabs.medium.com/?source=post_page---post_author_info--46048a2f8a2b---------------------------------------)\n\n[469 followers](https://noailabs.medium.com/followers?source=post_page---post_author_info--46048a2f8a2b---------------------------------------)\n\n· [37 following](https://noailabs.medium.com/following?source=post_page---post_author_info--46048a2f8a2b---------------------------------------)\n\nTech/biz consulting, analytics, research for founders, startups, corps and govs.\n\n## No responses yet\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Press](mailto:pressinquiries@medium.com)\n\n[Blog](https://blog.medium.com/?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----46048a2f8a2b---------------------------------------)\n\n[Rules](https:/","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fnoailabs.medium.com%2Fnew-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b","id":"https://noailabs.medium.com/new-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b","publishedDate":"2025-07-28T00:00:00.000Z","summary":"The article introduces GEPA (Genetic-Pareto), a novel prompt optimization method that leverages natural language reflection to improve large language model (LLM) performance. Unlike traditional reinforcement learning (RL) approaches like GRPO, which require extensive trial-and-error and numerous rollouts, GEPA uses natural language to diagnose issues, propose prompt updates, and learn high-level rules efficiently. By reflecting on system trajectories—such as reasoning steps and tool outputs—GEPA can achieve significant performance gains with fewer rollouts, outperforming RL-based methods by up to 20% and existing prompt optimizers like MIPROv2 by over 10%. The approach demonstrates that natural language reflection enhances the interpretability and effectiveness of prompt tuning, offering a promising alternative to RL for optimizing LLMs across various tasks.","title":"New Research: Reflective Prompt Evolution Can Outperform RL","url":"https://noailabs.medium.com/new-research-reflective-prompt-evolution-can-outperform-rl-46048a2f8a2b"}],"status":"completed","toolCallId":"call_nPvYUtzuK7cz4Nr12k3r42dY","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Here’s the performance picture the paper reports, distilled and specific.\n\nHeadline results\n- Versus RL (GRPO):\n  - Average gain: +10% across tasks\n  - Best-case gain: up to +19–20%\n  - Sample efficiency: up to 35× fewer rollouts than GRPO (which used 24k rollouts, often with LoRA)\n- Versus prompt optimization SOTA (MIPROv2):\n  - Average gain: >10% across two LLMs\n  - Aggregate optimization gain: GEPA ≈ +14% vs MIPROv2 ≈ +7% (i.e., roughly double the net improvement)\n  - Prompts tended to be shorter (reported ~33% shorter in a figure), lowering inference cost\n\nBenchmarks and models\n- Tasks: HotpotQA (multi-hop QA), HoVer (retrieval-augmented verification), IFBench (instruction following), PUPA (privacy-aware delegation)\n- Base models: Qwen3 8B (open) and GPT-4.1 mini (proprietary)\n- Configuration: GEPA optimizes module prompts in compound systems; GRPO baselines used 24k rollouts; MIPROv2 as a strong prompt-optimizer baseline\n\nLearning curves (qualitative takeaways from figures)\n- GEPA learns much faster with fewer samples; its curve overtakes GRPO early\n- On HotpotQA and HoVer with Qwen3 8B, GEPA’s final validation/test scores are clearly above both GRPO and MIPROv2\n- Even a single reflective update often yields a large jump; subsequent iterations refine further\n\nGeneralization and robustness\n- Multi-objective selection (accuracy, cost/latency, safety, stability) maintains diverse, high-performing prompt variants, improving robustness vs greedy prompt search\n- Improvements hold on held-out test sets (star markers in the figures), not just validation slices\n\nAblations / qualitative findings\n- Instruction-focused edits carried most of the gains; in places, instruction-only optimization beat methods that jointly tune instructions and few-shot examples\n- GEPA’s edits are interpretable “rules” tied to concrete failure modes (e.g., second-hop retrieval guidance), which transfer across many inputs\n\nBeyond benchmarks\n- Code optimization: As an inference-time reflective search loop, GEPA showed promising results on NPUEval and KernelBench by iteratively refining code with compiler/runtime feedback\n\nWhat to take away\n- If you have a compound LLM system and can log rich traces, GEPA tends to beat RL-style GRPO in both sample efficiency and final quality, and also outperforms leading prompt optimizers, while keeping prompts shorter and more interpretable.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_07f7fb24dd33c1350068caefd1470c81a1876a348568b7a089"}},"resumableStreamId":"STREAM:e4b2bf90-84da-46cb-8799-3503c0171c7d","role":"assistant","status":"done","threadId":"5e763140-e699-40d6-970e-751838959582","timeToFirstToken":20.769,"tokens":566,"tokensPerSecond":12.88853466924741,"updated_at":1758130084496,"userId":"google:104458561188320708185","id":"j9710n3qgwzwbnnb0mx7bdctyx7qs37h"},{"_creationTime":1758130502260.3176,"_id":"j971xgx80qyce30654aj9h2jpx7qrfa9","attachmentIds":[],"content":"what are the critics of this paper?","created_at":1758130502260,"messageId":"cfcff414-9208-4beb-9a8c-38b2beb57f4d","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"5e763140-e699-40d6-970e-751838959582","updated_at":1758130502260,"userId":"google:104458561188320708185","id":"j971xgx80qyce30654aj9h2jpx7qrfa9","providerMetadata":{}},{"_creationTime":1758130502260.3179,"_id":"j9729wgz8hez7gzfvz5hczfced7qsgkn","attachmentIds":[],"byok":false,"content":"Here are the main criticisms and open questions I’ve seen raised (and that are reasonable to raise) about GEPA, along with what the paper itself acknowledges as limitations.\n\nMethodological and empirical criticisms\n- Benchmark scope and external validity\n  - Critique: The paper evaluates on four tasks (HotpotQA, HoVer, IFBench, PUPA) with two base LLMs. These are good, but still a narrow slice of agentic workloads (e.g., complex planning, long-horizon tool use with partial observability, safety-constrained dialog, code agents with long dependency chains).\n  - Why it matters: GEPA’s advantage may hinge on richly textual traces. In settings with less readable trajectories (multimodal, binary outcomes, no tool logs) or highly stochastic environments, benefits may shrink.\n\n- RL baseline choice and setup\n  - Critique: The main RL baseline is GRPO at a fixed (large) rollout budget. Stronger or more tailored RL variants (better credit assignment, curriculum, or trajectory-level rewards; model-assisted reward rationales) might close the gap. Also, if GEPA leverages more information than the scalar reward (logs, rationales), the comparison isn’t apples-to-apples.\n  - Why it matters: If RL had access to the same rich textual signals (e.g., learned reward models over rationales), results could differ.\n\n- Reflection quality and evaluator leakage\n  - Critique: GEPA’s reflection can read evaluator messages, tool errors, and sometimes rationales. If those carry answer-key information or overly specific hints, the optimizer risks “teaching to the test” (spec leakage).\n  - Why it matters: Overly informative feedback can inflate gains and reduce true generalization.\n\n- Pareto selection stability\n  - Critique: Multi-objective Pareto fronts can be sensitive to metric noise and dataset slicing. Small validation sets or noisy metrics may preserve spurious “strategies,” increasing variance and selection instability.\n  - Why it matters: Could lead to brittle prompts that don’t survive to production.\n\n- Noisy, self-referential edits\n  - Critique: The same model family that executes tasks also generates reflections and edits. This can produce confident but wrong rules, compounding errors (“self-justification loops”).\n  - Why it matters: Without strong guardrails, the population can drift into brittle heuristics.\n\n- Compute/accounting fairness\n  - Critique: GEPA uses fewer rollouts, but each rollout may include extra LLM calls for reflection and candidate generation. If total token and wall-clock budgets are fully equalized, advantage might narrow.\n  - Why it matters: Real cost is tokens × calls × tool usage, not just “rollouts.”\n\n- Limited ablations on what actually helps\n  - Critique: The paper highlights instruction edits’ impact, but fewer ablations isolate contributions of: reflection prompt design, module-level vs monolithic edits, recombination policy, Pareto size, or logging granularity.\n  - Why it matters: Practitioners need to know which knobs matter most.\n\nConceptual/theoretical criticisms\n- Reliance on language as a learning substrate\n  - Critique: GEPA assumes problems, traces, and fixes are well-expressible in text. For low-level control, token-level shaping, or continuous-action domains, prompts may be too blunt; weight-space training may be required.\n  - Why it matters: Limits the method’s domain of applicability.\n\n- Interpretability vs correctness\n  - Critique: Textual “rules” feel interpretable, but their correctness and coverage aren’t guaranteed. Human-readable doesn’t mean causally valid.\n  - Why it matters: Can create a false sense of safety/auditability.\n\n- Genetic diversity vs convergence\n  - Critique: Maintaining many strategies helps exploration, but can slow convergence or complicate deployment if you end up with multiple specialized prompts that need routing or A/B logic.\n  - Why it matters: Operational complexity increases.\n\nPractical deployment criticisms\n- Governance and drift\n  - Critique: Frequent prompt changes can conflict with compliance, security review, or red-team sign-offs. Evolved prompts may slowly erode safety constraints unless audited rigorously.\n  - Why it matters: Enterprises often require strict change control.\n\n- Tooling and logging requirements\n  - Critique: GEPA’s effectiveness depends on rich, high-quality traces (tool I/O, evaluator rationales). Many production systems can’t expose such logs due to privacy, cost, or vendor limitations.\n  - Why it matters: Limits real-world applicability.\n\n- Portability across models\n  - Critique: The paper claims some cross-model transfer, but models differ in instruction following and tool-use priors. Edits that help one model may not translate cleanly.\n  - Why it matters: Reduces reuse across providers or versions.\n\nSecurity and safety criticisms\n- Speculative overfitting to evaluator quirks\n  - Critique: If evaluators are automated or heuristic, GEPA may evolve prompts that “game” evaluators (e.g., stylistic artifacts) rather than improve true task performance.\n  - Why it matters: Inflated offline gains, weaker human-judged quality.\n\n- Prompt injection and robustness\n  - Critique: More elaborate prompts can expand the attack surface for injection or jailbreaks. GEPA may inadvertently remove defensive phrasing while chasing accuracy/cost.\n  - Why it matters: Safety regressions are costly.\n\nWhat the paper itself acknowledges\n- Domain limits: Weight-space methods may win in data-rich regimes or for fine-grained token-level control where prompts are insufficient.\n- Reflection dependence: The quality of lessons depends on trace richness and reflector prompting.\n- Future work: Hybridization with weight finetuning/RL using GEPA’s distilled rules, better feedback engineering, and integrating few-shot example optimization.\n\nHow to mitigate these concerns in practice\n- Use a strict data hygiene and de-leakage protocol: separate train/val/test, mask gold answers from logs, limit evaluator rationales, and run held-out human evals.\n- Equalize total cost, not just rollouts: compare at matched token budgets and latency.\n- Add cross-model and cross-slice validation: test evolved prompts on different models and out-of-domain slices.\n- Enforce safety invariants: maintain a locked safety block that edits cannot touch; run safety regression tests each generation.\n- Keep edits small and auditable: require rationales linked to concrete failures; auto-ablate edits before merging.\n- Control Pareto complexity: cap frontier size, add tie-breaking with stability metrics, and periodically prune.\n\nBottom line\n- GEPA is a strong, practical alternative to RL when you can harvest rich textual traces and need sample efficiency, but its advantages are likely to shrink when traces are sparse, evaluators leak structure, or when problems require weight-level adaptation. Careful evaluation design (cost parity, leak controls, human evals) and safety governance are essential to avoid overstating gains or introducing regressions.","created_at":1758130502261,"messageId":"22a8aa7a-b731-4667-ad39-b6bc4b31d47e","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":null,"domains":null,"queries":["GEPA Reflective Prompt Evolution Can Outperform Reinforcement Learning arXiv 2507.19457 critiques discussion reviews 2025","\"GEPA\" prompt evolution Pareto criticism blog 2025-07","GEPA paper weaknesses limitations authors discuss arXiv 2507.19457"]},"result":[{"content":"# Computer Science > Computation and Language\n\n**arXiv:2507.19457** (cs)\n\n\\[Submitted on 25 Jul 2025\\]\n\n# Title:GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\nAuthors: [Lakshya A Agrawal](https://arxiv.org/search/cs?searchtype=author&query=Agrawal,+L+A), [Shangyin Tan](https://arxiv.org/search/cs?searchtype=author&query=Tan,+S), [Dilara Soylu](https://arxiv.org/search/cs?searchtype=author&query=Soylu,+D), [Noah Ziems](https://arxiv.org/search/cs?searchtype=author&query=Ziems,+N), [Rishi Khare](https://arxiv.org/search/cs?searchtype=author&query=Khare,+R), [Krista Opsahl-Ong](https://arxiv.org/search/cs?searchtype=author&query=Opsahl-Ong,+K), [Arnav Singhvi](https://arxiv.org/search/cs?searchtype=author&query=Singhvi,+A), [Herumb Shandilya](https://arxiv.org/search/cs?searchtype=author&query=Shandilya,+H), [Michael J Ryan](https://arxiv.org/search/cs?searchtype=author&query=Ryan,+M+J), [Meng Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang,+M), [Christopher Potts](https://arxiv.org/search/cs?searchtype=author&query=Potts,+C), [Koushik Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen,+K), [Alexandros G. Dimakis](https://arxiv.org/search/cs?searchtype=author&query=Dimakis,+A+G), [Ion Stoica](https://arxiv.org/search/cs?searchtype=author&query=Stoica,+I), [Dan Klein](https://arxiv.org/search/cs?searchtype=author&query=Klein,+D), [Matei Zaharia](https://arxiv.org/search/cs?searchtype=author&query=Zaharia,+M), [Omar Khattab](https://arxiv.org/search/cs?searchtype=author&query=Khattab,+O)\n\nView a PDF of the paper titled GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning, by Lakshya A Agrawal and 16 other authors\n\n[View PDF](https://arxiv.org/pdf/2507.19457)\n\n> Abstract:Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.\n\n| | |\n| --- | --- |\n| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE) |\n| ACM classes: | I.2.7; I.2.6; I.2.4; I.2.8 |\n| Cite as: | [arXiv:2507.19457](https://arxiv.org/abs/2507.19457) \\[cs.CL\\] |\n| (or [arXiv:2507.19457v1](https://arxiv.org/abs/2507.19457v1) \\[cs.CL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2507.19457](https://doi.org/10.48550/arXiv.2507.19457) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Lakshya A Agrawal \\[ [view email](https://arxiv.org/show-email/af5f91b3/2507.19457)\\] **\\[v1\\]**\nFri, 25 Jul 2025 17:42:32 UTC (1,632 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning, by Lakshya A Agrawal and 16 other authors\n\n- [View PDF](https://arxiv.org/pdf/2507.19457)\n- [TeX Source](https://arxiv.org/src/2507.19457)\n- [Other Formats](https://arxiv.org/format/2507.19457)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[< prev](https://arxiv.org/prevnext?id=2507.19457&function=prev&context=cs.CL)  \\|  [next >](https://arxiv.org/prevnext?id=2507.19457&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2025-07](https://arxiv.org/list/cs.CL/2025-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2507.19457?context=cs) [cs.AI](https://arxiv.org/abs/2507.19457?context=cs.AI) [cs.LG](https://arxiv.org/abs/2507.19457?context=cs.LG) [cs.SE](https://arxiv.org/abs/2507.19457?context=cs.SE)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2507.19457)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2507.19457)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2507.19457)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2507.19457) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.19457","id":"https://arxiv.org/abs/2507.19457","publishedDate":"2025-07-25T00:00:00.000Z","summary":"The paper introduces GEPA (Genetic-Pareto), a novel prompt optimization method for large language models (LLMs) that leverages natural language reflection rather than traditional reinforcement learning (RL). Unlike RL approaches like GRPO, which require extensive rollouts, GEPA uses natural language reflection to diagnose issues, propose prompt improvements, and learn high-level rules through trial and error. This approach enables significant efficiency, achieving up to 35 times fewer rollouts while outperforming RL-based methods by 10-20% across various tasks. GEPA also surpasses existing prompt optimizers like MIPROv2 by over 10%, demonstrating its effectiveness in enhancing LLM performance and its potential as an inference-time strategy for code optimization.","title":"GEPA: Reflective Prompt Evolution Can Outperform Reinforcement ...","url":"https://arxiv.org/abs/2507.19457"},{"content":"Preprint.\nGEPA: REFLECTIVE PROMPT EVOLUTION CAN OUTPERFORM\nREINFORCEMENT LEARNING\nLakshya A Agrawal1, Shangyin Tan1, Dilara Soylu2, Noah Ziems4,\nRishi Khare1, Krista Opsahl-Ong5, Arnav Singhvi2,5, Herumb Shandilya2,\nMichael J Ryan2, Meng Jiang4, Christopher Potts2, Koushik Sen1,\nAlexandros G. Dimakis1,3, Ion Stoica1, Dan Klein1, Matei Zaharia1,5, Omar Khattab6\n1UC Berkeley 2Stanford University 3BespokeLabs.ai 4Notre Dame 5Databricks 6MIT\nABSTRACT\nLarge language models (LLMs) are increasingly adapted to downstream tasks via reinforcement\nlearning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thou\u0002sands of rollouts to learn new tasks. We argue that the interpretable nature of language can often\nprovide a much richer learning medium for LLMs, compared with policy gradients derived from\nsparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that\nthoroughly incorporates natural language reflection to learn high-level rules from trial and error.\nGiven any AI system containing one or more LLM prompts, GEPA samples system-level trajecto\u0002ries (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons from the Pareto\nfrontier of its own attempts. As a result of GEPA’s design, it can often turn even just a few rollouts\ninto a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up\nto 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time\nsearch strategy for code optimization.\n1 INTRODUCTION\n0 5000 10000 15000 20000 25000\nNumber of Rollouts\n42.5\n45.0\n47.5\n50.0\n52.5\n55.0\n57.5\n60.0\n62.5\nScore\nHotpotQA, Qwen3 8B\nOptimization Method\nBaseline\nMIPROv2\nGRPO\nGEPA\nValidation Performance\nTest-set Performance\n(a) HotpotQA, Qwen3 8B\n0 5000 10000 15000 20000 25000\nNumber of Rollouts\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\nScore\nHoVer, Qwen3 8B\nOptimization Method\nBaseline\nMIPROv2\nGRPO\nGEPA\nValidation Performance\nTest-set Performance\n(b) HoVer, Qwen3 8B\nFigure 1: A comparison of the learning behavior of our proposed GEPA prompt optimizer against a state-of-the-art\nprompt optimizer (MIPROv2) and the GRPO (24,000 rollouts) algorithm. As more rollouts are sampled, the prompt\noptimizers can learn much more quickly than GRPO. GEPA substantially outperforms both GRPO and MIPROv2 in\nfinal score. The Test-set star markers demonstrate the performance gap in a held-out set of questions.\nLarge language models (LLMs) have enabled the development of agents and systems that combine fuzzy natural\u0002language behavior specification with tools like retrieval and code execution. These types of systems raise the question\nof how LLMs should be “optimized” for the best downstream performance within their harness. One popular ap\u0002proach for adapting LLMs to downstream tasks is reinforcement learning with verifiable rewards (RLVR), including\n1\narXiv:2507.19457v1 [cs.CL] 25 Jul 2025\nPreprint.\nalgorithms such as Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Such RL methods cast success\nmetrics as a scalar reward observed at the end of each rollout (Lambert, 2025) and use these rewards to estimate\ngradients for policy improvement.\nWhile these RL approaches are effective, they typically require tens of thousands of rollouts in practice to fit new\ntasks. For example, recent works leveraging GRPO across a range of tasks typically use up to hundreds of thousands\nof rollouts for training (Chen et al., 2025b; Wu et al., 2025c; Zhang et al., 2025; Jin et al., 2025; Si et al., 2025; Wang\net al., 2025a; Java et al., 2025; Chen et al., 2025a; Wu et al., 2025a; Sha et al., 2025; Lin et al., 2025; Peng et al.,\n2025; Song et al., 2025). This sample inefficiency can quickly become a serious bottleneck: many downstream LLM\napplications invoke expensive tool calls, have limited inference budget for sampling from the LLM itself, or simply\ncannot finetune the weights of the largest or best-performing LLMs.\nWe observe that the rollouts sampled from even highly sophisticated LLM systems can be serialized into traces of\nnatural (and formal) language, as they contain nothing but the instructions of each LLM module, the resulting LLM\nreasoning chains, tool calls, and potentially the internal workings of the reward function (for example, compiler error\nmessages, before they are collapsed into scalar rewards). Because such serialized trajectories can be readily under\u0002stood by modern LLMs, we argue that algorithms that learn deliberately in natural language by reflecting on these\ntrajectories can potentially make much more effective use of the strong language priors that LLMs have, compared\nwith standard RL approaches.\nTo operationalize this, we introduce GEPA (Genetic-Pareto), a reflective prompt optimizer for compound AI systems\nthat merges textual reflection with multi-objective evolutionary search. GEPA iteratively mutates every prompt within\nthe AI system in light of natural language feedback drawn from new rollouts. In each mutation, the candidate prompt\nis derived from an ancestor, accumulating high-level lessons derived from observations and LLM feedback. To avoid\nthe local optima that afflict greedy prompt updates, GEPA maintains a Pareto front: instead of evolving only the global\nbest prompt, it stochastically explores the top-performing prompts for each problem instance, thereby diversifying\nstrategies and encouraging robust generalization.\nWe evaluate GEPA on four diverse tasks—multi-hop reasoning (HotpotQA; Yang et al. 2018), instruction following\n(IFBench; Pyatkin et al. 2025b), privacy-aware delegation (PUPA; Li et al. 2025a), and retrieval-augmented verifica\u0002tion (HoVer; Jiang et al. 2020)—using both open (Qwen3 8B; Yang et al. 2025; Team 2025) and proprietary (GPT-4.1\nmini; OpenAI 2025) models. Our results show that GEPA demonstrates robust generalization and is highly sample\u0002efficient: on Qwen3 8B, GEPA outperforms GRPO (24,000 rollouts with LoRA) by up to 19% while requiring up to\n35× fewer rollouts. Overall, GEPA achieves an average improvement of +10% over GRPO across all tasks. Further\u0002more, GEPA surpasses the previous state-of-the-art prompt optimizer, MIPROv2 (Opsahl-Ong et al., 2024), on every\nbenchmark and model, obtaining aggregate optimization gains of +14%, more than doubling the gains achieved by\nMIPROv2 (+7%).\nEven qualitatively, GEPA generated prompts can be highly effective. Figure 2 highlights excerpts from a prompt\ncrafted by GEPA for the query creation module of a multi-hop question answering system (Used in HotpotQA). We\nalso find that in most cases, even a single reflective prompt update can give large improvements (as highlighted in the\noptimization trajectory in Figure 5). These results demonstrate that reflective prompt evolution using language feed\u0002back enables substantial sample efficiency and robust generalization, providing a practical path to optimizing complex,\nreal-world AI workflows in data- or budget-constrained environments. Finally, we also show promising preliminary\nresults demonstrating GEPA’s use as an inference-time search strategy for code optimization over NPUEval (Kalade\n& Schelle, 2025) and KernelBench (Ouyang et al., 2025).\n2 PROBLEM STATEMENT\nCompound AI Systems. We follow related work in defining a compound AI system as any modular system com\u0002posed of one or more language model (LLM) invocations, potentially interleaved with external tool calls, orchestrated\nthrough arbitrary control flow. This definition subsumes a broad class of real-world LLM-based AI systems, including\nagents, multi-agent systems, and general-purpose scaffolding techniques like ReAct (Yao et al., 2023), Archon (Saad\u0002Falcon et al., 2025), etc. Following Soylu et al. (2024); Khattab et al. (2024); Opsahl-Ong et al. (2024); Tan et al.\n(2025), we formalize such a system as Φ = (M, C, X , Y), where M = ⟨M1, . . . , M|M|⟩ denotes language modules,\nC specifies control flow logic, and X , Y are global input/output schemas. Each module Mi = (πi, θi, Xi, Yi) is an\nLLM subcomponent: πiis its (system) prompt including instructions and few-shot demonstrations; θithe underly\u0002ing model weights; Xi\n, Yi are input/output schemas. At runtime, C orchestrates the sequencing and invocation of\nmodules—e.g., passing outputs from one module to another, invoking modules conditionally, or leveraging tool APIs.\nThis way, C can invoke different modules in any order multiples of times.\n2\nPreprint.\nSeed Prompt for Second-Hop of Multi-Hop QA System\nGiven the fields question, summary 1, produce the fields query.\nGEPA’s Optimized Prompt for Second-Hop of Multi-Hop QA System, GPT-4.1 Mini\nYou will be given two input fields: question and summary 1. Your task: Generate a new search query (query) optimized for the second hop of a\nmulti-hop retrieval system.\n• The original user question is typically complex and requires information from multiple documents to answer.\n• The first hop query is the original question (used to retrieve initial documents).\n• Your goal: generate a query to retrieve documents not found in first hop but necessary to answer the question completely.\nInput Understanding: question is the original multi-hop question posed by the user. summary 1 is a concise summary of information from a document\nretrieved in the first hop, which partially addresses the question.\nPurpose and Context:\n• Your generated query aims to find the missing pieces of information needed to fully answer the question. . . .\n• The query must retrieve relevant documents NOT found in first hop . . . for final answer extraction.\nKey Observations and Lessons:\n• First-hop documents often cover one entity or aspect.\n• Remaining relevant documents often involve connected or higher-level concepts mentioned in summary 1 but not explicitly asked in","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2507.19457","id":"https://arxiv.org/pdf/2507.19457","publishedDate":"2025-07-25T00:00:00.000Z","summary":"The paper introduces GEPA (Genetic-Pareto), a novel prompt optimization method that leverages natural language reflection to improve large language models (LLMs) more efficiently than traditional reinforcement learning (RL) approaches like GRPO. Unlike RL methods that rely on sparse scalar rewards and require thousands of rollouts, GEPA uses natural language to analyze system trajectories (e.g., reasoning steps, tool calls) and iteratively refine prompts by diagnosing issues, proposing updates, and combining lessons from multiple attempts. Experimental results across four tasks show that GEPA outperforms GRPO by up to 20% while using up to 35 times fewer rollouts, and also surpasses the leading prompt optimizer MIPROv2 by over 10%. The approach demonstrates significant improvements in prompt learning efficiency and effectiveness, highlighting the potential of language-based reflection for optimizing LLMs in downstream applications.","title":"[PDF] GEPA: Reflective Prompt Evolution Can Outperform Reinforcement ...","url":"https://arxiv.org/pdf/2507.19457"},{"content":"Reflective Prompt Evolution can outperform reinforcement learning Press enter or click to view image in full size https://arxiv.org/pdf/2507.19457 GEPA (Genetic-Pareto Prompt Evolution) is a novel technique for optimizing prompts in LLM systems. It uses language reflection and evolutionary principles, often outperforming reinforcement learning (RL) methods in both sample efficiency and overall performance on complex tasks. Motivation and Background Traditional RL-based techniques — like Group Relative Policy Optimization (GRPO) — adapt LLMs to new tasks by tuning weights through thousands of reward-driven rollouts. However, RL compresses rich sequential behaviour into single reward values, missing out on the natural expressiveness of language. GEPA argues that systems relying on language should learn and improve through language itself: using natural language diagnoses to iteratively evolve and improve prompts. How GEPA Works GEPA processes traces of LLM behaviour (e.g., reasoning steps, tool calls, outputs) and prompts itself to reflect: “How could this prompt be improved?” The approach consists of three key steps: Genetic Optimization: Candidates (prompt versions) are created through mutation (reflective changes) and crossover (merging improvements), not weight updates. Natural Language Reflection: GEPA analyzes system trajectories as text, extracting issues and lessons in human-readable form. Pareto Sampling: At each iteration, the system samples from a pool of top-performing prompts, maintaining diversity and avoiding local optima. Press enter or click to view image in full size https://arxiv.org/pdf/2507.19457 Algorithmic Outline def GEPA_optimize(base_prompts, task_dataset, eval_metric, rollouts_budget): candidate_pool = [base_prompts] for rollout in range(rollouts_budget): parent = select_pareto_candidate(candidate_pool) feedback = reflect_on_trace(parent.run(task_dataset)) child = mutate_or_merge_prompts(parent, feedback) if eval_metric(child) &gt; eval_metric(parent): candidate_pool.append(child) return best_candidate(candidate_pool) Results: GEPA vs Reinforcement Learning Experiments demonstrate that GEPA: Achieves up to 19% higher task scores than GRPO on HotpotQA, IFBench, PUPA, and HoVer benchmarks. Requires up to 35× fewer rollouts to reach optimal test set performance, often matching RL’s best score with as few as 300–1200 rollouts instead of tens of thousands. Outperforms previous prompt optimizers such as MIPROv2 by 10% or more across multiple LLMs. comparison : Press enter or click to view image in full size Why Reflection Beats Rewards GEPA leverages the core strength of LLMs: understanding and generating language. By treating prompt optimization as a language process — where each candidate is just another “idea” — the system improves much more rapidly than brute-force reward aggregation. Pareto-based selection maintains diversity and drives generalization, making GEPA robust even on out-of-domain evaluation. Press enter or click to view image in full size https://arxiv.org/pdf/2507.19457 Technical Takeaways Efficiency: GEPA needs 400–1200 rollouts for major benchmarks, while RL typically needs 24,000 or more, dramatically reducing cost and compute. Interpretability: Each candidate prompt’s evolution and diagnosis is human-readable, facilitating easier debugging and improvement. Practical Benefits: GEPA’s prompts are up to 9× shorter than those produced by many traditional optimizers, making deployment faster and cheaper. ⁶ Example: GEPA in Action Below is a simplified code pattern for a reflective GEPA iteration: def reflect(prompt, trace): \"\"\" Analyze LLM trace, output suggestions in natural language. \"\"\" # Example reflection as prompt suggestion = llm(f\"Analyze: {trace}\\nHow can the prompt be improved?\") return update_prompt(prompt, suggestion) Conclusion GEPA represents a major leap for language-centric AI system optimization. By moving beyond RL’s scalar reward limitations, and allowing systems to learn from their own words, it achieves faster, more robust, and more interpretable results on a wide variety of compound LLM tasks. Appendix https://arxiv.org/abs/2507.19457 https://huggingface.co/papers/2507.19457","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fmedium.com%2F%40sulbha.jindal%2Fllm-prompt-optimization-genetic-pareto-gepa-paper-reviw-29ee52d9b5db","id":"https://medium.com/@sulbha.jindal/llm-prompt-optimization-genetic-pareto-gepa-paper-reviw-29ee52d9b5db","publishedDate":"2025-08-26T00:00:00.000Z","summary":"The webpage reviews the paper on GEPA (Genetic-Pareto Prompt Evolution), a novel method for optimizing prompts in large language models (LLMs). Unlike traditional reinforcement learning approaches, GEPA employs evolutionary principles combined with language reflection, allowing prompts to evolve through mutation, crossover, and self-assessment via natural language analysis. This approach leverages the language capabilities of LLMs to diagnose and improve prompts iteratively. Experimental results show that GEPA outperforms RL-based methods like GRPO, achieving up to 19% higher task scores and requiring significantly fewer rollouts—up to 35 times less—while maintaining diversity and robustness. The method emphasizes efficiency, natural language reflection, and Pareto sampling to enhance prompt quality and generalization across tasks.","title":"LLM Prompt optimization: Genetic Pareto (GEPA) — Paper Review","url":"https://medium.com/@sulbha.jindal/llm-prompt-optimization-genetic-pareto-gepa-paper-reviw-29ee52d9b5db"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F1mb8e5w%2F250719457_gepa_reflective_prompt_evolution_can%2F","id":"https://www.reddit.com/r/MachineLearning/comments/1mb8e5w/250719457_gepa_reflective_prompt_evolution_can/","publishedDate":"2025-07-28T00:00:00.000Z","summary":"Failed to fetch content","title":"[2507.19457] GEPA: Reflective Prompt Evolution Can Outperform ...","url":"https://www.reddit.com/r/MachineLearning/comments/1mb8e5w/250719457_gepa_reflective_prompt_evolution_can/"},{"content":"[Papers](https://huggingface.co/papers)\n\narxiv:2507.19457\n\n# GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\nPublished on Jul 25\n\n·Submitted by[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)on Jul 28\n\n[#3 Paper of the day](https://huggingface.co/papers?date=2025-07-28)\n\n[Upvote\\\n\\\n27](https://huggingface.co/login?next=%2Fpapers%2F2507.19457)\n\n- +19\n\nAuthors:\n\n[Lakshya A Agrawal](https://huggingface.co/LakshyAAAgrawal)\n,\n\nShangyin Tan\n\n,\n\nDilara Soylu\n\n,\n\nNoah Ziems\n\n,\n\nRishi Khare\n\n,\n\nKrista Opsahl-Ong\n\n,\n\nArnav Singhvi\n\n,\n\nHerumb Shandilya\n\n,\n\nMichael J Ryan\n\n,\n\nMeng Jiang\n\n,\n\nChristopher Potts\n\n,\n\nKoushik Sen\n\n,\n\nAlexandros G. Dimakis\n\n,\n\nIon Stoica\n\n,\n\nDan Klein\n\n,\n\nMatei Zaharia\n\n,\n\nOmar Khattab\n\n## Abstract\n\nGEPA, a prompt optimizer using natural language reflection, outperforms RL methods like GRPO and MIPROv2 with fewer rollouts by learning high-level rules from trial and error.\n\nAI-generated summary\n\n[Large language models](https://huggingface.co/papers?q=Large%20language%20models) (LLMs) are increasingly adapted to downstream tasks via\n[reinforcement learning](https://huggingface.co/papers?q=reinforcement%20learning) (RL) methods like [Group Relative Policy Optimization](https://huggingface.co/papers?q=Group%20Relative%20Policy%20Optimization)\n( [GRPO](https://huggingface.co/papers?q=GRPO)), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce [GEPA](https://huggingface.co/papers?q=GEPA) ( [Genetic-Pareto](https://huggingface.co/papers?q=Genetic-Pareto)), a prompt\noptimizer that thoroughly incorporates [natural language reflection](https://huggingface.co/papers?q=natural%20language%20reflection) to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, [GEPA](https://huggingface.co/papers?q=GEPA) samples [system-level trajectories](https://huggingface.co/papers?q=system-level%20trajectories) (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the [Pareto frontier](https://huggingface.co/papers?q=Pareto%20frontier) of its own attempts. As a result of [GEPA](https://huggingface.co/papers?q=GEPA)'s design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, [GEPA](https://huggingface.co/papers?q=GEPA) outperforms [GRPO](https://huggingface.co/papers?q=GRPO) by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. [GEPA](https://huggingface.co/papers?q=GEPA) also outperforms the leading [prompt optimizer](https://huggingface.co/papers?q=prompt%20optimizer),\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for [code optimization](https://huggingface.co/papers?q=code%20optimization).\n\n[View arXiv page](https://arxiv.org/abs/2507.19457) [View PDF](https://arxiv.org/pdf/2507.19457) [Add to collection](https://huggingface.co/login?next=%2Fpapers%2F2507.19457)\n\n### Community\n\n[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)\nPaper authorPaper submitter [Jul 28](https://huggingface.co/huggingface.co#6887e6cdeed12b6b6149240b)\n\nLarge language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPA's design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.\n\nReply\n\n[librarian-bot](https://huggingface.co/librarian-bot)\n[Jul 29](https://huggingface.co/huggingface.co#688825f0fc34a7523d2ce6e8)\n\nThis is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper.\n\nThe following papers were recommended by the Semantic Scholar API\n\n- [A Technical Survey of Reinforcement Learning Techniques for Large Language Models](https://huggingface.co/papers/2507.04136) (2025)\n- [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://huggingface.co/papers/2507.00726) (2025)\n- [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://huggingface.co/papers/2507.02804) (2025)\n- [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://huggingface.co/papers/2506.08125) (2025)\n- [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://huggingface.co/papers/2507.00417) (2025)\n- [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://huggingface.co/papers/2507.14897) (2025)\n- [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://huggingface.co/papers/2507.02834) (2025)\n\nPlease give a thumbs up to this comment if you found it helpful!\n\nIf you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\n\nYou can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`\n\nReply\n\n[millexplore](https://huggingface.co/millexplore)\n[Jul 31](https://huggingface.co/huggingface.co#688b0d36934fecfbd1df1c10)\n\nNo code? :)\n\n- 1 reply\n\n·\n\n[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)\nPaper author [23 days ago](https://huggingface.co/huggingface.co#68ac1872957cdf6c1c630450)\n\n[https://github.com/gepa-ai/gepa](https://github.com/gepa-ai/gepa)\n\n[LakshyAAAgrawal](https://huggingface.co/LakshyAAAgrawal)\nPaper authorPaper submitter [23 days ago](https://huggingface.co/huggingface.co#68ac17dba9c8febf9a1937d4)\n\nWe have released the code at [https://github.com/gepa-ai/gepa](https://github.com/gepa-ai/gepa)\n\n❤️\n\n1\n\n1\n\n+\n\nReply\n\nEditPreview\n\nUpload images, audio, and videos by dragging in the text input, pasting, or clicking here.\n\nTap or paste here to upload images\n\nComment\n\n· [Sign up](https://huggingface.co/join?next=%2Fpapers%2F2507.19457) or\n[log in](https://huggingface.co/login?next=%2Fpapers%2F2507.19457) to comment\n\n[Upvote\\\n\\\n27](https://huggingface.co/login?next=%2Fpapers%2F2507.19457)\n\n- +15\n\n## Models citing this paper 0\n\nNo model linking this paper\n\nCite arxiv.org/abs/2507.19457 in a model README.md to link it from this page.\n\n## Datasets citing this paper 0\n\nNo dataset linking this paper\n\nCite arxiv.org/abs/2507.19457 in a dataset README.md to link it from this page.\n\n### Spaces citing this paper 0\n\nNo Space linking this paper\n\nCite arxiv.org/abs/2507.19457 in a Space README.md to link it from this page.\n\n## Collections including this paper 5","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fhuggingface.co%2Fpapers%2F2507.19457","id":"https://huggingface.co/papers/2507.19457","publishedDate":"2025-07-28T00:00:00.000Z","summary":"The webpage presents \"GEPA,\" a novel prompt optimization method that leverages natural language reflection to improve large language model (LLM) performance. Unlike traditional reinforcement learning approaches like GRPO, which require extensive rollouts, GEPA learns high-level rules through natural language analysis of system trajectories, enabling more efficient and effective prompt refinement. Across multiple tasks, GEPA outperforms existing methods by up to 20% while using significantly fewer rollouts, demonstrating its potential as a powerful, interpretable alternative for prompt tuning and code optimization.","title":"Reflective Prompt Evolution Can Outperform Reinforcement Learning","url":"https://huggingface.co/papers/2507.19457"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F394049542_GEPA_Reflective_Prompt_Evolution_Can_Outperform_Reinforcement_Learning","id":"https://www.researchgate.net/publication/394049542_GEPA_Reflective_Prompt_Evolution_Can_Outperform_Reinforcement_Learning","publishedDate":"2025-07-30T00:00:00.000Z","summary":"Failed to fetch content","title":"GEPA: Reflective Prompt Evolution Can Outperform ...","url":"https://www.researchgate.net/publication/394049542_GEPA_Reflective_Prompt_Evolution_Can_Outperform_Reinforcement_Learning"},{"content":"# Computer Science > Machine Learning\n\n**arXiv:2503.14321** (cs)\n\n\\[Submitted on 18 Mar 2025 ( [v1](https://arxiv.org/abs/2503.14321v1)), last revised 24 May 2025 (this version, v2)\\]\n\n# Title:COPA: Comparing the incomparable in multi-objective model evaluation\n\nAuthors: [Adrián Javaloy](https://arxiv.org/search/cs?searchtype=author&query=Javaloy,+A), [Antonio Vergari](https://arxiv.org/search/cs?searchtype=author&query=Vergari,+A), [Isabel Valera](https://arxiv.org/search/cs?searchtype=author&query=Valera,+I)\n\nView a PDF of the paper titled COPA: Comparing the incomparable in multi-objective model evaluation, by Adri\\\\'an Javaloy and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.14321)\n\n> Abstract:As machine learning (ML) practitioners, we often have hundreds of (trained) ML models at hand from which we need to choose one, based on various objectives such as accuracy, robustness, fairness, scalability, etc. However, how to compare, aggregate and, ultimately, trade-off these objectives is usually a time-consuming task that requires of expert knowledge, as they may be measured in different units or scales. In this work, we investigate how objectives can be automatically normalized and aggregated to systematically navigate their Pareto front. To do so, we make incomparable objectives comparable using their CDFs, approximated by their relative rankings. As a result, we can aggregate them while matching user-specific preferences, allowing practitioners to meaningfully navigate and search for models in the Pareto front. We demonstrate the potential impact of our approach, COPA, in both model selection and benchmarking tasks across diverse ML areas such as fair ML, domain generalization, AutoML and foundation models, where classical ways to normalize and aggregate objectives fall short.\n\n| | |\n| --- | --- |\n| Comments: | 24 pages, 18 figures. Under submission |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2503.14321](https://arxiv.org/abs/2503.14321) \\[cs.LG\\] |\n| (or [arXiv:2503.14321v2](https://arxiv.org/abs/2503.14321v2) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.14321](https://doi.org/10.48550/arXiv.2503.14321) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Adrián Javaloy \\[ [view email](https://arxiv.org/show-email/ba745e74/2503.14321)\\] **[\\[v1\\]](https://arxiv.org/abs/2503.14321v1)**\nTue, 18 Mar 2025 14:51:42 UTC (1,897 KB)\n**\\[v2\\]**\nSat, 24 May 2025 12:46:13 UTC (3,116 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled COPA: Comparing the incomparable in multi-objective model evaluation, by Adri\\\\'an Javaloy and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.14321)\n- [TeX Source](https://arxiv.org/src/2503.14321)\n- [Other Formats](https://arxiv.org/format/2503.14321)\n\n[view license](http://creativecommons.org/licenses/by-nc-nd/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[< prev](https://arxiv.org/prevnext?id=2503.14321&function=prev&context=cs.LG)  \\|  [next >](https://arxiv.org/prevnext?id=2503.14321&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-03](https://arxiv.org/list/cs.LG/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.14321?context=cs) [cs.AI](https://arxiv.org/abs/2503.14321?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.14321)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.14321)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.14321)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.14321) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.14321","id":"https://arxiv.org/abs/2503.14321","publishedDate":"2025-03-18T00:00:00.000Z","summary":"The paper \"COPA: Comparing the Incomparable in Multi-Objective Model Evaluation\" introduces a novel method for evaluating and selecting machine learning models across multiple objectives such as accuracy, robustness, fairness, and scalability. Traditional approaches struggle to compare objectives measured in different units or scales. COPA addresses this by normalizing objectives using their cumulative distribution functions (CDFs), approximated through relative rankings, making them comparable. This allows for systematic aggregation of objectives aligned with user preferences, facilitating meaningful navigation of the Pareto front. The approach is demonstrated across various ML domains—including fair ML, domain generalization, AutoML, and foundation models—showing its effectiveness in model selection and benchmarking where classical normalization methods fall short. Overall, COPA offers a principled, automated framework for multi-objective model evaluation, enabling more informed decision-making in complex ML scenarios.","title":"COPA: Comparing the Incomparable to Explore the Pareto Front","url":"https://arxiv.org/abs/2503.14321"},{"content":"# Computer Science > Computation and Language\n\n**arXiv:2503.20194** (cs)\n\n\\[Submitted on 26 Mar 2025\\]\n\n# Title:GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization\n\nAuthors: [Zhouhong Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu,+Z), [Xingzhou Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+X), [Xiaoran Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi,+X), [Tao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+T), [Suhang Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng,+S), [Tianyu Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+T), [Hongwei Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng,+H), [Yanghua Xiao](https://arxiv.org/search/cs?searchtype=author&query=Xiao,+Y)\n\nView a PDF of the paper titled GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization, by Zhouhong Gu and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.20194) [HTML (experimental)](https://arxiv.org/html/2503.20194v1)\n\n> Abstract:Recent advances in large language models have highlighted the critical need for precise control over model outputs through predefined constraints. While existing methods attempt to achieve this through either direct instruction-response synthesis or preferential response optimization, they often struggle with constraint understanding and adaptation. This limitation becomes particularly evident when handling fine-grained constraints, leading to either hallucination or brittle performance. We introduce Generative Adversarial Policy Optimization (GAPO), a novel framework that combines GAN-based training dynamics with an encoder-only reward model to progressively learn and adapt to increasingly complex constraints. GAPO leverages adversarial training to automatically generate training samples of varying difficulty while utilizing the encoder-only architecture to better capture prompt-response relationships. Extensive experiments demonstrate GAPO's superior performance across multiple benchmarks, particularly in scenarios requiring fine-grained constraint handling, where it significantly outperforms existing methods like PPO, DPO, and KTO. Our results suggest that GAPO's unique approach to preferential prompt learning offers a more robust and effective solution for controlling LLM outputs. Code is avaliable in [this https URL](https://github.com/MikeGu721/GAPO).\n\n| | |\n| --- | --- |\n| Subjects: | Computation and Language (cs.CL) |\n| Cite as: | [arXiv:2503.20194](https://arxiv.org/abs/2503.20194) \\[cs.CL\\] |\n| (or [arXiv:2503.20194v1](https://arxiv.org/abs/2503.20194v1) \\[cs.CL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.20194](https://doi.org/10.48550/arXiv.2503.20194) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Zhouhong Gu \\[ [view email](https://arxiv.org/show-email/72e1078c/2503.20194)\\] **\\[v1\\]**\nWed, 26 Mar 2025 03:37:52 UTC (11,445 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization, by Zhouhong Gu and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.20194)\n- [HTML (experimental)](https://arxiv.org/html/2503.20194v1)\n- [TeX Source](https://arxiv.org/src/2503.20194)\n- [Other Formats](https://arxiv.org/format/2503.20194)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[< prev](https://arxiv.org/prevnext?id=2503.20194&function=prev&context=cs.CL)  \\|  [next >](https://arxiv.org/prevnext?id=2503.20194&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2025-03](https://arxiv.org/list/cs.CL/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.20194?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.20194)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.20194)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.20194)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.20194) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.20194","id":"https://arxiv.org/abs/2503.20194","publishedDate":"2025-03-26T00:00:00.000Z","summary":"The paper introduces GAPO (Generative Adversarial Policy Optimization), a novel framework designed to improve control over large language model (LLM) outputs by learning and adapting to complex, fine-grained constraints. Unlike traditional methods that struggle with understanding and adhering to detailed constraints, GAPO combines adversarial training (via GANs) with an encoder-only reward model to generate training samples of varying difficulty and better capture prompt-response relationships. This approach enables the model to progressively learn preferential prompts, leading to more accurate and reliable output control. Extensive experiments demonstrate that GAPO outperforms existing techniques like PPO, DPO, and KTO, especially in scenarios requiring precise constraint handling. The framework offers a robust solution for enhancing the fidelity and safety of LLM outputs, with code available on GitHub.","title":"GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization","url":"https://arxiv.org/abs/2503.20194"},{"content":"# Computer Science > Computation and Language\n\n**arXiv:2503.23503** (cs)\n\n\\[Submitted on 30 Mar 2025\\]\n\n# Title:Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models\n\nAuthors: [Sid Bharthulwar](https://arxiv.org/search/cs?searchtype=author&query=Bharthulwar,+S), [John Rho](https://arxiv.org/search/cs?searchtype=author&query=Rho,+J), [Katrina Brown](https://arxiv.org/search/cs?searchtype=author&query=Brown,+K)\n\nView a PDF of the paper titled Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models, by Sid Bharthulwar and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.23503) [HTML (experimental)](https://arxiv.org/html/2503.23503v1)\n\n> Abstract:We present a framework for optimizing prompts in vision-language models to elicit multimodal reasoning without model retraining. Using an evolutionary algorithm to guide prompt updates downstream of visual tasks, our approach improves upon baseline prompt-updating algorithms, which lack evolution-style \"survival of the fittest\" iteration. Crucially, we find this approach enables the language model to independently discover progressive problem-solving techniques across several evolution generations. For example, the model reasons that to \"break down\" visually complex spatial tasks, making a tool call to a Python interpreter to perform tasks (such as cropping, image segmentation, or saturation changes) would improve performance significantly. Our experimentation shows that explicitly evoking this \"tool calling\" call, via system-level XML $...\\\\texttt{} ... \\\\texttt{}...$ tags, can effectively flag Python interpreter access for the same language model to generate relevant programs, generating advanced multimodal functionality. This functionality can be crystallized into a system-level prompt that induces improved performance at inference time, and our experimentation suggests up to $\\\\approx 50\\\\%$ relative improvement across select visual tasks. Downstream performance is trained and evaluated across subtasks from MathVista, M3CoT, and GeoBench-VLM datasets. Importantly, our approach shows that evolutionary prompt optimization guides language models towards self-reasoning discoveries, which result in improved zero-shot generalization across tasks.\n\n| | |\n| --- | --- |\n| Comments: | Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs |\n| Subjects: | Computation and Language (cs.CL) |\n| Cite as: | [arXiv:2503.23503](https://arxiv.org/abs/2503.23503) \\[cs.CL\\] |\n| (or [arXiv:2503.23503v1](https://arxiv.org/abs/2503.23503v1) \\[cs.CL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.23503](https://doi.org/10.48550/arXiv.2503.23503) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Katrina Brown \\[ [view email](https://arxiv.org/show-email/1bc61d83/2503.23503)\\] **\\[v1\\]**\nSun, 30 Mar 2025 16:25:45 UTC (1,919 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models, by Sid Bharthulwar and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.23503)\n- [HTML (experimental)](https://arxiv.org/html/2503.23503v1)\n- [TeX Source](https://arxiv.org/src/2503.23503)\n- [Other Formats](https://arxiv.org/format/2503.23503)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[< prev](https://arxiv.org/prevnext?id=2503.23503&function=prev&context=cs.CL)  \\|  [next >](https://arxiv.org/prevnext?id=2503.23503&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2025-03](https://arxiv.org/list/cs.CL/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.23503?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.23503)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.23503)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.23503)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.23503) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.23503","id":"https://arxiv.org/abs/2503.23503","publishedDate":"2025-03-30T00:00:00.000Z","summary":"The paper introduces an evolutionary prompt optimization framework that enhances vision-language models' multimodal reasoning capabilities without retraining. By iteratively refining prompts through an evolutionary algorithm, the approach enables models to autonomously develop advanced problem-solving strategies, such as calling external tools like Python interpreters for complex spatial tasks. Explicitly prompting models to invoke tools via system-level tags significantly improves performance—up to approximately 50%—across various visual tasks. The method demonstrates that evolutionary prompt tuning guides models toward self-discovered reasoning techniques, leading to better zero-shot generalization on datasets like MathVista, M3CoT, and GeoBench-VLM. This work highlights a promising avenue for improving multimodal reasoning in language models through prompt evolution and tool integration.","title":"Evolutionary Prompt Optimization Discovers Emergent Multimodal Reasoning Strategies in Vision-Language Models","url":"https://arxiv.org/abs/2503.23503"},{"content":"# [ArXivIQ](https://arxiviq.substack.com/)\n\nSubscribeSign in\n\n# GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\n\nJul 31, 2025\n\n3\n\n[2](https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution/comments)\n\n[Share](javascript:void(0))\n\n**Authors:** _Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab_ **Paper:** [https://arxiv.org/abs/2507.19457](https://arxiv.org/abs/2507.19457)\n\n## TL;DR\n\n**What was done?** The authors introduced GEPA (Genetic-Pareto), a novel algorithm for optimizing prompts in complex, multi-module AI systems. Instead of relying on traditional reinforcement learning (RL), GEPA employs a language-driven, evolutionary approach. Its core innovation is \"reflective prompt mutation,\" where an LLM analyzes its own performance—including reasoning steps, tool usage, and detailed evaluation feedback—in natural language to diagnose failures and propose targeted improvements to its instructional prompts. This process is guided by a genetic algorithm that uses Pareto selection to maintain a diverse set of high-performing prompts, preventing the optimizer from getting stuck in local optima.\n\n**Why it matters?** This work signals a potential paradigm shift in how we optimize LLM-based agents. GEPA demonstrates that learning through language-based self-reflection is dramatically more sample-efficient than learning from sparse, scalar rewards. It outperforms the RL method GRPO by an average of 10% while using up to 35x fewer \"rollouts\" (system executions). It also surpasses the state-of-the-art prompt optimizer MIPROv2 ( [https://aclanthology.org/2024.emnlp-main.525/](https://aclanthology.org/2024.emnlp-main.525/)), and surprisingly shows that evolving detailed instructions alone can be more effective than optimizing few-shot examples. This approach makes adapting powerful AI systems far more practical and affordable, especially in settings where data is scarce or system executions are expensive.\n\n## Details\n\n### The High Cost of Learning by Doing\n\nOptimizing the performance of sophisticated AI agents—systems that combine multiple LLM modules, tool calls, and complex logic—is a central challenge in modern AI. A popular approach has been reinforcement learning (RL), where an agent learns through trial and error, guided by a scalar reward signal. However, this method often proves to be a brute-force endeavor, requiring tens or even hundreds of thousands of system executions (\"rollouts\") to achieve meaningful improvements. This high sample cost is a major bottleneck, making RL impractical for many real-world applications where each rollout may be computationally expensive, time-consuming, or financially costly.\n\nA new paper from a large collaboration of researchers across UC Berkeley, Stanford, Databricks, and MIT challenges this paradigm. The authors argue that for systems built on Large Language Models (LLMs), the very language they process offers a far richer and more efficient learning medium than a simple numerical reward. Their proposed algorithm, GEPA (Genetic-Pareto), demonstrates that an AI system can learn more effectively by \"reflecting\" on its behavior in natural language, leading to a method that is not only more powerful but also vastly more efficient.\n\nArXivIQ is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\n\nSubscribe\n\n### GEPA: Learning by Reflective Evolution\n\nGEPA introduces a novel optimization framework built on three key principles: Genetic Prompt Evolution, Natural Language Reflection, and Pareto-based Candidate Selection (Figure 3).\n\n1. **Natural Language Reflection:** At the heart of GEPA is its ability to learn from detailed, textual feedback. Instead of just receiving a score, the system's entire execution trace is fed back to an LLM for analysis. This feedback isn't just a number; it's a rich, textual record of the entire process, including the LLM's own reasoning chains, the specific tool calls it made, and even detailed diagnostic information from the evaluation environment, like compiler error messages or failed unit tests. This \"reflector\" LLM analyzes the textual feedback to diagnose what went wrong (or right) and proposes specific, targeted edits to the system's instructional prompts.\n\nAn example of a GEPA-generated prompt (Figure 2) reveals a level of detail and strategic insight far beyond a simple instruction, including sections on \"Key Observations,\" \"Purpose and Context,\" and \"Practical Strategy.\"\n\n2. **Genetic-Pareto Optimization:** To guide this learning process, GEPA employs an evolutionary search strategy. It maintains a pool of candidate prompt sets and iteratively \"mutates\" them based on the insights from the reflection step. To avoid the common pitfall of getting stuck in a local optimum, GEPA uses a strategy akin to an \"illumination\" search, known as Pareto-based selection (Algorithm 2).\n\nInstead of just greedily climbing the single highest peak it can find, GEPA tries to illuminate the entire mountain range by identifying candidates that are the best for at least one specific problem instance. This encourages diverse strategies and ultimately leads to a more robust and general solution. The impact of this choice is starkly visualized in the paper, showing how Pareto selection leads to a balanced, exploratory search tree compared to the narrow, stalled search of a naive greedy approach (Figure 6).\n\n\n### A New Benchmark for Sample Efficiency and Performance\n\nThe experimental results are compelling, showcasing GEPA's strengths across four diverse tasks— [HotpotQA](https://arxiv.org/abs/1809.09600), IFBench, [HoVer](https://aclanthology.org/2020.findings-emnlp.309/), and [PUPA](https://aclanthology.org/2025.naacl-long.173/) — and two different LLMs (Qwen3 8B and GPT-4.1 Mini).\n\n- **Outperforming Reinforcement Learning:** GEPA outperforms the RL baseline GRPO by an average of 10%, with gains of up to 19% on specific tasks. Most impressively, it achieves this while using up to **35 times fewer rollouts** (Table 1).\n\nThe learning curves (Figure 1) clearly show GEPA reaching superior performance far more rapidly than GRPO.\n\n- **Surpassing State-of-the-Art Prompt Optimization:** GEPA also consistently outperforms MIPROv2, a leading joint instruction and few-shot prompt optimizer. It more than doubles the aggregate performance gains seen with MIPROv2 and achieves this with prompts that are, on average, 33% shorter (Figure 15), translating directly to lower inference costs.\n\n- **The Power of Instructions:** A particularly surprising finding is that GEPA's instruction-only optimization outperforms MIPROv2's joint optimization of both instructions and few-shot examples. This suggests that as LLMs become better at following complex instructions, evolving a detailed, reflective set of instructions may be a more powerful and efficient strategy than curating in-context examples.\n\n\nThe paper also presents promising preliminary results for using GEPA as an inference-time search strategy for highly technical domains like code optimization. When applied to generating CUDA and NPU kernels, GEPA was able to iteratively refine code based on compiler feedback to achieve significant performance improvements over strong baselines (Figure 7, 8).\n\n### Limitations and Future Directions\n\nThe authors are candid about the method's limitations. The boundary between prompt-based learning and traditional weight-based fine-tuning remains an open question; in data-abundant regimes, full fine-tuning may still have the upper hand. The paper also suggests that GEPA could be further improved by incorporating few-shot example optimization or by developing more sophisticated \"feedback engineering\" to extract the most valuable learning signals from system traces.\n\nA key avenue for future work is the integration of reflective prompt evolution with weight-space adaptation. A hybrid approach, where GEPA's language-based insights guide more efficient RL or fine-tuning rollouts, could unify these paradigms and lead to even greater performance and efficiency.\n\n### Conclusion\n\n\"GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning\" presents a significant and practical advance in the field of AI system optimization. By treating language not just as an interface but as the primary medium for learning and reflection, the authors have developed a method that is demonstrably more sample-efficient, performant, and cost-effective than existing approaches.\n\nUltimately, GEPA's success suggests that the future of AI optimization may lie less in brute-force statistical methods and more in endowing our systems with the capacity for self-reflection. By learning from language, not just from numbers, GEPA is a significant step towards creating AI that improves not just by doing, but by understanding. This paper is a valuable contribution and a recommended read for anyone working on the frontier of LLM-based systems and agentic AI.\n\nArXivIQ is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\n\nSubscribe\n\n3\n\n[2](https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution/comments)\n\n[Share](javascript:void(0))\n\n#### Discussion about this post\n\nCommentsRestacks\n\n[Rod](https://substack.com/profile/27495567-rod?utm_source=substack-feed-item)\n\n[Jul 31](https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution/comment/140849637)\n\n\"the future of AI optimization may lie less in brute-force statistical methods and more in endowing our systems with the capacity for self-reflection. \"\n\nCounterpoint: Language allows sufficient dimensionality to escape local minimums when using inefficient but expansive search algorithms like genetic algorithms. I","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiviq.substack.com%2Fp%2Fgepa-reflective-prompt-evolution","id":"https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage discusses GEPA (Genetic-Pareto), a new algorithm for optimizing prompts in complex AI systems that leverage large language models (LLMs). Unlike traditional reinforcement learning, GEPA employs an evolutionary, language-driven approach where the system \"reflects\" on its own performance through natural language analysis—examining reasoning, tool use, and feedback—to diagnose and improve prompts. Guided by a Pareto-based genetic algorithm, GEPA maintains diverse high-quality prompts, avoiding local optima. The method significantly outperforms reinforcement learning (e.g., GRPO) by being more sample-efficient—requiring up to 35 times fewer system executions—and surpasses existing prompt optimization techniques like MIPROv2. This approach demonstrates that evolving detailed instructions via language reflection can be more effective than optimizing few-shot examples, making AI system adaptation more practical and cost-effective, especially when data or computational resources are limited.","title":"GEPA: Reflective Prompt Evolution Can Outperform Reinforcement ...","url":"https://arxiviq.substack.com/p/gepa-reflective-prompt-evolution"}],"status":"completed","toolCallId":"call_EHLyz9vQ4DuAe23LAwybLCQj","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Here are the main criticisms and open questions I’ve seen raised (and that are reasonable to raise) about GEPA, along with what the paper itself acknowledges as limitations.\n\nMethodological and empirical criticisms\n- Benchmark scope and external validity\n  - Critique: The paper evaluates on four tasks (HotpotQA, HoVer, IFBench, PUPA) with two base LLMs. These are good, but still a narrow slice of agentic workloads (e.g., complex planning, long-horizon tool use with partial observability, safety-constrained dialog, code agents with long dependency chains).\n  - Why it matters: GEPA’s advantage may hinge on richly textual traces. In settings with less readable trajectories (multimodal, binary outcomes, no tool logs) or highly stochastic environments, benefits may shrink.\n\n- RL baseline choice and setup\n  - Critique: The main RL baseline is GRPO at a fixed (large) rollout budget. Stronger or more tailored RL variants (better credit assignment, curriculum, or trajectory-level rewards; model-assisted reward rationales) might close the gap. Also, if GEPA leverages more information than the scalar reward (logs, rationales), the comparison isn’t apples-to-apples.\n  - Why it matters: If RL had access to the same rich textual signals (e.g., learned reward models over rationales), results could differ.\n\n- Reflection quality and evaluator leakage\n  - Critique: GEPA’s reflection can read evaluator messages, tool errors, and sometimes rationales. If those carry answer-key information or overly specific hints, the optimizer risks “teaching to the test” (spec leakage).\n  - Why it matters: Overly informative feedback can inflate gains and reduce true generalization.\n\n- Pareto selection stability\n  - Critique: Multi-objective Pareto fronts can be sensitive to metric noise and dataset slicing. Small validation sets or noisy metrics may preserve spurious “strategies,” increasing variance and selection instability.\n  - Why it matters: Could lead to brittle prompts that don’t survive to production.\n\n- Noisy, self-referential edits\n  - Critique: The same model family that executes tasks also generates reflections and edits. This can produce confident but wrong rules, compounding errors (“self-justification loops”).\n  - Why it matters: Without strong guardrails, the population can drift into brittle heuristics.\n\n- Compute/accounting fairness\n  - Critique: GEPA uses fewer rollouts, but each rollout may include extra LLM calls for reflection and candidate generation. If total token and wall-clock budgets are fully equalized, advantage might narrow.\n  - Why it matters: Real cost is tokens × calls × tool usage, not just “rollouts.”\n\n- Limited ablations on what actually helps\n  - Critique: The paper highlights instruction edits’ impact, but fewer ablations isolate contributions of: reflection prompt design, module-level vs monolithic edits, recombination policy, Pareto size, or logging granularity.\n  - Why it matters: Practitioners need to know which knobs matter most.\n\nConceptual/theoretical criticisms\n- Reliance on language as a learning substrate\n  - Critique: GEPA assumes problems, traces, and fixes are well-expressible in text. For low-level control, token-level shaping, or continuous-action domains, prompts may be too blunt; weight-space training may be required.\n  - Why it matters: Limits the method’s domain of applicability.\n\n- Interpretability vs correctness\n  - Critique: Textual “rules” feel interpretable, but their correctness and coverage aren’t guaranteed. Human-readable doesn’t mean causally valid.\n  - Why it matters: Can create a false sense of safety/auditability.\n\n- Genetic diversity vs convergence\n  - Critique: Maintaining many strategies helps exploration, but can slow convergence or complicate deployment if you end up with multiple specialized prompts that need routing or A/B logic.\n  - Why it matters: Operational complexity increases.\n\nPractical deployment criticisms\n- Governance and drift\n  - Critique: Frequent prompt changes can conflict with compliance, security review, or red-team sign-offs. Evolved prompts may slowly erode safety constraints unless audited rigorously.\n  - Why it matters: Enterprises often require strict change control.\n\n- Tooling and logging requirements\n  - Critique: GEPA’s effectiveness depends on rich, high-quality traces (tool I/O, evaluator rationales). Many production systems can’t expose such logs due to privacy, cost, or vendor limitations.\n  - Why it matters: Limits real-world applicability.\n\n- Portability across models\n  - Critique: The paper claims some cross-model transfer, but models differ in instruction following and tool-use priors. Edits that help one model may not translate cleanly.\n  - Why it matters: Reduces reuse across providers or versions.\n\nSecurity and safety criticisms\n- Speculative overfitting to evaluator quirks\n  - Critique: If evaluators are automated or heuristic, GEPA may evolve prompts that “game” evaluators (e.g., stylistic artifacts) rather than improve true task performance.\n  - Why it matters: Inflated offline gains, weaker human-judged quality.\n\n- Prompt injection and robustness\n  - Critique: More elaborate prompts can expand the attack surface for injection or jailbreaks. GEPA may inadvertently remove defensive phrasing while chasing accuracy/cost.\n  - Why it matters: Safety regressions are costly.\n\nWhat the paper itself acknowledges\n- Domain limits: Weight-space methods may win in data-rich regimes or for fine-grained token-level control where prompts are insufficient.\n- Reflection dependence: The quality of lessons depends on trace richness and reflector prompting.\n- Future work: Hybridization with weight finetuning/RL using GEPA’s distilled rules, better feedback engineering, and integrating few-shot example optimization.\n\nHow to mitigate these concerns in practice\n- Use a strict data hygiene and de-leakage protocol: separate train/val/test, mask gold answers from logs, limit evaluator rationales, and run held-out human evals.\n- Equalize total cost, not just rollouts: compare at matched token budgets and latency.\n- Add cross-model and cross-slice validation: test evolved prompts on different models and out-of-domain slices.\n- Enforce safety invariants: maintain a locked safety block that edits cannot touch; run safety regression tests each generation.\n- Keep edits small and auditable: require rationales linked to concrete failures; auto-ablate edits before merging.\n- Control Pareto complexity: cap frontier size, add tie-breaking with stability metrics, and periodically prune.\n\nBottom line\n- GEPA is a strong, practical alternative to RL when you can harvest rich textual traces and need sample efficiency, but its advantages are likely to shrink when traces are sparse, evaluators leak structure, or when problems require weight-level adaptation. Careful evaluation design (cost parity, leak controls, human evals) and safety governance are essential to avoid overstating gains or introducing regressions.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_01b028bf0b651bfa0068caf155d7dc819684b17288580fa16c"}},"resumableStreamId":"STREAM:a64cc7e0-d008-4d42-95cf-0acf24df99fd","role":"assistant","status":"done","threadId":"5e763140-e699-40d6-970e-751838959582","timeToFirstToken":1.264,"tokens":1437,"tokensPerSecond":25.066285235835892,"updated_at":1758130502261,"userId":"google:104458561188320708185","id":"j9729wgz8hez7gzfvz5hczfced7qsgkn"}],"version":"6.0.0"}