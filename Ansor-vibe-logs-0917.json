{"threads":[{"_creationTime":1758131208369.9058,"_id":"jd76ee1w7a6fh1mx0geq12dc1x7qrfcy","createdAt":1758131208370,"generationStatus":"completed","lastMessageAt":1758133741625,"model":"gpt-5-chat","pinned":false,"threadId":"6083fcc5-0473-4553-8df2-fd08145846b5","title":"Ansor Paper Explanation","updatedAt":1758133741625,"userId":"google:104458561188320708185","userSetTitle":false,"visibility":"visible","id":"6083fcc5-0473-4553-8df2-fd08145846b5","last_message_at":1758133741625,"created_at":1758131208370,"updated_at":1758133741625,"status":"completed","user_edited_title":false}],"messages":[{"_creationTime":1758131208369.906,"_id":"j971xcyrn7b26kdg6rx0ffkcz17qs367","attachmentIds":[],"content":"https://arxiv.org/abs/2006.06762\nAnsor: Generating High-Performance Tensor Programs for Deep Learning\n\nExplain this paper to me in depth using the Feynman technique, as if you were its author.","created_at":1758131208370,"messageId":"b847d2bf-e0de-4c98-b903-e9942691937e","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"6083fcc5-0473-4553-8df2-fd08145846b5","updated_at":1758131208370,"userId":"google:104458561188320708185","id":"j971xcyrn7b26kdg6rx0ffkcz17qs367","providerMetadata":{}},{"_creationTime":1758131208369.9062,"_id":"j97e9pyaxm0dyt2vcy9htdqkk57qs70y","attachmentIds":[],"byok":false,"content":"Imagine you want to run deep learning models fast on any hardware—CPUs, GPUs, ARM phones, whatever. At the lowest level, that means turning each operator (conv2d, matmul, layernorm, etc.) into a “tensor program” that uses the hardware well: blocking for caches, vectorizing, unrolling, parallelizing, using tensor cores, and so on. Getting this right by hand is painfully hard and never-ending—every operator shape and every device has its own sweet spot. Traditional solutions either:\n\n- Depend on vendor libraries (cuDNN, oneDNN). Great when they cover your operator/shape, but brittle and slow to extend to new ops.\n- Use search, but with tight constraints: template-based methods like AutoTVM need human-written schedule templates per op/backend and usually explore a small, biased space; other auto-schedulers prune too early by evaluating incomplete programs and thus miss winning combinations.\n\nAnsor is our attempt to break that bottleneck. If I explain it like I’m teaching a curious colleague (Feynman style), the core ideas reduce to three questions:\n\n1) What exactly are we searching over?\n2) How do we explore such a huge space efficiently?\n3) How do we allocate limited tuning time across all the subgraphs of a whole DNN so end-to-end latency actually improves?\n\nAnswering them one by one:\n\n1) The search space: hierarchical “sketches,” then details\n- Start from the pure mathematical definition of an operator (tensor expression). Example: matmul defines C[i, j] = sum_k A[i, k] * B[k, j].\n- A good implementation is a composition of choices: tiling loops, reordering, fusing, caching into shared/local memory, vectorizing, unrolling, binding to threads/warps/blocks, tensorization, etc. The combinations explode.\n- We separate the space into two layers:\n  - High-level structures (sketches): Think of these as blueprints that encode the structure most responsible for big performance differences. For a conv, a sketch might decide the tiling hierarchy, where reduction happens, whether to cache reads/writes, and which axes bind to CUDA blocks/warps. Sketch rules are general—derived from static analysis of the tensor expression graph—so we don’t hand-write per-op templates. We enumerate multiple plausible sketches automatically.\n  - Low-level details (attrs): Once we pick a sketch, we still need to choose numeric parameters: tile sizes, unroll factors, vector widths, shared-memory layout, etc. These are sampled and later refined.\n- This split is crucial: it keeps coverage large (we don’t prune away entire families up front) while making sampling tractable.\n\nWhy this is different from template-based approaches:\n- Templates bake in one family of schedules per op (often a human’s educated guess).\n- Our sketch rules are generic and compose across ops. That lets us cover far more structures, including ones humans didn’t pre-encode—so we can find programs outside prior search spaces.\n\n2) Exploration: sample, measure, learn, evolve\n- We never evaluate “partial” schedules (which biases search); we always generate complete programs you can compile and run.\n- Pipeline:\n  - Initial sampling: For each task (operator or fused subgraph), generate several sketches; for each sketch, sample multiple concrete schedules by filling in details.\n  - Cost model: We train a learned cost model (supervised regression) to predict runtime from schedule features. It starts cold, gets better as we measure more candidates, and guides us toward promising regions. Features summarize loop structure, memory traffic, reuse, parallel mappings, vectorization, etc.\n  - Evolutionary search: We refine candidates with mutation and crossover over schedule parameters, using the cost model as a fitness proxy. This helps fine-tune tile sizes, unrolls, vector widths, and bindings that interact subtly with each other and the hardware.\n  - Measurement loop: We periodically select high-scoring candidates, compile them, run them on the real device, record true latencies, and update the cost model. Rinse and repeat until the time budget is used or convergence.\n\nKey effect:\n- Because we always consider complete programs and let the model learn from real measurements, we avoid early pruning errors and can recover surprising combinations (e.g., unusual tile shapes or caching choices) that actually win.\n\n3) Whole-model scheduling: prioritize what matters\n- A DNN compiles into many “tasks” after graph fusion (e.g., dozens of fused operator subgraphs).\n- Tuning every task evenly wastes time. Instead, we use a task scheduler that allocates the tuning budget dynamically:\n  - Estimate how much improving a task would reduce end-to-end latency (sensitivity), combining current per-task performance with its frequency/criticality in the graph.\n  - Use a gradient-descent-like allocation to pick the next task to tune—the one with the highest marginal payoff.\n- Practically, this means we spend more time on big matmuls and dominant fused convs, less on small tails—so the overall model speed improves faster.\n\nConcrete contributions\n- Hierarchical search space construction from tensor expressions (sketch rules + parameterized refinements), eliminating per-op manual templates.\n- Evolutionary search guided by a learned cost model over complete schedules.\n- A task scheduler that optimizes end-to-end latency, not just per-op microbenchmarks.\n\nWhat this buys you (results)\n- Across Intel CPUs, ARM CPUs, and NVIDIA GPUs, Ansor finds implementations that:\n  - Beat prior auto-tuners and match or surpass vendor libraries when they are not ideally tuned for your exact shapes.\n  - Discover winners outside existing search spaces.\n  - Improve end-to-end DNN latency by up to roughly 3.8x (Intel), 2.6x (ARM), and 1.7x (NVIDIA) versus state-of-the-art baselines at the time.\n- Despite a larger search space, the better cost model and task scheduler make search time competitive or faster to a good solution.\n\nHow it looks from a user’s perspective\n- Provide the operator definitions (tensor expressions) or the model in TVM’s Relay.\n- Ansor auto-generates the search space; you don’t write schedule templates.\n- Give it a time budget. It will:\n  - Partition the model into fused tasks,\n  - Prioritize tasks,\n  - Generate, predict, run, learn, and iterate,\n  - Emit tuned kernels that integrate into your model runtime.\n\nWhy it works (intuitions)\n- Structure matters most: Choosing the right loop nest, cache placement, and parallel mapping explains a big chunk of performance. Sketch rules get those structural differences on the table.\n- Fine-tuning matters next: Hardware prefers very specific tile/unroll/vector sizes. Evolutionary search plus a learned cost model can quickly zoom in on those.\n- End-to-end matters always: Optimizing the wrong tasks wastes time; the scheduler prevents that.\n\nWhat it doesn’t assume\n- It doesn’t assume a fixed set of operator-specific templates or hardware-specific hand tuning. The same mechanism applies to new ops as long as they’re expressible as tensor computations.\n- It doesn’t rely on analytical models alone; it grounds predictions in live measurements.\n\nLimitations and future directions\n- Tuning still costs time; you trade compute time now for faster inference later. We mitigate with a good cost model and scheduling, but there’s still a budget to plan.\n- Extremely dynamic shapes and control flow need special handling; later TVM work extends support here.\n- Specialized intrinsics (e.g., tensor cores, AMX) require rule support to get fully exploited; we and the community keep adding these.\n- Sparse and low-precision operators were identified as natural next steps; subsequent efforts have pushed in that direction.\n\nMental model: a compiler that learns\n- Think of Ansor as a compiler that tailors schedules by trial, measurement, and learned intuition, not by human-crafted templates. It searches over structures and parameters, learns from real runs, and focuses its effort where it most improves your end-to-end model.\n\nIf you want, I can map these ideas to a concrete operator (e.g., depthwise conv2d on a GPU) and walk through how sketches and mutations evolve into a fast kernel.","created_at":1758131208371,"messageId":"d56d5e18-6bd2-4fdc-9b17-3292899dc6b8","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":null,"queries":["Ansor Generating High-Performance Tensor Programs for Deep Learning arXiv 2006.06762 summary key contributions auto-scheduler TVM 2020","Auto-scheduler Ansor TVM paper details sketch rule cost model evolutionary search"]},"result":[{"content":"# Computer Science > Machine Learning\n\n**arXiv:2006.06762** (cs)\n\n\\[Submitted on 11 Jun 2020 ( [v1](https://arxiv.org/abs/2006.06762v1)), last revised 15 Oct 2023 (this version, v5)\\]\n\n# Title:Ansor: Generating High-Performance Tensor Programs for Deep Learning\n\nAuthors: [Lianmin Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng,+L), [Chengfan Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia,+C), [Minmin Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun,+M), [Zhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+Z), [Cody Hao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+C+H), [Ameer Haj-Ali](https://arxiv.org/search/cs?searchtype=author&query=Haj-Ali,+A), [Yida Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+Y), [Jun Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+J), [Danyang Zhuo](https://arxiv.org/search/cs?searchtype=author&query=Zhuo,+D), [Koushik Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen,+K), [Joseph E. Gonzalez](https://arxiv.org/search/cs?searchtype=author&query=Gonzalez,+J+E), [Ion Stoica](https://arxiv.org/search/cs?searchtype=author&query=Stoica,+I)\n\nView a PDF of the paper titled Ansor: Generating High-Performance Tensor Programs for Deep Learning, by Lianmin Zheng and 11 other authors\n\n[View PDF](https://arxiv.org/pdf/2006.06762)\n\n> Abstract:High-performance tensor programs are crucial to guarantee efficient execution of deep neural networks. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously challenging. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs. These approaches either require significant engineering effort to develop platform-specific optimization code or fall short of finding high-performance programs due to restricted search space and ineffective exploration strategy.\n> We present Ansor, a tensor program generation framework for deep learning applications. Compared with existing search strategies, Ansor explores many more optimization combinations by sampling programs from a hierarchical representation of the search space. Ansor then fine-tunes the sampled programs with evolutionary search and a learned cost model to identify the best programs. Ansor can find high-performance programs that are outside the search space of existing state-of-the-art approaches. In addition, Ansor utilizes a task scheduler to simultaneously optimize multiple subgraphs in deep neural networks. We show that Ansor improves the execution performance of deep neural networks relative to the state-of-the-art on the Intel CPU, ARM CPU, and NVIDIA GPU by up to $3.8\\\\times$, $2.6\\\\times$, and $1.7\\\\times$, respectively.\n\n| | |\n| --- | --- |\n| Comments: | OSDI 2020 |\n| Subjects: | Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Performance (cs.PF); Programming Languages (cs.PL); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2006.06762](https://arxiv.org/abs/2006.06762) \\[cs.LG\\] |\n| (or [arXiv:2006.06762v5](https://arxiv.org/abs/2006.06762v5) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2006.06762](https://doi.org/10.48550/arXiv.2006.06762) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Lianmin Zheng \\[ [view email](https://arxiv.org/show-email/6c34870d/2006.06762)\\] **[\\[v1\\]](https://arxiv.org/abs/2006.06762v1)**\nThu, 11 Jun 2020 19:40:09 UTC (955 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2006.06762v2)**\nMon, 15 Jun 2020 16:21:22 UTC (954 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2006.06762v3)**\nFri, 30 Oct 2020 18:32:44 UTC (2,666 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2006.06762v4)**\nWed, 4 Nov 2020 07:29:29 UTC (2,721 KB)\n**\\[v5\\]**\nSun, 15 Oct 2023 07:00:36 UTC (748 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Ansor: Generating High-Performance Tensor Programs for Deep Learning, by Lianmin Zheng and 11 other authors\n\n- [View PDF](https://arxiv.org/pdf/2006.06762)\n- [TeX Source](https://arxiv.org/src/2006.06762)\n- [Other Formats](https://arxiv.org/format/2006.06762)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[< prev](https://arxiv.org/prevnext?id=2006.06762&function=prev&context=cs.LG)  \\|  [next >](https://arxiv.org/prevnext?id=2006.06762&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2020-06](https://arxiv.org/list/cs.LG/2020-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2006.06762?context=cs) [cs.NE](https://arxiv.org/abs/2006.06762?context=cs.NE) [cs.PF](https://arxiv.org/abs/2006.06762?context=cs.PF) [cs.PL](https://arxiv.org/abs/2006.06762?context=cs.PL) [stat](https://arxiv.org/abs/2006.06762?context=stat) [stat.ML](https://arxiv.org/abs/2006.06762?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2006.06762)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2006.06762)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2006.06762)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2006.html#abs-2006-06762) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2006-06762)\n\n[Lianmin Zheng](https://dblp.uni-trier.de/search/author?author=Lianmin%20Zheng) [Zhao Wu](https://dblp.uni-trier.de/search/author?author=Zhao%20Wu) [Cody Hao Yu](https://dblp.uni-trier.de/search/author?author=Cody%20Hao%20Yu) [Yida Wang](https://dblp.uni-trier.de/search/author?author=Yida%20Wang) [Jun Yang](https://dblp.uni-trier.de/search/author?author=Jun%20Yang)\n\n…\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2006.06762) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2006.06762","id":"https://arxiv.org/abs/2006.06762","publishedDate":"2020-06-11T00:00:00.000Z","summary":"The paper introduces Ansor, a framework designed to generate high-performance tensor programs crucial for efficient deep learning model execution across various hardware platforms. Traditional methods rely on vendor libraries or limited search strategies, which often require extensive engineering effort or fail to find optimal solutions. Ansor addresses these challenges by exploring a vast search space through hierarchical sampling, combined with evolutionary algorithms and a learned cost model to fine-tune program performance. It can discover tensor programs that outperform existing approaches, achieving up to 3.8× speedup on Intel CPUs, 2.6× on ARM CPUs, and 1.7× on NVIDIA GPUs. Additionally, Ansor features a task scheduler that optimizes multiple subgraphs simultaneously, enhancing overall neural network performance. The framework demonstrates significant improvements in deep learning workloads, making it a valuable tool for efficient tensor program generation across diverse hardware.","title":"Generating High-Performance Tensor Programs for Deep Learning","url":"https://arxiv.org/abs/2006.06762"},{"content":"This paper is included in the Proceedings of the \n14th USENIX Symposium on Operating Systems \nDesign and Implementation\nNovember 4–6, 2020\n978-1-939133-19-9\nOpen access to the Proceedings of the \n14th USENIX Symposium on Operating \nSystems Design and Implementation \nis sponsored by USENIX\nAnsor: Generating High-Performance\nTensor Programs for Deep Learning\nLianmin Zheng, UC Berkeley; Chengfan Jia, Minmin Sun, and Zhao Wu, Alibaba Group; \nCody Hao Yu, Amazon Web Services, Inc; Ameer Haj-Ali, UC Berkeley; Yida Wang,\nAmazon Web Services; Jun Yang, Alibaba Group; Danyang Zhuo, UC Berkeley and\nDuke University; Koushik Sen, Joseph E. Gonzalez, and Ion Stoica, UC Berkeley\nhttps://www.usenix.org/conference/osdi20/presentation/zheng\nAnsor: Generating High-Performance Tensor Programs for Deep Learning\nLianmin Zheng 1, Chengfan Jia 2, Minmin Sun 2, Zhao Wu 2, Cody Hao Yu 3 ,\nAmeer Haj-Ali 1, Yida Wang 3, Jun Yang 2, Danyang Zhuo 1,4 ,\nKoushik Sen 1, Joseph E. Gonzalez 1, Ion Stoica 1\n1 UC Berkeley, 2Alibaba Group, 3Amazon Web Services, 4 Duke University\nAbstract\nHigh-performance tensor programs are crucial to guarantee\nefficient execution of deep neural networks. However, obtain\u0002ing performant tensor programs for different operators on\nvarious hardware platforms is notoriously challenging. Cur\u0002rently, deep learning systems rely on vendor-provided kernel\nlibraries or various search strategies to get performant tensor\nprograms. These approaches either require significant engi\u0002neering effort to develop platform-specific optimization code\nor fall short of finding high-performance programs due to\nrestricted search space and ineffective exploration strategy.\nWe present Ansor, a tensor program generation framework\nfor deep learning applications. Compared with existing search\nstrategies, Ansor explores many more optimization combina\u0002tions by sampling programs from a hierarchical representation\nof the search space. Ansor then fine-tunes the sampled pro\u0002grams with evolutionary search and a learned cost model to\nidentify the best programs. Ansor can find high-performance\nprograms that are outside the search space of existing state-of\u0002the-art approaches. In addition, Ansor utilizes a task scheduler\nto simultaneously optimize multiple subgraphs in deep neural\nnetworks. We show that Ansor improves the execution perfor\u0002mance of deep neural networks relative to the state-of-the-art\non the Intel CPU, ARM CPU, and NVIDIA GPU by up to\n3.8⇥, 2.6⇥, and 1.7⇥, respectively.\n1 Introduction\nLow-latency execution of deep neural networks (DNN) plays\na critical role in autonomous driving [14], augmented real\u0002ity [3], language translation [15], and other applications of\nAI. DNNs can be expressed as a directed acyclic compu\u0002tational graph (DAG), in which nodes represent the opera\u0002tors (e.g., convolution, matrix multiplication) and directed\nedges represent the dependencies between operators. Existing\ndeep learning frameworks (e.g., Tensorflow [1], PyTorch [39],\nMXNet [10]) map the operators in DNNs to vendor-provided\nkernel libraries (e.g., cuDNN [13], MKL-DNN [27]) to\nachieve high performance. However, these kernel libraries\nrequire significant engineering effort to manually tune for\neach hardware platform and operator. The significant manual\neffort required to produce efficient operator implementations\nfor each target accelerator limits the development and innova\u0002tion of new operators [7] and specialized accelerators [35].\nGiven the importance of DNNs’ performance, researchers\nand industry practitioners have turned to search-based com\u0002pilation [2, 11, 32, 49, 59] for automated generation of tensor\nprograms, i.e., low-level implementations of tensor operators.\nFor an operator or a (sub-)graph of multiple operators, users\ndefine the computation in a high-level declarative language\n(§2), and the compiler then searches for programs tailored\ntowards different hardware platforms.\nTo find performant tensor programs, it is necessary for a\nsearch-based approach to explore a large enough search space\nto cover all the useful tensor program optimizations. However,\nexisting approaches fail to capture many effective optimiza\u0002tion combinations, because they rely on either predefined\nmanually-written templates (e.g., TVM [12], FlexTensor [59])\nor aggressive pruning by evaluating incomplete programs\n(e.g., Halide auto-scheduler [2]), which prevents them from\ncovering a comprehensive search space (§2). The rules they\nuse to construct the search space are also limited.\nIn this paper, we explore a novel search strategy for gener\u0002ating high-performance tensor programs. It can automatically\ngenerate a large search space with comprehensive coverage of\noptimizations and gives every tensor program in the space a\nchance to be chosen. It thus enables to find high-performance\nprograms that existing approaches miss.\nRealizing this goal faces multiple challenges. First, it re\u0002quires automatically constructing a large search space to cover\nas many tensor programs as possible for a given computation\ndefinition. Second, we need to search efficiently without com\u0002paring incomplete programs in the large search space that can\nbe orders of magnitude larger than what existing templates\ncan cover. Finally, when optimizing an entire DNN with many\nsubgraphs, we should recognize and prioritize the subgraphs\nthat are critical to the end-to-end performance.\nUSENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 863\nTo this end, we design and implement Ansor, a framework\nfor automated tensor program generation. Ansor utilizes a\nhierarchical representation to cover a large search space. This\nrepresentation decouples high-level structures and low-level\ndetails, enabling flexible enumeration of high-level structures\nand efficient sampling of low-level details. The space is con\u0002structed automatically for a given computation definition.\nAnsor then samples complete programs from the search space\nand fine-tunes these programs with evolutionary search and a\nlearned cost model. To optimize the performance of DNNs\nwith multiple subgraphs, Ansor dynamically prioritizes sub\u0002graphs of the DNNs that are more likely to improve the end\u0002to-end performance.\nWe evaluate Ansor on both standard deep learning bench\u0002marks and emerging new workloads against manual libraries\nand state-of-the-art search-based frameworks. Experiment re\u0002sults show that Ansor improves the execution performance\nof DNNs on the Intel CPU, ARM CPU, and NVIDIA GPU\nby up to 3.8⇥, 2.6⇥, and 1.7⇥, respectively. For most com\u0002putation definitions, the best program found by Ansor is out\u0002side the search space of existing search-based approaches.\nThe results also show that, compared with existing search\u0002based approaches, Ansor searches more efficiently, generating\nhigher-performance programs in a shorter time, despite its\nlarger search space. Ansor can match the performance of a\nstate-of-the-art framework with an order of magnitude less\nsearch time. Besides, Ansor enables automatic extension to\nnew operators by only requiring their mathematical definitions\nwithout manual templates.\nIn summary, this paper makes the following contributions:\n• A mechanism to generate a large hierarchical search\nspace of tensor programs for a computational graph.\n• An evolutionary strategy with a learned cost model to\nfine-tune the performance of tensor programs.\n• A scheduling algorithm based on gradient descent to\nprioritize important subgraphs when optimizing the end\u0002to-end performance of DNNs.\n• An implementation and comprehensive evaluation of the\nAnsor system demonstrating that the above techniques\noutperform state-of-the-art systems on a variety of DNNs\nand hardware platforms.\n2 Background\nThe deep learning ecosystem is embracing a rapidly growing\ndiversity of hardware platforms including CPUs, GPUs, FP\u0002GAs, and ASICs. In order to deploy DNNs on these platforms,\nhigh-performance tensor programs are needed for the opera\u0002tors used in DNNs. The required operator set typically con\u0002tains a mixture of standard operators (e.g., matmul, conv2d)\nand novel operators invented by machine learning researchers\n(e.g., capsule conv2d [23], dilated conv2d [57]).\nC = compute((N, M), lambda i, j: sum(A[i, k]*B[k, j], [k]))\nMatrix Multiplication !\", % = ∑) (\", )*), %\nFigure 1: The computation definition of matrix multiplication.\nTo deliver portable performance of these operators on a\nwide range of hardware platforms in a productive way, multi\u0002ple compiler techniques have been introduced (e.g., TVM [11],\nHalide [41], Tensor Comprehensions [49]). Users define the\ncomputation in a form similar to mathematical expressions\nusing a high-level declarative language, and the compiler gen\u0002erates optimized tensor programs according to the definition.\nFigure 1 shows the computation definition of matrix multipli\u0002cation in the TVM tensor expression language. Users mainly\nneed to define the shapes of the tensors and how each element\nin the output tensor is computed.\nHowever, automatically generating high-performance ten\u0002sor programs from a high-level definition is extremely dif\u0002ficult. Depending on the architecture of the target platform,\nthe compiler needs to search in an extremely large and com\u0002plicated space containing combinatorial choices of optimiza\u0002tions (e.g., tile structure, tile size, vectorization, paralleliza\u0002tion). Finding high-performance programs requires the search\nstrategy to cover a comprehensive space and explore it effi\u0002ciently. We describe two recent and effective approaches in\nthis section and other related work in §8.\nTemplate-guided search. In template-guided search, the\nsearch space is defined by manual templates. As shown in Fig\u0002ure 2a, the compiler (e.g., TVM) requires the user to manually\nwrite a template for a computation definition. The template\ndefines the structure of the tensor programs with some tunable\nparameters (e.g., tile size and unrolling factor). The compiler\nthen searches for the best values of these parameters for a spe\u0002cifi","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.usenix.org%2Fsystem%2Ffiles%2Fosdi20-zheng.pdf","id":"https://www.usenix.org/system/files/osdi20-zheng.pdf","publishedDate":"2020-11-04T00:00:00.000Z","summary":"This paper introduces Ansor, a framework for automatically generating high-performance tensor programs crucial for efficient deep neural network (DNN) execution across diverse hardware platforms. Traditional methods rely on vendor-provided libraries or limited search strategies, which require extensive manual tuning or fail to find optimal solutions. Ansor improves upon these by exploring a vast search space through hierarchical sampling, fine-tuning candidate programs with evolutionary algorithms and a learned cost model. It also optimizes multiple subgraphs simultaneously via a task scheduler. Experimental results demonstrate that Ansor significantly outperforms existing approaches, achieving up to 3.8×, 2.6×, and 1.7× speedups on Intel CPUs, ARM CPUs, and NVIDIA GPUs, respectively. The framework addresses the challenge of automating high-performance tensor program generation, reducing manual effort, and enabling faster, more efficient DNN deployment across various hardware architectures.","title":"[PDF] Ansor: Generating High-Performance Tensor Programs for Deep ...","url":"https://www.usenix.org/system/files/osdi20-zheng.pdf"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F342169422_Ansor_Generating_High-Performance_Tensor_Programs_for_Deep_Learning","id":"https://www.researchgate.net/publication/342169422_Ansor_Generating_High-Performance_Tensor_Programs_for_Deep_Learning","publishedDate":null,"summary":"Failed to fetch content","title":"(PDF) Ansor : Generating High-Performance Tensor Programs for ...","url":"https://www.researchgate.net/publication/342169422_Ansor_Generating_High-Performance_Tensor_Programs_for_Deep_Learning"},{"content":"# Ansor: Generating High-Performance Tensor Programs for Deep Learning\n\nAuthors:\n\nLianmin Zheng, _UC Berkeley;_ Chengfan Jia, Minmin Sun, and Zhao Wu, _Alibaba Group;_ Cody Hao Yu, _Amazon Web Services;_ Ameer Haj-Ali, _UC Berkeley;_ Yida Wang, _Amazon Web Services;_ Jun Yang, _Alibaba Group;_ Danyang Zhuo, _UC Berkeley and Duke University;_ Koushik Sen, Joseph E. Gonzalez, and Ion Stoica, _UC Berkeley_\n\nAbstract:\n\nHigh-performance tensor programs are crucial to guarantee efficient execution of deep neural networks. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously challenging. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs.\nThese approaches either require significant engineering effort to develop platform-specific optimization code or fall short of finding high-performance programs due to restricted search space and ineffective exploration strategy.\n\nWe present Ansor, a tensor program generation framework for deep learning applications.\nCompared with existing search strategies, Ansor explores many more optimization combinations by sampling programs from a hierarchical representation of the search space. Ansor then fine-tunes the sampled programs with evolutionary search and a learned cost model to identify the best programs. Ansor can find high-performance programs that are outside the search space of existing state-of-the-art approaches. In addition, Ansor utilizes a task scheduler to simultaneously optimize multiple subgraphs in deep neural networks. We show that Ansor improves the execution performance of deep neural networks relative to the state-of-the-art on the Intel CPU, ARM CPU, and NVIDIA GPU by up to 3.8x, 2.6x, and 1.7x, respectively.\n\n## [Lianmin Zheng, UC Berkeley](https://www.usenix.org/conference/osdi20/speaker-or-organizer/lianmin-zheng-uc-berkeley)\n\n## [Chengfan Jia, Alibaba Group](https://www.usenix.org/conference/osdi20/speaker-or-organizer/chengfan-jia-alibaba-group)\n\n## [Minmin Sun, Alibaba Group](https://www.usenix.org/conference/osdi20/speaker-or-organizer/minmin-sun-alibaba-group)\n\n## [Zhao Wu, Alibaba Group](https://www.usenix.org/conference/osdi20/speaker-or-organizer/zhao-wu-alibaba-group)\n\n## [Cody Hao Yu, Amazon Web Services](https://www.usenix.org/conference/osdi20/speaker-or-organizer/cody-hao-yu-amazon-web-services)\n\n## [Ameer Haj-Ali, UC Berkeley](https://www.usenix.org/conference/osdi20/speaker-or-organizer/ameer-haj-ali-uc-berkeley)\n\n## [Yida Wang, Amazon Web Services](https://www.usenix.org/conference/osdi20/speaker-or-organizer/yida-wang-amazon-web-services)\n\n## [Jun Yang, Alibaba Group](https://www.usenix.org/conference/osdi20/speaker-or-organizer/jun-yang-alibaba-group)\n\n## [Danyang Zhuo, UC Berkeley and Duke University](https://www.usenix.org/conference/osdi20/speaker-or-organizer/danyang-zhuo-uc-berkeley-and-duke-university)\n\n## [Koushik Sen, UC Berkeley](https://www.usenix.org/conference/osdi20/speaker-or-organizer/koushik-sen-uc-berkeley)\n\n## [Joseph E. Gonzalez, UC Berkeley](https://www.usenix.org/conference/osdi20/speaker-or-organizer/joseph-e-gonzalez-uc-berkeley)\n\n## [Ion Stoica, UC Berkeley](https://www.usenix.org/conference/osdi20/speaker-or-organizer/ion-stoica-uc-berkeley)\n\n## Open Access Media\n\nUSENIX is committed to Open Access to the research presented at our events. Papers and proceedings are freely available to everyone once the event begins. Any video, audio, and/or slides that are posted after the event are also free and open to everyone. [Support USENIX](https://www.usenix.org/annual-fund) and our commitment to Open Access.\n\nBibTeX\n\n@inproceedings {258858,\n\nauthor = {Lianmin Zheng and Chengfan Jia and Minmin Sun and Zhao Wu and Cody Hao Yu and Ameer Haj-Ali and Yida Wang and Jun Yang and Danyang Zhuo and Koushik Sen and Joseph E. Gonzalez and Ion Stoica},\n\ntitle = {Ansor: Generating {High-Performance} Tensor Programs for Deep Learning},\n\nbooktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},\n\nyear = {2020},\n\nisbn = {978-1-939133-19-9},\n\npages = {863--879},\n\nurl = {https://www.usenix.org/conference/osdi20/presentation/zheng},\n\npublisher = {USENIX Association},\n\nmonth = nov\n\n}\n\n[Download](https://www.usenix.org/biblio/export/bibtex/258858)\n\n![PDF icon](https://www.usenix.org/modules/file/icons/application-pdf.png)[Zheng PDF](https://www.usenix.org/system/files/osdi20-zheng.pdf)\n\n[View the slides](https://www.usenix.org/sites/default/files/conference/protected-files/osdi20_slides_zheng.pdf)\n\n![](https://www.usenix.org/sites/default/files/usenix_artifact_evaluation_evaluated_125.png)\n\n## Presentation Video  [iframe](http://www.youtube.com/embed/A2hJ_Mj02zk?width%3D745%26amp%3Bheight%3D420%26amp%3Btheme%3Ddark%26amp%3Bautoplay%3D0%26amp%3Bvq%3Dhd720%26amp%3Brel%3D0%26amp%3Bshowinfo%3D0%26amp%3Bmodestbranding%3D0%26amp%3Biv_load_policy%3D1%26amp%3Bcontrols%3D1%26amp%3Bautohide%3D2%26amp%3Bwmode%3Dopaque)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.usenix.org%2Fconference%2Fosdi20%2Fpresentation%2Fzheng","id":"https://www.usenix.org/conference/osdi20/presentation/zheng","publishedDate":"2020-01-01T00:00:00.000Z","summary":"Ansor is a framework designed to generate high-performance tensor programs for deep learning across various hardware platforms, including CPUs and GPUs. It addresses the challenge of optimizing tensor computations by exploring a vast search space of optimization strategies through hierarchical sampling, combined with evolutionary search and a learned cost model to identify the best programs. Unlike traditional methods relying on vendor libraries or limited search strategies, Ansor can discover programs outside existing search spaces, leading to significant performance improvements—up to 3.8x on Intel CPUs, 2.6x on ARM CPUs, and 1.7x on NVIDIA GPUs. Additionally, it employs a task scheduler to optimize multiple subgraphs simultaneously, enhancing overall neural network execution efficiency.","title":"Ansor: Generating High-Performance Tensor Programs for Deep Learning","url":"https://www.usenix.org/conference/osdi20/presentation/zheng"},{"content":"# Introducing TVM Auto-scheduler (a.k.a. Ansor)\n\nMar 3, 2021\n• Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu\n\nOptimizing the execution speed of deep neural networks is extremely hard with the growing\nmodel size, operator diversity, and hardware heterogeneity.\nFrom a computational perspective, deep neural networks are just layers and layers of tensor computations.\nThese tensor computations, such as matmul and conv2d, can be easily described by mathematical expressions.\nHowever, providing high-performance implementations for them on modern hardware can be very challenging.\nWe have to apply various low-level optimizations and utilize special hardware intrinsics to achieve high performance.\nIt takes huge engineering effort to build linear algebra and neural network acceleration libraries like CuBLAS, CuDNN, oneMKL, and oneDNN.\n\nOur life will be much easier if we can just write mathematical expressions and have something\nmagically turn them into efficient code implementations.\nThree years ago, deep learning compiler TVM and its search module AutoTVM were built as the first step towards this goal.\nAutoTVM employs a template-based search algorithm to find efficient implementations for a given tensor computation.\nHowever, it is a template-based approach, so it still requires domain experts to implement a non-trivial manual template\nfor every operator on every platform.\nToday, there are more than 15k lines of code for these templates in the TVM code repository.\nBesides being very hard to develop, these templates often have inefficient and limited search spaces,\nmaking them unable to achieve optimal performance.\n\nTo address the limitations of AutoTVM, we started project Ansor aiming at a fully automated auto-scheduler for\ngenerating code for tensor computations.\nAnsor auto-scheduler only takes tensor expressions as input and generates high-performance code without manual templates.\nWe made innovations in the search space construction and search algorithm.\nAs a result, the auto-scheduler can achieve better performance with less search time in a more automated way.\n\nAnsor auto-scheduler is now integrated into Apache TVM as `tvm.auto_scheduler` package.\nThis is a joint effort by collaborators from UC Berkeley, Alibaba, AWS and OctoML.\nDetailed tutorials are available for Intel CPUs, ARM CPUs, NVIDIA GPUs, and Mali GPUs on the TVM website \\[1\\].\nIn this blog post, we will give a high-level introduction and show some benchmark results.\n\n# System Overview\n\n## AutoTVM vs Auto-scheduler\n\nTable 1. Workflow Comparision\n\nTable 1 compares the workflow for generating code for an operator in AutoTVM and auto-scheduler.\nIn AutoTVM, the developer has to go through three steps.\nIn step 1, the developer has to write the compute definition in TVM’s tensor expression language.\nThis part is relatively easy because TVM’s tensor expression language looks just like math expressions.\nIn step 2, the developer has to write a schedule template, which typically consists of 20-100 lines of tricky DSL code.\nThis part requires domain expertise of both the target hardware architecture and operator semantics, so it is difficult.\nThe last step, step 3, is automated by a search algorithm.\n\nIn auto-scheduler, we eliminate the most difficult step 2 by automatic search space construction and accelerate step 3 with a better search algorithm.\nBy doing automatic search space construction, we not only eliminate huge manual effort,\nbut also enabling the exploration of much more optimization combinations.\nThis automation does not come for free, because we still need to design rules to generate the search space.\nHowever, these rules are very general. They are based on static analysis of the tensor expressions.\nWe only need to design a few general rules once and can apply them to almost all tensor computations in deep learning.\n\n## Search Process\n\nFigure 1. Search Process Overview\n\nFigure 1. shows the search process of auto-scheduler when optimizing a whole neural network.\nThe system takes deep learning models as input.\nIt then partitions the big model into small subgraphs with Relay’s operator fusion pass.\nA task scheduler is utilized to allocate the time resource for optimizing many subgraphs.\nAt each iteration, it picks a subgraph that has the most potential to increase the end-to-end performance.\nFor this subgraph, we analyze its tensor expression and generate several sketches for it.\nThen we run evolutionary search with a learned cost model to get a batch of optimized programs.\nThe optimized programs are sent to actual hardware for measurements.\nWhen the measurements are finished, the profiling results are used as feedback to update all components of the system.\nThis process is repeated iteratively until the optimization converges or we run out of time budget.\nMore technical details can be found in our paper \\[3\\] and our code.\n\nIt is worth notiing that since the auto-scheduler generates schedules from scratch,\nit reuses the existing computation definitions in TOPI but not schedule templates.\n\n# Benchmark Results\n\nIn this section, we benchmark the performance of AutoTVM and Auto-scheduler.\nThe CPU benchmark is done on an AWS c5.9xlarge, which is equipped with an Intel 18-core skylake 8124-m CPU.\nThe GPU benchmark is done on an AWS g4dn.4xlarge, which is equipped with an NVIDIA T4 GPU.\nAll benchmark code, raw data, tuning logs can be found in this repo \\[2\\].\n\n## Performance of the generated code\n\nWe benchmark the fp32 single-batch inference latency on three networks.\nFigure 2 shows the relative speedup of auto-scheduler against AutoTVM.\nWe can see auto-scheduler outperforms AutoTVM in all cases with 1.02x to 8.95x speedup.\nThis is because auto-scheduler explores a larger search space, which covers more efficient combinations\nof optimizations that are missed in TOPI manual templates.\nThe BERT-base@GPU is an extreme case where the manual templates are very badly designed.\nIn other words, the manual template for dense layers does not perform well for the shapes in BERT model.\n\nFigure 2. Code Performance Comparision (Higher is better)\n\n## Search Time\n\nAs we know, the search-based approaches can be very time-consuming, so we also care about the search time.\nIt typically takes several hours to let the search converge for a single neural network.\nFigure 3 compares the search time of AutoTVM and auto-scheduler.\nAuto-scheduler requires much less time to converge in most cases, despite its larger search space.\nThis is mainly because of auto-scheduler has a better cost model and task scheduler.\n\nFigure 3. Search Time Comparision (Lower is better)\n\n## More Results\n\nThe repo above serves as an internal benchmark tool for TVM, so it only compares the latest AutoTVM and AutoScheduler.\nYou can find results for more libraries and backends in our paper \\[3\\].\nRecently, this blog post \\[4\\] also tried auto-scheduler on an Apple M1 chip and got some good results.\n\n# Conclusion\n\nWe build TVM auto-scheduler, a system that automatically generates high-performance code for tensor expressions.\nCompared with the predecessor AutoTVM, auto-scheduler does not require manual templates.\nBesides, auto-scheduler is capable of generating schedules with better performance in a shorter time.\nWe achieve this by making innovations in the search space construction and search algorithm.\n\nWe are excited about the current performance of auto-scheduler.\nIn the future, we are interested in extending the ability of auto-scheduler to support\nsparse operators, low-precision operators, and dynamic shape better.\n\n# Links\n\n\\[1\\] Tutorials: [https://tvm.apache.org/docs/tutorials/index.html#autoscheduler-template-free-auto-scheduling](https://tvm.apache.org/docs/tutorials/index.html#autoscheduler-template-free-auto-scheduling)\n\\[2\\] Benchmark repo: [https://github.com/tlc-pack/TLCBench](https://github.com/tlc-pack/TLCBench)\n\\[3\\] OSDI Paper: [Ansor : Generating High-Performance Tensor Programs for Deep Learning](https://arxiv.org/abs/2006.06762)\n\\[4\\] Results on Apple M1 chip: [https://medium.com/octoml/on-the-apple-m1-beating-apples-core-ml-4-with-30-model-performance-improvements-9d94af7d1b2d](https://medium.com/octoml/on-the-apple-m1-beating-apples-core-ml-4-with-30-model-performance-improvements-9d94af7d1b2d).\n\n- ##### © 2024 Apache Software Foundation \\| All right reserved\n\n\n- Copyright © 2024 The Apache Software Foundation. Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Ftvm.apache.org%2F2021%2F03%2F03%2Fintro-auto-scheduler","id":"https://tvm.apache.org/2021/03/03/intro-auto-scheduler","publishedDate":"2021-03-03T00:00:00.000Z","summary":"The webpage introduces the TVM Auto-scheduler, also known as Ansor, a tool designed to automate and optimize the generation of high-performance code for tensor computations in deep neural networks. Traditional methods like AutoTVM required extensive manual effort, including writing complex schedule templates and domain expertise, which limited scalability and performance. Ansor addresses these challenges by automating the search space construction and employing advanced search algorithms, significantly reducing manual intervention and improving efficiency. Integrated into Apache TVM, Ansor enables users to generate optimized code directly from tensor expressions across various hardware platforms such as CPUs and GPUs, with minimal manual tuning. Developed through collaborations with UC Berkeley, Alibaba, AWS, and OctoML, Ansor streamlines deep learning model deployment by making high-performance code generation more accessible and effective.","title":"Introducing TVM Auto-scheduler (a.k.a. Ansor)","url":"https://tvm.apache.org/2021/03/03/intro-auto-scheduler"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdiscuss.tvm.apache.org%2Ft%2Frfc-ansor-an-auto-scheduler-for-tvm-autotvm-v2-0%2F7005","id":"https://discuss.tvm.apache.org/t/rfc-ansor-an-auto-scheduler-for-tvm-autotvm-v2-0/7005","publishedDate":"2020-06-17T00:00:00.000Z","summary":"Failed to fetch content","title":"[RFC] Ansor: An Auto-scheduler for TVM (AutoTVM v2.0)","url":"https://discuss.tvm.apache.org/t/rfc-ansor-an-auto-scheduler-for-tvm-autotvm-v2-0/7005"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fhomepages.dcc.ufmg.br%2F~michaelcanesche%2Fpaper%2Fbennu_paper.pdf","id":"https://homepages.dcc.ufmg.br/~michaelcanesche/paper/bennu_paper.pdf","publishedDate":null,"summary":"Failed to fetch content","title":"[PDF] A Unified Search Technique for the Ansor Optimizer - UFMG","url":"https://homepages.dcc.ufmg.br/~michaelcanesche/paper/bennu_paper.pdf"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F367548394_TensorIR_An_Abstraction_for_Automatic_Tensorized_Program_Optimization","id":"https://www.researchgate.net/publication/367548394_TensorIR_An_Abstraction_for_Automatic_Tensorized_Program_Optimization","publishedDate":"2025-08-17T00:00:00.000Z","summary":"Failed to fetch content","title":"An Abstraction for Automatic Tensorized Program Optimization","url":"https://www.researchgate.net/publication/367548394_TensorIR_An_Abstraction_for_Automatic_Tensorized_Program_Optimization"}],"status":"completed","toolCallId":"call_8Bl1s1G44zr954UBQsN7OFqI","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Imagine you want to run deep learning models fast on any hardware—CPUs, GPUs, ARM phones, whatever. At the lowest level, that means turning each operator (conv2d, matmul, layernorm, etc.) into a “tensor program” that uses the hardware well: blocking for caches, vectorizing, unrolling, parallelizing, using tensor cores, and so on. Getting this right by hand is painfully hard and never-ending—every operator shape and every device has its own sweet spot. Traditional solutions either:\n\n- Depend on vendor libraries (cuDNN, oneDNN). Great when they cover your operator/shape, but brittle and slow to extend to new ops.\n- Use search, but with tight constraints: template-based methods like AutoTVM need human-written schedule templates per op/backend and usually explore a small, biased space; other auto-schedulers prune too early by evaluating incomplete programs and thus miss winning combinations.\n\nAnsor is our attempt to break that bottleneck. If I explain it like I’m teaching a curious colleague (Feynman style), the core ideas reduce to three questions:\n\n1) What exactly are we searching over?\n2) How do we explore such a huge space efficiently?\n3) How do we allocate limited tuning time across all the subgraphs of a whole DNN so end-to-end latency actually improves?\n\nAnswering them one by one:\n\n1) The search space: hierarchical “sketches,” then details\n- Start from the pure mathematical definition of an operator (tensor expression). Example: matmul defines C[i, j] = sum_k A[i, k] * B[k, j].\n- A good implementation is a composition of choices: tiling loops, reordering, fusing, caching into shared/local memory, vectorizing, unrolling, binding to threads/warps/blocks, tensorization, etc. The combinations explode.\n- We separate the space into two layers:\n  - High-level structures (sketches): Think of these as blueprints that encode the structure most responsible for big performance differences. For a conv, a sketch might decide the tiling hierarchy, where reduction happens, whether to cache reads/writes, and which axes bind to CUDA blocks/warps. Sketch rules are general—derived from static analysis of the tensor expression graph—so we don’t hand-write per-op templates. We enumerate multiple plausible sketches automatically.\n  - Low-level details (attrs): Once we pick a sketch, we still need to choose numeric parameters: tile sizes, unroll factors, vector widths, shared-memory layout, etc. These are sampled and later refined.\n- This split is crucial: it keeps coverage large (we don’t prune away entire families up front) while making sampling tractable.\n\nWhy this is different from template-based approaches:\n- Templates bake in one family of schedules per op (often a human’s educated guess).\n- Our sketch rules are generic and compose across ops. That lets us cover far more structures, including ones humans didn’t pre-encode—so we can find programs outside prior search spaces.\n\n2) Exploration: sample, measure, learn, evolve\n- We never evaluate “partial” schedules (which biases search); we always generate complete programs you can compile and run.\n- Pipeline:\n  - Initial sampling: For each task (operator or fused subgraph), generate several sketches; for each sketch, sample multiple concrete schedules by filling in details.\n  - Cost model: We train a learned cost model (supervised regression) to predict runtime from schedule features. It starts cold, gets better as we measure more candidates, and guides us toward promising regions. Features summarize loop structure, memory traffic, reuse, parallel mappings, vectorization, etc.\n  - Evolutionary search: We refine candidates with mutation and crossover over schedule parameters, using the cost model as a fitness proxy. This helps fine-tune tile sizes, unrolls, vector widths, and bindings that interact subtly with each other and the hardware.\n  - Measurement loop: We periodically select high-scoring candidates, compile them, run them on the real device, record true latencies, and update the cost model. Rinse and repeat until the time budget is used or convergence.\n\nKey effect:\n- Because we always consider complete programs and let the model learn from real measurements, we avoid early pruning errors and can recover surprising combinations (e.g., unusual tile shapes or caching choices) that actually win.\n\n3) Whole-model scheduling: prioritize what matters\n- A DNN compiles into many “tasks” after graph fusion (e.g., dozens of fused operator subgraphs).\n- Tuning every task evenly wastes time. Instead, we use a task scheduler that allocates the tuning budget dynamically:\n  - Estimate how much improving a task would reduce end-to-end latency (sensitivity), combining current per-task performance with its frequency/criticality in the graph.\n  - Use a gradient-descent-like allocation to pick the next task to tune—the one with the highest marginal payoff.\n- Practically, this means we spend more time on big matmuls and dominant fused convs, less on small tails—so the overall model speed improves faster.\n\nConcrete contributions\n- Hierarchical search space construction from tensor expressions (sketch rules + parameterized refinements), eliminating per-op manual templates.\n- Evolutionary search guided by a learned cost model over complete schedules.\n- A task scheduler that optimizes end-to-end latency, not just per-op microbenchmarks.\n\nWhat this buys you (results)\n- Across Intel CPUs, ARM CPUs, and NVIDIA GPUs, Ansor finds implementations that:\n  - Beat prior auto-tuners and match or surpass vendor libraries when they are not ideally tuned for your exact shapes.\n  - Discover winners outside existing search spaces.\n  - Improve end-to-end DNN latency by up to roughly 3.8x (Intel), 2.6x (ARM), and 1.7x (NVIDIA) versus state-of-the-art baselines at the time.\n- Despite a larger search space, the better cost model and task scheduler make search time competitive or faster to a good solution.\n\nHow it looks from a user’s perspective\n- Provide the operator definitions (tensor expressions) or the model in TVM’s Relay.\n- Ansor auto-generates the search space; you don’t write schedule templates.\n- Give it a time budget. It will:\n  - Partition the model into fused tasks,\n  - Prioritize tasks,\n  - Generate, predict, run, learn, and iterate,\n  - Emit tuned kernels that integrate into your model runtime.\n\nWhy it works (intuitions)\n- Structure matters most: Choosing the right loop nest, cache placement, and parallel mapping explains a big chunk of performance. Sketch rules get those structural differences on the table.\n- Fine-tuning matters next: Hardware prefers very specific tile/unroll/vector sizes. Evolutionary search plus a learned cost model can quickly zoom in on those.\n- End-to-end matters always: Optimizing the wrong tasks wastes time; the scheduler prevents that.\n\nWhat it doesn’t assume\n- It doesn’t assume a fixed set of operator-specific templates or hardware-specific hand tuning. The same mechanism applies to new ops as long as they’re expressible as tensor computations.\n- It doesn’t rely on analytical models alone; it grounds predictions in live measurements.\n\nLimitations and future directions\n- Tuning still costs time; you trade compute time now for faster inference later. We mitigate with a good cost model and scheduling, but there’s still a budget to plan.\n- Extremely dynamic shapes and control flow need special handling; later TVM work extends support here.\n- Specialized intrinsics (e.g., tensor cores, AMX) require rule support to get fully exploited; we and the community keep adding these.\n- Sparse and low-precision operators were identified as natural next steps; subsequent efforts have pushed in that direction.\n\nMental model: a compiler that learns\n- Think of Ansor as a compiler that tailors schedules by trial, measurement, and learned intuition, not by human-crafted templates. It searches over structures and parameters, learns from real runs, and focuses its effort where it most improves your end-to-end model.\n\nIf you want, I can map these ideas to a concrete operator (e.g., depthwise conv2d on a GPU) and walk through how sketches and mutations evolve into a fast kernel.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_046a0d7197c2355e0068caf413b87c819e808bccabc3c013f9"}},"resumableStreamId":"STREAM:668ccdd0-4b1b-493e-b9fa-440c72b01c0e","role":"assistant","status":"done","threadId":"6083fcc5-0473-4553-8df2-fd08145846b5","timeToFirstToken":1.583,"tokens":1711,"tokensPerSecond":39.837019790454015,"updated_at":1758131208371,"userId":"google:104458561188320708185","id":"j97e9pyaxm0dyt2vcy9htdqkk57qs70y"},{"_creationTime":1758133126174.8567,"_id":"j9715pvgy0fjfmgfzxbvbh0xwh7qsvsr","attachmentIds":[],"content":"show me the diagram For Ansor in the paper and explain it to me","created_at":1758133126175,"messageId":"d975058b-1e2a-4295-bb2a-ece3d04bdccc","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"6083fcc5-0473-4553-8df2-fd08145846b5","updated_at":1758133126175,"userId":"google:104458561188320708185","id":"j9715pvgy0fjfmgfzxbvbh0xwh7qsvsr","providerMetadata":{}},{"_creationTime":1758133126174.857,"_id":"j97evaebn82394egpmmqq25qvs7qsh9c","attachmentIds":[],"byok":false,"content":"I can’t display the paper’s original figure here, but I’ll describe the main Ansor system diagram in the paper and walk you through each block so you can mentally “see” it and understand how data flows.\n\nWhat the diagram shows at a glance\n- Inputs on the left:\n  - A tensor program (the pure compute definition of an op or fused subgraph), e.g., conv2d, matmul, or a fused pattern emitted by TVM’s Relay.\n  - A target hardware backend (e.g., CUDA GPU, x86 CPU, ARM CPU).\n\n- Middle pipeline (the core of Ansor):\n  1) Sketch Generation:\n     - A set of generic sketch rules analyzes the tensor program’s compute DAG and enumerates several high-level “sketches” (loop-structure blueprints). Each sketch encodes decisions like multi-level tiling, compute-at/compute-inline placement, memory caches (shared/local), and parallel bindings (e.g., blocks/warps/threads on GPUs).\n  2) Initial Sampling (Attribute Filling):\n     - For each sketch, Ansor samples concrete schedule parameters: specific tile sizes, vector widths, unroll factors, thread bindings, cache layouts, etc., to create complete candidate schedules.\n  3) Cost Model Scoring:\n     - A learned regression model extracts structural features from each complete schedule (loop nest, memory traffic, reuse distance, parallel patterns, vectorization, etc.) and predicts its runtime cost to rank candidates.\n  4) Evolutionary Search:\n     - Using the cost model as the fitness function, Ansor mutates and crosses over the schedule parameters (e.g., changes tile sizes, swaps bindings, toggles unroll) to generate improved candidates within each sketch family.\n  5) Measurement and Feedback:\n     - Top-ranked candidates are compiled to real kernels, executed on the target device, and measured.\n     - Measured latencies are fed back to retrain/update the cost model, closing the loop.\n\n- On the right (outputs):\n  - The best schedule/kernels per task are emitted and cached in a log.\n  - These tuned kernels integrate into the model runtime, producing the final accelerated program.\n\n- Overlaid control (top/bottom band in the figure):\n  - Task Scheduler for end-to-end models:\n    - When optimizing a whole network, the diagram shows a controller that allocates tuning time across many tasks (fused subgraphs). It estimates each task’s marginal impact on end-to-end latency and picks which task to tune next. This controller sits “outside” the per-task loop and repeatedly invokes the middle pipeline for the selected task.\n\nHow the pieces connect step by step\n1) Task selection (whole-model mode):\n   - The task scheduler examines all tasks (t1, t2, …) and their current best latencies, plus their frequency and position in the graph. It selects a task with the highest expected payoff.\n\n2) Sketch generation:\n   - From the selected task’s compute DAG, rules generate several sketches (S1, S2, S3, …). Think “different loop-structure families” that could plausibly be good.\n\n3) Candidate creation:\n   - For each sketch, Ansor samples N concrete schedules by choosing numeric attributes (tile sizes, unroll, vector width, thread/block mappings, etc.).\n\n4) Cost model ranking:\n   - Extract features from each schedule; the learned model predicts latency and ranks them.\n\n5) Evolution + selection:\n   - Keep a population of candidates. Apply mutation/crossover to produce new ones; score them again with the cost model. Repeat for a few generations.\n\n6) Real measurements:\n   - Take the top K candidates, compile and run them on the hardware, collect actual runtimes.\n\n7) Update model and records:\n   - Add the new (features, measured time) pairs to the training set. Retrain/update the cost model so it better matches the device. Update the best-known schedule for this task in the tuning log.\n\n8) Iterate or switch tasks:\n   - If the budget remains, the task scheduler may pick the same task again or switch to another task where the marginal benefit is now higher.\n\nKey ideas the diagram emphasizes\n- Two-layer search: sketches (structure) then parameters (details).\n- Closed-loop learning: sampling → predicting → measuring → learning → better sampling.\n- Evolutionary refinement guided by a learned cost model, not just random search.\n- Whole-model prioritization so tuning effort maximizes end-to-end speedup.\n\nIf you’d like, I can sketch a simplified ASCII diagram to approximate the figure’s layout, or pull out one sub-figure (like the evolutionary loop) and walk through a concrete example with a conv2d on GPU.","created_at":1758133126176,"messageId":"47dff6e8-a454-4c7a-a8d2-f6f679375aa1","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"I can’t display the paper’s original figure here, but I’ll describe the main Ansor system diagram in the paper and walk you through each block so you can mentally “see” it and understand how data flows.\n\nWhat the diagram shows at a glance\n- Inputs on the left:\n  - A tensor program (the pure compute definition of an op or fused subgraph), e.g., conv2d, matmul, or a fused pattern emitted by TVM’s Relay.\n  - A target hardware backend (e.g., CUDA GPU, x86 CPU, ARM CPU).\n\n- Middle pipeline (the core of Ansor):\n  1) Sketch Generation:\n     - A set of generic sketch rules analyzes the tensor program’s compute DAG and enumerates several high-level “sketches” (loop-structure blueprints). Each sketch encodes decisions like multi-level tiling, compute-at/compute-inline placement, memory caches (shared/local), and parallel bindings (e.g., blocks/warps/threads on GPUs).\n  2) Initial Sampling (Attribute Filling):\n     - For each sketch, Ansor samples concrete schedule parameters: specific tile sizes, vector widths, unroll factors, thread bindings, cache layouts, etc., to create complete candidate schedules.\n  3) Cost Model Scoring:\n     - A learned regression model extracts structural features from each complete schedule (loop nest, memory traffic, reuse distance, parallel patterns, vectorization, etc.) and predicts its runtime cost to rank candidates.\n  4) Evolutionary Search:\n     - Using the cost model as the fitness function, Ansor mutates and crosses over the schedule parameters (e.g., changes tile sizes, swaps bindings, toggles unroll) to generate improved candidates within each sketch family.\n  5) Measurement and Feedback:\n     - Top-ranked candidates are compiled to real kernels, executed on the target device, and measured.\n     - Measured latencies are fed back to retrain/update the cost model, closing the loop.\n\n- On the right (outputs):\n  - The best schedule/kernels per task are emitted and cached in a log.\n  - These tuned kernels integrate into the model runtime, producing the final accelerated program.\n\n- Overlaid control (top/bottom band in the figure):\n  - Task Scheduler for end-to-end models:\n    - When optimizing a whole network, the diagram shows a controller that allocates tuning time across many tasks (fused subgraphs). It estimates each task’s marginal impact on end-to-end latency and picks which task to tune next. This controller sits “outside” the per-task loop and repeatedly invokes the middle pipeline for the selected task.\n\nHow the pieces connect step by step\n1) Task selection (whole-model mode):\n   - The task scheduler examines all tasks (t1, t2, …) and their current best latencies, plus their frequency and position in the graph. It selects a task with the highest expected payoff.\n\n2) Sketch generation:\n   - From the selected task’s compute DAG, rules generate several sketches (S1, S2, S3, …). Think “different loop-structure families” that could plausibly be good.\n\n3) Candidate creation:\n   - For each sketch, Ansor samples N concrete schedules by choosing numeric attributes (tile sizes, unroll, vector width, thread/block mappings, etc.).\n\n4) Cost model ranking:\n   - Extract features from each schedule; the learned model predicts latency and ranks them.\n\n5) Evolution + selection:\n   - Keep a population of candidates. Apply mutation/crossover to produce new ones; score them again with the cost model. Repeat for a few generations.\n\n6) Real measurements:\n   - Take the top K candidates, compile and run them on the hardware, collect actual runtimes.\n\n7) Update model and records:\n   - Add the new (features, measured time) pairs to the training set. Retrain/update the cost model so it better matches the device. Update the best-known schedule for this task in the tuning log.\n\n8) Iterate or switch tasks:\n   - If the budget remains, the task scheduler may pick the same task again or switch to another task where the marginal benefit is now higher.\n\nKey ideas the diagram emphasizes\n- Two-layer search: sketches (structure) then parameters (details).\n- Closed-loop learning: sampling → predicting → measuring → learning → better sampling.\n- Evolutionary refinement guided by a learned cost model, not just random search.\n- Whole-model prioritization so tuning effort maximizes end-to-end speedup.\n\nIf you’d like, I can sketch a simplified ASCII diagram to approximate the figure’s layout, or pull out one sub-figure (like the evolutionary loop) and walk through a concrete example with a conv2d on GPU.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0470bdd60f55d64c0068cafb86c89c819e8b117557ed9d5cac"}},"resumableStreamId":"STREAM:46693f5f-da7e-43cd-bc90-d9347b143ba6","role":"assistant","status":"done","threadId":"6083fcc5-0473-4553-8df2-fd08145846b5","timeToFirstToken":1.21,"tokens":974,"tokensPerSecond":76.86237373737374,"updated_at":1758133126176,"userId":"google:104458561188320708185","id":"j97evaebn82394egpmmqq25qvs7qsh9c"},{"_creationTime":1758133215148.9026,"_id":"j976b131yq8bkhm5atkatkwb9x7qs42z","attachmentIds":[],"content":"explain to me the workflow pipeline of Ansor","created_at":1758133215149,"messageId":"3b220599-1702-4894-85db-7313b4c2160c","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"6083fcc5-0473-4553-8df2-fd08145846b5","updated_at":1758133215149,"userId":"google:104458561188320708185","id":"j976b131yq8bkhm5atkatkwb9x7qs42z","providerMetadata":{}},{"_creationTime":1758133215148.9028,"_id":"j97de43e4b52kqa3ncxnrj071d7qsmd4","attachmentIds":[],"byok":false,"content":"Here’s the Ansor workflow end to end, from a model or operator to tuned kernels. I’ll keep it concrete and sequenced so you can map each step to what the system actually does.\n\nHigh-level overview\n- Input: A tensor program (operator or fused subgraph) plus a target backend (CPU/GPU).\n- Output: Optimized schedules (kernels) for each task and a tuned end-to-end model.\n- Core loop: Generate complete candidate schedules, predict their runtime, measure the best, learn from feedback, and repeat—while a task scheduler decides which task to tune next for maximum overall speedup.\n\nStep-by-step pipeline\n1) Frontend and task extraction\n- Start with a model (e.g., in TVM Relay) or a single operator.\n- Apply graph-level passes (fuse compatible ops). The model becomes a set of tuning tasks, each a compute DAG (tensor expression) representing a fused subgraph or op.\n\n2) Task scheduling across the whole model\n- Estimate each task’s marginal impact on end-to-end latency from current measurements and usage frequency.\n- Pick the next task to tune—the one with the highest expected payoff.\n- This selection repeats throughout tuning to allocate time where it matters most.\n\n3) Sketch generation (search space: structure)\n- Analyze the selected task’s compute DAG.\n- Apply generic “sketch rules” to enumerate multiple high-level loop-structure blueprints (sketches). These encode:\n  - Multi-level tiling hierarchies\n  - Compute-at / inline placement\n  - Cache reads/writes into shared/local memory\n  - Parallel bindings (threads/warps/blocks on GPU; cores/vec on CPU)\n  - Tensorization opportunities when available\n- Result: a set of diverse structural families that could yield high performance.\n\n4) Attribute sampling (search space: parameters)\n- For each sketch, sample concrete numeric parameters to produce complete schedules:\n  - Tile sizes and split factors\n  - Loop reorderings where allowed\n  - Vector widths, unroll factors\n  - Thread/block/warp bindings\n  - Cache line/layout choices\n- Each sample is a fully specified program you can compile and run.\n\n5) Cost model scoring\n- Extract structural and hardware-relevant features from each complete schedule (loop nest statistics, memory traffic and reuse, parallel structure, vectorization, etc.).\n- A learned regression cost model predicts runtime and ranks candidates.\n- The model starts cold and is updated as real measurements come in.\n\n6) Evolutionary search (model-guided refinement)\n- Maintain a population of high-scoring candidates.\n- Apply mutations (e.g., tweak tile sizes, change vector width, flip unroll) and crossovers (combine parameters from two schedules).\n- Use the cost model as a fitness proxy to quickly explore promising neighborhoods without compiling everything.\n\n7) Measurement and feedback on real hardware\n- Select the top K candidates according to the cost model.\n- Compile them into kernels and run micro-benchmarks on the target device.\n- Record true runtimes and any relevant performance counters.\n\n8) Cost model update and log persistence\n- Add (features, measured runtime) pairs to the training set; retrain/update the model to better fit the device.\n- Update the best-known schedule for the task and persist it to a tuning log/database for reuse.\n\n9) Iterate and re-allocate\n- If the time budget remains for this task, continue evolving, measuring, and updating.\n- Otherwise, return to the task scheduler, which may select a different task that now offers higher marginal end-to-end gain.\n\n10) Build the final runtime\n- After tuning, compile the model with the best schedules found for each task.\n- The tuned kernels are cached, so future builds or runs can reuse them without re-tuning (unless shapes/hardware change).\n\nWhy this pipeline works\n- Separation of concerns: Structural choices (sketches) capture big performance wins; parameter tuning finds the precise hardware-sweet-spot values.\n- Always-complete candidates: Avoids bias from evaluating partial schedules and missing good combinations.\n- Learn-from-measurement loop: The cost model quickly adapts to the target device, guiding search efficiently.\n- End-to-end prioritization: The task scheduler ensures tuning time converts into real model speedups, not just better microbenchmarks.\n\nIf you want, I can walk through a concrete operator (e.g., conv2d on CUDA) and illustrate one sketch, the sampled parameters, typical mutations, and how measurements converge to a fast kernel.","created_at":1758133215150,"messageId":"6eb474d8-c57f-4cdf-8c27-48432c62b464","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Here’s the Ansor workflow end to end, from a model or operator to tuned kernels. I’ll keep it concrete and sequenced so you can map each step to what the system actually does.\n\nHigh-level overview\n- Input: A tensor program (operator or fused subgraph) plus a target backend (CPU/GPU).\n- Output: Optimized schedules (kernels) for each task and a tuned end-to-end model.\n- Core loop: Generate complete candidate schedules, predict their runtime, measure the best, learn from feedback, and repeat—while a task scheduler decides which task to tune next for maximum overall speedup.\n\nStep-by-step pipeline\n1) Frontend and task extraction\n- Start with a model (e.g., in TVM Relay) or a single operator.\n- Apply graph-level passes (fuse compatible ops). The model becomes a set of tuning tasks, each a compute DAG (tensor expression) representing a fused subgraph or op.\n\n2) Task scheduling across the whole model\n- Estimate each task’s marginal impact on end-to-end latency from current measurements and usage frequency.\n- Pick the next task to tune—the one with the highest expected payoff.\n- This selection repeats throughout tuning to allocate time where it matters most.\n\n3) Sketch generation (search space: structure)\n- Analyze the selected task’s compute DAG.\n- Apply generic “sketch rules” to enumerate multiple high-level loop-structure blueprints (sketches). These encode:\n  - Multi-level tiling hierarchies\n  - Compute-at / inline placement\n  - Cache reads/writes into shared/local memory\n  - Parallel bindings (threads/warps/blocks on GPU; cores/vec on CPU)\n  - Tensorization opportunities when available\n- Result: a set of diverse structural families that could yield high performance.\n\n4) Attribute sampling (search space: parameters)\n- For each sketch, sample concrete numeric parameters to produce complete schedules:\n  - Tile sizes and split factors\n  - Loop reorderings where allowed\n  - Vector widths, unroll factors\n  - Thread/block/warp bindings\n  - Cache line/layout choices\n- Each sample is a fully specified program you can compile and run.\n\n5) Cost model scoring\n- Extract structural and hardware-relevant features from each complete schedule (loop nest statistics, memory traffic and reuse, parallel structure, vectorization, etc.).\n- A learned regression cost model predicts runtime and ranks candidates.\n- The model starts cold and is updated as real measurements come in.\n\n6) Evolutionary search (model-guided refinement)\n- Maintain a population of high-scoring candidates.\n- Apply mutations (e.g., tweak tile sizes, change vector width, flip unroll) and crossovers (combine parameters from two schedules).\n- Use the cost model as a fitness proxy to quickly explore promising neighborhoods without compiling everything.\n\n7) Measurement and feedback on real hardware\n- Select the top K candidates according to the cost model.\n- Compile them into kernels and run micro-benchmarks on the target device.\n- Record true runtimes and any relevant performance counters.\n\n8) Cost model update and log persistence\n- Add (features, measured runtime) pairs to the training set; retrain/update the model to better fit the device.\n- Update the best-known schedule for the task and persist it to a tuning log/database for reuse.\n\n9) Iterate and re-allocate\n- If the time budget remains for this task, continue evolving, measuring, and updating.\n- Otherwise, return to the task scheduler, which may select a different task that now offers higher marginal end-to-end gain.\n\n10) Build the final runtime\n- After tuning, compile the model with the best schedules found for each task.\n- The tuned kernels are cached, so future builds or runs can reuse them without re-tuning (unless shapes/hardware change).\n\nWhy this pipeline works\n- Separation of concerns: Structural choices (sketches) capture big performance wins; parameter tuning finds the precise hardware-sweet-spot values.\n- Always-complete candidates: Avoids bias from evaluating partial schedules and missing good combinations.\n- Learn-from-measurement loop: The cost model quickly adapts to the target device, guiding search efficiently.\n- End-to-end prioritization: The task scheduler ensures tuning time converts into real model speedups, not just better microbenchmarks.\n\nIf you want, I can walk through a concrete operator (e.g., conv2d on CUDA) and illustrate one sketch, the sampled parameters, typical mutations, and how measurements converge to a fast kernel.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_008d732db20f71520068cafbdfb24c8190947d0d9386271519"}},"resumableStreamId":"STREAM:093318cb-9d0e-4670-85c2-4c2b72b87e0b","role":"assistant","status":"done","threadId":"6083fcc5-0473-4553-8df2-fd08145846b5","timeToFirstToken":1.001,"tokens":929,"tokensPerSecond":69.07576771507175,"updated_at":1758133215150,"userId":"google:104458561188320708185","id":"j97de43e4b52kqa3ncxnrj071d7qsmd4"},{"_creationTime":1758133683390.549,"_id":"j9716yzs8e4yna5sq9hkagjy097qrf8c","attachmentIds":[],"content":"what contributions does ANSOR have?","created_at":1758133683390,"messageId":"7a874fe4-3bae-49fa-9c98-9104b52943ac","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"6083fcc5-0473-4553-8df2-fd08145846b5","updated_at":1758133683390,"userId":"google:104458561188320708185","id":"j9716yzs8e4yna5sq9hkagjy097qrf8c","providerMetadata":{}},{"_creationTime":1758133683390.5493,"_id":"j97c3v183edaxr7ajm5tnxxsjx7qrfgr","attachmentIds":[],"byok":false,"content":"Key contributions of Ansor (as presented in the paper “Ansor: Generating High-Performance Tensor Programs for Deep Learning”):\n\n- Template-free auto-scheduling via generic sketch rules\n  - Introduces a method to automatically construct the search space for any tensor program from its compute DAG.\n  - Uses “sketches” (high-level loop-structure blueprints) derived by general rules rather than hand-written, per-operator templates.\n  - Greatly broadens coverage across operators and shapes while reducing human effort.\n\n- Two-level search space: structure first, then parameters\n  - Separates decisions into:\n    - Structural choices (loop tiling hierarchy, compute-at/inline, cache placement, thread/warp/block bindings, tensorization opportunities).\n    - Numeric/details (tile sizes, unroll factors, vector widths, etc.).\n  - This separation yields a large, expressive yet tractable search space.\n\n- Learning-guided evolutionary search over complete programs\n  - Always generates complete, compilable schedules; avoids evaluating partial schedules that can bias search.\n  - Trains a learned cost model (supervised regression on schedule features) to predict runtime.\n  - Combines cost-model guidance with evolutionary operators (mutation/crossover) to efficiently refine candidates.\n\n- Closed-loop measurement with continual cost-model updating\n  - Compiles and runs top-ranked candidates on the real device.\n  - Feeds measured latencies back to retrain the cost model, improving prediction fidelity over time and hardware.\n\n- Whole-model task scheduler for end-to-end speedup\n  - Proposes a scheduler that allocates tuning time across many fused tasks in a DNN based on estimated marginal impact on total latency.\n  - Focuses effort on the tasks that most improve end-to-end performance rather than evenly tuning everything.\n\n- Broad applicability across hardware and ops\n  - Works for CPUs (x86, ARM) and GPUs (CUDA), and for a wide variety of dense tensor operators and fused subgraphs, without per-op templates.\n\n- Strong empirical results\n  - Demonstrates that Ansor finds kernels that outperform prior auto-tuners and often match or surpass vendor libraries for many workloads.\n  - Shows significant end-to-end model speedups on multiple hardware backends within practical tuning budgets.\n\n- Practical system integration\n  - Integrates into TVM: takes Relay graphs/tensor expressions as input; emits tuned kernels and caches logs for reuse.\n\nIn short: Ansor contributes a template-free, learning-guided auto-scheduler that searches a large, general space of schedules using sketches and evolutionary refinement, measures on real hardware, and prioritizes tasks to maximize end-to-end model speedups.","created_at":1758133683391,"messageId":"43381828-788b-4fe5-8e21-5c0702c9ce64","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Key contributions of Ansor (as presented in the paper “Ansor: Generating High-Performance Tensor Programs for Deep Learning”):\n\n- Template-free auto-scheduling via generic sketch rules\n  - Introduces a method to automatically construct the search space for any tensor program from its compute DAG.\n  - Uses “sketches” (high-level loop-structure blueprints) derived by general rules rather than hand-written, per-operator templates.\n  - Greatly broadens coverage across operators and shapes while reducing human effort.\n\n- Two-level search space: structure first, then parameters\n  - Separates decisions into:\n    - Structural choices (loop tiling hierarchy, compute-at/inline, cache placement, thread/warp/block bindings, tensorization opportunities).\n    - Numeric/details (tile sizes, unroll factors, vector widths, etc.).\n  - This separation yields a large, expressive yet tractable search space.\n\n- Learning-guided evolutionary search over complete programs\n  - Always generates complete, compilable schedules; avoids evaluating partial schedules that can bias search.\n  - Trains a learned cost model (supervised regression on schedule features) to predict runtime.\n  - Combines cost-model guidance with evolutionary operators (mutation/crossover) to efficiently refine candidates.\n\n- Closed-loop measurement with continual cost-model updating\n  - Compiles and runs top-ranked candidates on the real device.\n  - Feeds measured latencies back to retrain the cost model, improving prediction fidelity over time and hardware.\n\n- Whole-model task scheduler for end-to-end speedup\n  - Proposes a scheduler that allocates tuning time across many fused tasks in a DNN based on estimated marginal impact on total latency.\n  - Focuses effort on the tasks that most improve end-to-end performance rather than evenly tuning everything.\n\n- Broad applicability across hardware and ops\n  - Works for CPUs (x86, ARM) and GPUs (CUDA), and for a wide variety of dense tensor operators and fused subgraphs, without per-op templates.\n\n- Strong empirical results\n  - Demonstrates that Ansor finds kernels that outperform prior auto-tuners and often match or surpass vendor libraries for many workloads.\n  - Shows significant end-to-end model speedups on multiple hardware backends within practical tuning budgets.\n\n- Practical system integration\n  - Integrates into TVM: takes Relay graphs/tensor expressions as input; emits tuned kernels and caches logs for reuse.\n\nIn short: Ansor contributes a template-free, learning-guided auto-scheduler that searches a large, general space of schedules using sketches and evolutionary refinement, measures on real hardware, and prioritizes tasks to maximize end-to-end model speedups.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0ca9e5630e2307a90068cafdb40a4c819e861b1f984104a185"}},"resumableStreamId":"STREAM:27d91e11-6503-4cdf-a26c-8ea9fa411c8a","role":"assistant","status":"done","threadId":"6083fcc5-0473-4553-8df2-fd08145846b5","timeToFirstToken":1.313,"tokens":543,"tokensPerSecond":37.337550711682596,"updated_at":1758133683391,"userId":"google:104458561188320708185","id":"j97c3v183edaxr7ajm5tnxxsjx7qrfgr"},{"_creationTime":1758133741622.7036,"_id":"j97djd1hdhqmah151cz9z66mdx7qrdeb","attachmentIds":[],"content":"what are the critics on Ansor","created_at":1758133741625,"messageId":"2486ca2d-94c2-4024-a6d2-21c3e3dba1c5","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"6083fcc5-0473-4553-8df2-fd08145846b5","updated_at":1758133741625,"userId":"google:104458561188320708185","id":"j97djd1hdhqmah151cz9z66mdx7qrdeb","providerMetadata":{}},{"_creationTime":1758133741622.7039,"_id":"j97280j6a2q125ytz0yda33bk17qrbtk","attachmentIds":[],"byok":false,"content":"Here are the main critiques and limitations that practitioners and researchers often point out about Ansor. I’ll separate inherent trade-offs from practical pain points observed in use.\n\nConceptual and algorithmic limitations\n- Tuning cost and stability\n  - Ansor still relies on iterative compile–run measurements. For large models or many target shapes/devices, the wall-clock tuning time and hardware wear can be significant.\n  - Convergence can be noisy: small codegen or driver changes may shift the best schedule; results are not always perfectly reproducible across machines or software versions.\n\n- Cost model fidelity gaps\n  - The learned cost model uses structural features of schedules; it doesn’t perfectly capture microarchitectural nuances (e.g., bank conflicts, cache associativity quirks, warp scheduling, occupancy-vs-register pressure trade-offs).\n  - Mispredictions can misguide evolutionary search into local optima, delaying discovery of real winners.\n\n- Coverage vs. optimality\n  - Sketch rules are generic, but not exhaustive. If an optimal kernel requires a structure not encompassed by the sketch rules (e.g., a niche data layout transform, exotic pipelining, or novel warp-level cooperation), Ansor may never discover it.\n  - Tensorization and special units (Tensor Cores, AMX, DotProd, DPAS) require explicit rule support; without it, Ansor may underuse key hardware features.\n\n- Sensitivity to shape and hardware\n  - Schedules often don’t transfer well across input shapes or batch sizes. Dynamic-shape workloads may need multiple tuned variants or a conservative compromise.\n  - Portability across hardware generations is imperfect; a Pareto-best kernel on Turing might be mediocre on Ampere or Hopper unless rules and search are updated.\n\n- Evolutionary search overhead\n  - Evolutionary operators over a large parameter space can be sample-inefficient without strong priors. Even with the cost model, many candidates are explored before good ones are measured.\n\nSystem and usability critiques\n- Engineering complexity\n  - Integrating new intrinsics or memory scopes requires writing or extending sketch rules and tensorization patterns—expert effort that can be nontrivial.\n  - Debugging failed candidates (compilation errors, invalid schedules) can be time-consuming; while Ansor aims to generate only valid complete schedules, corner cases still occur.\n\n- Limited insight into “why” a kernel wins\n  - Ansor outputs a best schedule, but the rationale is learned and implicit. It’s not always easy to extract human-understandable explanations or general design rules from the cost model.\n\n- End-to-end scheduling assumptions\n  - The task scheduler estimates marginal end-to-end benefit assuming current profiling and independence between tasks. Interactions like kernel–kernel interference, stream overlap, or multi-tenant contention are not fully modeled.\n  - Memory footprint and compilation time aren’t first-class objectives; emphasis is on latency. For edge devices, code size or power may be equally important.\n\n- Maintenance with evolving stacks\n  - Changes in LLVM, CUDA, or TVM IR can shift performance or break optimizations; keeping Ansor’s rules and features aligned with upstream toolchains requires ongoing maintenance.\n\n- Vendor libraries still win on niches\n  - For some shapes and ops where vendors invest heavily (e.g., very large GEMMs using proprietary heuristics, or highly optimized convolutions), cuDNN/oneDNN/cuBLAS may outperform Ansor unless substantial tuning time and tensorization support are provided.\n\nResearch directions often proposed to address these issues\n- Better priors and transfer\n  - Use meta-learning, retrieval, or schedule embeddings to warm-start from similar tasks/shapes/hardware and cut tuning time.\n- Richer cost models\n  - Incorporate static pipeline/occupancy models, hardware counters, or lightweight simulators; use GNNs over IR to capture dependency structure more faithfully.\n- Broader sketch spaces with safety\n  - Expand rule sets to cover more advanced warp-level patterns, asynchronous copy pipelines, and tensor-core tiling—while keeping validity guarantees.\n- Multi-objective and robust tuning\n  - Optimize for latency, power, memory footprint, and robustness to shape jitter; produce a small portfolio of kernels with runtime dispatch.\n- System-level co-optimization\n  - Model launch overheads, kernel fusion across tasks, stream scheduling, and memory bandwidth sharing to better predict end-to-end gains.\n\nBottom line\nAnsor made an important leap by removing per-op templates and learning over complete schedules, but it trades human templates for compute and measurement. Its search space, cost model, and device support must evolve to keep pace with new hardware and operator patterns. In practice, it delivers strong results for many dense ops, but may require nontrivial tuning time and additional rule work to fully exploit specialized units or highly dynamic workloads.","created_at":1758133741626,"messageId":"2a6e4ab0-8295-42fc-a3ad-bf691de5dd91","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Here are the main critiques and limitations that practitioners and researchers often point out about Ansor. I’ll separate inherent trade-offs from practical pain points observed in use.\n\nConceptual and algorithmic limitations\n- Tuning cost and stability\n  - Ansor still relies on iterative compile–run measurements. For large models or many target shapes/devices, the wall-clock tuning time and hardware wear can be significant.\n  - Convergence can be noisy: small codegen or driver changes may shift the best schedule; results are not always perfectly reproducible across machines or software versions.\n\n- Cost model fidelity gaps\n  - The learned cost model uses structural features of schedules; it doesn’t perfectly capture microarchitectural nuances (e.g., bank conflicts, cache associativity quirks, warp scheduling, occupancy-vs-register pressure trade-offs).\n  - Mispredictions can misguide evolutionary search into local optima, delaying discovery of real winners.\n\n- Coverage vs. optimality\n  - Sketch rules are generic, but not exhaustive. If an optimal kernel requires a structure not encompassed by the sketch rules (e.g., a niche data layout transform, exotic pipelining, or novel warp-level cooperation), Ansor may never discover it.\n  - Tensorization and special units (Tensor Cores, AMX, DotProd, DPAS) require explicit rule support; without it, Ansor may underuse key hardware features.\n\n- Sensitivity to shape and hardware\n  - Schedules often don’t transfer well across input shapes or batch sizes. Dynamic-shape workloads may need multiple tuned variants or a conservative compromise.\n  - Portability across hardware generations is imperfect; a Pareto-best kernel on Turing might be mediocre on Ampere or Hopper unless rules and search are updated.\n\n- Evolutionary search overhead\n  - Evolutionary operators over a large parameter space can be sample-inefficient without strong priors. Even with the cost model, many candidates are explored before good ones are measured.\n\nSystem and usability critiques\n- Engineering complexity\n  - Integrating new intrinsics or memory scopes requires writing or extending sketch rules and tensorization patterns—expert effort that can be nontrivial.\n  - Debugging failed candidates (compilation errors, invalid schedules) can be time-consuming; while Ansor aims to generate only valid complete schedules, corner cases still occur.\n\n- Limited insight into “why” a kernel wins\n  - Ansor outputs a best schedule, but the rationale is learned and implicit. It’s not always easy to extract human-understandable explanations or general design rules from the cost model.\n\n- End-to-end scheduling assumptions\n  - The task scheduler estimates marginal end-to-end benefit assuming current profiling and independence between tasks. Interactions like kernel–kernel interference, stream overlap, or multi-tenant contention are not fully modeled.\n  - Memory footprint and compilation time aren’t first-class objectives; emphasis is on latency. For edge devices, code size or power may be equally important.\n\n- Maintenance with evolving stacks\n  - Changes in LLVM, CUDA, or TVM IR can shift performance or break optimizations; keeping Ansor’s rules and features aligned with upstream toolchains requires ongoing maintenance.\n\n- Vendor libraries still win on niches\n  - For some shapes and ops where vendors invest heavily (e.g., very large GEMMs using proprietary heuristics, or highly optimized convolutions), cuDNN/oneDNN/cuBLAS may outperform Ansor unless substantial tuning time and tensorization support are provided.\n\nResearch directions often proposed to address these issues\n- Better priors and transfer\n  - Use meta-learning, retrieval, or schedule embeddings to warm-start from similar tasks/shapes/hardware and cut tuning time.\n- Richer cost models\n  - Incorporate static pipeline/occupancy models, hardware counters, or lightweight simulators; use GNNs over IR to capture dependency structure more faithfully.\n- Broader sketch spaces with safety\n  - Expand rule sets to cover more advanced warp-level patterns, asynchronous copy pipelines, and tensor-core tiling—while keeping validity guarantees.\n- Multi-objective and robust tuning\n  - Optimize for latency, power, memory footprint, and robustness to shape jitter; produce a small portfolio of kernels with runtime dispatch.\n- System-level co-optimization\n  - Model launch overheads, kernel fusion across tasks, stream scheduling, and memory bandwidth sharing to better predict end-to-end gains.\n\nBottom line\nAnsor made an important leap by removing per-op templates and learning over complete schedules, but it trades human templates for compute and measurement. Its search space, cost model, and device support must evolve to keep pace with new hardware and operator patterns. In practice, it delivers strong results for many dense ops, but may require nontrivial tuning time and additional rule work to fully exploit specialized units or highly dynamic workloads.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_091888a46c4970e30068cafdee2d9c81a08bb16704933945d9"}},"resumableStreamId":"STREAM:6d8f3d18-710a-495b-a987-20df28da529e","role":"assistant","status":"done","threadId":"6083fcc5-0473-4553-8df2-fd08145846b5","timeToFirstToken":1.805,"tokens":982,"tokensPerSecond":36.51100535395598,"updated_at":1758133741626,"userId":"google:104458561188320708185","id":"j97280j6a2q125ytz0yda33bk17qrbtk"}],"version":"6.0.0"}