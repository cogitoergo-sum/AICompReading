{"threads":[{"_creationTime":1758732660365.6526,"_id":"jd7fwqeesn1r0mdag42gm9j7cn7r7jex","branchParentPublicMessageId":"56d10bc3-4cc6-462e-88d1-086d79485619","branchParentThreadId":"jd7akezesxrbs5kc9fcn4ze9717r7xgx","createdAt":1758732660365,"generationStatus":"completed","lastMessageAt":1758733286895,"model":"gpt-5-chat","modelParams":{},"pinned":false,"threadId":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","title":"ECLIP paper spatial partitions comparison of Nvidia MPS/MIG vs AMD CU Masking","updatedAt":1758733286895,"userId":"google:104458561188320708185","userSetTitle":false,"visibility":"visible","id":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","last_message_at":1758733286895,"created_at":1758732660365,"updated_at":1758733286895,"status":"completed","user_edited_title":false},{"_creationTime":1758684871844.142,"_id":"jd7akezesxrbs5kc9fcn4ze9717r7xgx","createdAt":1758684871844,"generationStatus":"completed","lastMessageAt":1758731821520,"model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"pinned":false,"threadId":"020ae027-8a02-4dc0-9799-45305ca05185","title":"ECLIP paper spatial partitions comparison of Nvidia MPS/MIG vs AMD CU Masking","updatedAt":1758731821520,"userId":"google:104458561188320708185","userSetTitle":false,"visibility":"visible","id":"020ae027-8a02-4dc0-9799-45305ca05185","last_message_at":1758731821520,"created_at":1758684871844,"updated_at":1758731821520,"status":"completed","user_edited_title":false}],"messages":[{"_creationTime":1758684871844.1423,"_id":"j979csmn6cyyw4ev4mgj66jzgd7r7wm5","attachmentIds":[],"content":"in the ECLIP paper ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs\nhttps://arxiv.org/abs/2506.12598\nwhat do they mean by \"Unlike\n Nvidia’s MPS or MIG, which applies spatial partitions at the process\n level, AMD’s CU Masking feature within the ROCm runtime enables\n spatial partitioning at a finer granularity of streams by specifying\n which CUs to execute kernels on. \"","created_at":1758684871844,"messageId":"f6384d3a-28aa-4014-b8f7-3325153dff9c","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758684871844,"userId":"google:104458561188320708185","id":"j979csmn6cyyw4ev4mgj66jzgd7r7wm5","providerMetadata":{}},{"_creationTime":1758684871844.1426,"_id":"j979vn2sqbjs149yg1ztmjw8tn7r7jve","attachmentIds":[],"byok":false,"content":"They’re contrasting what is being partitioned and at what control granularity:\n\n- NVIDIA MPS/MIG (process-level spatial partitioning)\n  - MIG (Multi-Instance GPU) carves a GPU into fixed “slices” (sets of SMs, memory slices, etc.). Each slice is then bound to a process. All CUDA work launched by that process runs only on that slice. Changing the partitioning means reconfiguring MIG instances, which is relatively heavyweight, and the unit of control is the process.\n  - MPS allows multiple processes to share an SM pool with lower context-switch overhead, but you still don’t select specific SMs per kernel/stream; isolation and partitioning decisions are effectively per process or global, not per individual stream.\n\n- AMD ROCm CU Masking (stream-level spatial partitioning)\n  - On AMD GPUs, “compute units” (CUs) are the execution blocks roughly analogous to NVIDIA SMs.\n  - ROCm exposes a “CU Mask” you can attach to a stream/queue that says “kernels launched on this stream may execute only on these specific CUs.”\n  - Because the mask is applied at the stream (command queue) level, you can:\n    - Partition the GPU among multiple co-located workloads at finer granularity than processes.\n    - Change which CUs a particular subset of kernels (those launched on a given stream) can use without reconfiguring the whole GPU or rebinding processes.\n    - Even within a single process, assign different kernels or model stages to different CU subsets by launching them on different masked streams.\n\nIn short: NVIDIA’s common mechanisms (MIG/MPS) assign hardware slices or sharing policies to entire processes, while AMD’s CU Masking lets you pin kernels to specific CUs via per-stream masks. That finer granularity enables ECLIP to do “kernel-wise” or “stage-wise” resource allocation and quickly reshuffle CU assignments with low overhead, rather than tearing down and recreating process-level partitions.","created_at":1758684871845,"messageId":"56d10bc3-4cc6-462e-88d1-086d79485619","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":["arxiv.org","rocm.docs.amd.com","docs.amd.com","nvidia.com","developer.nvidia.com"],"queries":["ECLIP Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs paper 2506.12598 CU Masking ROCm streams explanation","AMD CU Masking ROCm stream granularity vs Nvidia MPS MIG process level spatial partitioning explanation"]},"result":[{"content":"# Electrical Engineering and Systems Science > Systems and Control\n\n**arXiv:2506.12598** (eess)\n\n\\[Submitted on 14 Jun 2025\\]\n\n# Title:ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs\n\nAuthors: [Ryan Quach](https://arxiv.org/search/eess?searchtype=author&query=Quach,+R), [Yidi Wang](https://arxiv.org/search/eess?searchtype=author&query=Wang,+Y), [Ali Jahanshahi](https://arxiv.org/search/eess?searchtype=author&query=Jahanshahi,+A), [Daniel Wong](https://arxiv.org/search/eess?searchtype=author&query=Wong,+D), [Hyoseung Kim](https://arxiv.org/search/eess?searchtype=author&query=Kim,+H)\n\nView a PDF of the paper titled ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs, by Ryan Quach and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2506.12598) [HTML (experimental)](https://arxiv.org/html/2506.12598v1)\n\n> Abstract:As AI inference becomes mainstream, research has begun to focus on improving the energy consumption of inference servers. Inference kernels commonly underutilize a GPU's compute resources and waste power from idling components. To improve utilization and energy efficiency, multiple models can co-locate and share the GPU. However, typical GPU spatial partitioning techniques often experience significant overheads when reconfiguring spatial partitions, which can waste additional energy through repartitioning overheads or non-optimal partition configurations. In this paper, we present ECLIP, a framework to enable low-overhead energy-efficient kernel-wise resource partitioning between co-located inference kernels. ECLIP minimizes repartitioning overheads by pre-allocating pools of CU masked streams and assigns optimal CU assignments to groups of kernels through our resource allocation optimizer. Overall, ECLIP achieves an average of 13% improvement to throughput and 25% improvement to energy efficiency.\n\n| | |\n| --- | --- |\n| Comments: | Accepted to ISLPED 2025 |\n| Subjects: | Systems and Control (eess.SY) |\n| Cite as: | [arXiv:2506.12598](https://arxiv.org/abs/2506.12598) \\[eess.SY\\] |\n| (or [arXiv:2506.12598v1](https://arxiv.org/abs/2506.12598v1) \\[eess.SY\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2506.12598](https://doi.org/10.48550/arXiv.2506.12598) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Ryan Quach \\[ [view email](https://arxiv.org/show-email/05675750/2506.12598)\\] **\\[v1\\]**\nSat, 14 Jun 2025 18:34:48 UTC (458 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs, by Ryan Quach and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2506.12598)\n- [HTML (experimental)](https://arxiv.org/html/2506.12598v1)\n- [TeX Source](https://arxiv.org/src/2506.12598)\n- [Other Formats](https://arxiv.org/format/2506.12598)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\neess.SY\n\n[< prev](https://arxiv.org/prevnext?id=2506.12598&function=prev&context=eess.SY)  \\|  [next >](https://arxiv.org/prevnext?id=2506.12598&function=next&context=eess.SY)\n\n[new](https://arxiv.org/list/eess.SY/new) \\| [recent](https://arxiv.org/list/eess.SY/recent) \\| [2025-06](https://arxiv.org/list/eess.SY/2025-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2506.12598?context=cs) [cs.SY](https://arxiv.org/abs/2506.12598?context=cs.SY) [eess](https://arxiv.org/abs/2506.12598?context=eess)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2506.12598)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2506.12598)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2506.12598)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2506.12598) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2506.12598","id":"https://arxiv.org/abs/2506.12598","publishedDate":"2025-06-14T00:00:00.000Z","summary":"This webpage is an arXiv entry for the paper \"ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs\" (arXiv:2506.12598). The paper, submitted on June 14, 2025, by Ryan Quach and others, presents ECLIP, a framework designed to improve the energy efficiency of AI inference servers by enabling low-overhead kernel-wise resource partitioning on GPUs. ECLIP achieves an average of 13% improvement to throughput and 25% improvement to energy efficiency. The paper was accepted to ISLPED 2025.\n","title":"ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs","url":"https://arxiv.org/abs/2506.12598"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2508.18556","id":"https://arxiv.org/abs/2508.18556","publishedDate":"2025-08-25T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"Managing Multi Instance GPUs for High Throughput and Energy Savings","url":"https://arxiv.org/abs/2508.18556"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2109.01611.pdf","id":"https://arxiv.org/pdf/2109.01611.pdf","publishedDate":"2023-10-26T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"Multi-model Machine Learning Inference Serving with GPU Spatial Partitioning","url":"https://arxiv.org/pdf/2109.01611.pdf"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2409.06646v1","id":"https://arxiv.org/html/2409.06646v1","publishedDate":"2024-07-12T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"Optimal Workload Placement on Multi-Instance GPUs","url":"https://arxiv.org/html/2409.06646v1"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2509.17357","id":"https://arxiv.org/abs/2509.17357","publishedDate":"2025-09-22T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via Partially Disaggregated Prefill","url":"https://arxiv.org/abs/2509.17357"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2105.10312v1.pdf","id":"https://arxiv.org/pdf/2105.10312v1.pdf","publishedDate":"2021-05-21T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"Contention-Aware GPU Partitioning and Task-to-Partition Allocation for Real-Time Workloads","url":"https://arxiv.org/pdf/2105.10312v1.pdf"}],"status":"completed","toolCallId":"call_Ugk0Gyx47banwSJHpfUM4Otj","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"They’re contrasting what is being partitioned and at what control granularity:\n\n- NVIDIA MPS/MIG (process-level spatial partitioning)\n  - MIG (Multi-Instance GPU) carves a GPU into fixed “slices” (sets of SMs, memory slices, etc.). Each slice is then bound to a process. All CUDA work launched by that process runs only on that slice. Changing the partitioning means reconfiguring MIG instances, which is relatively heavyweight, and the unit of control is the process.\n  - MPS allows multiple processes to share an SM pool with lower context-switch overhead, but you still don’t select specific SMs per kernel/stream; isolation and partitioning decisions are effectively per process or global, not per individual stream.\n\n- AMD ROCm CU Masking (stream-level spatial partitioning)\n  - On AMD GPUs, “compute units” (CUs) are the execution blocks roughly analogous to NVIDIA SMs.\n  - ROCm exposes a “CU Mask” you can attach to a stream/queue that says “kernels launched on this stream may execute only on these specific CUs.”\n  - Because the mask is applied at the stream (command queue) level, you can:\n    - Partition the GPU among multiple co-located workloads at finer granularity than processes.\n    - Change which CUs a particular subset of kernels (those launched on a given stream) can use without reconfiguring the whole GPU or rebinding processes.\n    - Even within a single process, assign different kernels or model stages to different CU subsets by launching them on different masked streams.\n\nIn short: NVIDIA’s common mechanisms (MIG/MPS) assign hardware slices or sharing policies to entire processes, while AMD’s CU Masking lets you pin kernels to specific CUs via per-stream masks. That finer granularity enables ECLIP to do “kernel-wise” or “stage-wise” resource allocation and quickly reshuffle CU assignments with low overhead, rather than tearing down and recreating process-level partitions.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d366d08d2c81a19ceab23ad73c351408f28b28fc7b22db"}},"resumableStreamId":"STREAM:3c9efb14-45c8-4768-9127-ebaedd0b9aa5","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":0.769,"tokens":415,"tokensPerSecond":24.031501534541665,"updated_at":1758684871845,"userId":"google:104458561188320708185","id":"j979vn2sqbjs149yg1ztmjw8tn7r7jve"},{"_creationTime":1758686770549.2673,"_id":"j97ex610e3dr5dtc41mjhrt2kx7r77ct","attachmentIds":[],"content":"kernel-granular spatial partitioning by (i) creating pools of streams\n with pre-defined CU masks, (ii) a runtime scheduler that intercepts\n and redirects kernels to appropriate CU-masked streams while main\ntaining inter-kernel data dependencies, and (iii) a resource alloca\ntion optimizer that assigns optimal CU assignments for groups of\n kernels while considering co-location scenarios, and other hardware\n constraints. \n\nwhat is this part saying","created_at":1758686770549,"messageId":"7d2e4bc8-ab30-4c71-b6cd-a9c2783d58a7","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758686770549,"userId":"google:104458561188320708185","id":"j97ex610e3dr5dtc41mjhrt2kx7r77ct","providerMetadata":{}},{"_creationTime":1758686770549.2676,"_id":"j973mdy3f8bkrjbtbm3mnv295x7r721p","attachmentIds":[],"byok":false,"content":"They’re describing how ECLIP actually enforces fine-grained (kernel-level) GPU partitioning on AMD GPUs using CU masks, and the three components that make it work:\n\n1) Pools of streams with pre-defined CU masks\n- They pre-create multiple GPU streams (command queues), each tagged with a CU mask (a selected subset of compute units).\n- Think of each stream as a “lane” that only runs on certain CUs. Different lanes cover different CU subsets (e.g., CU 0–7, CU 8–15, etc.).\n- This gives ready-to-use “partitions” without reconfiguring the GPU each time.\n\n2) A runtime scheduler that intercepts and redirects kernels\n- When the application launches a kernel, ECLIP intercepts that launch.\n- Based on the current plan, it reroutes the kernel to one of the pre-masked streams so it runs only on the intended CUs.\n- While doing this, it preserves data dependencies between kernels (e.g., by managing events, stream waits, or DAG ordering) so correctness and execution order are maintained.\n\n3) A resource allocation optimizer\n- This component decides how many and which CUs to assign to kernels (or groups of kernels/stages) to meet performance/throughput goals.\n- It accounts for co-location (multiple models/jobs running together), contention, and hardware limits (e.g., available CUs, memory bandwidth, cache, occupancy).\n- It outputs an assignment plan (e.g., Kernel group A → mask S1, group B → mask S2), which the scheduler then enforces via the masked stream pool.\n\nPut simply: they pre-make “lanes” on the GPU (streams with CU masks), a scheduler routes each kernel into the right lane while keeping dependencies correct, and an optimizer decides the best lane sizes/assignments given multiple concurrent workloads and hardware constraints.","created_at":1758686770550,"messageId":"6901809d-aeb6-42a4-a567-3af73a5ea766","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"They’re describing how ECLIP actually enforces fine-grained (kernel-level) GPU partitioning on AMD GPUs using CU masks, and the three components that make it work:\n\n1) Pools of streams with pre-defined CU masks\n- They pre-create multiple GPU streams (command queues), each tagged with a CU mask (a selected subset of compute units).\n- Think of each stream as a “lane” that only runs on certain CUs. Different lanes cover different CU subsets (e.g., CU 0–7, CU 8–15, etc.).\n- This gives ready-to-use “partitions” without reconfiguring the GPU each time.\n\n2) A runtime scheduler that intercepts and redirects kernels\n- When the application launches a kernel, ECLIP intercepts that launch.\n- Based on the current plan, it reroutes the kernel to one of the pre-masked streams so it runs only on the intended CUs.\n- While doing this, it preserves data dependencies between kernels (e.g., by managing events, stream waits, or DAG ordering) so correctness and execution order are maintained.\n\n3) A resource allocation optimizer\n- This component decides how many and which CUs to assign to kernels (or groups of kernels/stages) to meet performance/throughput goals.\n- It accounts for co-location (multiple models/jobs running together), contention, and hardware limits (e.g., available CUs, memory bandwidth, cache, occupancy).\n- It outputs an assignment plan (e.g., Kernel group A → mask S1, group B → mask S2), which the scheduler then enforces via the masked stream pool.\n\nPut simply: they pre-make “lanes” on the GPU (streams with CU masks), a scheduler routes each kernel into the right lane while keeping dependencies correct, and an optimizer decides the best lane sizes/assignments given multiple concurrent workloads and hardware constraints.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d36e33329c819199bfcf0608b3213106c797be4894d17f"}},"resumableStreamId":"STREAM:995f42dd-8883-4e47-9ed4-312abfd22224","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":0.981,"tokens":391,"tokensPerSecond":56.955571740713765,"updated_at":1758686770550,"userId":"google:104458561188320708185","id":"j973mdy3f8bkrjbtbm3mnv295x7r721p"},{"_creationTime":1758686783602.2856,"_id":"j97cfgnmqrgd8dpcnbf17p3fcn7r7tnt","attachmentIds":[],"content":"what is a CU mask","created_at":1758686783602,"messageId":"afff2840-fec7-4bad-b077-25b29dd77474","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758686783602,"userId":"google:104458561188320708185","id":"j97cfgnmqrgd8dpcnbf17p3fcn7r7tnt","providerMetadata":{}},{"_creationTime":1758686783602.286,"_id":"j97fc2jqn9pxzk3d2fj928mwvs7r6q2s","attachmentIds":[],"byok":false,"content":"A CU mask is a bitmask that specifies which Compute Units (CUs) on an AMD GPU are allowed to execute work from a given queue/stream.\n\n- Compute Units (CUs) are AMD’s parallel execution blocks (analogous to NVIDIA SMs).\n- In ROCm, you can attach a CU mask to a stream/command queue. Each bit in the mask corresponds to a CU (1 = usable, 0 = excluded).\n- Kernels launched on that masked stream will be scheduled only on the CUs enabled by the mask.\n\nWhy it matters:\n- It enables spatial partitioning: you can carve the GPU into subsets of CUs and direct different kernels or workloads to different subsets.\n- It can be done per stream (not just per process), so you get fine-grained control—useful for co-locating multiple models, isolating stages, or managing interference.\n\nExample conceptually:\n- GPU with 32 CUs → a 32-bit mask.\n- Mask 0x0000FFFF means “use CUs 0–15”; mask 0xFFFF0000 means “use CUs 16–31.”\n- Create two streams with those masks; kernels launched on each stream will run on their respective CU halves.","created_at":1758686783603,"messageId":"8201522d-86b8-44d8-a8a7-8eb6ddc26261","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"A CU mask is a bitmask that specifies which Compute Units (CUs) on an AMD GPU are allowed to execute work from a given queue/stream.\n\n- Compute Units (CUs) are AMD’s parallel execution blocks (analogous to NVIDIA SMs).\n- In ROCm, you can attach a CU mask to a stream/command queue. Each bit in the mask corresponds to a CU (1 = usable, 0 = excluded).\n- Kernels launched on that masked stream will be scheduled only on the CUs enabled by the mask.\n\nWhy it matters:\n- It enables spatial partitioning: you can carve the GPU into subsets of CUs and direct different kernels or workloads to different subsets.\n- It can be done per stream (not just per process), so you get fine-grained control—useful for co-locating multiple models, isolating stages, or managing interference.\n\nExample conceptually:\n- GPU with 32 CUs → a 32-bit mask.\n- Mask 0x0000FFFF means “use CUs 0–15”; mask 0xFFFF0000 means “use CUs 16–31.”\n- Create two streams with those masks; kernels launched on each stream will run on their respective CU halves.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d36e402114819586112ea4f40e42cf05c0e15f121bc616"}},"resumableStreamId":"STREAM:c42098e9-8ad3-4214-851c-6e63d76fdfe8","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":0.841,"tokens":261,"tokensPerSecond":53.22185970636215,"updated_at":1758686783603,"userId":"google:104458561188320708185","id":"j97fc2jqn9pxzk3d2fj928mwvs7r6q2s"},{"_creationTime":1758686819692.591,"_id":"j977fgpg2gs3r8srte12qk9gah7r6tfc","attachmentIds":[],"content":"how do those masks allow the hardware to run different applications in paralle","created_at":1758686819692,"messageId":"7741a109-13d5-45d9-b070-796824c31e16","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758686819692,"userId":"google:104458561188320708185","id":"j977fgpg2gs3r8srte12qk9gah7r6tfc","providerMetadata":{}},{"_creationTime":1758686819692.5913,"_id":"j973j14b4e3xvc8e100grcbgrd7r6ec8","attachmentIds":[],"byok":false,"content":"Short answer: CU masks carve the GPU’s execution resources into disjoint subsets and bind each app’s work to a subset. The hardware scheduler can then dispatch kernels from different apps to different CU subsets at the same time, so they run in parallel without contending for the same CUs.\n\nHow it works step by step:\n- Partitioning: A CU mask is a per-queue/stream allowlist of CUs. If App A’s kernels are issued on a stream with mask M1 and App B’s kernels on a stream with mask M2, and M1 and M2 are disjoint (no overlapping bits), then A and B are confined to different CUs.\n- Dispatch: The GPU’s work distributor only schedules waves/warps from a queue onto CUs that are enabled by that queue’s mask. So A’s waves can land only on CUs in M1; B’s only on CUs in M2.\n- Concurrency: Because the allowed CU sets don’t overlap, the hardware can keep CUs from M1 busy with A’s kernels while simultaneously keeping CUs from M2 busy with B’s kernels. No per-kernel preemption is needed; both progress independently.\n- Isolation and fairness: Masks reduce interference by preventing one app from occupying the other’s CUs. This yields more predictable latency/throughput than unconstrained sharing. The runtime can also resize masks (reassign CUs) between kernels to adapt to load.\n\nNotes and limits:\n- Other shared resources (L2 cache, memory controllers, DRAM bandwidth) are still shared, so masks don’t eliminate all interference.\n- If masks overlap, apps may still contend on the overlapping CUs.\n- To use masks effectively, each app issues work on streams/queues configured with the desired mask; a runtime can intercept kernel launches and route them to the right masked stream.","created_at":1758686819693,"messageId":"11b91854-362f-435e-964c-24e8086c1d7a","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Short answer: CU masks carve the GPU’s execution resources into disjoint subsets and bind each app’s work to a subset. The hardware scheduler can then dispatch kernels from different apps to different CU subsets at the same time, so they run in parallel without contending for the same CUs.\n\nHow it works step by step:\n- Partitioning: A CU mask is a per-queue/stream allowlist of CUs. If App A’s kernels are issued on a stream with mask M1 and App B’s kernels on a stream with mask M2, and M1 and M2 are disjoint (no overlapping bits), then A and B are confined to different CUs.\n- Dispatch: The GPU’s work distributor only schedules waves/warps from a queue onto CUs that are enabled by that queue’s mask. So A’s waves can land only on CUs in M1; B’s only on CUs in M2.\n- Concurrency: Because the allowed CU sets don’t overlap, the hardware can keep CUs from M1 busy with A’s kernels while simultaneously keeping CUs from M2 busy with B’s kernels. No per-kernel preemption is needed; both progress independently.\n- Isolation and fairness: Masks reduce interference by preventing one app from occupying the other’s CUs. This yields more predictable latency/throughput than unconstrained sharing. The runtime can also resize masks (reassign CUs) between kernels to adapt to load.\n\nNotes and limits:\n- Other shared resources (L2 cache, memory controllers, DRAM bandwidth) are still shared, so masks don’t eliminate all interference.\n- If masks overlap, apps may still contend on the overlapping CUs.\n- To use masks effectively, each app issues work on streams/queues configured with the desired mask; a runtime can intercept kernel launches and route them to the right masked stream.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d36e6442a48190a43697a0be3292eb0a16755c0193a163"}},"resumableStreamId":"STREAM:324c2dd0-7aeb-49a0-817f-c8b1d9ad104a","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":1.076,"tokens":384,"tokensPerSecond":69.54002173125679,"updated_at":1758686819693,"userId":"google:104458561188320708185","id":"j973j14b4e3xvc8e100grcbgrd7r6ec8"},{"_creationTime":1758686928429.9124,"_id":"j97ahmxg5qee4r2v3xq1dgmn8h7r7gdt","attachmentIds":[],"content":"what is this\nHIP (Heterogeneous-Compute Interface\n for Portability) ","created_at":1758686935569,"messageId":"acf04f98-6047-41ee-a5c2-ff2742e13ba3","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758686935569,"userId":"google:104458561188320708185","id":"j97ahmxg5qee4r2v3xq1dgmn8h7r7gdt","providerMetadata":{}},{"_creationTime":1758686935569.3696,"_id":"j974fk8wb7dpxgga44m1gyh1d17r7d6d","attachmentIds":[],"byok":false,"content":"HIP (Heterogeneous-Compute Interface for Portability) is AMD’s C++ programming model and runtime API for GPU computing. It’s designed to be source-compatible with CUDA-style code so developers can write (or port) GPU kernels once and build them for:\n\n- AMD GPUs via ROCm (the AMD toolchain), and\n- NVIDIA GPUs via a CUDA backend (using HIP’s CUDA path).\n\nKey points:\n- Language/API: Very similar to CUDA (kernels, grids/blocks, threads, streams, events, memory APIs). Many CUDA calls have near drop-in HIP equivalents.\n- Tooling: \n  - hipcc/hipclang compilers\n  - hipify tools to convert CUDA code to HIP automatically\n- Runtime: Provides device management, streams/queues, events, memory alloc/copy, kernel launch, etc., across supported backends.\n- Goal: Portability without large rewrites, enabling one codebase to target multiple GPU vendors.\n\nIn the context of ROCm, HIP is the main way applications submit kernels and manage GPU execution (including advanced features like assigning CU masks to HIP streams on AMD hardware).","created_at":1758686935570,"messageId":"e66bcc51-d4f5-4b74-9ef9-1499847e8039","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"HIP (Heterogeneous-Compute Interface for Portability) is AMD’s C++ programming model and runtime API for GPU computing. It’s designed to be source-compatible with CUDA-style code so developers can write (or port) GPU kernels once and build them for:\n\n- AMD GPUs via ROCm (the AMD toolchain), and\n- NVIDIA GPUs via a CUDA backend (using HIP’s CUDA path).\n\nKey points:\n- Language/API: Very similar to CUDA (kernels, grids/blocks, threads, streams, events, memory APIs). Many CUDA calls have near drop-in HIP equivalents.\n- Tooling: \n  - hipcc/hipclang compilers\n  - hipify tools to convert CUDA code to HIP automatically\n- Runtime: Provides device management, streams/queues, events, memory alloc/copy, kernel launch, etc., across supported backends.\n- Goal: Portability without large rewrites, enabling one codebase to target multiple GPU vendors.\n\nIn the context of ROCm, HIP is the main way applications submit kernels and manage GPU execution (including advanced features like assigning CU masks to HIP streams on AMD hardware).","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d36ed81744819e92d2ad5201b42ac00f927760107a677b"}},"resumableStreamId":"STREAM:30902c83-5781-4815-b399-54bec0a8ccc5","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":0.71,"tokens":235,"tokensPerSecond":57.64042187883248,"updated_at":1758686935570,"userId":"google:104458561188320708185","id":"j974fk8wb7dpxgga44m1gyh1d17r7d6d"},{"_creationTime":1758687022797.622,"_id":"j9780wsdmgzkq9dndf7aaxpg2n7r70rb","attachmentIds":[],"content":"what is the cuda equivelent to amd's shader engine","created_at":1758687022797,"messageId":"83a9e99a-6da5-45bf-8ca1-a466846ded72","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758687022797,"userId":"google:104458561188320708185","id":"j9780wsdmgzkq9dndf7aaxpg2n7r70rb","providerMetadata":{}},{"_creationTime":1758687022797.6223,"_id":"j972rgmjtynvzwwjpahmzg7mk17r6t18","attachmentIds":[],"byok":false,"content":"Closest rough mapping:\n\n- AMD Shader Engine (SE): A top-level partition of the GPU that contains a group of front-end/raster units and a cluster of Compute Units (CUs). Modern AMD GPUs often have multiple SEs; each SE contains several Shader Arrays/Workgroup Processors (WGPs) made up of CUs.\n\n- NVIDIA rough equivalent: Graphics Processing Cluster (GPC).\n  - A GPC is a top-level partition that includes front-end/raster units and a cluster of Streaming Multiprocessors (SMs).\n  - Within a GPC, SMs are further organized into Texture Processing Clusters (TPCs) on many architectures.\n\nSo:\n- AMD CU ≈ NVIDIA SM (execution block)\n- AMD Shader Array/WGP (grouping of CUs) ≈ NVIDIA TPC (grouping of SMs)\n- AMD Shader Engine (grouping of shader arrays + front-end) ≈ NVIDIA GPC (grouping of TPCs + front-end)\n\nNote: The mapping isn’t 1:1 across all generations, but SE ↔ GPC is the closest conceptual match.","created_at":1758687022798,"messageId":"8c20205d-4809-4180-97e3-ae1708573e1f","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Closest rough mapping:\n\n- AMD Shader Engine (SE): A top-level partition of the GPU that contains a group of front-end/raster units and a cluster of Compute Units (CUs). Modern AMD GPUs often have multiple SEs; each SE contains several Shader Arrays/Workgroup Processors (WGPs) made up of CUs.\n\n- NVIDIA rough equivalent: Graphics Processing Cluster (GPC).\n  - A GPC is a top-level partition that includes front-end/raster units and a cluster of Streaming Multiprocessors (SMs).\n  - Within a GPC, SMs are further organized into Texture Processing Clusters (TPCs) on many architectures.\n\nSo:\n- AMD CU ≈ NVIDIA SM (execution block)\n- AMD Shader Array/WGP (grouping of CUs) ≈ NVIDIA TPC (grouping of SMs)\n- AMD Shader Engine (grouping of shader arrays + front-end) ≈ NVIDIA GPC (grouping of TPCs + front-end)\n\nNote: The mapping isn’t 1:1 across all generations, but SE ↔ GPC is the closest conceptual match.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d36f2f6ca0819e8c67e995d01733530e6b847f16f0d904"}},"resumableStreamId":"STREAM:5d610886-8387-406b-89a3-d4c9ab88742b","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":0.795,"tokens":235,"tokensPerSecond":44.98468606431853,"updated_at":1758687022798,"userId":"google:104458561188320708185","id":"j972rgmjtynvzwwjpahmzg7mk17r6t18"},{"_creationTime":1758687413037.3054,"_id":"j970jaq80nv28r52wbm6fcwx0x7r7xqa","attachmentIds":[],"content":"what is HSA barrier packets","created_at":1758687413037,"messageId":"b12cb57a-fd3f-4dba-90d2-10fc2aa524d9","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758687413037,"userId":"google:104458561188320708185","id":"j970jaq80nv28r52wbm6fcwx0x7r7xqa","providerMetadata":{}},{"_creationTime":1758687413037.3057,"_id":"j97f72hgxbr6gntyzf8va9z5857r695e","attachmentIds":[],"byok":false,"content":"HSA barrier packets are special commands in the HSA (Heterogeneous System Architecture) queue that enforce ordering and dependency constraints between other packets (like kernel dispatches and memory copies). They act as synchronization points for work submitted to an HSA agent (e.g., a GPU).\n\nKey ideas:\n- Packets: In HSA, work is submitted as packets to a ring buffer (the AQL queue). Examples: kernel dispatch packets, agent-to-host copies, host-to-agent copies, and barrier packets.\n- Barrier purpose: A barrier packet doesn’t perform computation itself. Instead, it ensures that:\n  - All specified prior packets complete before subsequent packets begin, and/or\n  - All specified signals reach certain states before allowing later work to proceed.\n- How it works:\n  - A barrier packet can list up to a small number of dependency signals (e.g., completion signals from earlier dispatches).\n  - When the GPU scheduler reaches the barrier in the queue, it waits until those signals indicate completion, then it “releases” the barrier so following packets can execute.\n- Types/variants you’ll see:\n  - Barrier-and (wait until all listed signals are satisfied).\n  - Barrier-or (wait until any one of the listed signals is satisfied).\n  - Memory fence scopes can be specified (system, agent, work-group) to ensure visibility/ordering of memory effects across CPU/GPU or between kernels.\n- Why they’re used:\n  - To maintain inter-kernel dependencies when launching kernels asynchronously.\n  - To sequence DMA copies relative to kernels (e.g., ensure input copy finishes before compute starts; ensure compute finishes before output copy).\n  - To coordinate work across multiple queues/streams using signals.\n\nIn ROCm/HIP context:\n- Under the hood, HIP streams and events often translate to HSA AQL packets and signals. A HIP event “wait” can become an HSA barrier packet that waits on the event’s signal, providing correct ordering across queues with minimal overhead.","created_at":1758687413038,"messageId":"f67dd2db-452f-406d-91a6-334e0bcb61cf","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"HSA barrier packets are special commands in the HSA (Heterogeneous System Architecture) queue that enforce ordering and dependency constraints between other packets (like kernel dispatches and memory copies). They act as synchronization points for work submitted to an HSA agent (e.g., a GPU).\n\nKey ideas:\n- Packets: In HSA, work is submitted as packets to a ring buffer (the AQL queue). Examples: kernel dispatch packets, agent-to-host copies, host-to-agent copies, and barrier packets.\n- Barrier purpose: A barrier packet doesn’t perform computation itself. Instead, it ensures that:\n  - All specified prior packets complete before subsequent packets begin, and/or\n  - All specified signals reach certain states before allowing later work to proceed.\n- How it works:\n  - A barrier packet can list up to a small number of dependency signals (e.g., completion signals from earlier dispatches).\n  - When the GPU scheduler reaches the barrier in the queue, it waits until those signals indicate completion, then it “releases” the barrier so following packets can execute.\n- Types/variants you’ll see:\n  - Barrier-and (wait until all listed signals are satisfied).\n  - Barrier-or (wait until any one of the listed signals is satisfied).\n  - Memory fence scopes can be specified (system, agent, work-group) to ensure visibility/ordering of memory effects across CPU/GPU or between kernels.\n- Why they’re used:\n  - To maintain inter-kernel dependencies when launching kernels asynchronously.\n  - To sequence DMA copies relative to kernels (e.g., ensure input copy finishes before compute starts; ensure compute finishes before output copy).\n  - To coordinate work across multiple queues/streams using signals.\n\nIn ROCm/HIP context:\n- Under the hood, HIP streams and events often translate to HSA AQL packets and signals. A HIP event “wait” can become an HSA barrier packet that waits on the event’s signal, providing correct ordering across queues with minimal overhead.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d370b59e20819cb3f635e640522aa80b4dae4d7f228fcb"}},"resumableStreamId":"STREAM:e523a4bb-7f58-4581-97f7-047fc596341d","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":0.755,"tokens":410,"tokensPerSecond":40.95085896923691,"updated_at":1758687413038,"userId":"google:104458561188320708185","id":"j97f72hgxbr6gntyzf8va9z5857r695e"},{"_creationTime":1758730958087.1252,"_id":"j97bnw09g0026tzfxkjw2x5bq57r7k4v","attachmentIds":[],"content":"what is the structure of a nvidia gpu, starting from top to bottom, with the number of each specified for an a100","created_at":1758730958087,"messageId":"c1c7cf50-68d8-4088-8952-f9b63eb7f049","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758730958087,"userId":"google:104458561188320708185","id":"j97bnw09g0026tzfxkjw2x5bq57r7k4v","providerMetadata":{}},{"_creationTime":1758730958087.1255,"_id":"j97d53xvwyw2z9awx8y49a15fn7r6m9v","attachmentIds":[],"byok":false,"content":"Here’s a practical, top‑to‑bottom view of the NVIDIA A100 (Ampere) GPU’s main hierarchy and counts for the common 40/80 GB PCIe/SXM variants.\n\nTop-level chip\n- 1 GPU die (GA100)\n- 1 HBM2(e) memory subsystem: 5 HBM stacks (each 8‑Hi on 80 GB models, 6‑Hi on 40 GB), 5120‑bit total bus (5 × 1024‑bit)\n\nGraphics/compute front-end hierarchy\n- 7 GPCs (Graphics Processing Clusters)\n  - Each GPC contains:\n    - 1 Raster Engine (for graphics; A100 is mostly used for compute but the unit exists)\n    - 8 TPCs (Texture Processing Clusters)\n      - Each TPC contains:\n        - 2 SMs (Streaming Multiprocessors)\n          - So per GPC: 8 TPC × 2 SM = 16 SMs\n\nTotals (logical, pre‑fuse)\n- Up to 7 GPC × 16 SM/GPC = 112 SMs maximum on GA100\n- Shipping A100 parts expose 108 SMs (4 SMs disabled for yield/power), i.e., 54 TPCs enabled\n\nPer-SM (Streaming Multiprocessor) resources (A100)\n- Tensor Cores: 4 per SM (3rd‑gen tensor cores)\n- FP32 CUDA cores: 64 per SM\n- FP64 CUDA cores: 2 per SM (separate FP64 units)\n- Load/Store, SFUs, schedulers/dispatchers, register file, shared memory/L1 cache\n- Shared memory/L1: up to 164 KB per SM (configurable)\n- Warp schedulers: 4 per SM; issue width 2\n- Instruction/warp slots, etc.\n\nChip-level aggregates (for the 108‑SM A100)\n- SMs: 108\n- Tensor Cores: 108 × 4 = 432\n- FP32 CUDA cores: 108 × 64 = 6912\n- L2 cache: 40 MB total\n- HBM bandwidth:\n  - A100 40 GB: up to ~1555 GB/s (HBM2, 1215 MHz)\n  - A100 80 GB: up to ~2039 GB/s (HBM2e, 1593 MHz)\n- NVLink/NVSwitch:\n  - SXM A100: 12 NVLink 3.0 links (600 GB/s bidirectional total)\n  - PCIe A100: PCIe Gen4 x16; no NVLink on some PCIe models\n\nMemory/IO partitions\n- 20 memory controllers/partitions (each 256‑bit), grouped to service the 5 HBM stacks\n- 40 MB L2 cache sliced across memory partitions (banked/sliced, shared by all SMs)\n- Copy engines (DMA), video engines, display (often disabled), and firmware/management complex\n\nSummary hierarchy with counts (A100 as shipped)\n- GPU: 1\n- GPCs: 7\n- TPCs per GPC: 8\n- SMs per TPC: 2\n- Enabled SMs total: 108 (out of 112 max)\n- Tensor Cores per SM: 4 → 432 total\n- FP32 cores per SM: 64 → 6912 total\n- L2 cache: 40 MB shared\n- HBM stacks: 5 (40 GB or 80 GB total capacity depending on stack height/speed)\n\nNotes\n- The theoretical max (112 SMs) reflects the full GA100; most A100 SKUs have 108 enabled.\n- The graphics-oriented units (Raster Engine, TPC naming) are part of NVIDIA’s consistent block hierarchy even when the product is used primarily for compute.","created_at":1758730958088,"messageId":"17b158b3-bd14-4881-ac24-d6980042a4aa","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Here’s a practical, top‑to‑bottom view of the NVIDIA A100 (Ampere) GPU’s main hierarchy and counts for the common 40/80 GB PCIe/SXM variants.\n\nTop-level chip\n- 1 GPU die (GA100)\n- 1 HBM2(e) memory subsystem: 5 HBM stacks (each 8‑Hi on 80 GB models, 6‑Hi on 40 GB), 5120‑bit total bus (5 × 1024‑bit)\n\nGraphics/compute front-end hierarchy\n- 7 GPCs (Graphics Processing Clusters)\n  - Each GPC contains:\n    - 1 Raster Engine (for graphics; A100 is mostly used for compute but the unit exists)\n    - 8 TPCs (Texture Processing Clusters)\n      - Each TPC contains:\n        - 2 SMs (Streaming Multiprocessors)\n          - So per GPC: 8 TPC × 2 SM = 16 SMs\n\nTotals (logical, pre‑fuse)\n- Up to 7 GPC × 16 SM/GPC = 112 SMs maximum on GA100\n- Shipping A100 parts expose 108 SMs (4 SMs disabled for yield/power), i.e., 54 TPCs enabled\n\nPer-SM (Streaming Multiprocessor) resources (A100)\n- Tensor Cores: 4 per SM (3rd‑gen tensor cores)\n- FP32 CUDA cores: 64 per SM\n- FP64 CUDA cores: 2 per SM (separate FP64 units)\n- Load/Store, SFUs, schedulers/dispatchers, register file, shared memory/L1 cache\n- Shared memory/L1: up to 164 KB per SM (configurable)\n- Warp schedulers: 4 per SM; issue width 2\n- Instruction/warp slots, etc.\n\nChip-level aggregates (for the 108‑SM A100)\n- SMs: 108\n- Tensor Cores: 108 × 4 = 432\n- FP32 CUDA cores: 108 × 64 = 6912\n- L2 cache: 40 MB total\n- HBM bandwidth:\n  - A100 40 GB: up to ~1555 GB/s (HBM2, 1215 MHz)\n  - A100 80 GB: up to ~2039 GB/s (HBM2e, 1593 MHz)\n- NVLink/NVSwitch:\n  - SXM A100: 12 NVLink 3.0 links (600 GB/s bidirectional total)\n  - PCIe A100: PCIe Gen4 x16; no NVLink on some PCIe models\n\nMemory/IO partitions\n- 20 memory controllers/partitions (each 256‑bit), grouped to service the 5 HBM stacks\n- 40 MB L2 cache sliced across memory partitions (banked/sliced, shared by all SMs)\n- Copy engines (DMA), video engines, display (often disabled), and firmware/management complex\n\nSummary hierarchy with counts (A100 as shipped)\n- GPU: 1\n- GPCs: 7\n- TPCs per GPC: 8\n- SMs per TPC: 2\n- Enabled SMs total: 108 (out of 112 max)\n- Tensor Cores per SM: 4 → 432 total\n- FP32 cores per SM: 64 → 6912 total\n- L2 cache: 40 MB shared\n- HBM stacks: 5 (40 GB or 80 GB total capacity depending on stack height/speed)\n\nNotes\n- The theoretical max (112 SMs) reflects the full GA100; most A100 SKUs have 108 enabled.\n- The graphics-oriented units (Raster Engine, TPC naming) are part of NVIDIA’s consistent block hierarchy even when the product is used primarily for compute.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d41acebc44819e9f73176e5b9a725003226f9b42e310a0"}},"resumableStreamId":"STREAM:3dad56a4-68ff-4b11-813c-c1dac343b835","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":1.001,"tokens":836,"tokensPerSecond":46.934650797215355,"updated_at":1758730958088,"userId":"google:104458561188320708185","id":"j97d53xvwyw2z9awx8y49a15fn7r6m9v"},{"_creationTime":1758730994568.2957,"_id":"j970xhs83qznawrartjzavpbrh7r7yj7","attachmentIds":[],"content":"what about for an AMD GPU","created_at":1758730994568,"messageId":"d8b6e3d1-f5cb-4d1b-b8d4-04d65c2b0785","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758730994568,"userId":"google:104458561188320708185","id":"j970xhs83qznawrartjzavpbrh7r7yj7","providerMetadata":{}},{"_creationTime":1758730994568.296,"_id":"j97dkv879xw055j1bd1h77zgy97r6m8p","attachmentIds":[],"byok":false,"content":"Here’s the common top-to-bottom hierarchy for modern AMD datacenter GPUs (CDNA family, e.g., MI100/MI200/MI300). I’ll give both the general structure and concrete numbers for representative parts so you can map it.\n\nGeneral AMD GPU hierarchy (compute-focused, CDNA/CDNA2/CDNA3)\n- GPU package\n  - Multiple GPU dies/chiplets (varies by generation; MI250 has 2 GCDs; MI300A/X have multiple compute chiplets)\n  - HBM stacks (each 1024-bit interface) around the compute dies\n- Per GPU die (GCD)\n  - Shader Engines (SEs)\n    - Within each SE:\n      - Shader Arrays (also called SA) or Workgroup Processor (WGP) arrays\n        - Workgroup Processors (WGPs)\n          - Each WGP = 2 Compute Units (CUs) operating in lockstep for wave64 (CDNA uses WGP as the scheduling unit)\n            - Compute Unit (CU)\n              - SIMD units (e.g., 4× SIMD32 per CU on many gens; WGP effectively 8× SIMD32)\n              - Scalar unit, LDS (shared memory), register files, texture/load-store units (varies on pure compute GPUs), instruction caches\n  - L2 cache slices (shared across the die)\n  - Memory controllers/partitions for HBM\n  - SDMA (copy) engines, command processor, schedulers, fabric links (Infinity Fabric/XGMI)\n\nConcrete examples\n\n1) AMD Instinct MI100 (CDNA1, single die)\n- Dies: 1 GCD\n- Shader Engines: 4 SE\n- WGPs per SE: 14\n- Total WGPs: 56\n- CUs: 112 (since 1 WGP = 2 CUs)\n- Matrix (Matrix Core/Matrix FMA) units: per CU/WGP (architecture-specific)\n- L2 cache: 32 MB\n- HBM: 4 stacks (32 GB total), 4096-bit bus, ~1.23 TB/s bandwidth\n\n2) AMD Instinct MI250/MI250X (CDNA2, dual-die MCM)\n- Dies: 2 GCDs per package (often called “GPU 0” and “GPU 1”)\n- Per GCD:\n  - Shader Engines: 4\n  - WGPs per SE: 14 (typical)\n  - Total WGPs per GCD: 56\n  - CUs per GCD: 112\n- Package totals (both GCDs combined):\n  - WGPs: 112\n  - CUs: 224\n  - L2 cache: 64 MB aggregate (32 MB per GCD)\n  - HBM: 8 stacks total (64 GB on MI250, 128 GB on MI250X), ~3.2–3.6 TB/s bandwidth aggregate\n- Inter-die link: Infinity Fabric/XGMI inside the package\n\n3) AMD Instinct MI300X (CDNA3, chiplet-based)\n- Compute chiplets: 8 Compute Dies (CDs) arranged around HBM and I/O dies\n- Logical grouping still exposes WGP/CU hierarchy, but counts differ:\n  - Total CUs: 304\n  - Hence total WGPs: 152\n  - Organized across multiple “Shader Engine”-like logical partitions per compute die\n- L2 cache: large, distributed across chiplets (hundreds of MB-class cache including HBM-resident features; exact L2 per die is vendor-doc specific)\n- HBM: 8 stacks, 192 GB total, >5 TB/s bandwidth\n- Interconnect: on-package Infinity Fabric between compute, I/O, and memory chiplets\n\nQuick mapping of units\n- Shader Engine (SE): top-level partition of a die (closest NVIDIA analog: GPC)\n- Shader Array / WGP array: mid-level grouping within an SE (closest NVIDIA analog: TPC/SM cluster)\n- Workgroup Processor (WGP): the primary scheduling block for CDNA; each contains two CUs\n- Compute Unit (CU): execution block (closest NVIDIA analog: SM). Contains SIMD lanes, LDS (shared memory), registers, schedulers.\n\nNotes\n- For HIP/ROCm scheduling and CU masking, masks are typically in terms of CUs (or WGPs on newer gens), enabling spatial partitioning at that granularity.\n- Counts can vary slightly by SKU (some CUs may be disabled for yield/power).\n- Graphics-focused RDNA parts use similar terms but add graphics front-end blocks; CDNA trims graphics and focuses on compute.\n\nIf you have a specific AMD GPU in mind (e.g., MI210 vs MI250X vs MI300A/X), I can list the exact SE/WGP/CU counts for that model.","created_at":1758730994569,"messageId":"963265a5-df28-460b-b941-bd5391bf6f0d","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Here’s the common top-to-bottom hierarchy for modern AMD datacenter GPUs (CDNA family, e.g., MI100/MI200/MI300). I’ll give both the general structure and concrete numbers for representative parts so you can map it.\n\nGeneral AMD GPU hierarchy (compute-focused, CDNA/CDNA2/CDNA3)\n- GPU package\n  - Multiple GPU dies/chiplets (varies by generation; MI250 has 2 GCDs; MI300A/X have multiple compute chiplets)\n  - HBM stacks (each 1024-bit interface) around the compute dies\n- Per GPU die (GCD)\n  - Shader Engines (SEs)\n    - Within each SE:\n      - Shader Arrays (also called SA) or Workgroup Processor (WGP) arrays\n        - Workgroup Processors (WGPs)\n          - Each WGP = 2 Compute Units (CUs) operating in lockstep for wave64 (CDNA uses WGP as the scheduling unit)\n            - Compute Unit (CU)\n              - SIMD units (e.g., 4× SIMD32 per CU on many gens; WGP effectively 8× SIMD32)\n              - Scalar unit, LDS (shared memory), register files, texture/load-store units (varies on pure compute GPUs), instruction caches\n  - L2 cache slices (shared across the die)\n  - Memory controllers/partitions for HBM\n  - SDMA (copy) engines, command processor, schedulers, fabric links (Infinity Fabric/XGMI)\n\nConcrete examples\n\n1) AMD Instinct MI100 (CDNA1, single die)\n- Dies: 1 GCD\n- Shader Engines: 4 SE\n- WGPs per SE: 14\n- Total WGPs: 56\n- CUs: 112 (since 1 WGP = 2 CUs)\n- Matrix (Matrix Core/Matrix FMA) units: per CU/WGP (architecture-specific)\n- L2 cache: 32 MB\n- HBM: 4 stacks (32 GB total), 4096-bit bus, ~1.23 TB/s bandwidth\n\n2) AMD Instinct MI250/MI250X (CDNA2, dual-die MCM)\n- Dies: 2 GCDs per package (often called “GPU 0” and “GPU 1”)\n- Per GCD:\n  - Shader Engines: 4\n  - WGPs per SE: 14 (typical)\n  - Total WGPs per GCD: 56\n  - CUs per GCD: 112\n- Package totals (both GCDs combined):\n  - WGPs: 112\n  - CUs: 224\n  - L2 cache: 64 MB aggregate (32 MB per GCD)\n  - HBM: 8 stacks total (64 GB on MI250, 128 GB on MI250X), ~3.2–3.6 TB/s bandwidth aggregate\n- Inter-die link: Infinity Fabric/XGMI inside the package\n\n3) AMD Instinct MI300X (CDNA3, chiplet-based)\n- Compute chiplets: 8 Compute Dies (CDs) arranged around HBM and I/O dies\n- Logical grouping still exposes WGP/CU hierarchy, but counts differ:\n  - Total CUs: 304\n  - Hence total WGPs: 152\n  - Organized across multiple “Shader Engine”-like logical partitions per compute die\n- L2 cache: large, distributed across chiplets (hundreds of MB-class cache including HBM-resident features; exact L2 per die is vendor-doc specific)\n- HBM: 8 stacks, 192 GB total, >5 TB/s bandwidth\n- Interconnect: on-package Infinity Fabric between compute, I/O, and memory chiplets\n\nQuick mapping of units\n- Shader Engine (SE): top-level partition of a die (closest NVIDIA analog: GPC)\n- Shader Array / WGP array: mid-level grouping within an SE (closest NVIDIA analog: TPC/SM cluster)\n- Workgroup Processor (WGP): the primary scheduling block for CDNA; each contains two CUs\n- Compute Unit (CU): execution block (closest NVIDIA analog: SM). Contains SIMD lanes, LDS (shared memory), registers, schedulers.\n\nNotes\n- For HIP/ROCm scheduling and CU masking, masks are typically in terms of CUs (or WGPs on newer gens), enabling spatial partitioning at that granularity.\n- Counts can vary slightly by SKU (some CUs may be disabled for yield/power).\n- Graphics-focused RDNA parts use similar terms but add graphics front-end blocks; CDNA trims graphics and focuses on compute.\n\nIf you have a specific AMD GPU in mind (e.g., MI210 vs MI250X vs MI300A/X), I can list the exact SE/WGP/CU counts for that model.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d41af370708191ba566a148241495d09ca605469172794"}},"resumableStreamId":"STREAM:39bcdf21-4374-4daa-831c-cae8a3ee09fe","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":0.982,"tokens":1029,"tokensPerSecond":46.63494221617947,"updated_at":1758730994569,"userId":"google:104458561188320708185","id":"j97dkv879xw055j1bd1h77zgy97r6m8p"},{"_creationTime":1758731074057.9915,"_id":"j979aqj5k1z1bez9xfk42619a17r6q35","attachmentIds":[],"content":"so a compute unit is equivalent to a SM for nvidia?","created_at":1758731074058,"messageId":"f9aba7d0-306f-4938-9675-4e3942381021","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758731074058,"userId":"google:104458561188320708185","id":"j979aqj5k1z1bez9xfk42619a17r6q35","providerMetadata":{}},{"_creationTime":1758731074057.9917,"_id":"j972ykxh1ets49b3z3ek104n3s7r7ws9","attachmentIds":[],"byok":false,"content":"Mostly, yes—conceptually.\n\n- Role: AMD’s Compute Unit (CU) is the fundamental execution block, analogous to NVIDIA’s Streaming Multiprocessor (SM).\n- Caveat by generation:\n  - Older GCN/CDNA1: scheduler targets CUs directly → CU ≈ SM.\n  - Newer AMD (RDNA/CDNA2/CDNA3): two CUs are paired into a Workgroup Processor (WGP), which is the main scheduling unit. In that sense, WGP ≈ SM for scheduling, while each CU is roughly half an SM’s worth of resources.\n- Practical mapping:\n  - AMD CU ≈ half of an NVIDIA SM (on WGP-era parts)\n  - AMD WGP (2 CUs) ≈ NVIDIA SM (for how kernels are scheduled/issued)\n\nSo “CU ≈ SM” is a good shorthand, but for precise comparisons on modern AMD, think in terms of WGPs.","created_at":1758731074059,"messageId":"0ea4ca13-7a06-401f-b077-1c05a771951c","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Mostly, yes—conceptually.\n\n- Role: AMD’s Compute Unit (CU) is the fundamental execution block, analogous to NVIDIA’s Streaming Multiprocessor (SM).\n- Caveat by generation:\n  - Older GCN/CDNA1: scheduler targets CUs directly → CU ≈ SM.\n  - Newer AMD (RDNA/CDNA2/CDNA3): two CUs are paired into a Workgroup Processor (WGP), which is the main scheduling unit. In that sense, WGP ≈ SM for scheduling, while each CU is roughly half an SM’s worth of resources.\n- Practical mapping:\n  - AMD CU ≈ half of an NVIDIA SM (on WGP-era parts)\n  - AMD WGP (2 CUs) ≈ NVIDIA SM (for how kernels are scheduled/issued)\n\nSo “CU ≈ SM” is a good shorthand, but for precise comparisons on modern AMD, think in terms of WGPs.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d41b42ab248191b58bd72d2b221c0a05e6b4d17bcc8bac"}},"resumableStreamId":"STREAM:c09b11d2-1565-436e-aa9c-2bdc8ee869ff","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":0.83,"tokens":200,"tokensPerSecond":51.62622612287042,"updated_at":1758731074059,"userId":"google:104458561188320708185","id":"j972ykxh1ets49b3z3ek104n3s7r7ws9"},{"_creationTime":1758731105200.3413,"_id":"j97ap3byy2bjv3smf0dnnrfe8s7r6r8f","attachmentIds":[],"content":"what is the equivalent to the shader engine in nvidia","created_at":1758731105200,"messageId":"7d953c4e-09b0-410c-98a1-baf2cdc59908","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758731105200,"userId":"google:104458561188320708185","id":"j97ap3byy2bjv3smf0dnnrfe8s7r6r8f","providerMetadata":{}},{"_creationTime":1758731105200.3416,"_id":"j97bmpz1dv0r19gdqfjvd8e8557r7wg8","attachmentIds":[],"byok":false,"content":"NVIDIA’s closest equivalent to AMD’s Shader Engine is the Graphics Processing Cluster (GPC).\n\n- AMD Shader Engine (SE): A top-level partition that groups front-end units and multiple shader arrays/WGPs/CUs.\n- NVIDIA GPC: A top-level partition that groups a raster/front-end and multiple TPCs (each TPC contains SMs).\n\nSo, SE ≈ GPC in terms of hierarchy and function.","created_at":1758731105201,"messageId":"25f4f876-d1c9-4c23-a24f-12127785af92","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"NVIDIA’s closest equivalent to AMD’s Shader Engine is the Graphics Processing Cluster (GPC).\n\n- AMD Shader Engine (SE): A top-level partition that groups front-end units and multiple shader arrays/WGPs/CUs.\n- NVIDIA GPC: A top-level partition that groups a raster/front-end and multiple TPCs (each TPC contains SMs).\n\nSo, SE ≈ GPC in terms of hierarchy and function.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d41b61f73c81969b7864c73257d9f10ba123b91d4b7488"}},"resumableStreamId":"STREAM:15a2e072-dafc-4e03-be3f-4de61f94adad","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":1.628,"tokens":93,"tokensPerSecond":53.94431554524362,"updated_at":1758731105201,"userId":"google:104458561188320708185","id":"j97bmpz1dv0r19gdqfjvd8e8557r7wg8"},{"_creationTime":1758731137848.301,"_id":"j9784mmsa49jf7jvz3g15fvqcd7r69qr","attachmentIds":[],"content":"for nvidia, scheduling occurrs at the SM level, what is the GPC for?","created_at":1758731137848,"messageId":"f3077a1d-cc38-4d15-b8d6-8235228a0f73","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758731137848,"userId":"google:104458561188320708185","id":"j9784mmsa49jf7jvz3g15fvqcd7r69qr","providerMetadata":{}},{"_creationTime":1758731137848.3013,"_id":"j972vmv2t26tnx9s9a318dkr2n7r7w68","attachmentIds":[],"byok":false,"content":"GPCs are top-level organization and front-end/throughput scaling units, not the place where thread blocks are scheduled.\n\nWhat a GPC is for:\n- Front-end replication: Each GPC has its own front-end (raster/geometry for graphics, work distribution logic, instruction caches, etc.). Replicating this per GPC lets the chip scale in parallel rather than funneling everything through one front-end.\n- Work distribution domain: The global scheduler distributes thread blocks (CTAs) to SMs, but typically within GPC boundaries. A GPC provides a locality domain so dispatch, caches, and fabric traffic are managed efficiently.\n- Cache/fabric locality: L1/SMs sit under a GPC; traffic aggregates into that GPC’s crossbar/partition toward L2/memory. Keeping work within a GPC can reduce cross-GPC traffic and improve latency/bandwidth utilization.\n- Graphics pipeline partitioning: For graphics, each GPC has a raster engine and TPCs (with SMs) so multiple primitives/tiles can be processed in parallel across GPCs.\n- Scalability and binning: NVIDIA can enable/disable GPCs to create different SKUs and scale performance.\n\nIn short: SMs are where blocks are scheduled and executed; GPCs group sets of SMs with a duplicated front-end and local interconnect so the GPU can scale and keep scheduling/graphics work distributed and localized.","created_at":1758731137849,"messageId":"2b90e130-af31-4905-bf08-8ea644099a5b","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"GPCs are top-level organization and front-end/throughput scaling units, not the place where thread blocks are scheduled.\n\nWhat a GPC is for:\n- Front-end replication: Each GPC has its own front-end (raster/geometry for graphics, work distribution logic, instruction caches, etc.). Replicating this per GPC lets the chip scale in parallel rather than funneling everything through one front-end.\n- Work distribution domain: The global scheduler distributes thread blocks (CTAs) to SMs, but typically within GPC boundaries. A GPC provides a locality domain so dispatch, caches, and fabric traffic are managed efficiently.\n- Cache/fabric locality: L1/SMs sit under a GPC; traffic aggregates into that GPC’s crossbar/partition toward L2/memory. Keeping work within a GPC can reduce cross-GPC traffic and improve latency/bandwidth utilization.\n- Graphics pipeline partitioning: For graphics, each GPC has a raster engine and TPCs (with SMs) so multiple primitives/tiles can be processed in parallel across GPCs.\n- Scalability and binning: NVIDIA can enable/disable GPCs to create different SKUs and scale performance.\n\nIn short: SMs are where blocks are scheduled and executed; GPCs group sets of SMs with a duplicated front-end and local interconnect so the GPU can scale and keep scheduling/graphics work distributed and localized.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d41b8281a08196a680c6fe1fcb9e61097c6825f82c5048"}},"resumableStreamId":"STREAM:bfc16183-1fc3-48a4-a6a6-a4e731ad34d2","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":1.151,"tokens":299,"tokensPerSecond":49.791840133222316,"updated_at":1758731137849,"userId":"google:104458561188320708185","id":"j972vmv2t26tnx9s9a318dkr2n7r7w68"},{"_creationTime":1758731158054.308,"_id":"j973tmr27qqqy6c5ed9k39tqkd7r679r","attachmentIds":[],"content":"how about for amd gpu","created_at":1758731158054,"messageId":"c04636af-cc6a-42ae-88f8-8217efd43cd1","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758731158054,"userId":"google:104458561188320708185","id":"j973tmr27qqqy6c5ed9k39tqkd7r679r","providerMetadata":{}},{"_creationTime":1758731158054.3083,"_id":"j97b6xkbm42h2pmrhc1nxp8bxd7r7ajj","attachmentIds":[],"byok":false,"content":"On AMD GPUs, the closest analogue to NVIDIA’s GPC is the Shader Engine (SE).\n\nWhat the SE is for (role similar to GPC):\n- Front-end replication and scaling: Each SE contains its own front-end resources for dispatching and feeding work to its subordinate arrays (shader arrays/WGP arrays). Replicating this per SE lets the GPU scale throughput without a single front-end bottleneck.\n- Work distribution domain: The command processor/work distributor assigns workgroups to execution resources largely within an SE’s domain, helping locality and balancing across SEs.\n- Local interconnect/cache domain: Traffic from the SM-analogues (WGPs/CUs) aggregates within the SE before heading to shared L2/memory fabric, improving locality much like a GPC domain does on NVIDIA.\n- Graphics partitioning (for graphics-capable parts): Each SE houses rasterization and related front-end units (on RDNA/GCN), allowing multiple parallel “raster engines” similar to NVIDIA’s per-GPC raster engines.\n- SKU scaling: Vendors can enable/disable CUs/WGPs and even vary counts per SE to create product variants.\n\nExecution granularity reminder:\n- Scheduling/compute issue happens at the WGP level on modern AMD (CDNA/RDNA), where each WGP contains 2 CUs. That’s analogous to NVIDIA scheduling at the SM.\n- Hierarchy (typical): GPU die → multiple Shader Engines (SEs) → per SE: multiple Shader Arrays/WGP arrays → WGPs → CUs → SIMD units.\n\nSo, SE on AMD serves the same overarching purpose as GPC on NVIDIA: a top-level partition to replicate front-end and scale/balance work across clusters of execution units.","created_at":1758731158055,"messageId":"2a72bec2-eca2-49b4-aaa2-402c849c1b9c","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"On AMD GPUs, the closest analogue to NVIDIA’s GPC is the Shader Engine (SE).\n\nWhat the SE is for (role similar to GPC):\n- Front-end replication and scaling: Each SE contains its own front-end resources for dispatching and feeding work to its subordinate arrays (shader arrays/WGP arrays). Replicating this per SE lets the GPU scale throughput without a single front-end bottleneck.\n- Work distribution domain: The command processor/work distributor assigns workgroups to execution resources largely within an SE’s domain, helping locality and balancing across SEs.\n- Local interconnect/cache domain: Traffic from the SM-analogues (WGPs/CUs) aggregates within the SE before heading to shared L2/memory fabric, improving locality much like a GPC domain does on NVIDIA.\n- Graphics partitioning (for graphics-capable parts): Each SE houses rasterization and related front-end units (on RDNA/GCN), allowing multiple parallel “raster engines” similar to NVIDIA’s per-GPC raster engines.\n- SKU scaling: Vendors can enable/disable CUs/WGPs and even vary counts per SE to create product variants.\n\nExecution granularity reminder:\n- Scheduling/compute issue happens at the WGP level on modern AMD (CDNA/RDNA), where each WGP contains 2 CUs. That’s analogous to NVIDIA scheduling at the SM.\n- Hierarchy (typical): GPU die → multiple Shader Engines (SEs) → per SE: multiple Shader Arrays/WGP arrays → WGPs → CUs → SIMD units.\n\nSo, SE on AMD serves the same overarching purpose as GPC on NVIDIA: a top-level partition to replicate front-end and scale/balance work across clusters of execution units.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d41b96ab5881a1a75d49cc2104423e06654604b7219e56"}},"resumableStreamId":"STREAM:0c496014-62ad-454c-b769-e0e7174aa4af","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":0.683,"tokens":356,"tokensPerSecond":51.616644918080326,"updated_at":1758731158055,"userId":"google:104458561188320708185","id":"j97b6xkbm42h2pmrhc1nxp8bxd7r7ajj"},{"_creationTime":1758731179971.1611,"_id":"j971p5sm56ayjrhezywpdj3s9s7r7z0v","attachmentIds":[],"content":"find me a diagram that shows each layer of nvidia and amd gpu","created_at":1758731179971,"messageId":"0d6d9d3d-d5f0-4ed9-9bd2-e1561c457248","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758731179971,"userId":"google:104458561188320708185","id":"j971p5sm56ayjrhezywpdj3s9s7r7z0v","providerMetadata":{}},{"_creationTime":1758731179971.1614,"_id":"j97emmsda414tq1rp69pd8pe0s7r6w7z","attachmentIds":[],"byok":false,"content":"Here are good, vendor-sourced diagrams you can open that show the hierarchy layers for NVIDIA and AMD GPUs:\n\nNVIDIA (A100/Ampere)\n- GA100 full-chip block diagram (shows GPC → TPC → SM): NVIDIA A100 whitepaper, Figure 6\n  https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n- GA10x (GA102) hierarchy overview (GPC, TPC, SM): GA102 whitepaper, “GPC, TPC, and SM High-Level Architecture” (Figure 2, Figure 3)\n  https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf\n- Optional (newer reference for hierarchy visuals): Blackwell GB202 GPC/TPC/SM diagrams (Figures 3–5)\n  https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf\n\nAMD (CDNA/Instinct)\n- MI100 (CDNA1) block diagram (shows Shader Engines → CUs, L2, memory ctrls): AMD CDNA whitepaper, Figure 2\n  https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf\n- CDNA4 (MI350 series) XCD (compute die) conceptual block + package layout (SE/arrays/CUs at chiplet level): AMD CDNA 4 whitepaper, Figures 1–2\n  https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-4-architecture-whitepaper.pdf\n\nWhat to look for in each:\n- NVIDIA: GPU → GPC → TPC → SM → warp schedulers/cores; plus L2 slices, memory partitions/HBM, NVLink.\n- AMD CDNA: GPU/package → (for monolithic dies) Shader Engines → WGPs/CUs; for chiplet parts: package → XCDs (compute chiplets) → CU arrays; plus L2 slices, HBM stacks, Infinity Fabric/IODs.\n\nIf you want, tell me which exact models you’re targeting (e.g., A100 vs H100; MI250 vs MI300X), and I’ll extract the specific figure numbers/pages and annotate the hierarchy from those diagrams.","created_at":1758731179972,"messageId":"e79e99dd-e2f9-4e38-a777-f3057a04c4f0","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"news","domains":null,"queries":["NVIDIA GPU hierarchy diagram GPC TPC SM A100 GA100 block diagram","AMD GPU hierarchy diagram Shader Engine Shader Array WGP CU CDNA MI100 block diagram","NVIDIA GPC TPC SM diagram official whitepaper","AMD CDNA architecture block diagram shader engine"]},"result":[{"content":"V1.0\nNVIDIA A100 Tensor Core GPU \nArchitecture\nUNPRECEDENTED ACCELERATION AT EVERY SCALE\nii\nNVIDIA A100 Tensor Core GPU Architecture\nTable of Contents\nIntroduction 7\nIntroducing NVIDIA A100 Tensor Core GPU - our 8th Generation Data Center GPU for the \nAge of Elastic Computing 9\nNVIDIA A100 Tensor Core GPU Overview 11\nNext-generation Data Center and Cloud GPU 11\nIndustry-leading Performance for AI, HPC, and Data Analytics 12\nA100 GPU Key Features Summary 14\nA100 GPU Streaming Multiprocessor (SM) 15\n40 GB HBM2 and 40 MB L2 Cache 16\nMulti-Instance GPU (MIG) 16\nThird-Generation NVLink 16\nSupport for NVIDIA Magnum IO™ and Mellanox Interconnect Solutions 17\nPCIe Gen 4 with SR-IOV 17\nImproved Error and Fault Detection, Isolation, and Containment 17\nAsynchronous Copy 17\nAsynchronous Barrier 17\nTask Graph Acceleration 18\nNVIDIA A100 Tensor Core GPU Architecture In-Depth 19\nA100 SM Architecture 20\nThird-Generation NVIDIA Tensor Core 23\nA100 Tensor Cores Boost Throughput 24\nA100 Tensor Cores Support All DL Data Types 26\nA100 Tensor Cores Accelerate HPC 28\nMixed Precision Tensor Cores for HPC 28\nA100 Introduces Fine-Grained Structured Sparsity 31\nSparse Matrix Definition 31\nSparse Matrix Multiply-Accumulate (MMA) Operations 32\nCombined L1 Data Cache and Shared Memory 33\nSimultaneous Execution of FP32 and INT32 Operations 34\nA100 HBM2 and L2 Cache Memory Architectures 34\niii\nNVIDIA A100 Tensor Core GPU Architecture\nA100 HBM2 DRAM Subsystem 34\nECC Memory Resiliency 35\nA100 L2 Cache 35\nMaximizing Tensor Core Performance and Efficiency for Deep Learning Applications 37\nStrong Scaling Deep Learning Performance 38\nNew NVIDIA Ampere Architecture Features Improved Tensor Core Performance 38\nCompute Capability 43\nMIG (Multi-Instance GPU) Architecture 44\nBackground 44\nMIG Capability of NVIDIA Ampere GPU Architecture 45\nImportant Use Cases for MIG 45\nMIG Architecture and GPU Instances in Detail 47\nCompute Instances 49\nCompute Instances Enable Simultaneous Context Execution 51\nMIG Migration 52\nThird-Generation NVLink 52\nPCIe Gen 4 with SR-IOV 53\nError and Fault Detection, Isolation, and Containment 53\nAdditional A100 Architecture Features 54\nNVJPG Decode for DL Training 54\nOptical Flow Accelerator 55\nAtomics Improvements 56\nNVDEC for DL 56\nCUDA Advances for NVIDIA Ampere Architecture GPUs 58\nCUDA Task Graph Acceleration 58\nCUDA Task Graph Basics 58\nTask Graph Acceleration on NVIDIA Ampere Architecture GPUs 59\nCUDA Asynchronous Copy Operation 61\nAsynchronous Barriers 63\nL2 Cache Residency Control 64\nCooperative Groups 66\nConclusion 68\nAppendix A - NVIDIA DGX A100 69\niv\nNVIDIA A100 Tensor Core GPU Architecture\nNVIDIA DGX A100 - The Universal System for AI Infrastructure 69\nGame-changing Performance 70\nUnmatched Data Center Scalability 71\nFully Optimized DGX Software Stack 71\nNVIDIA DGX A100 System Specifications 74\nAppendix B - Sparse Neural Network Primer 76\nPruning and Sparsity 77\nFine-Grained and Coarse-Grained Sparsity 77\nv\nNVIDIA A100 Tensor Core GPU Architecture\nList of Figures\nFigure 1. Modern cloud datacenter workloads require NVIDIA GPU acceleration ................... 8\nFigure 2. New Technologies in NVIDIA A100.......................................................................10\nFigure 3. NVIDIA A100 GPU on new SXM4 Module ............................................................12\nFigure 4. Unified AI Acceleration for BERT-LARGE Training and Inference.......................... 13\nFigure 5. A100 GPU HPC application speedups compared to NVIDIA Tesla V100............... 14\nFigure 6. GA100 Full GPU with 128 SMs (A100 Tensor Core GPU has 108 SMs)................ 20\nFigure 7. GA100 Streaming Multiprocessor (SM).................................................................22\nFigure 8. A100 vs V100 Tensor Core Operations.................................................................25\nFigure 9. TensorFloat-32 (TF32).........................................................................................27\nFigure 10. Iterations of TCAIRS Solver to Converge to FP64 Accuracy .............................. 30\nFigure 11. TCAIRS solver speedup over the baseline FP64 direct solver............................ 30\nFigure 12. A100 Fine-Grained Structured Sparsity.............................................................32\nFigure 13. Example Dense MMA and Sparse MMA operations...........................................33\nFigure 14. A100 Tensor Core Throughput and Efficiency ...................................................39\nFigure 15. A100 SM Data Movement Efficiency .................................................................40\nFigure 16. A100 L2 cache residency controls.....................................................................41\nFigure 17. A100 Compute Data Compression....................................................................41\nFigure 18. A100 strong-scaling innovations........................................................................42\nFigure 19. Software-based MPS in Pascal vs Hardware-Accelerated MPS in Volta............. 44\nFigure 20. CSP Multi-user node Today ..............................................................................46\nFigure 21. Example CSP MIG Configuration......................................................................47\nFigure 22. Example MIG compute configuration with three GPU Instances......................... 48\nFigure 23. MIG Configuration with multiple independent GPU Compute workloads............. 49\nFigure 24. Example MIG partitioning process.....................................................................50\nFigure 25. Example MIG config with three GPU Instances and four Compute Instances. .... 51\nFigure 26. NVIDIA DGX A100 with Eight A100 GPUs.........................................................53\nFigure 27. Illustration of optical flow and stereo disparity....................................................55\nFigure 28. Execution Breakdown for Sequential 2us Kernels..............................................59\nFigure 29. Impact of Task Graph acceleration on CPU launch latency................................ 60\nFigure 30. Grid-to-Grid Latency Speedup using CUDA graphs...........................................61\nFigure 31. A100 Asynchronous Copy vs No Asynchronous Copy....................................... 62\nFigure 32. Synchronous vs Asynchronous Copy to Shared Memory................................... 63\nFigure 33. A100 Asynchronous Barriers.............................................................................64\nFigure 34. A100 L2 residency control example...................................................................66\nFigure 35. Warp-Wide Reduction.......................................................................................67\nFigure 36. NVIDIA DGX 100 System .................................................................................69\nFigure 37. DGX A100 Delivers unprecedented AI performance for training and inference. .. 70\nFigure 38. NVIDIA DGX Software Stack ............................................................................72\nFigure 39. Dense Neural Network......................................................................................76\nFigure 40. Fine-Grained Sparsity.......................................................................................78\nFigure 41. Coarse Grained Sparsity...................................................................................79\nFigure 42. Fine Grained Structured Sparsity ......................................................................80\nvi\nNVIDIA A100 Tensor Core GPU Architecture\nList of Tables\nTable 1. NVIDIA A100 Tensor Core GPU Performance Specs............................................15\nTable 2. A100 speedup over V100 (TC=Tensor Core, GPUs at respective clock speeds).... 23\nTable 3. A100 Tensor Core Input / Output Formats and Performance vs FP32 FFMA. ........ 27\nTable 4. Comparison of NVIDIA Data Center GPUs ...........................................................36\nTable 5. Compute Capability: GP100 vs GV100 vs GA100................................................43\nTable 6. NVJPG Decode Rate at different video formats.....................................................55\nTable 7. GA100 HW decode support..................................................................................56\nTable 8. Decode performance @ GPU boost clock (1410 MHz).......................................... 56\nTable 9. A100 vs V100 Decode Comparison @ 1080p30 ...................................................57\nTable 10. NVIDIA DGX A100 System Specifications.........................................................74\nTable 11. Accuracy achieved on various networks with 2:4 fine grained structured sparsity81\nIntroduction to the NVIDIA A100 Tensor Core GPU\n7\nNVIDIA A100 Tensor Core GPU Architecture\nIntroduction\nThe diversity of compute-intensive applications running in modern cloud data centers has driven \nthe explosion of NVIDIA GPU-accelerated cloud computing. Such intensive applications include \nAI deep learning training and inference, data analytics, scientific computing, genomics, edge \nvideo analytics and 5G services, graphics rendering, cloud gaming, and many more. From \nscaling-up AI training and scientific computing, to scaling-out inference applications, to enabling \nreal-time conversational AI, NVIDIA GPUs provide the necessary horsepower to accelerate \nnumerous complex and unpredictable workloads running in today’s cloud data centers.\nNVIDIA® GPUs are the leading computational engines powering the AI revolution, providing \ntremendous speedups for AI training and inference workloads. In addition, NVIDIA GPUs \naccelerate many types of HPC and data analytics applications and systems, allowing customers \nto effectively analyze, visualize, and turn data into insights. NVIDIA’s accelerated computing \nplatforms are central to many of the world’s most important and fastest-growing industries.\nHPC has grown beyond supercomputers running computat","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fimages.nvidia.com%2Faem-dam%2Fen-zz%2FSolutions%2Fdata-center%2Fnvidia-ampere-architecture-whitepaper.pdf","id":"https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf","publishedDate":null,"summary":"This document provides an overview of the NVIDIA A100 Tensor Core GPU architecture, the 8th generation data center GPU designed for elastic computing. It details key features such as the Streaming Multiprocessor (SM), HBM2 memory, Multi-Instance GPU (MIG), and third-generation NVLink. The A100 architecture incorporates advancements like fine-grained structured sparsity, improved Tensor Core performance, and support for various data types. The document also covers the NVIDIA DGX A100 system, its specifications, and a primer on sparse neural networks.\n","title":"[PDF] NVIDIA A100 Tensor Core GPU Architecture","url":"https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fforwardevery.day%2F2025%2F05%2F01%2Fnvidia-gpu-compute-hierarchy%2F","id":"https://forwardevery.day/2025/05/01/nvidia-gpu-compute-hierarchy/","publishedDate":"2025-05-01T00:00:00.000Z","summary":"Failed to fetch content","title":"Deep Dive into GPU Compute Hierarchy - Forward Everyday","url":"https://forwardevery.day/2025/05/01/nvidia-gpu-compute-hierarchy/"},{"content":"Add speed and simplicity to your Machine Learning workflow today\n\n[Get started](http://console.paperspace.com/signup?ref=blog.paperspace.com) [Contact Sales](https://paperspace.com/contact-sales?ref=blog.paperspace.com)\n\nIn this article, we'll take a technical examination of the technology behind graphics cards, their components, architecture, and how they relate to machine learning.\n\nThe task of a graphics card is very complex, yet its concepts and components are simple to comprehend. We will look at the essential components of a video card and what they accomplish. And at each stage, we'll use the NVIDIA A100 - 40 GB as an example of the current state of the art of graphics cards. The A100 arguably represents the best single GPU available for deep learning on the market.\n\n# Graphics card breakdown\n\nA graphics card, often known as a video card, graphics adapter, display card, or display adapter, is a type of expansion card that processes data and produces graphical output. As a result, it's commonly used for video editing, gaming, and 3D rendering. However, it's become the go-to powerhouse for machine learning applications and cryptocurrency mining in recent years. A graphics card accomplishes these highly demanding tasks with the help of the following components:\n\n- Graphics Processing Unit (GPU)\n- Data Processing Unit (DPU)\n- Video Memory (VRAM)\n- Video BIOS (VBIOS)\n- Voltage regulator module (VRM)\n- Motherboard interface\n- Interconnection interface\n- Network interface & controller\n- Output Interfaces\n- Cooling system\n\nNVidia A100 hardware breakdown ( [source](https://www.nvidia.com/en-in/data-center/products/converged-accelerator/?ref=blog.paperspace.com))\n\n## Graphics Processing Unit (GPU)\n\nFrequently mistaken for the graphics card itself. The GPU, unlike a computer's CPU, is designed to handle more complex mathematical and geometric calculations required for graphics rendering. GPUs, on average, have more transistors and a larger density of computing cores with more Arithmetic Logic Units (ALU) than a normal CPU.\n\nThere are four classifications of these units:\n\n- Streaming Multiprocessors (SMs)\n- Load/Store (LD/ST) units\n- Special Function Units (SFU)\n- Texture Mapping Unit (TMU)\n\n1) **A** **Streaming Multiprocessor** (SM) is a type of execution entity that consists of a collection of cores that share a register space, as well as shared memory and an L1 cache. Multiple threads can be simultaneously executed by a core in an SM. When it comes to SM's core, there are two major rivals:\n\n- Compute Uniﬁed Device Architecture (CUDA) or Tensor cores by _NVIDIA_\n- Stream Processors by _AMD_\n\nNVIDIA's CUDA Cores and Tensor Cores, in general, are believed to be more stable and optimized, particularly for machine learning applications. CUDA cores have been present on every Nvidia GPU released in the last decade, but Tensor Cores are a newer addition. Tensor cores are much quicker than CUDA cores at computing. In fact, CUDA cores can only do one operation every clock cycle, but tensor cores can perform several operations per cycle. In terms of accuracy and processing speed, CUDA cores are not as powerful as Tensor cores for machine learning models, but for some applications they are more than enough. As a result, these are the best options for training machine learning models.\n\nNVIDIA A100 Streaming Multiprocessor’s core architecture ( [Source](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/?ref=blog.paperspace.com))\n\nThe performances of these cores are measured in the FLOPS unit (floating point operations per second). For these measurements, The NVIDIA A100 achieves record breaking values:\n\n| **Core** | **Specification** |\n| --- | --- |\n| FP64 | 9.7 TFLOPS |\n| FP64 Tensor Core | 19.5 TFLOPS |\n| FP32 | 19.5 TFLOPS |\n| FP32 Tensor Core | 156 TFLOPS |\n| BFLOAT16 Tensor Core | 312 TFLOPS |\n| FP16 Tensor Core | 312 TFLOPS |\n| INT8 Tensor Core | 624 TFLOPS |\n\nAccording the [NVIDIA documentation](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf?ref=blog.paperspace.com), using sparsity format for data representation can even help double some of these values.\n\nInside the A100, cache management is done in a particular way to make data transfer between cores and VRAM as fast and smooth as possible. For this purpose, the A100 GPU has 3 levels of cache L0, L1 and L2:\n\nThe L0 instruction cache is private to a single streaming multiprocessor sub-processing block, the L1 instruction cache is private to an SM, and the L2 cache is unified, shared across all SMs,  and reserved for both instruction and data. The L2 cache in the A100 is larger than all previous GPU's caches and comes with a size of 40MB and it acts as the bridge between the L1 private cache and the 40GB HBM2 VRAM which we will see in details later in this article.\n\nCache hierarchy inside the NVIDIA A100 (40GB VRAM version) ( [Source](https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s21819-optimizing-applications-for-nvidia-ampere-gpu-architecture.pdf?ref=blog.paperspace.com))\n\n2) **Load/Store (LD/ST)** units allow threads to perform multiple data loading and storing to memory operations per single clock cycle. In the A100, these unit introduce a new method for asynchronous copy of data, which gives the possibility to load data that can be shared globally between threads without consuming extra thread resources. This newly introduced method offers an increase of around 20% of data loading times between shared memory and local caches.\n\n3) **Special Function Units** (SFUs) efficiently perform structured arithmetic or mathematical functions on vectored data, – for example sine, cosine, reciprocal, and square root.\n\n4) **Texture Mapping Unit** (TMU) handles application-specific tasks such image rotation, resizing, adding distortion and noise, and moving 3D plane objects.\n\n## Data Processing Unit (DPU)\n\nThe DPU is a non-standard component of the graphics card. Data Processing units are a newly introduced class of programmable processor that joined CPUs and GPUs as the three main components of computing. So, a DPU is a stand-alone processor that is generally implemented in ML and Data centers. It offers a set of accelerated software abilities to manage: networking, storage, security. The A100 graphics card has on board the latest BlueField-2 DPU, which can give great advantages when it comes to handling workloads with massive multiple-input multiple-outputs (MIMO), AI-on-5G deployments, and even more specialized workloads such as signal processing or multi-node training.\n\nNVIDIA A100 BlueField-2 Data Processing Unit architecture ( [Source](https://www.nvidia.com/en-in/networking/products/data-processing-unit/?ref=blog.paperspace.com))\n\n## Video Memory (VRAM)\n\nIn its broadest definition, video random-access memory (VRAM) is analogous to system RAM. VRAM is a sort of cache utilized by the GPU to hold massive amounts of data required for graphics or other applications. All data saved in VRAM is transitory. Traditional VRAM is frequently much faster than the system RAM. And, more importantly, it's physically close to the GPU. It's directly soldered to the graphics card's PCB. This enables remarkably fast data transport with minimal latency, allowing for high-resolution graphics rendering or deep learning model training.\n\nNVIDIA GeForce RTX 3050 VRAM Positionning ( [Source](https://www.nvidia.com/fr-fr/geforce/graphics-cards/30-series/rtx-3050/?ref=blog.paperspace.com))\n\nOn current graphics cards, VRAM comes in a variety of sizes, speeds, and bus widths. Currently, multiple technologies are implemented; GDDR and HMB have their own respective variations. GDDR (SGRAM Double Data Rate) has been the industry standard for more than a decade. It achieves high clock speeds, but at the expense of physical space and higher than average power consumption. On the other hand, HBM (High Bandwidth Memory) is the state of the art for VRAM technologies. It consumes less power and has the ability to be stacked to increase memory size while taking less real estate on the graphics card. It also allows higher bandwidth and lower clock speeds. The NVIDIA A100 is backed with the latest generation of HBM memories, the HBM2e with a size of 80GB, and a bandwidth up to 1935 GB/s. This is a 73% increase in comparison with the previous version Tesla V100.\n\nAdd speed and simplicity to your Machine Learning workflow today\n\n[Get started](http://console.paperspace.com/signup?ref=blog.paperspace.com) [Contact Sales](https://paperspace.com/contact-sales?ref=blog.paperspace.com)\n\n## Voltage regulator module (VRM)\n\nIt ensures that the GPU receives the necessary power at a constant voltage. A low-quality VRM can create a series of problems, including GPU shutdowns under stress, limited overclocking performances, and even shortened GPU lifespan. The graphics card receives 12 volts of electricity from a modern power supply unit (PSU). GPUs, on the other hand, are sensitive to voltage and cannot sustain that value. This is where the VRM comes into play. It reduces the 12-volt power supply to 1.1 volts before sending it to the GPU cores and memory. The power stage of the A100 with all its VRMs can sustain a power delivery up to 300 Watts.\n\nUsing the 8-Pin power connector the A100 receives power from the power supply unit, then forwards the current to the VRMs, that supplies the power to the GPU and DPU, as a 1.1 VDC current, rated at a maximum enforced limit of 300 W and a theoretical limit of 400 W.\n\n## Motherboard interface\n\nThis is the sub-component of the graphics card that plugs into the system's motherboard. It is via this interface, or 'slot’, that the graphics card and the computer interchange data and control commands. At the start of the 2000s, many types of interfaces were implemented by different manufacturers: PCI, PCIe, PCI-X or AGP. But, now PCIe has become the go-to interface for ","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fblog.paperspace.com%2Fa-complete-anatomy-of-a-graphics-card-case-study-of-the-nvidia-a100%2F","id":"https://blog.paperspace.com/a-complete-anatomy-of-a-graphics-card-case-study-of-the-nvidia-a100/","publishedDate":null,"summary":"This article provides a technical overview of graphics cards, using the NVIDIA A100 as a case study. It breaks down the essential components of a graphics card, including the GPU, DPU, VRAM, VBIOS, VRM, motherboard interface, interconnection interface, network interface & controller, output interfaces, and cooling system. The article focuses on the GPU, explaining its role in handling complex calculations and highlighting the different units within it, such as Streaming Multiprocessors (SMs), Load/Store units, Special Function Units (SFU), and Texture Mapping Units (TMU). It also compares CUDA and Tensor cores, with a table showing the A100's performance in FLOPS across different core types.\n","title":"A complete anatomy of a graphics card: Case study of the NVIDIA ...","url":"https://blog.paperspace.com/a-complete-anatomy-of-a-graphics-card-case-study-of-the-nvidia-a100/"},{"content":"Latest\n\n- [\\[ June 14, 2024 \\] The Increasing Impatience Of The Speed Of The PCI-Express Roadmap \\\nConnect](https://www.nextplatform.com/2024/06/14/the-increasing-impatience-of-the-speed-of-the-pci-express-roadmap/)\n- [\\[ June 13, 2024 \\] Stacking Up Intel Gaudi Against Nvidia GPUs For AI \\\nAI](https://www.nextplatform.com/2024/06/13/stacking-up-intel-gaudi-against-nvidia-gpus-for-ai/)\n- [\\[ June 11, 2024 \\] Software And Services For Profits, AI Hardware Is Only Table Stakes \\\nCompute](https://www.nextplatform.com/2024/06/11/software-and-services-for-profits-ai-hardware-is-only-table-stakes/)\n- [\\[ June 11, 2024 \\] Running AI And Edge Workloads On CPUs \\\nCompute](https://www.nextplatform.com/2024/06/11/running-ai-and-edge-workloads-on-cpus/)\n- [\\[ June 10, 2024 \\] Lots Of Questions On Google’s “Trillium” TPU v6, A Few Answers \\\nAI](https://www.nextplatform.com/2024/06/10/lots-of-questions-on-googles-trillium-tpu-v6-a-few-answers/)\n- [\\[ June 7, 2024 \\] Cisco Pushes Nvidia Enterprise AI, But Has Its Own Network Agenda \\\nCompute](https://www.nextplatform.com/2024/06/07/cisco-pushes-nvidia-enterprise-ai-but-has-its-own-network-agenda/)\n- [\\[ June 6, 2024 \\] New Chips Built For The Age Of AI \\\nCompute](https://www.nextplatform.com/2024/06/06/new-chips-built-for-the-age-of-ai/)\n- [\\[ June 5, 2024 \\] HPE Is Also Having Trouble Making Money With AI Servers \\\nCompute](https://www.nextplatform.com/2024/06/05/hpe-is-also-having-trouble-making-money-with-ai-servers/)\n\n![](https://www.nextplatform.com/wp-content/uploads/2020/05/nvidia-ampere-logo-1030x438.jpg)\n\nWhen you have 54.2 billion transistors to play with, you can pack a lot of different functionality into a computing device, and this is precisely what Nvidia has done with vigor and enthusiasm with the new “Ampere” GA100 GPU aimed at acceleration in the datacenter. We covered the initial announcement of the GA100 GPU and its Tesla A100 accelerator implementation two weeks ago, and now we are going to get into the architectural deep dive and then, of course, a thorough analysis of the price/performance of the A100 device compared to its predecessors in the Tesla lineup.\n\nWhen we wrote our original story about the GA100 GPU and the A100 accelerator two weeks ago, the feeds and speeds of the devices were not all out, and subsequent to that announcement we have had several sessions with Nvidia, including a Q&A session with Ian Buck, vice president and general manager of the Tesla datacenter business, and a deep dive session with Jonah Alben, senior vice president of GPU engineering at the company. A lot of the details about the GA100 and its accelerator card are in [the Ampere architecture whitepaper](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf), which we are supplementing with material from these sessions and our own analysis.\n\nAs with prior GPUs aimed at the datacenter dating all the way back to the “Fermi” chips a decade ago, which were the first in the true Tesla lineup, there is a hierarchy of compute engines, caching, and coordination logic in the Ampere design. The base organizing unit is the Streaming Multiprocessor, or SM, which has a number of different compute engines that sit side by side, waiting for work to be issued to them in parallel. This SM is what the HPC experts look at as the equivalent to a core in CPU land, for example when sizing up the “core” count on supercomputers in the Top 500 rankings that come out twice a year. Any collection of things behind the L1 instruction and data caches is a “core” to this way of thinking, and what Nvidia calls cores we might, in CPU Land, call a “unit.” (People from Nvidia sometimes call it a unit, too.)\n\nIn the old days, the elements within an SM were called variously Streaming Processors, or SPs, or CUDA cores, but these SPs now have a lot of different compute elements within them with different data formats and processing types, not just 32-bit single precision CUDA cores. Putting on our architecture hat, we think a credible argument can be made that _each of the SPs_ within an SM should be considered a “core” as we know it, not the SM as others are doing it and very roughly and somewhat analogously to a core in CPU Land. It’s a finer point, we realize. What we care about ultimately is oomph per device, cost per oomph, and cost per oomph per watt.\n\nAn side: If you want to look at the history of the GPU architecture in Tesla devices since the “G80” chip that started off the general purpose GPU computing revolution, [we did a big review of that back in February 2018](https://www.nextplatform.com/2018/02/28/engine-hpc-machine-learning/) when we dove into the Volta GPU after talking to the architects at Nvidia.\n\nWithout further ado, here is what the Ampere SM and its four SPs look like:\n\n[![](https://www.nextplatform.com/wp-content/uploads/2020/05/nvidia-ampere-sp-sm-block-diagram.jpg)](http://www.nextplatform.com/wp-content/uploads/2020/05/nvidia-ampere-sp-sm-block-diagram.jpg)\n\nEach SP has sixteen 32-bit integer units (INT32), sixteen 32-bit floating point units (FP32), and eight 64-bit (FP64) units. This is very similar to the Volta SM. Like crazy similar, as you can see here:\n\n[![](https://www.nextplatform.com/wp-content/uploads/2020/05/nvidia-volta-sp-sm-block-diagram.jpg)](http://www.nextplatform.com/wp-content/uploads/2020/05/nvidia-volta-sp-sm-block-diagram.jpg)\n\nWe never did know the size of the Level 0 caches on Volta, and we don’t know the size on the Ampere GPU, either. But the feeds and speeds on the warp scheduler, dispatch unit, and register files on the Ampere SPs look the same as on the Volta chips. The L1 instruction cache sizes are similarly not revealed for the two devices, but we do know that the L1 data cache across the SM was 128 KB with Volta and is now 50 percent higher, at 192 KB, in Ampere. The load/store and texture units are essentially the same.\n\nAt a high level, Ampere is very similar to Volta, but there are a lot of tweaks in all of these elements of the Ampere SPs and the SMs that make them different in the details, such as the fusing of the pair of Tensor Core units in the SP to make them capable of doing a very high rate of 64-bit math. There are, as we have talked about already, a lot of innovations with data formats and sparse matrix acceleration that give high performance on AI workloads equivalent to doubling up the FP units or the Tensor Core units – to do so within the what is presumably about the same transistor budget. So don’t think we are not impressed. We are.\n\nThat sparse matrix acceleration, for instance, is a particularly neat trick, as shown below:\n\n[![](https://www.nextplatform.com/wp-content/uploads/2020/05/nvidia-ampere-sparse-matrix.jpg)](http://www.nextplatform.com/wp-content/uploads/2020/05/nvidia-ampere-sparse-matrix.jpg)\n\nAfter a machine learning model is trained on the GPUs, there is a set of dense weights that are output by the model. Nvidia has created an automagic pruning technique that reduces the density of the matrix by half, using a 2:4 non-zero pattern, that does not result in the decrease in the effectiveness of the weights for inference, and by skipping the zeros introduced, the Tensor Core unit can do twice the floating point or integer math than it would have done with the denser matrix. We are not sure what happens with matrices that are already sparse, which happens with certain AI and HPC workloads. But presumably if they were using data in the right formats, these workloads could also see the 2X sparse matrix speedup.\n\nThe Volta chip had 84 SMs in total, with 80 of them exposed and four of them duds, which helped Nvidia get better chip yield out of the 12 nanometer processes from foundry partner Taiwan Semiconductor Manufacturing Corp, which were absolutely cutting edge three years ago. With the move to 7 nanometer processes at TSMC, the Ampere chip can have a lot more SMs on a chip, and in fact there are 128 in total. That is a 52.4 percent increase in SMs, and that is the big part of the improvement in performance moving from Volta to Ampere. The production Volta and Ampere devices had 80 SMs and 108 SMs, respectively, so at first this is more like an underlying 35 percent base increase in performance just from the increase in the number of computing units on the device.\n\nHere’s what the fully loaded Ampere GA100 looks like with all of its 128 SMs:\n\n[![](https://www.nextplatform.com/wp-content/uploads/2020/05/nvidia-ampere-ga100-block-diagram.jpg)](http://www.nextplatform.com/wp-content/uploads/2020/05/nvidia-ampere-ga100-block-diagram.jpg)\n\nThe hierarchy of compute in the Ampere GPU goes like this. Two SMs together comprise a Texture Processor Cluster, or TPC. Eight of these TPCs (and therefore sixteen of the SMs) comprise a higher level block called a GPU Processing Cluster, and eight of these make up the full GPU. That number is significant in that there are also eight Multi-Instance GPU, or MIG, slices that can independently be used as virtual inference engines and virtual GPUs for desktop infrastructure. The Volta GV100 GPU had six GPCs, which could have also been their own MIG in theory, but Nvidia did not allocate separate paths across caches and into memory controllers directly to each GPC in the GV100 as it is doing with the GA100. This is what a MIG really is – a more isolated and independent GPC – and it is a subtle change that nonetheless is an important one because it eliminates some of the contention for memory capacity and bandwidth when the device is being run as eight smaller devices instead of one big fat wonking one. This may be a subtle change, but it is an important one.\n\nWith the full arrays of SMs, the Ampere chip has 8,192 FP32 and INT32 units, 4,096 FP64 units, and 512 Tensor Core units. This is one hell of a dense device. With only 108 of the SMs activated, has 6,912 FP32 and INT32 units, 3,456 FP64 units, and 432 Tensor Core units that can run workloa","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.nextplatform.com%2F2020%2F05%2F28%2Fdiving-deep-into-the-nvidia-ampere-gpu-architecture%2F","id":"https://www.nextplatform.com/2020/05/28/diving-deep-into-the-nvidia-ampere-gpu-architecture/","publishedDate":"2020-05-28T00:00:00.000Z","summary":"This webpage dives deep into the Nvidia Ampere GPU architecture, specifically the GA100 GPU and its Tesla A100 accelerator. It covers the architecture's hierarchy, including the Streaming Multiprocessor (SM) and its compute engines, and compares the A100's performance to its predecessors. The article references Nvidia's whitepaper and insights from Q&A sessions with Nvidia executives.\n","title":"Diving Deep Into The Nvidia Ampere GPU Architecture","url":"https://www.nextplatform.com/2020/05/28/diving-deep-into-the-nvidia-ampere-gpu-architecture/"},{"content":"V2.1\nNVIDIA AMPERE GA102 GPU \nARCHITECTURE\nSecond-Generation RTX\nNVIDIA Ampere GA102 GPU Architecture ii\nTable of Contents\nIntroduction 5\nGA102 Key Features 7\n2x FP32 Processing 7\nSecond-Generation RT Core 7\nThird-Generation Tensor Cores 8\nGDDR6X and GDDR6 Memory 8\nThird-Generation NVLink® 8\nPCIe Gen 4 9\nAmpere GPU Architecture In-Depth 10\nGPC, TPC, and SM High-Level Architecture 10\nROP Optimizations 11\nGA10x SM Architecture 11\n2x FP32 Throughput 12\nLarger and Faster Unified Shared Memory and L1 Data Cache 13\nPerformance Per Watt 16\nSecond-Generation Ray Tracing Engine in GA10x GPUs 17\nAmpere Architecture RTX Processors in Action 19\nGA10x GPU Hardware Acceleration for Ray-Traced Motion Blur 20\nThird-Generation Tensor Cores in GA10x GPUs 24\nComparison of Turing vs GA10x GPU Tensor Cores 24\nNVIDIA Ampere Architecture Tensor Cores Support New DL Data Types 26\nFine-Grained Structured Sparsity 26\nNVIDIA DLSS 8K 28\nGDDR6X Memory 30\nRTX IO 32\nIntroducing NVIDIA RTX IO 33\nHow NVIDIA RTX IO Works 34\nDisplay and Video Engine 38\nDisplayPort 1.4a with DSC 1.2a 38\nHDMI 2.1 with DSC 1.2a 38\nFifth Generation NVDEC - Hardware-Accelerated Video Decoding 39\nAV1 Hardware Decode 40\nSeventh Generation NVENC - Hardware-Accelerated Video Encoding 40\nNVIDIA Ampere GA102 GPU Architecture iii\nConclusion 42\nAppendix A - Additional GeForce GA10x GPU Specifications 44\nGeForce RTX 3090 44\nGeForce RTX 3070 46\nAppendix B - New Memory Error Detection and Replay (EDR) Technology 49\nAppendix C - RTX A6000 GPU Performance 50\nList of Figures\nFigure 1. Ampere GA10x Architecture - A Giant Leap ............................................................. 6\nFigure 2. GA102 Full GPU with 84 SMs .................................................................................10\nFigure 3. GA10x Streaming Multiprocessor (SM) ...................................................................12\nFigure 4. NVIDIA Ampere GA10x Architecture Power Efficiency ............................................16\nFigure 5. GeForce RTX 3080 vs GeForce RTX 2080 Super RT Performance........................17\nFigure 6. Second-Generation RT Core in GA10x GPUs .........................................................18\nFigure 7. Turing RTX Technology Improves Performance......................................................19\nFigure 8. Ampere Architecture RTX Technology Further Improves Performance ...................20\nFigure 9. Ampere Architecture Motion Blur Hardware Acceleration ........................................21\nFigure 10. Basic Ray Tracing vs Ray Tracing with Motion Blur .............................................22\nFigure 11. Rendering Without vs With Motion Blur on GA10x ...............................................23\nFigure 12. Ampere Architecture Tensor Core vs Turing Tensor Core....................................25\nFigure 13. Fine-Grained Structured Sparsity.........................................................................27\nFigure 14. Watch Dogs: Legion with 8K DLSS compared to 4K and 1080p resolution. .........28\nFigure 15. Built for 8K Gaming..............................................................................................29\nFigure 16. GDDR6X Improved Performance and Efficiency using PAM4 Signaling ..............30\nFigure 17. GDDR6X New Signaling, New Coding, New Algorithms ......................................31\nFigure 18. Games Bottlenecked by Traditional I/O................................................................32\nFigure 19. Compressed Data Needed, but CPU Cannot Keep Up ........................................33\nFigure 20. RTX IO Delivers 100X Throughput, 20X Lower CPU Utilization ...........................34\nFigure 21. Level Load Time Comparison ..............................................................................35\nFigure 22. NVIDIA A40 data center GPU for visual computing..............................................36\nFigure 23. Video Decode and Encode Formats Supported on GA10x GPUs ........................39\nFigure 24. GA104 Full GPU with 48 SMs..............................................................................46\nFigure 25. Old Overclocking Method vs Overclocking with EDR ...........................................49\nFigure 26. SPECviewperf® 2020 Performance - RTX A6000 vs RTX 6000 ..........................50\nFigure 27. Rendering Performance - RTX A6000 vs RTX 6000 ............................................51\nFigure 28. HPC Performance - RTX A6000 vs RTX 6000.....................................................51\nFigure 29. Deep Learning Performance - RTX A6000 vs RTX 6000 .....................................52\nList of Tables\nTable 1. Comparative X-Factors for FP32 Throughput ..........................................................13\nTable 2. GeForce RTX 3080 vs GeForce RTX 2080 / 2080 Super........................................14\nNVIDIA Ampere GA102 GPU Architecture iv\nTable 3. NVIDIA RTX A6000 and NVIDIA A40 Specs ...........................................................15\nTable 4. Ray Tracing Feature Comparison............................................................................18\nTable 5. Comparing RTX A6000 vs RTX 6000 Motion Blur Rendering Time .........................23\nTable 6. Comparison of NVIDIA Turing vs Ampere Architecture Tensor Core.......................25\nTable 7. DisplayPort Versions - Spec Comparison................................................................38\nTable 8 . HDMI Versions - Spec Comparison.........................................................................38\nTable 9. Comparison of GeForce RTX 3090 to NVIDIA Titan RTX........................................44\nTable 10. Comparison of GeForce RTX 3070 to GeForce RTX 2070 Super........................47\nIntroduction to the NVIDIA Ampere GA102 GPU Architecture\nNVIDIA Ampere GA102 GPU Architecture 5\nIntroduction\nSince inventing the world’s first GPU (Graphics Processing Unit) in 1999, NVIDIA GPUs have \nbeen at the forefront of 3D graphics and GPU-accelerated computing. Each NVIDIA GPU \nArchitecture is carefully designed to provide breakthrough levels of performance and efficiency. \nThe family of new NVIDIA® Ampere architecture GPUs is designed to accelerate many different \ntypes of computationally intensive applications and workloads. The first NVIDIA Ampere \narchitecture GPU, the A100, was released in May 2020 and provides tremendous speedups for \nAI training and inference, HPC workloads, and data analytics applications. The A100 GPU is \ndescribed in detail in the NVIDIA A100 GPU Tensor Core Architecture Whitepaper.\nThe newest members of the NVIDIA Ampere architecture GPU family, GA102 and GA104, are \ndescribed in this whitepaper. GA102 and GA104 are part of the new NVIDIA “GA10x” class of \nAmpere architecture GPUs. GA10x GPUs build on the revolutionary NVIDIA Turing™ GPU \narchitecture. Turing was the world’s first GPU architecture to offer high performance real-time \nray tracing, AI-accelerated graphics, energy-efficient inference acceleration for the data center, \nand professional graphics rendering all in one product. \nGA10x GPUs add many new features and deliver significantly faster performance than Turing \nGPUs. In addition, GA10x GPUs are carefully crafted to provide the best performance per area \nand energy efficiency for traditional graphics workloads, and even more so for real-time ray \ntracing workloads. Compared to the Turing GPU Architecture, the NVIDIA Ampere Architecture \nis up to 1.7x faster in traditional raster graphics workloads and up to 2x faster in ray tracing.\nGA102 is the most powerful Ampere architecture GPU in the GA10x lineup and is used in the \nGeForce RTX 3090, GeForce RTX 3080, NVIDIA RTX A6000, and the NVIDIA A40 data center \nGPU. The GeForce RTX 3070 GPU uses the new GA104 GPU.\nThe GeForce RTX 3090 is the highest performing GPU in the GeForce RTX lineup and has \nbeen built for 8K HDR gaming. With 10496 CUDA Cores, 24GB of GDDR6X memory, and the \nnew DLSS 8K mode enabled, it can run many games at 8K@60 fps. The GeForce RTX 3080 \nprovides up to 2x the performance of the GeForce RTX 2080, delivering the greatest \ngenerational leap of any GPU that has ever been made. The GeForce RTX 3070 offers \nperformance that rivals NVIDIA’s previous generation flagship GPU, the GeForce RTX 2080 Ti.\nNew HDMI 2.1 and AV1 decode features in GA10x GPUs allow users to stream content at 8K \nwith HDR. \nThe NVIDIA® RTX™ A6000 combines 84 second-generation RT Cores, 336 third-generation \nTensor Cores, and 10,752 CUDA Cores with 48 GB of fast GDDR6 for accelerated rendering, \ngraphics, AI, and compute performance. Two RTX A6000s can be connected with NVIDIA \nNVLink® to provide 96 GB of combined GPU memory for handling extremely large rendering, \nAI, VR, and visual computing workloads. In total, RTX A6000 delivers the key capabilities\ndesigners, engineers and artists need to tackle the most complex workloads from their desktop \nworkstation.\nIntroduction to the NVIDIA Ampere GA102 GPU Architecture\nNVIDIA Ampere GA102 GPU Architecture 6 \nFinally, the NVIDIA A40 GPU is an evolutionary leap in performance and multi-workload \ncapabilities for the data center, combining best-in-class professional graphics with powerful \ncompute and AI acceleration to meet today’s design, creative, and scientific challenges. \nIncluding the same core counts and memory size as the RTX A6000, the A40 will power the \nnext generation of virtual workstations and server-based workloads. NVIDIA A40 is up to 2X \nmore power efficient than the previous generation, and it brings state-of-the-art features for ray\u0002traced rendering, simulation, virtual production, and more to professionals.\nFigure 1. Ampere GA10x Architecture - A Giant Leap \nGA102 Key Features\nNVIDIA Ampere GA102 GPU Architecture 7\nGA102 Key Features\nFabricated on Samsung’s 8nm 8N NVIDIA Custom Process, the NVIDIA Ampere architecture\u0002based GA102 GPU includes 28.3 billion transistors with a die size of 628.4 mm2\n. Like all \nGeForce RTX GPUs, at the heart","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.nvidia.com%2Fcontent%2Fdam%2Fen-zz%2FSolutions%2Fgeforce%2Fampere%2Fpdf%2FNVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf","id":"https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf","publishedDate":"2023-01-09T00:00:00.000Z","summary":"This webpage is a technical overview of the NVIDIA Ampere GA102 GPU architecture. It details key features like 2x FP32 processing, second-generation RT cores, third-generation Tensor Cores, GDDR6X memory, and PCIe Gen 4. The document dives into the architecture, including the SM architecture, ray tracing engine, and Tensor Cores. It also covers features like RTX IO, display and video engines, and hardware-accelerated video decoding and encoding. The document includes comparisons to previous generation GPUs and provides specifications for GeForce RTX 3090 and 3070.\n","title":"NVIDIA AMPERE GA102 GPU ARCHITECTURE","url":"https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf"},{"content":"INTRODUCING\nAMD CDNA ARCHITECTURE \nThe All-New AMD GPU Architecture for \nthe Modern Era of HPC & AI \n2\nTable of Contents\nTable of Contents 2\nIntroduction 2\nAMD CDNA Architecture Overview 2\nL2 Cache and Memory 3\nCommunication and Scaling 3\nAMD CDNA Architecture Compute Units 4\nAMD ROCm™ Open Software Ecosystem 6\nConclusion 9\nFigure 1 - Domain specific architectures : optimal efficiency through domain-specific optimizations\nIntroduction\nOver the last thirty years, graphics processors have evolved from specialized, fixed-function architectures into fully programmable \naccelerators. While early designs focused on limited computation such as lighting transformations, the GPUs of today are general-purpose \nand deliver dramatic performance gains for computationally rich software written in standard languages like C++ and Fortran. The industry \nhas simultaneously evolved to take advantage of GPUs for critical fields like machine learning and scientific computing – that will drive \ninnovation for decades to come.\nAt a technical level, this translates into two major trends in hardware and software. First of all, GPUs have transformed from fixed\u0002function pipelines, to flexible computing engines like the GCN architecture that offer both scalar and vector processing. Simultaneously, \nthe software ecosystem has rapidly expanded to empower standard computing paradigms by providing critical tooling that encompasses \neverything from math libraries and compilers, to debuggers, to cluster management and deployment.\nAMD builds high performance computing solutions for the broad range of use cases, from PC gamers to data center platforms. While \ntraditional GPUs have been built to be general purpose in nature, designed to drive both graphics-oriented as well as compute-oriented \nworkloads, AMD recently introduced a different approach to fuel even higher efficiencies. There are now two architectures: AMD RDNA™ is \noptimized for gaming to maximize frames per second and AMD CDNA is optimized for computing to push the limits of flops per second.\n3\nAMD CDNA Architecture Overview\nGraphic processing units (GPUs) built on the AMD CDNA architecture target compute applications such as high performance computing \n(HPC) and AI & machine learning (ML) that run on everything from individual servers to the world’s largest Exascale supercomputers. \nThe overall system architecture is designed for extreme scalability and carefully boosts performance over the previous generations while \nadvancing programmability and productivity with the extremely robust AMD ROCm™ software stack. Figure 2 below illustrates the 7nm \nAMD Instinct™ MI100 accelerator, which is the first incarnation of the AMD CDNA architecture. \nFigure 2 – Block diagram of the AMD Instinct™ MI100 accelerator, the first GPUs powered by the AMD CDNA architecture.\nThe AMD Instinct MI100 GPU is organized into several main blocks that are all tied together with an on-die fabric. A PCI-Express® 4.0 \ninterface connects the GPU to a host processor such as 2nd Gen AMD EPYC™ CPUs using a 16GT/s link that delivers up to 32GB/s in each \ndirection. The command processor receives API-level commands and generates work for the various components of the GPU. At a \nhigh-level, the three main functions of any compute processor, whether CPU or GPU, are compute, memory, and communication and each \nfunction is associated with a different set of blocks.\nThe AMD Instinct™ MI100 accelerator is the world’s fastest HPC GPU, and a culmination of the AMD CDNA architecture, with all-new Matrix \nCore Technology, and AMD ROCm™ open ecosystem to deliver new levels of performance, portability, and productivity. AMD CDNA is an \nall-new GPU architecture from AMD to drive accelerated computing into the era of exascale computing. The new architecture augments \nscalar and vector processing with new Matrix Core Engines and adds Infinity Fabric™ technology to scale up to larger systems. The open \nROCm ecosystem puts customers in control and is a robust, mature platform that is easy to develop for and capable of running the most \ncritical applications. The overall result is that the MI100 is the first GPU to break the 10TFLOP/s FP64 barrier designed as the stepping \nstone to the next generation of Exascale systems that will deliver pioneering discoveries in machine learning and scientific computing.1\nShader Engine\nCU\nCU\nCU\nCU\nShader Engine\nCU\nCU\nCU\nCU\nShader Engine\nCU\nCU\nCU\nCU\nShader Engine\nCU\nCU\nCU\nCU\nDMA\nHWS\nACE\nACE\nACE\nACE\nMemory Controller Memory Controller\nMemory Controller Memory Controller\nCU\nCU\nCU\nCU\nShader Engine\nCU\nCU\nCU\nCU\nShader Engine\nCU\nCU\nCU\nCU\nShader Engine\nCU\nCU\nCU\nCU\nShader Engine\nPCIe® Gen 4 Multimedia Engine XGMI Links\nINFINITY FABRIC\n4\nL2 Cache and Memory\nThe lowest levels of the memory hierarchy for the AMD Instinct™ MI100 accelerator live inside the CUs, but most scientific or machine \nlearning data sets are measured in gigabytes or terabytes and will quickly spill into memory. The memory hierarchy is responsible for \nholding the working data and efficiently delivering it to the compute arrays where the data is consumed in computation.\nThe L2 cache is shared across the whole chip and physically partitioned into multiple slices. For the MI100, the cache is 16-way set \nassociative and comprises 32 slices (twice as many as in MI50) in total for an aggregate capacity of 8MB. Each slice can sustain 64B/cycle \nfor an aggregate bandwidth over 3TB/s across the GPU.\nThe AMD CDNA architecture is targeted for the most demanding workloads, and the memory controllers and interfaces are designed for \nmaximum bandwidth, power efficiency, and extreme reliability. The memory controllers drive 4- or 8-high stacks of co-packaged HBM2 at \n2.4GT/s for an aggregate theoretical throughput of 1.23TB/s – 20% faster than the prior generation AMD accelerators, while keeping the \nGPU power budget constant.2 The 32GB of HBM2 memory is protected by robust hardware ECC that enables mission critical applications to \noperate at a massive scale.6\nCommunication and Scaling\nThe last leg of the system architecture is communication to scale up across multiple GPUs. While systems running small workloads may \nuse a single AMD Instinct™ MI100 accelerator, larger and more demanding problems such as training neural networks require more powerful \nsystems. As an example, the BERT neural network which is used for language modeling scales to thousands of processors in the most \nrecent MLPerf benchmarks.\nThe AMD CDNA architecture uses standards based high-speed AMD Infinity Fabric technology to connect to other GPUs. The Infinity Fabric \nlinks operate at 23GT/s and are 16-bits wide similar to the previous generation, but the MI100 brings a third link for full connectivity in quad \nGPU configurations offering greater bi-section bandwidth and enabling highly scalable systems. Unlike PCIe®, the AMD Infinity Fabric links \nsupport coherent GPU memory, which enables multiple GPUs to share an address space and tightly cooperate on a single problem.\nAs Figure 3 illustrates, the additional AMD Infinity Fabric links enable a fully connected 4-GPU building block, whereas the Radeon Instinct™ \nMI50 GPU could use only a ring topology. The fully connected topology boosts performance for common communication patterns such as \nall-reduce, and scatter/gather. These communication primitives are widely used in HPC and ML, e.g., the weight update communication \nphase of training neural networks found in DLRM.\nFigure 3 – Fully connected 4-GPU Infinity Fabric™ technology hives with the AMD Instinct™ MI100 GPUs\nThe 120 enhanced compute units (CUs) are built on the foundations of the GCN architecture and organized into four compute engines, and \nare responsible for all the compute in the AMD Instinct MI100 GPU. While inspired by the prior-generation GCN architecture, each CU is \nrearchitected and enhanced with a Matrix Core Engine that significantly boosts computational throughput for many different numerical \nformats. In aggregate across all the CUs, the MI100 can achieve a theoretical double-precision floating-point throughput of up to 11.5 \nTFLOP/s – the the first processor to break the 10TFLOP/s barrier.1 Unlike the graphics-oriented AMD RDNA™ family, the AMD CDNA family \nremoves all of the fixed-function hardware that is designed to accelerate graphics tasks such as rasterization, tessellation, graphics caches, \nblending, and even the display engine. However, the AMD CDNA family retains dedicated logic for HEVC, H.264, and VP9 decoding that is \nsometimes used for compute workloads that operate on multimedia data, such as machine learning for object detection.7 Removing the \nfixed-function graphics hardware frees up area and power to invest in additional compute units, boosting performance and efficiency.\n5\nFigure 4 –HPC and AI performance improvements with AMD Infinity Fabric technology8,9,10\nPCIe® 4.0 Infinity Fabric PCIe® 4.0 Infinity Fabric PCIe® 4.0\nImage Processing\nResNext101 FP32, 8 GPUs\nLanguage Modeling\nBERT FP32, 8 GPUs\nRecommendation Network\nDLRM FP32, 8 GPUs\nRelative performance to PCIe systemRelative performance to PCIe systemRelative performance to PCIe system\nInfinity Fabric\n27% Faster 44% Faster 42% Faster\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nAMD CDNA Architecture Compute Units\nThe command processor and scheduling logic translate higher-level API commands into compute tasks. These compute tasks in turn \nare implemented as compute arrays and managed by the Asynchronous Compute Engines (ACE). Each of the four ACEs maintains an \nindependent stream of commands and can dispatch wavefronts to the compute units.\nThe impressive 120 CUs of the AMD CDNA architecture are organized into four arrays of CUs. The CUs are derived from the earlier GCN \narchitecture and execute wavefronts that contain 64 work-items. However, the CUs are enhanced with new Matrix Core Engines that are \noptimized for operating","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.amd.com%2Fcontent%2Fdam%2Famd%2Fen%2Fdocuments%2Finstinct-business-docs%2Fwhite-papers%2Famd-cdna-white-paper.pdf","id":"https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf","publishedDate":null,"summary":"AMD has introduced the CDNA architecture, a new GPU architecture optimized for computing applications like HPC and AI/ML. CDNA is designed for extreme scalability and boosts performance with the AMD ROCm software stack. The first incarnation of the CDNA architecture is the AMD Instinct MI100 accelerator, which includes Matrix Core Technology and Infinity Fabric technology.\n","title":"[PDF] AMD CDNA ARCHITECTURE","url":"https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf"},{"content":"AMD«e\\\n\n\"AMD Instinct MIL00\" Instruction Set\nArchitecture\n\nReference Guide\n\n14-December-2020\n\nSpecification Agreement\nThis Specification Agreement (this \"Agreement\") is a legal agreement between Advanced Micro Devices, Inc. (\"AMD\") and \"You\"\nas the recipient of the attached AMD Specification (the \"Specification\"). If you are accessing the Specification as part of your\nperformance of work for another party, you acknowledge that you have authority to bind such party to the terms and\nconditions of this Agreement. If you accessed the Specification by any means or otherwise use or provide Feedback (defined\nbelow) on the Specification, You agree to the terms and conditions set forth in this Agreement. If You do not agree to the terms\nand conditions set forth in this Agreement, you are not licensed to use the Specification; do not use, access or provide Feedback\nabout the Specification. In consideration of Your use or access of the Specification (in whole or in part), the receipt and\nsufficiency of which are acknowledged, You agree as follows:\n1. You may review the Specification only (a) as a reference to assist You in planning and designing Your product, service or\ntechnology (\"Product\") to interface with an AMD product in compliance with the requirements as set forth in the\nSpecification and (b) to provide Feedback about the information disclosed in the Specification to AMD.\n2. Except as expressly set forth in Paragraph 1, all rights in and to the Specification are retained by AMD. This Agreement\ndoes not give You any rights under any AMD patents, copyrights, trademarks or other intellectual property rights. You\nmay not (i) duplicate any part of the Specification; (ii) remove this Agreement or any notices from the Specification, or (iii)\ngive any part of the Specification, or assign or otherwise provide Your rights under this Agreement, to anyone else.\n3. The Specification may contain preliminary information, errors, or inaccuracies, or may not include certain necessary\ninformation. Additionally, AMD reserves the right to discontinue or make changes to the Specification and its products at\nany time without notice. The Specification is provided entirely \"AS IS.\" AMD MAKES NO WARRANTY OF ANY KIND AND\nDISCLAIMS ALL EXPRESS, IMPLIED AND STATUTORY WARRANTIES, INCLUDING BUT NOT LIMITED TO IMPLIED\nWARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NONINFRINGEMENT, TITLE OR THOSE\nWARRANTIES ARISING AS A COURSE OF DEALING OR CUSTOM OF TRADE. AMD SHALL NOT BE LIABLE FOR DIRECT,\nINDIRECT, CONSEQUENTIAL, SPECIAL, INCIDENTAL, PUNITIVE OR EXEMPLARY DAMAGES OF ANY KIND (INCLUDING\nLOSS OF BUSINESS, LOSS OF INFORMATION OR DATA, LOST PROFITS, LOSS OF CAPITAL, LOSS OF GOODWILL)\nREGARDLESS OF THE FORM OF ACTION WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE) AND STRICT\nPRODUCT LIABILITY OR OTHERWISE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n4. Furthermore, AMD’s products are not designed, intended, authorized or warranted for use as components in systems\nintended for surgical implant into the body, or in other applications intended to support or sustain life, or in any other\napplication in which the failure of AMD’s product could create a situation where personal injury, death, or severe\nproperty or environmental damage may occur.\n5. You have no obligation to give AMD any suggestions, comments or feedback (\"Feedback\") relating to the Specification.\nHowever, any Feedback You voluntarily provide may be used by AMD without restriction, fee or obligation of\nconfidentiality. Accordingly, if You do give AMD Feedback on any version of the Specification, You agree AMD may freely\nuse, reproduce, license, distribute, and otherwise commercialize Your Feedback in any product, as well as has the right to\nsublicense third parties to do the same. Further, You will not give AMD any Feedback that You may have reason to believe\nis (i) subject to any patent, copyright or other intellectual property claim or right of any third party; or (ii) subject to\nlicense terms which seek to require any product or intellectual property incorporating or derived from Feedback or any\nProduct or other AMD intellectual property to be licensed to or otherwise provided to any third party.\n6. You shall adhere to all applicable U.S., European, and other export laws, including but not limited to the U.S. Export\nAdministration Regulations (\"EAR\"), (15 C.F.R. Sections 730 through 774), and E.U. Council Regulation (EC) No 428/2009 of\n5 May 2009. Further, pursuant to Section 740.6 of the EAR, You hereby certifies that, except pursuant to a license granted\nby the United States Department of Commerce Bureau of Industry and Security or as otherwise permitted pursuant to a\nLicense Exception under the U.S. Export Administration Regulations (\"EAR\"), You will not (1) export, re-export or release\nto a national of a country in Country Groups D:1, E:1 or E:2 any restricted technology, software, or source code You receive\nhereunder, or (2) export to Country Groups D:1, E:1 or E:2 the direct product of such technology or software, if such\nforeign produced direct product is subject to national security controls as identified on the Commerce Control List\n(currently found in Supplement 1 to Part 774 of EAR). For the most current Country Group listings, or for additional\n\"AMD Instinct MI100\" Instruction Set Architecture\nii of 275\ninformation about the EAR or Your obligations under those regulations, please refer to the U.S. Bureau of Industry and\nSecurity’s website at http://www.bis.doc.gov/.\n7. If You are a part of the U.S. Government, then the Specification is provided with \"RESTRICTED RIGHTS\" as set forth in\nsubparagraphs (c) (1) and (2) of the Commercial Computer Software-Restricted Rights clause at FAR 52.227-14 or\nsubparagraph (c) (1)(ii) of the Rights in Technical Data and Computer Software clause at DFARS 252.277-7013, as\napplicable.\n8. This Agreement is governed by the laws of the State of California without regard to its choice of law principles. Any\ndispute involving it must be brought in a court having jurisdiction of such dispute in Santa Clara County, California, and\nYou waive any defenses and rights allowing the dispute to be litigated elsewhere. If any part of this agreement is\nunenforceable, it will be considered modified to the extent necessary to make it enforceable, and the remainder shall\ncontinue in effect. The failure of AMD to enforce any rights granted hereunder or to take action against You in the event\nof any breach hereunder shall not be deemed a waiver by AMD as to subsequent enforcement of rights or subsequent\nactions in the event of future breaches. This Agreement is the entire agreement between You and AMD concerning the\nSpecification; it may be changed only by a written document signed by both You and an authorized representative of\nAMD.\nDISCLAIMER\nThe information contained herein is for informational purposes only, and is subject to change without notice. This\ndocument may contain technical inaccuracies, omissions and typographical errors, and AMD is under no obligation to\nupdate or otherwise correct this information. Advanced Micro Devices, Inc. makes no representations or warranties\nwith respect to the accuracy or completeness of the contents of this document, and assumes no liability of any kind,\nincluding the implied warranties of noninfringement, merchantability or fitness for particular purposes, with respect\nto the operation or use of AMD hardware, software or other products described herein. No license, including implied\nor arising by estoppel, to any intellectual property rights is granted by this document. Terms and limitations\napplicable to the purchase or use of AMD’s products or technology are as set forth in a signed agreement between the\nparties or in AMD’s Standard Terms and Conditions of Sale.\nAMD, the AMD Arrow logo, and combinations thereof are trademarks of Advanced Micro Devices, Inc. OpenCL is a\ntrademark of Apple Inc. used by permission by Khronos Group, Inc. OpenGL® and the oval logo are trademarks or\nregistered trademarks of Hewlett Packard Enterprise in the United States and/or other countries worldwide. DirectX is\na registered trademark of Microsoft Corporation in the US and other jurisdictions. Other product names used in this\npublication are for identification purposes only and may be trademarks of their respective companies.\n© 2018-2020 Advanced Micro Devices, Inc. All rights reserved.\nAdvanced Micro Devices, Inc.\n2485 Augustine Drive\nSanta Clara, CA, 95054\nwww.amd.com\n\"AMD Instinct MI100\" Instruction Set Architecture\niii of 275\nContents\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nAbout This Document. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nAudience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nOrganization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nConventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\nRelated Documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\nFeature Changes in MI100 devices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\nContact Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.1. Terminology. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2. Program Orga","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.amd.com%2Fcontent%2Fdam%2Famd%2Fen%2Fdocuments%2Finstinct-tech-docs%2Finstruction-set-architectures%2Finstinct-mi100-cdna1-shader-instruction-set-architecture.pdf","id":"https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/instinct-mi100-cdna1-shader-instruction-set-architecture.pdf","publishedDate":"2020-12-14T00:00:00.000Z","summary":"This document is the \"AMD Instinct MI100\" Instruction Set Architecture Reference Guide, dated December 14, 2020. It's a legal agreement between AMD and the recipient of the specification, outlining the terms of use. The recipient can use the specification to plan and design products that interface with AMD products and to provide feedback. AMD retains all rights to the specification and provides it \"AS IS,\" with no warranties. AMD is not liable for any damages. The document also states that AMD's products are not intended for critical applications like surgical implants. Any feedback provided to AMD can be used without restriction.\n","title":"[PDF] \"AMD Instinct MI100\" Instruction Set Architecture: Reference Guide","url":"https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/instinct-mi100-cdna1-shader-instruction-set-architecture.pdf"},{"content":"[Jump to content](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#bodyContent)\n\nFrom Wikipedia, the free encyclopedia\n\nAMD compute-focused GPU microarchitecture\n\n| [![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/06/AMD_CDNA_wordmark.png/440px-AMD_CDNA_wordmark.png)](https://en.wikipedia.org/wiki/File:AMD_CDNA_wordmark.png) |\n| Release date | November 16, 2020 (4 years ago) (2020-11-16) |\n| Designed by | [AMD](https://en.wikipedia.org/wiki/AMD) |\n| Fabrication process | - [TSMC](https://en.wikipedia.org/wiki/TSMC) [N7](https://en.wikipedia.org/wiki/7_nm_process) - TSMC [N6](https://en.wikipedia.org/wiki/7_nm_process) - TSMC [N5](https://en.wikipedia.org/wiki/5_nm_process)[\\[1\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-1) |\n| History |\n| Predecessor | [AMD FirePro](https://en.wikipedia.org/wiki/AMD_FirePro) |\n| Variant | [RDNA](https://en.wikipedia.org/wiki/RDNA_(microarchitecture)) (consumer, professional) |\n\nAMD CDNA\n\n**CDNA** ( **Compute DNA**) is a compute-centered [graphics processing unit](https://en.wikipedia.org/wiki/Graphics_processing_unit) (GPU) [microarchitecture](https://en.wikipedia.org/wiki/Microarchitecture) designed by [AMD](https://en.wikipedia.org/wiki/AMD) for datacenters. Mostly used in the [AMD Instinct](https://en.wikipedia.org/wiki/AMD_Instinct) line of data center graphics cards, CDNA is a successor to the [Graphics Core Next](https://en.wikipedia.org/wiki/Graphics_Core_Next) (GCN) microarchitecture; the other successor being [RDNA](https://en.wikipedia.org/wiki/RDNA_(microarchitecture)) (Radeon DNA), a consumer graphics focused microarchitecture.\n\nThe first generation of CDNA was announced on March 5th, 2020,[\\[2\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-2) and was featured in the AMD Instinct MI100, launched November 16th, 2020.[\\[3\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-3) This is CDNA 1's only produced product, manufactured on [TSMC](https://en.wikipedia.org/wiki/TSMC)'s [N7](https://en.wikipedia.org/wiki/7_nanometer) FinFET process.\n\nThe second iteration of the CDNA line implemented a [multi-chip module](https://en.wikipedia.org/wiki/Multi-chip_module) (MCM) approach, differing from its predecessor's monolithic approach. Featured in the AMD Instinct MI250X and MI250, this MCM design used an elevated fanout bridge (EFB)[\\[4\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-4) to connect the dies. These two products were announced November 8th, 2021, and launched November 11th. The CDNA 2 line includes an additional latecomer using a monolithic design, the MI210.[\\[5\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-5) The MI250X and MI250 were the first AMD products to use the [Open Compute Project](https://en.wikipedia.org/wiki/Open_Compute_Project) (OCP)'s OCP Accelerator Module (OAM) socket form factor. Lower wattage [PCIe](https://en.wikipedia.org/wiki/PCIe) versions are available.\n\nThe third iteration of CDNA switches to a MCM design utilizing different chiplets manufactured on multiple nodes. Currently consisting of the MI300X and MI300A, this product contains 15 unique dies and is connected with advanced 3D packaging techniques. The MI300 series was announced on January 5, 2023, and launched in H2 2023.\n\n## CDNA 1\n\n\\[ [edit](https://en.wikipedia.org/w/index.php?title=CDNA_(microarchitecture)&action=edit&section=1)\\]\n\n| Release date | November 16, 2020 (4 years ago) (2020-11-16) |\n| Fabrication process | TSMC [N7](https://en.wikipedia.org/wiki/7_nm_process) ( [FinFET](https://en.wikipedia.org/wiki/Fin_field-effect_transistor)) |\n| History |\n| Predecessor | [AMD FirePro](https://en.wikipedia.org/wiki/AMD_FirePro) |\n| Successor | [CDNA 2](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#CDNA_2) |\n\nAMD CDNA 1\n\nThe _CDNA_ family consists of one die, named [Arcturus](https://en.wikipedia.org/wiki/Arcturus). The die is 750 square millimetres, contains 25.6 billion transistors and is manufactured on TSMC's N7 node.[\\[6\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-6) The Arcturus die possesses 120 compute units and a 4096-bit memory bus, connected to four [HBM2](https://en.wikipedia.org/wiki/HBM2) placements, giving the die 32 GB of memory, and just over 1200 GB/s of memory bandwidth. Compared to its predecessor, CDNA has removed all hardware related to graphics acceleration. This removal includes but is not limited to: graphics caches, tessellation hardware, [render output units](https://en.wikipedia.org/wiki/Render_output_unit) (ROPs), and the display engine. CDNA retains the [VCN](https://en.wikipedia.org/wiki/Video_Core_Next) media engine for [HEVC](https://en.wikipedia.org/wiki/High_Efficiency_Video_Coding), [H.264](https://en.wikipedia.org/wiki/Advanced_Video_Coding), and [VP9](https://en.wikipedia.org/wiki/VP9) decoding.[\\[7\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-:0-7) CDNA has also added dedicated matrix compute hardware, similar to those added in [Nvidia](https://en.wikipedia.org/wiki/Nvidia)'s [Volta Architecure](https://en.wikipedia.org/wiki/Volta_(microarchitecture)).\n\n### Architecture\n\n\\[ [edit](https://en.wikipedia.org/w/index.php?title=CDNA_(microarchitecture)&action=edit&section=2)\\]\n\nThe 120 compute units (CUs) are organized into 4 asynchronous compute engines (ACEs), each ACE maintaining its own independent command execution and dispatch. At the CU level, CDNA compute units are organized similarly to GCN units. Each CU contains four SIMD16, that each execute their 64-thread wavefront (Wave64) over four cycles.\n\n#### Memory system\n\n\\[ [edit](https://en.wikipedia.org/w/index.php?title=CDNA_(microarchitecture)&action=edit&section=3)\\]\n\nCDNA has a 20% clock bump for the HBM, resulting in a roughly 200 GB/s bandwidth increase vs. [Vega 20](https://en.wikipedia.org/wiki/Radeon_RX_Vega_series) (GCN 5.0). The die has a shared 4 MB L2 cache that puts out 2 KB per clock to the CUs. At the CU level, each CU has its own L1 cache, a local data store (LDS) with 64 KB per CU and a 4 KB global data store (GDS), shared by all CUs. This GDS can be used to store control data, reduction operations or act as a small global shared surface.[\\[7\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-:0-7)[\\[8\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-8)\n\n##### Experimental PIM implementation\n\n\\[ [edit](https://en.wikipedia.org/w/index.php?title=CDNA_(microarchitecture)&action=edit&section=4)\\]\n\nIn October 2022, Samsung demonstrated a Processing-In-Memory (PIM) specialized version of the MI100. In December 2022 Samsung showed off a cluster of 96 modified MI100s, boasting large increases in processing throughput for various workloads and significant reduction in power consumption.[\\[9\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-9)\n\n### Changes from GCN\n\n\\[ [edit](https://en.wikipedia.org/w/index.php?title=CDNA_(microarchitecture)&action=edit&section=5)\\]\n\nThe individual compute units remain highly similar to GCN but with the addition of 4 matrix units per CU. Support for more datatypes were added, with [BF16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format), INT8 and INT4 being added.[\\[7\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-:0-7) For an extensive list of operations utilizing the matrix units and new datatypes, please reference the [CDNA ISA Reference Guide](https://developer.amd.com/wp-content/resources/CDNA1_Shader_ISA_14December2020.pdf).\n\n### Products\n\n\\[ [edit](https://en.wikipedia.org/w/index.php?title=CDNA_(microarchitecture)&action=edit&section=6)\\]\n\n| Model ( [Code name](https://en.wikipedia.org/wiki/Code_name)) | Released | [Architecture](https://en.wikipedia.org/wiki/Microarchitecture) & [fab](https://en.wikipedia.org/wiki/Semiconductor_device_fabrication) | Transistors & die size | Core | [Fillrate](https://en.wikipedia.org/wiki/Fillrate)[\\[a\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-Boost-10) | Processing power ( [TFLOPS](https://en.wikipedia.org/wiki/FLOPS)) | [Memory](https://en.wikipedia.org/wiki/Video_random-access_memory) | TBP | Software interface | Physical interface |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Vector[\\[a\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-Boost-10)[\\[b\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-FLOPS-11) | Matrix[\\[a\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-Boost-10)[\\[b\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-FLOPS-11) |\n| Config[\\[c\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-Core_config-12) | Clock[\\[a\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-Boost-10) ( [MHz](https://en.wikipedia.org/wiki/Hertz)) | Texture[\\[d\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-Texture_fill-13) ( [GT](https://en.wikipedia.org/wiki/Texel_(graphics))/s) | Pixel[\\[e\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-Pixel_fill-14) ( [GP](https://en.wikipedia.org/wiki/Pixel)/s) | [Half\\ \\ (FP16)](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) | [Single\\ \\ (FP32)](https://en.wikipedia.org/wiki/Single-precision_floating-point_format) | [Double\\ \\ (FP64)](https://en.wikipedia.org/wiki/Double-precision_floating-point_format) | INT8 | BF16 | FP16 | FP32 | FP64 | Bus type & width | Size ( [GB](https://en.wikipedia.org/wiki/Gigabyte)) | Clock ( [MT/s](https://en.wikipedia.org/wiki/Transfers_per_second)) | Bandwidth ( [GB](https://en.wikipedia.org/wiki/Gigabyte)/s) |\n| AMD Instinct MI100 (Arcturus)[\\[10\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-15)[\\[11\\]](https://en.wikipedia.org/wiki/CDNA_(microarchitecture)#cite_note-16) | Nov 16, 2020 | CDNA [TSMC](https://en.wikipedia.org/wiki/TSMC) [N7](h","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FCDNA_(microarchitecture)","id":"https://en.wikipedia.org/wiki/CDNA_(microarchitecture)","publishedDate":null,"summary":"CDNA (Compute DNA) is a compute-focused GPU microarchitecture designed by AMD for datacenters, primarily used in the AMD Instinct line. It succeeds the Graphics Core Next (GCN) microarchitecture, with RDNA being its consumer-focused counterpart. The first generation, CDNA 1, was released on November 16, 2020, and featured in the AMD Instinct MI100, using TSMC's N7 process. CDNA 2, released on November 11th, 2021, implemented a multi-chip module (MCM) design and included the MI250X, MI250, and MI210. The third iteration, CDNA 3, uses an MCM design with multiple chiplets and advanced 3D packaging, and includes the MI300X and MI300A, launched in H2 2023.\n","title":"CDNA (microarchitecture) - Wikipedia","url":"https://en.wikipedia.org/wiki/CDNA_(microarchitecture)"},{"content":"\n If you're like us and obsessed with PC hardware, gaming, and what's on the horizon, you probably know the name Kepler_L2. As one of the most trusted and respected leakers of GPU information, if they're posting anything related to specs or hardware, it's worth taking note of. The latest Kepler_L2 drop over on the Anandtech Forums (via Wccftech) offers images of GPU die configurations or block diagrams for AMD's upcoming RDNA 5 or UDNA generation of Radeon graphics. GPU die and configuration for AMD's upcoming flagship RDNA 5 or UDNA graphics card, image credit: Kepler_L2/Anandtech Forums. VIEW GALLERY - 4 IMAGES According to the image, the top RDNA 5 GPU will be a beast, featuring eight shader arrays and 16 shader engines, which deliver a total of 96 Compute Units. This would be AMD's flagship, a GeForce RTX 5090-like alternative, something it has chosen not to offer with its current RDNA 4-powered Radeon RX 9000 Series. There, the flagship Radeon RX 9070 XT features only 64 Compute Units. Read more: AMD's next-gen UDNA GPUs could have 50% more GPU cores than RDNA 4, Radeon 10090 XT incoming Read more: AMD's cancelled high-end Radeon RX 8900 XTX had 50% more shader engines than 7900 XTX Read more: AMD's next-gen RDNA 5 GPU leak: Radeon RX 10900 XT has 36GB GDDR7, TSMC 3nm, RTX 6090 killer The image also offers some insight into the memory configuration for AMD's next-gen flagship RDNA 5 or UDNA GPU, with 16 Unified Memory Controllers delivering a maximum bus or interface size of 512-bit. Now, will all of this hardware make its way into a commercial flagship Radeon RX 10090 XT desktop graphics card built for PC gaming? Or will it be limited to the workstation and AI space? GPU die and configuration for AMD's upcoming mid-range RDNA 5 or UDNA graphics card, image credit: Kepler_L2/Anandtech Forums. Time will tell, but it does suggest that AMD is committed to offering a flagship GPU next time around, after choosing to skip that SKU with RDNA 4. Moving down the line, the following diagram showcases a GPU die with 40 CUs (Compute Units), along with the memory configuration pointing to a 192-bit interface. This represents a significant downgrade from the flagship configuration. Still, with RDNA 5 rumored to be built using a more advanced and efficient node, as well as adopting faster and more efficient GDDR7 memory, this image could ultimately be our first look at the successor to the Radeon RX 9070 XT. GPU die and configuration for AMD's upcoming mainstream and entry-level RDNA 5 or UDNA graphics cards, image credit: Kepler_L2/Anandtech Forums. Moving down even further, we see images of RDNA 5's mainstream and entry-level dies with 24 and 12 CUs, respectively. Kepler_L2 also notes that RDNA 5 could feature bigger cache sizes per CU, which comes from the unified data center and consumer architectural design of UDNA. As part of its report on this new leak, Wccftech adds that a single RDNA 5 Compute Unit comprises a single RDNA 4 WGP (Work Group Processor). This would effectively double the CU counts that we're seeing here, similar to how NVIDIA's 'CUDA Core' counts doubled with the architectural changes introduced with its GeForce RTX 30 Series and Ampere architecture.","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.tweaktown.com%2Fnews%2F107380%2Famds-next-gen-rdna-5-flagship-gaming-gpu-pictured-96-compute-units-512-bit-memory-bus%2Findex.html","id":"https://www.tweaktown.com/news/107380/amds-next-gen-rdna-5-flagship-gaming-gpu-pictured-96-compute-units-512-bit-memory-bus/index.html","publishedDate":"2025-08-27T00:00:00.000Z","summary":"AMD's next-generation RDNA 5 flagship GPU is rumored to have 96 Compute Units and a 512-bit memory bus, according to a leak from Kepler_L2 on the Anandtech Forums. This suggests AMD is planning a high-end GPU to compete with the GeForce RTX 5090, a segment they skipped with the RDNA 4-powered Radeon RX 9000 Series. The leak also includes details on mid-range and entry-level RDNA 5 GPUs with 40, 24, and 12 Compute Units, respectively, and hints at larger cache sizes per CU.\n","title":"AMD's next-gen RDNA 5 flagship gaming GPU pictured, 96 ...","url":"https://www.tweaktown.com/news/107380/amds-next-gen-rdna-5-flagship-gaming-gpu-pictured-96-compute-units-512-bit-memory-bus/index.html"},{"content":"# [Chips and Cheese](https://chipsandcheese.com/)\n\nSubscribeSign in\n\n# AMD’s RDNA4 GPU Architecture at Hot Chips 2025\n\n[Chester Lam](https://substack.com/@chlamchowder)\n\nSep 13, 2025\n\n24\n\n[2](https://chipsandcheese.com/p/amds-rdna4-gpu-architecture-at-hot/comments)\n4\n\n[Share](javascript:void(0))\n\nRDNA4 is AMD’s latest graphics-focused architecture, and fills out their RX 9000 line of discrete GPUs. AMD noted that creating a good gaming GPU requires understanding both current workloads, as well as taking into account what workloads might look like five years in the future. Thus AMD has been trying to improve efficiency across rasterization, compute, and raytracing. Machine learning has gained importance including in games, so AMD’s new GPU architecture caters to ML workloads as well.\n\nFrom AMD’s perspective, RDNA4 represents a large efficiency leap in raytracing and machine learning, while also improving on the rasterization front. Improved compression helps keep the graphics architecture fed. Outside of the GPU’s core graphics acceleration responsibility, RDNA4 brings improved media and display capabilities to round out the package.\n\n# Media Engine\n\nThe Media Engine provides hardware accelerated video encode and decode for a wide range of codecs. High end RDNA4 parts like the RX 9070XT have two media engines. RDNA4’s media engines feature faster decoding speed, helping save power during video playback by racing to idle. For video encoding, AMD targeted better quality in H.264, H.265, and AV1, especially in low latency encoding.\n\nLow latency encoder modes are mostly beneficial for streaming, where delays caused by the media engine ultimately translate to a delayed stream. Reducing latency can make quality optimizations more challenging. Video codecs strive to encode differences between frames to economize storage. Buffering up more frames gives the encoder more opportunities to look for similar content across frames, and lets it allocate more bitrate budget for difficult sequences. But buffering up frames introduces latency. Another challenge is some popular streaming platforms mainly [use H.264](https://help.twitch.tv/s/article/broadcasting-guidelines?language=en_US), an older codec that’s less efficient than AV1. Newer codecs are being tested, so the situation may start to change as the next few decades fly by. But for now, H.264 remains important due to its wide support.\n\nTesting with an old gameplay clip from Elder Scrolls Online shows a clear advantage for RDNA4’s media engine when testing with the latency-constrained VBR mode and encoder tuned for low latency encoding (-usage lowlatency -rc vbr\\_latency). Netflix’s VMAF video quality metric gives higher scores for RDNA4 throughout the bitrate range. Closer inspection generally agrees with the VMAF metric.\n\nRDNA4 does a better job preserving high contrast outlines. Differences are especially visible around text, which RDNA4 handles better than its predecessor while using a lower bitrate. Neither result looks great with such a close look, with blurred text on both examples and fine detail crushed in video encoding artifacts. But it’s worth remembering that the latency-constrained VBR mode uses a VBV buffer of [up to three frames](https://github.com/GPUOpen-LibrariesAndSDKs/AMF/wiki/Rate-Control-Methods), while higher latency modes can use VBV buffer sizes covering multiple seconds of video. Encoding speed has improved slightly as well, jumping from ~190 to ~200 FPS from RDNA3.5 to RDNA4.\n\n# Display Engine\n\nThe display engine fetches on-screen frame data from memory, composites it into a final image, and drives it to the display outputs. It’s a basic task that most people take for granted, but the display engine is also a good place to perform various image enhancements. A traditional example is using a lookup table to apply color correction. Enhancements at the display engine are invisible to user software, and are typically carried out in hardware with minimal power cost. On RDNA4, AMD added a “Radeon Image Sharpening” filter, letting the display engine sharpen the final image. Using dedicated hardware at the display engine instead of the GPU’s programmable shaders means that the sharpening filter won’t impact performance and can be carried out with better power efficiency. And, AMD doesn’t need to rely on game developers to implement the effect. Sharpening can even apply to the desktop, though I’m not sure why anyone would want that.\n\nPower consumption is another important optimization area for display engines. Traditionally that’s been more of a concern for mobile products, where maximizing battery life under low load is a top priority. But RDNA4 has taken aim at multi-monitor idle power with its newer display engine. AMD’s presentation stated that they took advantage of variable refresh rates on FreeSync displays. They didn’t go into more detail, but it’s easy to imagine what AMD might be doing. High resolution and high refresh rate displays translate to high pixel rates. That in turn drives higher memory bandwidth demands. Dynamically lowering refresh rates could let RDNA4’s memory subsystem enter a low power state while still meeting refresh deadlines.\n\nPower and GDDR6 data rates for various refresh rate combinations. AMD’s monitoring software (and others) read out extremely low memory clocks when the memory bus is able to idle, so those readings aren’t listed.\n\nI have a RX 9070 hooked up to a Viotek GN24CW 1080P display via HDMI, and a MSI MAG271QX 1440P capable of refresh rates up to 360 Hz. The latter is connected via DisplayPort. The RX 9070 manages to keep memory at idle clocks even at high refresh rate settings. Moving the mouse causes the card to ramp up memory clocks and consume more power, hinting that RDNA4 is lowering refresh rates when screen contents don’t change. Additionally, RDNA4 gets an intermediate GDDR6 power state that lets it handle the 1080P 60 Hz + 1440P 240 Hz combination without going to maximum memory clocks. On RDNA2, it’s more of an all or nothing situation. The older card is more prone to ramping up memory clocks to handle high pixel rates, and power consumption remains high even when screen contents don’t change.\n\n# Compute Changes\n\nRDNA4’s Workgroup Processor retains the same high level layout as prior RDNA generations. However, it gets major improvements targeted towards raytracing, like improved raytracing units and wider BVH nodes, a dynamic register allocation mode, and a scheduler that no longer suffers false memory dependencies between waves. I covered those in previous articles. Besides those improvements, AMD’s presentation went over a couple other details worth discussing.\n\n## Scalar Floating Point Instructions\n\nAMD has a long history of using a scalar unit to offload operations that are constant across a wave. Scalar offload saves power by avoiding redundant computation, and frees up the vector unit to increase performance in compute-bound sequences. RDNA4’s scalar unit gains a few floating point instructions, expanding scalar offload opportunities. This capability debuted on RDNA3.5, but RDNA4 brings it to discrete GPUs.\n\nWhile not discussed in AMD’s presentation, scalar offload can bring additional performance benefits because scalar instructions sometimes have lower latency than their vector counterparts. Most basic vector instructions on RDNA4 have 5 cycle latency. FP32 adds and multiples on the scalar unit have 4 cycle latency. The biggest latency benefits still come from offloading integer operations though.\n\n## Split Barriers\n\nGPUs use barriers to synchronize threads and enforce memory ordering. For example, a s\\_barrier instruction on older AMD GPUs would cause a thread to wait until all of its peers in the workgroup also reached the s\\_barrier instruction. Barriers degrade performance because any thread that happened to reach the barrier faster would have to stall until its peers catch up.\n\nRDNA4 splits the barrier into separate “signal” and “wait” actions. Instead of s\\_barrier, RDNA4 has s\\_barrier\\_signal and s\\_barrier\\_wait. A thread can “signal” the barrier once it produces data that other threads might need. It can then do independent work, and only wait on the barrier once it needs to use data produced by other threads. The s\\_barrier\\_wait will then stall the thread until all other threads in the workgroup have signalled the barrier.\n\n# Memory Subsystem\n\nThe largest RDNA4 variants have a 8 MB L2 cache, representing a substantial L2 capacity increase compared to prior RDNA generations. RDNA3 and RDNA2 maxed out at 6 MB and 4 MB L2 capacities, respectively. AMD found that difficult workloads like raytracing benefit from the larger L2. Raytracing involves pointer chasing during BVH traversal, and it’s not surprising that it’s more sensitive to accesses getting serviced from the slower Infinity Cache as opposed to L2. In the initial scene in 3DMark’s DXR feature test, run in Explorer Mode, RDNA4 dramatically cuts down the amount of data that has to be fetched from beyond L2.\n\nRDNA2 still does a good job of keeping data in L2 in absolute terms. But it’s worth noting that hitting Infinity Cache on both platforms adds more than 50 ns of extra latency over a L2 hit. That’s well north of 100 cycles because both RDNA2 and RDNA4 run above 2 GHz. While AMD’s graphics strategy has shifted towards making the faster caches bigger, it still contrasts with Nvidia’s strategy of putting way more eggs in the L2 basket. Blackwell’s L2 cache serves the functions of both AMD’s L2 and Infinity Cache, and has latency between those two cache levels. Nvidia also has a flexible L1/shared memory allocation scheme that can give them more low latency caching capacity in front of L2, depending on a workload’s requested local storage (shared memory) capacity.\n\nA mid-level L1 cache was a familiar fixture on prior RDNA generations. It’s conspicuously missing from RDNA4, as well as AMD’s presentation. One possibility is that L","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fchipsandcheese.com%2Fp%2Famds-rdna4-gpu-architecture-at-hot","id":"https://chipsandcheese.com/p/amds-rdna4-gpu-architecture-at-hot","publishedDate":"2025-09-13T00:00:00.000Z","summary":"AMD's RDNA4 GPU architecture, designed for the RX 9000 series, focuses on improving efficiency in rasterization, compute, raytracing, and machine learning. It features an improved media engine with faster decoding and better encoding quality for H.264, H.265, and AV1, especially in low-latency modes beneficial for streaming. The display engine also sees enhancements, performing image improvements like color correction.\n","title":"AMD’s RDNA4 GPU Architecture at Hot Chips 2025","url":"https://chipsandcheese.com/p/amds-rdna4-gpu-architecture-at-hot"},{"content":"NVIDIA.\n\nNVIDIA RTX BLACKWELL\nGPU ARCHITECTURE\n\nBuilt for Neural Rendering\n\nV1.01\n\nii\nNVIDIA RTX Blackwell GPU Architecture\nContents\nINTRODUCTION ....................................................................................................................................................................... 4\nRTX Blackwell Neural Rendering Architecture........................................................................................................... 7\nBlackwell GB202 GPU....................................................................................................................................................... 8\nSM Architecture................................................................................................................................................................10\nGDDR7 Memory Subsystem........................................................................................................................................13\nBlackwell 5th Generation Tensor Cores .................................................................................................................15\nFP4 Support ...................................................................................................................................................................15\nBlackwell 4th Generation RT Cores ..........................................................................................................................16\nMega Geometry............................................................................................................................................................17\nLevel-of-Detail ..........................................................................................................................................................17\nSubdivision Surfaces..............................................................................................................................................21\nMega Geometry API and Architecture Support.........................................................................................21\nBlackwell RT Core Enhancements for Mega Geometry..........................................................................22\nLinear Swept Spheres (LSS)....................................................................................................................................22\nShader Execution Reordering (SER) 2.0..................................................................................................................24\nAI Management Processor (AMP).............................................................................................................................25\nNew RTX Blackwell Video and Display Features .................................................................................................27\nNinth-Generation NVENC.........................................................................................................................................28\nSixth-Generation NVDEC..........................................................................................................................................30\nDisplayPort 2.1b...........................................................................................................................................................30\nBlackwell Max-Q Power Efficiency Improvements............................................................................................31\nAdvanced Power Gating ...........................................................................................................................................31\nAccelerated Frequency Switching........................................................................................................................32\nLow Latency Sleep.......................................................................................................................................................33\nDLSS 4 .......................................................................................................................................................................................35\nDLSS 4 Multi Frame Generation ................................................................................................................................35\nTransformer Models........................................................................................................................................................36\nDLSS Super Resolution (SR)....................................................................................................................................37\nDLSS Ray Reconstruction (RR)...............................................................................................................................38\nDeep Learning Anti-Aliasing (DLAA)....................................................................................................................38\nNeural Shaders.......................................................................................................................................................................39\nRTX Neural Materials ......................................................................................................................................................41\nRTX Neural Texture Compression (NTC) ................................................................................................................41\nNeural Radiance Cache (NRC) .....................................................................................................................................42\nRTX Skin................................................................................................................................................................................43\nRTX Neural Faces..............................................................................................................................................................44\nAPPENDIX A: Blackwell GB202 GPU.............................................................................................................................46\nGeForce RTX 5090 Specifications........................................................................................................................46\nAPPENDIX B: Blackwell GB203 GPU.............................................................................................................................49\nGeForce RTX 5080 Specifications........................................................................................................................49\nGeForce RTX 5070 Ti Specifications...................................................................................................................51\nAPPENDIX C: Blackwell GB205 GPU.............................................................................................................................54\nGeForce RTX 5070 Specifications........................................................................................................................54\niii\nNVIDIA RTX Blackwell GPU Architecture\nList of Figures\nFigure 1. RTX Blackwell Design Goals....................................................................................................................... 5\nFigure 2. The Age of Neural Rendering Has Arrived - Significant AI TOPS Increase Per Frame..... 7\nFigure 3. GB202 GPU block diagram (full chip)..................................................................................................... 8\nFigure 4. The Blackwell GPC with Raster Engine, 8 TPCs, 16 SMs, and 16 ROPs.................................. 9\nFigure 5. The Blackwell Streaming Multiprocessor (SM) ...............................................................................11\nFigure 6. Ada SM vs Blackwell....................................................................................................................................12\nFigure 7. GDDR7 compared to prior generation GDDR6/6x .........................................................................13\nFigure 8. Blackwell 5th Generation Tensor Cores with FP4, double throughput of FP8.................16\nFigure 9. New Fourth-Generation RT Core Block Diagram (RTX Blackwell Architecture) ...............17\nFigure 10. BVH and Mesh Using Clusters...........................................................................................................18\nFigure 11. TLAS/BLAS Acceleration Structures and Cluster BLAS .........................................................19\nFigure 12. NVIDIA “Zorah Demo” using Mega Geometry and other new Blackwell Tech..............20\nFigure 13. Partitioned Top-Level Acceleration Structure (PTLAS) ..........................................................21\nFigure 14. Sequence of Disjoint Orthogonal Triangle Strips (DOTS)......................................................23\nFigure 15. Sequence of Linear-Swept Spheres (LSS)....................................................................................24\nFigure 16. Shader Execution Reordering (SER), conceptual diagram.....................................................24\nFigure 17. AI Management Processor (AMP) Schedules AI / Graphics Workloads ..........................26\nFigure 18. 4:2:2 Provides 2X the Color Information for 1.3X the RAW File Size of 4:2:0...............28\nFigure 19. Ninth-Generation NVENC Encoder Improves Encoding Speed...........................................29\nFigure 20. New Max Q Power Efficiency Innovations to Improve Battery Life. .................................31\nFigure 21. Advanced Clock, Power, and Rail Gating Provide Fine-Grain Control of Power...........32\nFigure 22. Accelerated Frequency Switching ...................................................................................................33\nFigure 23. Real-life Example of Running Inference on SLMs on Ada and Blackwell........................34\nFigure 24. DLSS Brings Upgrade","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fimages.nvidia.com%2Faem-dam%2FSolutions%2Fgeforce%2Fblackwell%2Fnvidia-rtx-blackwell-gpu-architecture.pdf","id":"https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf","publishedDate":null,"summary":"This webpage is an NVIDIA document detailing the RTX Blackwell GPU architecture. It covers various aspects including the Neural Rendering architecture, the GB202 GPU, SM architecture, GDDR7 memory subsystem, 5th generation Tensor Cores (with FP4 support), 4th generation RT Cores, Mega Geometry features, Shader Execution Reordering (SER) 2.0, AI Management Processor (AMP), new video and display features (Ninth-Generation NVENC, Sixth-Generation NVDEC, DisplayPort 2.1b), and power efficiency improvements (Advanced Power Gating, Accelerated Frequency Switching, Low Latency Sleep).\n","title":"[PDF] NVIDIA RTX BLACKWELL GPU ARCHITECTURE","url":"https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf"},{"content":"\n [Official NVIDIA] NVIDIA RTX Blackwell GPU Architecture Whitepaper \n Author: Nestledrink \n Time Posted (UTC): 2025-01-29 17:19:20+00:00 \n Score: 99 \n \n\n\n \n Link: https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf \n \n\n\n \n Comments \n \n\n\n \n \n Comment by: SBAstan1962\n | Posted (UTC): 2025-01-29\n | Score: 51\n \n It's kinda funny how Nvidia basically went back all the way to Pascal for the SM structure. Nvidia with Pascal: Eh, we don't need separate FP32 and INT32 cores, just have each core able to do both. Nvidia with Turing: Shit! We need dedicated INT32 cores! Let's have an entirely separate block for them. Nvidia with Ampere and Ada Lovelace: Shit! We totally overestimated the amount of INT32 operations that the GPU uses! Let's still have that dedicated block, but with cores able to do either FP32 or INT32 operations. Nvidia with Blackwell: Eh, we don't need separate FP32 and INT32 cores, just have each core able to do both. \n\n\n \n \n Comment by: Lagviper\n | Posted (UTC): 2025-01-30\n | Score: 11\n \n I spotted that too Thought it was funny. I guess Turing/Ampere/Ada kind of took a shortcut to make place for RT/ML on the die and had simplified cuda cores just for FP32 over just having everything FP32/INT32 back then. There must be something for better management of concurrency and scheduling than Pascal for sure. They talk about a RISC core for AMP that better coordinates. They also say that because its double the compute of Ada, it increases the performance of address generation workloads that are crucial for neural shading. So again something we might see in the future. I imagine each cuda core can individually choose between FP32 and INT32? Like pick 70% of cuda cores for FP32 and the rest concurrently doing INT32? \n\n\n \n \n Comment by: salmonmilks\n | Posted (UTC): 2025-01-30\n | Score: 5\n \n where does one obtain these knowledge of so many abbreviations \n\n\n \n \n Comment by: Lagviper\n | Posted (UTC): 2025-01-30\n | Score: 10\n \n 3 decades of following GPU tech… reading white papers, etc. Not really a useful skill lol \n\n \n \n \n \n\n \n \n Comment by: AdministrativeFun702\n | Posted (UTC): 2025-01-29\n | Score: 33\n \n Hmmm they gimped 5070TI L2 cache. Its same as 5070 and also 4070TI/super. I was hoping for 4080like performance but nope.Not gonna happen. I am sad. \n\n\n \n \n Comment by: iamthewhatt\n | Posted (UTC): 2025-01-29\n | Score: 12\n \n It will be very similar to how the 5080 compares to the 4080 tbh. There were no major hardware improvements, and the extra power can only take it so far. \n\n \n\n \n \n Comment by: Nestledrink\n | Posted (UTC): 2025-01-29\n | Score: 9\n \n NVIDIA claimed 5070 Ti to be 1.2x faster vs 4070 Ti. That will bring 5070 Ti to 95% of 4080 performance. and they also claimed 5070 to be 1.2x faster vs 4070. That will bring it to 95% of 4070 Ti performance. \n\n\n \n \n Comment by: No-Inspector-4886\n | Posted (UTC): 2025-01-29\n | Score: 11\n \n nah shit , rtx 5070 ti = rtx 4070 ti super rtx 5070 = rtx 4070 super \n\n\n \n \n Comment by: seruus\n | Posted (UTC): 2025-01-29\n | Score: 3\n \n Aren't the leaks pointing towards the 9070XT being slower than the 7900XT, being more an improved 7800XT instead? \n\n\n \n \n Comment by: No-Inspector-4886\n | Posted (UTC): 2025-01-29\n | Score: 1\n \n where did you see this leakes ;))? , ofc not. \n\n \n \n\n \n \n Comment by: Olde94\n | Posted (UTC): 2025-01-29\n | Score: 3\n \n 5070 had 16% fewer cores than 4070 super. I think it’ll be slower \n\n \n\n \n \n Comment by: signed7\n | Posted (UTC): 2025-01-29\n | Score: 2\n \n Ye looks like 9070xt would match the 5070ti after all if amd's 'matches 4080' claims are true \n\n \n \n\n \n \n Comment by: Gatlyng\n | Posted (UTC): 2025-01-30\n | Score: 1\n \n Yes, but those are also cherry picked results.  Besides, if 5070Ti really is 4080 level of performance for $500 less ($250 less than 5080 and 4080 Super), why would anyone buy the 5080? \n\n\n \n \n Comment by: Nestledrink\n | Posted (UTC): 2025-01-30\n | Score: 6\n \n NVIDIA has been pretty spot on with their updated numbers from January 15th. They published some numbers before but those seemed to be superseded by these new ones. You can check out the numbers here: [https://videocardz.com/newz/nvidia-official-geforce-rtx-50-vs-rtx-40-benchmarks-15-to-33-performance-uplift-without-dlss-multi-frame-generation](https://videocardz.com/newz/nvidia-official-geforce-rtx-50-vs-rtx-40-benchmarks-15-to-33-performance-uplift-without-dlss-multi-frame-generation) In these updated numbers, they claimed 5090 to be 1.33x faster vs 4090. TechPowerUp 4K benchmark showed 1.34x. They claimed 5080 to be 1.15x faster vs 4080. TechPowerUp 4K benchmark showed 1.149x. They are claiming 1.2x for 5070 Ti and 5070. If you extrapolate the math from TechPowerUp 4K chart [here](https://tpucdn.com/review/nvidia-geforce-rtx-5080-founders-edition/images/average-fps-3840-2160.png), then 5070 Ti will end up having 79.6 FPS average at 4K which is approximately 95% of RTX 4080 performance and RTX 5070 to have 63.2 FPS average at 4K which is approximately 95% of 4070 Ti performance. When 5070 Ti specs started to leak before the announcement, 5070 Ti looked to be the best value product in the 50 series stack due to the same GPU as 5080 and even have the same VRAM. \n\n \n\n \n \n Comment by: jonjohnjonjohn\n | Posted (UTC): 2025-01-30\n | Score: 2\n \n Because there won't be any founders editions of 5070ti I bet the partner cards will just end up costing close to what a 5080 does anyway \n\n \n \n \n\n \n \n Comment by: The_Zura\n | Posted (UTC): 2025-01-30\n | Score: 7\n \n 5070 Ti has 25% more bandwidth in exchange. \n\n \n\n \n \n Comment by: onurraydar\n | Posted (UTC): 2025-01-30\n | Score: 5\n \n Wasn't L2 there to compensate for lackluster bandwidth? With much higher bandwidth L2 isn't as necessary. \n\n\n \n \n Comment by: AdministrativeFun702\n | Posted (UTC): 2025-01-30\n | Score: 1\n \n We will see in reviews. \n\n\n \n \n Comment by: onurraydar\n | Posted (UTC): 2025-01-30\n | Score: 1\n \n We shall. I'm hoping it's within 5% of 4080 but I have a feeling it's gonna be like 10+% slower at this rate. \n\n \n \n \n\n \n \n Comment by: Accurate-End-5695\n | Posted (UTC): 2025-01-29\n | Score: 1\n \n Check Framechasers recent video. It's actually more impressive than people realize. \n\n \n \n\n \n \n Comment by: EmilMR\n | Posted (UTC): 2025-01-29\n | Score: 21\n \n Mega Geometry is supported by all RTX gpus but Blackwell RT cores have additional benefits. The biggest deal about Blackwell are Neural Shaders. The main changes they have done for it is unifying INT32 and FP32 unit. Still only either operation can be performed in a cycle. Then there is a RISCV processor than handles scheduling. It mentions that this is compatible with existing Windows HAGS. \n\n\n \n \n Comment by: SBAstan1962\n | Posted (UTC): 2025-01-29\n | Score: 7\n \n \\>It is really interesting but they are only now releasing these to developers and they get to have an actual hardware to play with them It's implied that these technologies were doable since Turing, but its adoption was held back by lack of DirectX support (likely with some pressure by AMD). Since AMD and Intel seem to be relenting on the idea of having dedicated neural and RT hardware, now modern PC APIs can actually make use of the full Tensor and RT core feature set. It'll likely still be a year or two until AAA games can actually make use of these technologies. \n\n \n\n \n \n Comment by: HomeMadeShock\n | Posted (UTC): 2025-01-29\n | Score: 2\n \n It does mention that prior generations can run ~some~ of neural shading, but it looks like Blackwell can take full advantage of all neural shading features  \n\n \n\n \n \n Comment by: _G_P_\n | Posted (UTC): 2025-02-05\n | Score: 1\n \n Does anyone know if there is any benchmark specific to the new RT and tensor cores, and how they compare in performance from the previous gen? I saw the \"Zorah\" demo, but I don't think it's available to the general public, and I'm wondering if anyone has it and ran it on Ada Lovelace (assuming it would even run). \n\n \n\n \n \n Comment by: [deleted]\n | Posted (UTC): 2025-01-29\n | Score: 1\n \n [removed] \n\n \n \n\n \n \n Comment by: speedballandcrack\n | Posted (UTC): 2025-01-29\n | Score: 17\n \n Dumb me read it as wallpaper \n\n\n \n \n Comment by: Hwistler\n | Posted (UTC): 2025-01-29\n | Score: 5\n \n Same. Quickly skimmed the title and thought, \"Sweet, this is probably one of those cool layout wallpapers!\" I am dumb. \n\n \n\n \n \n Comment by: ACTM\n | Posted (UTC): 2025-01-29\n | Score: 5\n \n [Here you go!](https://i.imgur.com/eaathpg.jpg) \n\n \n \n\n \n \n Comment by: leonard28259\n | Posted (UTC): 2025-01-29\n | Score: 12\n \n Should've used their AI GPUs to design a better architecture. \n\n\n \n \n Comment by: vr_wanderer\n | Posted (UTC): 2025-01-29\n | Score: 8\n \n They did....... for AI. \n\n \n \n\n \n \n Comment by: Lagviper\n | Posted (UTC): 2025-01-30\n | Score: 6\n \n So mega geometry is hardware embedded into the new RT cores. ASIC like acceleration. That's gonna be tough to beat for older RTX cards even though Alan Wake 2 promises to have mega geometry for all rtx cards. this will be very interesting to bench when it releases. * SER 2.0 * AI management processor * 4th generation RT core with triangle cluster intersection / compression engine, linear swept spheres * 5th generation tensor core with FP4 support. 2nd genereation FP8 transformer engine similar to datacenter blackwell GPUs. * Whole cuda cores are now both FP32/INT32, no longer split like ampere/Ada... (isn't this basically a 360 back to Pascal? Pascal FP32/Int32 → Turing FP32 + INT32 → Ampere/Ada FP32 + FP32/INT32 → Blackwell FP32/INT32,.... oh well, must be a logical reason). As per paper, this doubles the integer math throughput per clock cycle compared to Ada., SM built for neural shading. * Neural shaders seem heavily TOP dependant overall, so the above cuda core change makes the whole pipeline neural like and the new enhanced tensor cores are now accessible to graphic shaders, so the whole TOPs output of this card can be used for shading. That won't be possible on older RTX cards, even","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.reddit.com%2Fr%2Fnvidia%2Fcomments%2F1icz1s3%2Fofficial_nvidia_nvidia_rtx_blackwell_gpu%2F","id":"https://www.reddit.com/r/nvidia/comments/1icz1s3/official_nvidia_nvidia_rtx_blackwell_gpu/","publishedDate":"2025-01-29T00:00:00.000Z","summary":"This webpage is a Reddit post discussing the NVIDIA RTX Blackwell GPU Architecture Whitepaper. The post includes a link to the whitepaper and comments from users. Some users discuss the SM structure, comparing it to previous NVIDIA architectures like Pascal, Turing, Ampere, and Ada Lovelace. Others comment on the L2 cache of the 5070TI and its potential performance.\n","title":"[Official NVIDIA] NVIDIA RTX Blackwell GPU Architecture Whitepaper","url":"https://www.reddit.com/r/nvidia/comments/1icz1s3/official_nvidia_nvidia_rtx_blackwell_gpu/"},{"content":"[Skip to content](https://blogs.nvidia.com/blogs.nvidia.com#content)\n\n# NVIDIA Blackwell: Born for Extreme-Scale AI Inference\n\nNVIDIA Blackwell’s scale-up capabilities set the stage to scale out the world’s largest AI factories.\n\nSeptember 18, 2025 by [Kevin Acocella](https://blogs.nvidia.com/blog/author/kevinacocella/)\n\nYour browser does not support the video tag.\n\n**Share**\n\n[Email0](https://blogs.nvidia.com/blogs.nvidia.com#ea-share-count-email)\n\nThe NVIDIA Blackwell architecture is the reigning leader of the AI revolution.\n\nMany think of Blackwell as a chip, but it may be better to think of it as a platform powering large-scale AI infrastructure.\n\n## **Surging Demand and Model Complexity**\n\nBlackwell is the core of an entire system architecture designed specifically to power AI factories that produce intelligence using the largest and most complex AI models.\n\nToday’s frontier AI models have hundreds of billions of parameters and are estimated to serve nearly a billion users per week. The next generation of models are expected to have well over a trillion parameters — and are being trained on tens of trillions of [tokens](https://blogs.nvidia.com/blog/ai-tokens-explained/) of data drawn from text, image and video datasets.\n\nScaling out a data center — harnessing up to thousands of computers to share the work — is necessary to meet this demand. But far greater performance and energy efficiency can come from first scaling up: by making a bigger computer.\n\nBlackwell redefines the limits of just how big we can go.\n\nYour browser does not support the video tag.\n\nExponential growth of parameters in notable AI models over time. Data Source: Epoch (2025), with major processing by Our World In Data\n\n## **Today’s Most Challenging Form of Computing**\n\nAI factories are the machines of the next industrial revolution. Their work is AI inference — the most challenging form of computing known today — and their product is intelligence.\n\nThese factories require infrastructure that can adapt, scale out and maximize every bit of compute resource available.\n\nWhat does that look like?\n\nA symphony of compute, networking, storage, power and cooling — with integration at the silicon and systems levels, up and down racks — orchestrated by software that sees tens of thousands of Blackwell GPUs as one.\n\nThe new unit of the data center is [NVIDIA GB200 NVL72](https://www.nvidia.com/en-us/data-center/gb200-nvl72/), a rack-scale system that acts as a single, massive GPU.\n\nNVIDIA CEO Jensen Huang shows off the NVIDIA GB200 NVL72 system and the NVIDIA Grace Blackwell superchip during his keynote at CES 2025.\n\n## **Birth of a Superchip**\n\nAt the core, the NVIDIA Grace Blackwell superchip unites two Blackwell GPUs with one NVIDIA Grace CPU.\n\nFusing them into a unified compute module — a superchip — boosts performance by an order of magnitude. To do so requires a new high-speed interconnect technology introduced with the NVIDIA Hopper architecture: [NVIDIA NVLink](https://www.nvidia.com/en-us/data-center/nvlink/) chip-to-chip.\n\nThis technology unlocks seamless communication between the CPU and GPUs, enabling them to share memory directly, resulting in lower latency and higher throughput for AI workloads.\n\nYour browser does not support the video tag.\n\nIt takes a symphony of creation, cutting, assembly and inspection to build a superchip.\n\n## **A New Interconnect for the Superchip Era**\n\nScaling this performance across multiple superchips without bottlenecks was impossible with previous networking technology. So NVIDIA created a new kind of interconnect to keep performance bottlenecks from emerging and enable AI at scale.\n\nYour browser does not support the video tag.\n\n## **A Backbone That Clears Bottlenecks**\n\nThe NVIDIA NVLink Switch spine anchors GB200 NVL72 with a precisely engineered web of over 5,000 high-performance copper cables, connecting 72 GPUs across 18 compute trays to move data at a staggering 130 TB/s.\n\nThat’s fast enough to transfer the entire internet’s peak traffic in less than a second.\n\nTwo miles of copper wire is precisely cut, measured, assembled and tested to create the blisteringly fast NVIDIA NVLink Switch spine.\n\nThe spine cartridge is inspected before installation.\n\nYour browser does not support the video tag.\n\nThe spine, powered up, can move an entire internet’s worth of data in less than a second.\n\n## **Building One Giant GPU for Inference**\n\nThe integration of all this advanced hardware and software, compute and networking enables GB200 NVL72 systems to unlock new possibilities for AI at scale.\n\nEach rack weighs one-and-a-half tons — featuring more than 600,000 parts, two miles of wire and millions of lines of code converged.\n\nIt acts as one giant virtual GPU, making factory-scale AI inference possible, where every nanosecond and watt matters.\n\nYour browser does not support the video tag.\n\n## **GB200 NVL72 Everywhere**\n\nNVIDIA then deconstructed GB200 NVL72 so that partners and customers can configure and build their own NVL72 systems.\n\nEach NVL72 system is a two-ton, 1.2-million-part supercomputer. NVL72 systems are manufactured across more than 150 factories worldwide with 200 technology partners.\n\nFrom cloud giants to system builders, partners worldwide are producing NVIDIA Blackwell NVL72 systems.\n\n## **Time to Scale Out**\n\nTens of thousands of Blackwell NVL72 systems converge to create [AI factories](https://blogs.nvidia.com/blog/ai-factory/).\n\nWorking together isn’t enough. They must work as one.\n\n[NVIDIA Spectrum-X Ethernet](https://www.nvidia.com/en-us/networking/spectrumx/) and [Quantum-X800 InfiniBand](https://www.nvidia.com/en-us/networking/products/infiniband/quantum-x800/) switches make this unified effort possible at the data center level.\n\nEach GPU in an NVL72 system is connected directly to the factory’s data network, and to every other GPU in the system. GB200 NVL72 systems offer 400 Gbps of Ethernet or InfiniBand interconnect using [NVIDIA ConnectX-7 NICs](https://www.nvidia.com/en-us/networking/ethernet-adapters/).\n\nNVIDIA Quantum-X800 Switch, NVLink Switch, and Spectrum-X Ethernet unify one or many NVL72 systems to function as one.\n\n## **Opening Lines of Communication**\n\nScaling out AI factories requires many tools, each in service of one thing: unrestricted, parallel communication for every AI workload in the factory.\n\n[NVIDIA BlueField-3 DPUs](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/documents/datasheet-nvidia-bluefield-3-dpu.pdf) do their part to boost AI performance by offloading and accelerating the non-AI tasks that keep the factory running: the symphony of networking, storage and security.\n\nNVIDIA GB200 NVL72 powers an AI factory by CoreWeave, an NVIDIA Cloud Partner.\n\n## **The AI Factory Operating System**\n\nThe data center is now the computer. [NVIDIA Dynamo](https://www.nvidia.com/en-us/ai/dynamo/) is its operating system.\n\nDynamo orchestrates and coordinates AI inference requests across a large fleet of GPUs to ensure that AI factories run at the lowest possible cost to maximize productivity and revenue.\n\nIt can add, remove and shift GPUs across workloads in response to surges in customer use, and route queries to the GPUs best fit for the job.\n\nColossus, xAI’s AI supercomputer. Created in 122 days, it houses over 200,000 NVIDIA GPUs — an example of a full-stack, scale-out architecture.\n\nBlackwell is more than a chip. It’s the engine of AI factories.\n\nThe world’s largest-planned computing clusters are being built on the Blackwell and Blackwell Ultra architectures — with approximately 1,000 racks of NVIDIA GB300 systems produced each week.\n\nCategories: [Data Center](https://blogs.nvidia.com/blog/category/enterprise/) [Explainer](https://blogs.nvidia.com/blog/category/explainer/)\n\nTags: [AI Factory](https://blogs.nvidia.com/blog/tag/ai-factory/) [Artificial Intelligence](https://blogs.nvidia.com/blog/tag/artificial-intelligence/) [Hardware](https://blogs.nvidia.com/blog/tag/hardware/) [Inference](https://blogs.nvidia.com/blog/tag/inference/) [NVIDIA BlueField](https://blogs.nvidia.com/blog/tag/nvidia-bluefield/) [NVLink](https://blogs.nvidia.com/blog/tag/nvlink/)\n\nRelated News\n\n[At Climate Week NYC, NVIDIA Details AI’s Key Role in the Sustainable Energy Transition](https://blogs.nvidia.com/blog/ai-energy-innovation-climate-research/)\n\nEnergy efficiency in large language model inference has improved 100,000x in the past 10 years — demonstrating that accelerated computing is sustainable computing. At Climate Week NYC, taking place through... [Read Article](https://blogs.nvidia.com/blog/ai-energy-innovation-climate-research/)\n\n[NVIDIA, OpenAI Announce ‘the Biggest AI Infrastructure Deployment in History’](https://blogs.nvidia.com/blog/openai-nvidia/)\n\nOpenAI and NVIDIA just announced a landmark AI infrastructure partnership — an initiative that will scale OpenAI’s compute with multi-gigawatt data centers powered by millions of NVIDIA GPUs. To discuss... [Read Article](https://blogs.nvidia.com/blog/openai-nvidia/)\n\n[The UK’s ‘Goldilocks Moment for AI’: NVIDIA, UK and US Leaders Highlight AI Infrastructure Investments](https://blogs.nvidia.com/blog/uk-ai-ecosystem-celebration/)\n\nThe U.K. was the center of the AI world this week as NVIDIA, U.K. and U.S. leaders announced new initiatives toward making the nation an AI superpower. NVIDIA founder and... [Read Article](https://blogs.nvidia.com/blog/uk-ai-ecosystem-celebration/)\n\n[The AI Makers: NVIDIA Partners in UK Advance Physical and Agentic AI, Robotics, Life Sciences and More](https://blogs.nvidia.com/blog/uk-partner-ecosystem-ai-makers/)\n\nThe U.K. is driving investments in sovereign AI, using the technology to advance industries like manufacturing, life sciences and more. During NVIDIA founder and CEO Jensen Huang’s visit to the... [Read Article](https://blogs.nvidia.com/blog/uk-partner-ecosystem-ai-makers/)\n\n[Share This](https://x.com/intent/tweet?via=%username%&url=%url%&text=%prefix%%text%%suffix%&hashtags=%hashtags%","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fblogs.nvidia.com%2Fblog%2Fblackwell-ai-inference%2F","id":"https://blogs.nvidia.com/blog/blackwell-ai-inference/","publishedDate":"2025-09-18T00:00:00.000Z","summary":"NVIDIA Blackwell is a platform designed for large-scale AI infrastructure, particularly AI factories that handle the most complex AI models. It aims to meet the surging demand for AI by scaling up computing capabilities, offering significant performance and energy efficiency. The core of Blackwell is the NVIDIA Grace Blackwell superchip, which unites two Blackwell GPUs with one NVIDIA Grace CPU, boosting performance through a new high-speed interconnect technology called NVIDIA NVLink. The NVIDIA GB200 NVL72, a rack-scale system, acts as a single, massive GPU, and is anchored by the NVIDIA NVLink Switch, which uses over 5,000 high-performance copper cables to move data at 130 TB/s.\n","title":"NVIDIA Blackwell: Born for Extreme-Scale AI Inference","url":"https://blogs.nvidia.com/blog/blackwell-ai-inference/"},{"content":"September 12, 2025\n\n# Matrix Multiplication on Blackwell: Part 3 - The Optimizations Behind 85% of SOTA Performance\n\nAli Taha\n\nJiexiang Liu\n\nHengjie Wang\n\nAbdul Dakkak\n\nIn the prior blog posts in this series ( [part 1](https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-1-introduction) and [part 2](https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-2-using-hardware-features-to-optimize-matmul)) we talked about how to achieve ~300GFlops performance for Matmul on NVIDIA Blackwell GPUs. In this post, we continue on this journey and discuss how to leverage the 2SM technique along with pipelining to **increase our performance about 5x and get within 85% of state-of-the-art (SOTA)**.\n\nFigure 1: roadmap to 85% performance\n\n## Kernel 5: multicast and 2xSM MMA\n\nSince NVIDIA’s Hopper generation, Streaming Multiprocessors (SMs) can be grouped, and Cooperative Thread Arrays (CTAs) within the same SM group can access each other’s shared memory (also known as distributed shared memory access). This brings up two advanced optimizations on Blackwell:\n\n- Tensor Memory Accelerator (TMA) multicasting—supported since Hopper: SMs collaborate on loading a tile into shared memory.\n- 2xSM Matrix Multiply-Accumulate (MMA): 2 SM's tensor cores collaborate on one large MMA operation using inputs in SM's shared memory.\n\nLet’s examine both techniques and see how they are reflected in code. The first thing we need to do is inform the code about the size of the cluster (the `cluster_shape`). This can be done in Mojo using the `@__llvm_metadata` decorator:\n\nMojo\n\n@\\_\\_llvm\\_metadata(\\`nvvm.cluster\\_dim\\`=cluster\\_shape)\nfn blackwell\\_tma\\_pair\\_umma\\_kernel\\[\na\\_type,\n...other parameters...,\ncluster\\_shape\n\\]\n\nCopy\n\nThe above sets the “CTA cluster” launch shape for the kernel at compile time. CTAs within in the same cluster can access to each others’ shared memory.\n\n### CTA memory multicasting\n\nTo understand memory multicasting, let’s take a simple example and examine what we’d have to do without the multicasting feature. For visualization, we will use A, B, and C as 256x256 matrices. In this case, we will launch 4 CTA’s.\n\nFigure 2: matrix multiplication of two 256x256 matrices with 4 CTAs\n\nEach CTA will compute a 128x128 tile. Let’s examine which tiles CTA each loads throughout `K/BK`:\n\nFigure 3: CTA tile loading\n\nNote the redundancy in the memory loads. This is similar to the redundancy we noticed at the beginning of the [blog series](https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-2-using-hardware-features-to-optimize-matmul#shared), but this time the redundancy is occurring at tile granularity instead of element granularity.\n\nHow can we mitigate this? If we group these 4 CTAs into a 2x2 cluster, the SMs can now access each others’ shared memory, which allows each SM to load a tile from global memory and then broadcast the memory to its neighbors.\n\nWith this capability, we can adjust our load strategy and have the two CTA’s along the same row load exactly one half of the tile from A, and to broadcast that half to their neighbor. That way, both CTA’s do half the loads while still getting the full tile. Visually, this looks like:\n\nFigure 4: CTAs multicasting tile rows of A\n\nWe can extend this technique to save the loads for the B matrix as well:\n\nFigure 5: CTAs multicasting tile columns of B\n\nThis technique is called multicasting and is scalable. For example, when launching a 4x4 cluster we’d make each CTA load half of its tiles and get the rest of the data from its peers. Let’s look at how this is implemented in Mojo. We begin by declaring the TMA ops on the host side:\n\nMojo\n\na\\_tma\\_op = create\\_tma\\_tile\\[\na\\_type, 2, (BM // cluster\\_shape\\[1\\], BK), swizzle\\_mode=a\\_swizzle\n\\](ctx, a)\nb\\_tma\\_op = create\\_tma\\_tile\\[\nb\\_type, 2, (BN // cluster\\_shape\\[0\\], BK), swizzle\\_mode=b\\_swizzle\n\\](ctx, a)\n\nCopy\n\nSince each CTA will slice the shared memory tiles for A and B and broadcast them to corresponding peer CTAs, we divide the TMA tile by the cluster dimension. In the GPU kernel, we launch the multicast load using:\n\nMojo\n\nif elect\\_one\\_thread:\n....\na\\_tma\\_op.async\\_multicast\\_load(\na\\_smem\\_slice,\ntma\\_mbar\\[0\\],\n(UInt(i \\* BK), UInt(a\\_gmem\\_slice\\_coord)),\na\\_multicast\\_mask,\n)\n\nCopy\n\nThe API is similar to the `async_load` TMA API we showed in [our last blog](https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-2-using-hardware-features-to-optimize-matmul#tma) but with a few key differences, discussed in the following paragraphs.\n\nFirst, there is an additional TMA mask `a_multicast_mask`. The TMA mask is a 16 bit value describing the CTA index involved in the load. Each bit stands for one CTA and there can be at most 16 CTAs in one cluster. For example, in the above CTA 0 and 2 multicast the A tile so the CTA masks are both `0b101`. In Mojo we can compute that via:\n\nMojo\n\nvar rank\\_m = block\\_id\\_in\\_cluster.x\n\\# CLUSTER\\_M and CLUSTER\\_N are cluster dimensions\n@parameter\nfor i in range(CLUSTER\\_N):\na\\_multicast\\_mask \\|= 1 << (i \\* CLUSTER\\_M)\na\\_multicast\\_mask <<= rank\\_m\n\nCopy\n\nThe layout tensor `a_smem_slice` is sliced from the original tile by offsetting the base pointer:\n\nMojo\n\nalias a\\_tma\\_load\\_size = a\\_desc\\_layout.size()\nvar rank\\_n = block\\_id\\_in\\_cluster.y\nvar a\\_smem\\_slice = \\_\\_type\\_of(a\\_smem\\_tile)(\na\\_smem\\_tile.ptr + rank\\_n \\* a\\_tma\\_load\\_size\n)\n\nCopy\n\nWe also update the corresponding coordinates in global memory for this tile slice:\n\nMojo\n\n\\# Number of rows per slice\nalias a\\_tma\\_rows = a\\_desc\\_layout.shape\\[0\\].value()\na\\_gmem\\_slice\\_coord = block\\_idx.x \\* BM + Int(rank\\_n) \\* a\\_tma\\_rows\n\nCopy\n\n### 2xSM MMA\n\nWe have shown how distributed shared memory and multicast can help reduce the data volume from global memory to shared memory. However there is still one limitation—the tiles are duplicated in the distributed shared memory. Looking into CTAs 0 and 1 in the previous example, we see both CTAs keep a copy of `BNxBK` tile in shared memory, as shown in the left side of Figure 6 below. Given each CTA has access to the other’s shared memory, saving two copies of the same tile is a waste of resources.\n\nFigure 6: duplication of tiles in distributed shared memory\n\nBlackwell’s 2xSM MMA instruction ( `tcgen05.mma.cta_group::2`) is designed to address this problem. As shown [in the right side of Figure 6](https://www.modular.com/www.modular.com#figure-06), CTAs 0 and 1 works as a pair and each only loads half of the B tile. The 2xSM MMA instruction sees both halves in shared memory and coordinates the tensor cores on both SMs to complete MMA operations as large as two single SM MMAs. Comparing the left and right figures above, the 2 SMs are calculating the same MMA workload `2*BM x MMA_N x BK` , where `MMA_N` denotes the N dimension of MMA shape, and producing the same result, however the 2xSM instruction reduces the shared memory usage for the B tile by half.\n\nComparing to multicast load with single SM MMA, the 2xSM MMA load reduces shared memory traffic in two ways. First, CTAs 0 and 1 still load the same half of the B tile as in multicast load (see [Figure 5](https://www.modular.com/www.modular.com#figure-05)) but it’s only a regular TMA transfer, i.e. the multicast step is omitted. Moreover, the traffic from shared memory to tensor memory for the B tile is also halved as we only save one copy of it in the distributed shared memory.\n\nThe code to launch 2xSM MMA is quite similar to launching single SM MMA ( [see our previous blog](https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-2-using-hardware-features-to-optimize-matmul#mma)), with the exception of using `elect_one_cta`. Since two SMs collaborate on one MMA operation, only one CTA should launch the instruction and that’s the CTA with even ID in each pair of CTAs. Hence, the `elect_one_cta` is a predicate to select the launching CTA (also called the leader CTA).\n\nMojo\n\nif elect\\_one\\_cta:\n\\# wait for data arriving in shared memory\n...\nif elect\\_one\\_thread:\n@parameter\nfor j in range(num\\_k\\_mmas):\nvar c\\_scale = 0 if i == 0 and j == 0 else 1\nalias idx = IntTuple(0, MMA\\_K \\* j)\nalias a\\_offset = a\\_smem\\_layout(idx) \\* sizeof\\[a\\_type\\]()\nalias b\\_offset = b\\_smem\\_layout(idx) \\* sizeof\\[b\\_type\\]()\nmma\\[cta\\_group\\](\nadesc + a\\_offset,\nbdesc + b\\_offset,\ntmem\\_addr,\nidesc,\nc\\_scale=c\\_scale,\n)\nmma\\_arrive\\_multicast\\[cta\\_group\\](mma\\_mbar, c\\_mma\\_mask)\n\nCopy\n\nThe `mma` function above will invoke the `tcgen05.mma.cta_group::2` instruction when `cta_group` is 2. We also change the arrive function from `mma_arrive` ( [see the previous blog](https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-2-using-hardware-features-to-optimize-matmul#tensor-memory)) to `mma_arrive_multicast`, which takes `cta_group` and signals the leader CTA’s memory barrier when `cta_group=2`.\n\n#### Tensor memory layout\n\nWith the 2xSM MMA instruction, the two CTAs partition the M dimension ( `MMA_M`) evenly ( `BM = MMA_M/2` ) and each has half of the result of shape `BM x MMA_N` in tensor memory. The instruction supports `MMA_M` values of 128 and 256. For `MMA_M=256`, the TMEM layout for each half is the same as single SM MMA as shown in Figure 7 below. The top half is stored in the leader CTA with the lower half in its pair CTA. The output to global memory is the same as in kernel 4 from our last post.\n\nFigure 7: tensor memory layout\n\nFor `MMA_M = 128`, the layout is different than above. We skip the details here and [refer the reader to the code](https://github.com/modular/modular) since `MMA_M = 128` is less important in production.\n\nFigure 8: 20% SOTA performance with CTA memory multicast and 2xSM MMA\n\nWith this implementation, we are now at 360.2 TFLOPs and about 20% of our SOTA goal. If we look at the profile, even though we are using the advanced MMA instruction, we’re still limited by global memory throughput because the c","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.modular.com%2Fblog%2Fmatrix-multiplication-on-nvidias-blackwell-part-3-the-optimizations-behind-85-of-sota-performance","id":"https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-3-the-optimizations-behind-85-of-sota-performance","publishedDate":"2025-09-12T00:00:00.000Z","summary":"This blog post, the third in a series, details optimizations to achieve 85% of state-of-the-art (SOTA) performance for matrix multiplication on NVIDIA Blackwell GPUs. It focuses on two key techniques: Tensor Memory Accelerator (TMA) multicasting, which allows SMs to collaborate on loading tiles into shared memory, and 2xSM Matrix Multiply-Accumulate (MMA), where two SM's tensor cores work together on a large MMA operation. The post explains how these features are implemented in Mojo, including setting the CTA cluster launch shape and utilizing multicasting to reduce redundant memory loads by having CTAs share data.\n","title":"Modular: Matrix Multiplication on Blackwell: Part 3 - The Optimizations Behind 85% of SOTA Performance","url":"https://www.modular.com/blog/matrix-multiplication-on-nvidias-blackwell-part-3-the-optimizations-behind-85-of-sota-performance"},{"content":"INTRODUCING\nAMD CDNA™ 4 \nARCHITECTURE \nBreakthrough AI and HPC Acceleration with Enhanced AI \nCapabilities, Advanced Precisions, and High Efficiency\nWHITE PAPER | INTRODUCING AMD CDNA™ 4 ARCHITECTURE 2\nINTRODUCTION\nThe data center landscape has been fundamentally transformed by GPUs and accelerated computing. Accelerated \ncomputing was initially embraced in the scientific community to complement existing general-purpose CPUs for \nparticular workloads such as seismic analysis and molecular dynamics. With that foothold established, accelerators \nevolved to support increasingly more general programming languages such as C++ and Python and more diverse \napplications. Machine learning – especially in computer vision – was early to take advantage of new accelerators and \ngave rise to the field of deep learning that explicitly depends on accelerators such as GPUs to train and deploy neural \nnetworks. Between classic scientific computing and emerging machine learning and AI workloads, GPUs were in\u0002creasingly designed to operate at scale – powering the world’s largest supercomputers. The discovery of transformer\u0002based neural networks expanded the horizons for GPUs and ultimately led to the explosion of generative AI, which \nis one of the most demanding application areas to date and reaches well outside of scientific computing to both \nconsumer and business usage.\nThe demand for computational capabilities and efficiency is so great that architects can leave no stone unturned \nand are constantly revisiting basic assumptions in computing. Initially, accelerators adopted a new memory model \nbut retained familiar datatypes. Now, floating-point numerical representation is evolving alongside new algorithms \nto enable reduced precision thereby improving performance, memory footprint, and energy efficiency. At the same \ntime, GPUs have pushed the envelope in terms of integration – first adopting advanced packaging to tightly couple \nHBM to the accelerator and now using even more advanced packaging techniques to shift away from monolithic \nsilicon implementations.\nAMD has been at the forefront of this revolution, developing novel software and system architectures for GPUs to \nenable ever greater performance and capabilities. From the software side, the AMD CDNA™ 2 architecture unified \nCPU and GPU computing through cache coherency to unlock new applications and opportunities for the software \necosystem. The AMD CDNA 3 architecture revolutionized how accelerators are built, adopting advanced packaging to \nenable heterogeneous integration and repartitioning a processor across a dozen different chiplets. \nAs Figure 1 below shows, the AMD CDNA 4 architecture builds on the previous foundation of 3D packaging technolo\u0002gies and rebalances the elements of the processor across the heterogenous package. Each AMD Instinct™ MI350 \nSeries GPU integrates 8 vertically stacked accelerator complex dies (XCD) and 2 I/O Dies (IODs) containing system \ninfrastructure, tied together with the AMD on-package AMD Infinity Fabric™ technology and connecting to 8 stacks \nof 12-Hi high-bandwidth memory (HBM3E). The XCDs combine the latest process technology and support for new \nreduced precision datatypes to boost the throughput and efficiency for AI, while the repartitioned IOD helps improve \nlatency and efficiency for communication and memory traffic. Externally, the AMD Instinct MI350 Series family of \nproducts employ Infinity Fabric to directly connect 8 GPUs in a single node. \nWHITE PAPER | INTRODUCING AMD CDNA™ 4 ARCHITECTURE 3\nFigure 1. Advanced 3D Package and chiplet-based construction of the AMD Instinct MI350 Series GPU\nThis design provides the versatility to rapidly develop and deploy a family of AMD CDNA 4 architecture based \nproducts which address diverse customer needs – both ease-of-use and maximum performance. The AMD \nInstinct MI350 Series includes the air-cooled (AC) Instinct MI350X GPU that is 1000W offered in an OCP UBB8 \nbase board which was designed to be drop-in compatible with the previous generation AMD Instinct™ MI325X \nPlatforms and systems for the fast deployment into the existing ecosystem and quick time to value. For \nenhanced performance and efficiency with unparalleled density, the AMD Instinct MI355X GPU (1400W) offers \ndirect liquid-cooled (DLC) platforms, again offered in a on a UBB8 baseboard, for infrastructure that supports \ngreater power and cooling. The AMD Instinct MI355X GPU almost doubles the peak throughput for existing \nmachine learning focused 16-bit and 8-bit Matrix datatypes compared to the prior generation AMD Instinct \nMI325X GPU and introduces support for additional reduced precision numerical formats that offer 3.85x the \nperformance comparing 10TFLOP/s peak theoretical for FP6/FP4 to the 2.61 PFLOPS of FP8 peak theoretical \nperformance the MI325X.MI350-005 The MI355X also increases memory capacity generationally to 288GB HBM3E \nwith 8TB/s of bandwidth and over 1TB/s of communication bandwidth. The CDNA 4 architecture is designed \nto work with the open-source powered ROCm™ software ecosystem and provide excellent out-of-the-box \ncapabilities for scientific computing as well as enterprise orchestration through Kubernetes® and support for \nleading AI training and inference stacks and Day 0 support for popular generative AI models.\nAMD Infinity Cache™\nHBM3 Memory Stacks\nIODS\nIODs\nXCDs\nWHITE PAPER | INTRODUCING AMD CDNA™ 4 ARCHITECTURE 4\nThe AMD CDNA™ 3 architecture was a paradigm shift - adopting a modern, chiplet-based approach that fully \nleveraged heterogeneous integration and tied together myriad specialized chiplets with the Infinity Fabric \ninto a highly optimized computing platform. Compared to the monolithic approach that dominated the prior \nseveral decades, this was a fundamental rethinking of silicon design and architecture – one that will likely \nunlock performance and scalability for multiple generations.\nThe CDNA 4 architecture inherits this revolutionary foundation and demonstrates the advantages of this \nflexible strategy by carefully optimizing every component to deliver optimal performance, efficiency, and \nmanufacturability. The eight compute chiplets or XCDs benefit from the latest process technology and are \nimplemented in TSMC’s cutting-edge N3P process technology with a similar die size and footprint to the \nprior generation to achieve optimal performance and efficiency. The memory and communication functions \nin the IODs are dominated by the large AMD Infinity Cache™ and interconnects that do not effectively scale \nto fully take advantage of performance offered by the latest process technologies. Taking advantage of the \nindependent scaling in a heterogeneous architecture, these functions are kept on TSMC’s highly efficient \nand affordable N6 process but rebalanced across two massive IODs, rather than four, which optimizes the \nperformance and energy efficiency while maintaining an advantageous manufacturability profile. \nCHIPLET ARCHITECTURE\nWHITE PAPER | INTRODUCING AMD CDNA™ 4 ARCHITECTURE 5\nThe AMD CDNA™ 4 architecture highlights one of the major advantages of a chiplet-based, heterogeneous \napproach to building a compute platform – each chiplet can use the right process technology, which enables a \nmore efficient evolution over time. The accelerator complex dies (XCDs) house the computational portion of \nthe processor and the lowest levels of the cache hierarchy that are the most performance sensitive. The AMD \nCDNA 4 XCDs are built on the latest process technology, TSMC’s N3P, to take advantage of the improved logic \ndensity and performance compared to the N5 process used in the prior generation. \nAMD CDNA™ 4 COMPUTE\nThe AMD CDNA 4 architecture rebalances the XCDs as shown in Figure 2 to boost performance, especially \nfor the most demanding AI workloads, by increasing the capabilities of each Compute Unit (CU) – enabling \nhardware support for new datatypes and achieving dramatically higher computational throughput and \nefficiency for both vector and matrix workloads as seen in Table 1 below.\nIn the AMD CDNA 4 XCD, the number of CUs are slightly reduced compared to the prior generation, but \neach CU is more powerful through a combination of new features. Consequently, the global resources such \nas the scheduler, hardware queues and Asynchronous Compute Engines (ACE) that send compute shader \nworkgroups to the Compute Units (CUs) only need minor enhancements.\nEach XCD comprises 36 AMD CDNA 4 Compute Units organized as four arrays of 9 CUs, of which 32 are active, \nleaving four potentially disabled to enable high yield and efficiency operating frequencies. As with the prior \ngeneration, the L2 cache coalesces all the traffic within the XCD before fanning out to the Infinity Fabric that \nconnects to the rest of the system. The processor spans 8 XCDs, for up to 256 CUs – slightly less than the \nprior generation, but in some cases doubling the computational throughput compared to the AMD Instinct \nMI300 Series GPUs.MI350-005\nFigure 2. Conceptual block diagram of an AMD CDNA™ 4 architecture accelerator complex die (XCD)\nGLOBAL\nRESOURCES\n4MB L2 CACHE\nHWS \nHARDWARE\nSCHEDULER\nACE0\nCOMPUTE \nACCELERATOR\nUNIFIED COMPUTE SYSTEM\nHQDO-7 HQDO-7 HQDO-7 HQDO-7\nCU0 32KB L1 32KB L1 32KB L1\nACE1\nCOMPUTE \nACCELERATOR\nACE2\nCOMPUTE \nACCELERATOR\nACE3\nCOMPUTE \nACCELERATOR\nCU1 CU35\nWHITE PAPER | INTRODUCING AMD CDNA™ 4 ARCHITECTURE 6\nSince most workloads will span many CUs, the 64KB, 8-way set-associative instruction cache is shared \nbetween two adjacent CUs, making efficient use of the caches and area. \nThe greatest generational improvements in capabilities in the AMD CDNA 4 CUs are in the Matrix Cores and \nfocus on AI and machine learning, adding both hardware support for new standardized numerical formats \nand increasing the raw computational resources for existing data types. Lower precision numerical formats \nare one of the most efficient and powerful techniques to boost performance for AI. The smalle","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.amd.com%2Fcontent%2Fdam%2Famd%2Fen%2Fdocuments%2Finstinct-tech-docs%2Fwhite-papers%2Famd-cdna-4-architecture-whitepaper.pdf","id":"https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-4-architecture-whitepaper.pdf","publishedDate":"2025-06-12T00:00:00.000Z","summary":"This white paper introduces the AMD CDNA 4 architecture, designed for breakthrough AI and HPC acceleration. It builds on previous architectures with advanced 3D packaging and rebalances processor elements. The AMD Instinct MI350 Series GPU, based on this architecture, integrates 8 accelerator complex dies (XCDs) and 2 I/O Dies (IODs), connected by AMD Infinity Fabric and high-bandwidth memory (HBM3E). This design supports diverse customer needs and includes the air-cooled Instinct MI350X GPU.\n","title":"[PDF] AMD CDNA™ 4 ARCHITECTURE","url":"https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-4-architecture-whitepaper.pdf"},{"content":"Select LanguageRussianArabicChinese (Simplified)EnglishFrenchGermanJapaneseKoreanPolishPortugueseSpanishUkrainian\n\n# AMD UDNA Architecture Revealed in New Series of Block Diagrams\n\nAuthor [Maximum Games](https://en.gamegpu.com/view-user-profile?user=41287), August 29 2025. Published in [HARDWARE](https://en.gamegpu.com/backgammon/news/iron/)\n\n[Go to comments](https://en.gamegpu.com/iron/architectural-design-should-be-opened-in-a-new-series-of-block-diagrams#disqus_thread)\n\nAMD's UDNA architecture has been revealed in a new series of block diagrams published by a reputable leaker, revealing that the company is developing a highly scalable GPU-a solution covering four crystal configurations with different numbers **computing units** From **12 CU in basic versions** to **96 CU in the flagship model**. The approach is based on a modular structure that combines **shader arrays**, **rendering engines** и **unified memory controllers** with the ability to scale depending on the tasks - from mass gaming to data centers and AI acceleration.\n\nThe flagship version consists of eight shader arrays, each of which includes two engines with six compute units. This gives 16 engines and a total of **96 CU**, which corresponds to the published scheme. The center of the architecture occupies a single SoC block with **Graphics Command Processor (GCP)**, **graphics engine (GE)**, **hardware task scheduler (HWS)** и **shared L2 cache**. For exchange with memory are used **16 controllers of 32 bits**, which forms **512-bit bus**, typical for top models.\n\nThe middle level of architecture offers up to **40 CU** (four arrays, two engines with five CUs each) and **192-bit bus**, which hints at the use **six memory controllers**. Below are the solutions **24 and 12 CU**, which can be used in mobile and energy-efficient solutions. Memory configurations may vary from GDDR to LPDDR5X depending on the target market.\n\nThe charts also indicate a potential increase **local cache on CU** in server versions, as well as possible unification of architecture between series **Radeon and Instinct** This will allow AMD to scale UDNA more efficiently for both gaming GPU, as well as for AI and HPC accelerators. None of this data has been officially confirmed yet, and the final specifications may change. However, AMD's modular approach demonstrates the company's ambitions to create a universal GPU- new generation solutions.\n\n[To the top of the page](https://en.gamegpu.com/%D0%B6%D0%B5%D0%BB%D0%B5%D0%B7%D0%BE/arkhitektura-amd-udna-raskryta-v-novoj-serii-blok-skhem#startOfPage)\n\n- [Back](https://en.gamegpu.com/iron/rdna-5-amd-can-implement-multichip-gpu-with-2-5d-and-3-5d-upscaling-in-consumer-segment)\n- [Forward](https://en.gamegpu.com/iron/US-gets-share-in-Intel-Corporation-invests-5-billion-in-exchange-for-7-shares)\n\n### HARDWARE NEWS\n\n- [**Apple is preparing the first foldable iPhone**](https://en.gamegpu.com/iron/apple-ready-first-iphone)\n- [**Samsung is preparing a 25-inch OLED monitor for gamers**](https://en.gamegpu.com/iron/Samsung-gotovit-25-dyujmovyj-oled-monitor-for-gamers)\n- [**Snapdragon 8 Elite Gen 5 Will Get More Power, But No Improved Power Efficiency**](https://en.gamegpu.com/iron/Snapdragon-8-Elite-Gen-5-Gets-More-Power-But-Doesn%27t-Improve-Energy-Efficiency)\n- [**DDR4 is getting more expensive, DDR5 is getting cheaper: the memory market is entering a paradoxical phase**](https://en.gamegpu.com/iron/ddr4-is-getting-more-expensive--ddr5-is-getting-cheaper--the-memory-market-is-going-into-a-paradoxical-phase)\n- [**Ryzen 5 9500F and Ryzen 7 9700F Spotted with $217 and $294 Prices**](https://en.gamegpu.com/iron/ryzen-5-9500f-and-ryzen-7-9700f-zasvetilis-with-tsenami-217-and-294)\n- [**Reverse trend: current generation consoles are not getting cheaper, but are getting more expensive since their release**](https://en.gamegpu.com/iron/the-opposite-trend-is-that-next-generation-consoles-are-not-getting-cheaper-but-are-getting-more-expensive-from-the-moment-of-release)\n- [**User's Second Ryzen 9800X3D Burned Out on ASRock B850I Lightning WiFi Board**](https://en.gamegpu.com/iron/u-user-burned-out-second-ryzen-9800x3d-on-plate-asrock-b850i-lightning-wifi)\n- [**ASUS ROG Matrix GeForce RTX 5090 Coming in October in Limited Edition**](https://en.gamegpu.com/iron/asus-rog-matrix-geforce-rtx-5090-will-be-released-in-october-with-limited-tide)\n- [**LG 32U990A is the first 6K monitor with Thunderbolt 5 and HDR 600 certification**](https://en.gamegpu.com/iron/lg-32u990a-first-6k-monitor-with-thunderbolt-5-and-certified-hdr-600)\n- [**ASUS introduced Prime Radeon RX 9070 XT White Edition**](https://en.gamegpu.com/iron/asus-predstavila-prime-radeon-rx-9070-xt-white-edition)\n\n### SOFTWARE NEWS\n\n- [**Borderlands 4: Complete List of Achievements and Trophies**](https://en.gamegpu.com/game/borderlands-4-full-list-of-achievements-and-trophies)\n- [**Rockstar Removes Companion App Red Dead Redemption 2**](https://en.gamegpu.com/game/rockstar-removes-red-dead-redemption-2-companion)\n- [**Apple is preparing the first foldable iPhone**](https://en.gamegpu.com/iron/apple-ready-first-iphone)\n- [**Samsung is preparing a 25-inch OLED monitor for gamers**](https://en.gamegpu.com/iron/Samsung-gotovit-25-dyujmovyj-oled-monitor-for-gamers)\n- [**Snapdragon 8 Elite Gen 5 Will Get More Power, But No Improved Power Efficiency**](https://en.gamegpu.com/iron/Snapdragon-8-Elite-Gen-5-Gets-More-Power-But-Doesn%27t-Improve-Energy-Efficiency)\n- [**DDR4 is getting more expensive, DDR5 is getting cheaper: the memory market is entering a paradoxical phase**](https://en.gamegpu.com/iron/ddr4-is-getting-more-expensive--ddr5-is-getting-cheaper--the-memory-market-is-going-into-a-paradoxical-phase)\n- [**Ryzen 5 9500F and Ryzen 7 9700F Spotted with $217 and $294 Prices**](https://en.gamegpu.com/iron/ryzen-5-9500f-and-ryzen-7-9700f-zasvetilis-with-tsenami-217-and-294)\n- [**Reverse trend: current generation consoles are not getting cheaper, but are getting more expensive since their release**](https://en.gamegpu.com/iron/the-opposite-trend-is-that-next-generation-consoles-are-not-getting-cheaper-but-are-getting-more-expensive-from-the-moment-of-release)\n- [**User's Second Ryzen 9800X3D Burned Out on ASRock B850I Lightning WiFi Board**](https://en.gamegpu.com/iron/u-user-burned-out-second-ryzen-9800x3d-on-plate-asrock-b850i-lightning-wifi)\n- [**ASUS ROG Matrix GeForce RTX 5090 Coming in October in Limited Edition**](https://en.gamegpu.com/iron/asus-rog-matrix-geforce-rtx-5090-will-be-released-in-october-with-limited-tide)\n\n### Top materials GameGPU\n\n#### [The Elder Scrolls IV: Oblivion Remastered - PC performance graphics benchmarks of Graphics Cards and Processors...](https://en.gamegpu.com/action-/-fps-/-tps/the-elder-scrolls-iv-oblivion-remasterd-test-gpu-cpu)\n\nApr 23, 2025\n\\| [Action/FPS/TPS](https://en.gamegpu.com/backgammon/gpu-test-a/action-/-fps-/-tps/)\n\nViews : 105733\n\n#### [The Last of Us Part II Remastered - PC performance graphics benchmarks of Graphics Cards and Processors...](https://en.gamegpu.com/action-/-fps-/-tps/the-last-of-us-part-ii-remastered-test-gpu-cpu)\n\nApr 03, 2025\n\\| [Action/FPS/TPS](https://en.gamegpu.com/backgammon/gpu-test-a/action-/-fps-/-tps/)\n\nViews : 93334\n\n#### [Assassin’s Creed Shadows - PC performance graphics benchmarks of Graphics Cards and Processors...](https://en.gamegpu.com/action-/-fps-/-tps/assassin%27s-creed-shadows-test-gpu-cpu)\n\nMarch 18 2025\n\\| [Action/FPS/TPS](https://en.gamegpu.com/backgammon/gpu-test-a/action-/-fps-/-tps/)\n\nViews : 64479\n\n#### [DOOM: The Dark Ages - PC performance graphics benchmarks of Graphics Cards and Processors...](https://en.gamegpu.com/action-/-fps-/-tps/doom-the-dark-ages-test-gpu-cpu)\n\n12 May 2025\n\\| [Action/FPS/TPS](https://en.gamegpu.com/backgammon/gpu-test-a/action-/-fps-/-tps/)\n\nViews : 63264\n\n#### [Clair Obscur: Expedition 33 - PC performance graphics benchmarks of Graphics Cards and Processors...](https://en.gamegpu.com/action-/-fps-/-tps/clair-obscur-expedition-33-test-gpu-cpu)\n\nApr 24, 2025\n\\| [Action/FPS/TPS](https://en.gamegpu.com/backgammon/gpu-test-a/action-/-fps-/-tps/)\n\nViews : 34058\n\n#### [Subscribe to our Telegram-channel GameGPU!](https://en.gamegpu.com/action-/-fps-/-tps/napominaem-podpishites-na-our-telegram-canal-gamegpu)\n\n07 May 2025\n\\| [Action/FPS/TPS](https://en.gamegpu.com/backgammon/gpu-test-a/action-/-fps-/-tps/)\n\nViews : 21370\n\n### Top news GameGPU\n\n#### [Update Windows 11 causes SSD failures during long data writes...](https://en.gamegpu.com/iron/Windows-11-update-causes-ssd-failures-due-to-long-data-recording)\n\nAug 17, 2025\n\\| [HARDWARE](https://en.gamegpu.com/backgammon/news/iron/)\n\nViews : 5544\n\n#### [NVIDIA Didn't Introduce 18+ for Drivers: GeForce NOW Bug Causes...](https://en.gamegpu.com/iron/nvidia-didn%27t-enter-18-for-drivers-error-geforce-now-caused-false-panic)\n\nAug 03, 2025\n\\| [HARDWARE](https://en.gamegpu.com/backgammon/news/iron/)\n\nViews : 4298\n\n#### [Wuchang: Fallen Feathers has received a major 1.5 update...](https://en.gamegpu.com/game/wuchang-fallen-feathers-got-a-major-update-1-5)\n\nAug 12, 2025\n\\| [GAMES](https://en.gamegpu.com/backgammon/news/game/)\n\nViews : 3467\n\n#### [Helldivers 2 Coming to Xbox on August 26: Exact Time Revealed...](https://en.gamegpu.com/game/helldivers-2-coming-to-xbox-26-august-exact-launch-time-revealed)\n\nAug 24, 2025\n\\| [GAMES](https://en.gamegpu.com/backgammon/news/game/)\n\nViews : 3252\n\n#### [NVIDIA has cut prices on the RTX 5090, 5080, and 5070 in...](https://en.gamegpu.com/iron/nvidia-dropped-tseny-na-rtx-5090-5080-i-5070-v-europe-pochti-na-10)\n\nAug 19, 2025\n\\| [HARDWARE](https://en.gamegpu.com/backgammon/news/iron/)\n\nViews : 3177\n\n#### [Metal Gear Solid Delta: Snake Eater is limited to 60 FPS on...](https://en.gamegpu.com/game/metal-gear-solid-delta-snake-eater-limited-60-fps-na-pk-i-ne-podderzhivaet-ultrashirokij-format)\n\nAug 22, 2025\n\\| [GAMES](https://en.gamegpu.com/backgammon/news/game/)\n\nViews : 2594\n\n- [Correspondence table GPU/CPU](https:/","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fen.gamegpu.com%2Firon%2Farchitectural-design-should-be-opened-in-a-new-series-of-block-diagrams","id":"https://en.gamegpu.com/iron/architectural-design-should-be-opened-in-a-new-series-of-block-diagrams","publishedDate":"2025-08-29T00:00:00.000Z","summary":"AMD's UDNA architecture, revealed in new block diagrams, is a highly scalable GPU solution with configurations ranging from 12 to 96 compute units (CU). The flagship model features 96 CUs, eight shader arrays, and a 512-bit memory bus. The architecture's modular design combines shader arrays, rendering engines, and unified memory controllers, allowing for scaling across gaming, data centers, and AI applications. Lower-end configurations include 40, 24, and 12 CU options, with memory configurations varying based on the target market. The architecture may unify Radeon and Instinct series for more efficient scaling.\n","title":"AMD UDNA Architecture Revealed in New Series of Block Diagrams","url":"https://en.gamegpu.com/iron/architectural-design-should-be-opened-in-a-new-series-of-block-diagrams"},{"content":"[Facebook](https://www.facebook.com/ServeTheHome/)[Instagram](https://www.instagram.com/servethehome/)[Linkedin](https://www.linkedin.com/company/servethehome-com/)[RSS](http://feeds.feedburner.com/servethehome)[TikTok](https://www.tiktok.com/@servethehome)[Twitter](https://twitter.com/servethehome)[Youtube](https://www.youtube.com/ServeTheHomeVideo?sub_confirmation=1)\n\n- [Forums](https://forums.servethehome.com)\n- [AI](https://www.servethehome.com/category/ai-deep-learning/)\n- [Server](https://www.servethehome.com/category/server-parts/)\n - [Server Systems](https://www.servethehome.com/category/server-parts/server-systems/)\n - [Server CPUs](https://www.servethehome.com/category/server-parts/cpus/)\n - [Accelerators](https://www.servethehome.com/category/server-parts/accelerators/)\n - [Server Motherboards](https://www.servethehome.com/category/server-parts/motherboards/)\n - [Server Chassis](https://www.servethehome.com/category/server-parts/server-chassis/)\n - [Other Components](https://www.servethehome.com/category/server-parts/other-components/)\n- [5G Edge](https://www.servethehome.com/category/edge/)\n- [Storage](https://www.servethehome.com/category/storage/)\n- [Networking](https://www.servethehome.com/category/networking/)\n- [Workstation](https://www.servethehome.com/category/workstations/)\n - [Workstation Processors](https://www.servethehome.com/category/workstations/workstation-processors/)\n - [Workstation Motherboards](https://www.servethehome.com/category/workstations/workstation-motherboards/)\n- [Software](https://www.servethehome.com/category/server-software/)\n - [Operating Systems](https://www.servethehome.com/category/server-software/operating-systems/)\n - [Server Applications](https://www.servethehome.com/category/server-software/server-applications/)\n - [Virtualization](https://www.servethehome.com/category/server-software/virtualization-other/)\n- [Guides](https://www.servethehome.com/buyers-guides/)\n - [Buyer’s Guides](https://www.servethehome.com/category/client-tips-and-tricks/buyers-guides/)\n - [Tips](https://www.servethehome.com/category/client-tips-and-tricks/)\n - [Top Hardware Components for TrueNAS / FreeNAS NAS Servers](https://www.servethehome.com/buyers-guides/top-hardware-components-for-truenas-freenas-nas-servers/)\n - [Top Hardware Components for pfSense Appliances](https://www.servethehome.com/buyers-guides/top-hardware-components-pfsense-appliances/)\n - [Top Hardware Components for napp-it and Solarish NAS Servers](https://www.servethehome.com/buyers-guides/top-hardware-components-napp-omnios-nas-servers/)\n - [Top Picks for Windows Server 2016 Essentials Hardware](https://www.servethehome.com/buyers-guides/top-picks-windows-server-2016-essentials-hardware/)\n - [The DIY WordPress Hosting Server Hardware Guide](https://www.servethehome.com/buyers-guides/the-diy-wordpress-hosting-server-hardware-guide/)\n\nSearch\n\n[Facebook](https://www.facebook.com/ServeTheHome/)[Instagram](https://www.instagram.com/servethehome/)[Linkedin](https://www.linkedin.com/company/servethehome-com/)[RSS](http://feeds.feedburner.com/servethehome)[TikTok](https://www.tiktok.com/@servethehome)[Twitter](https://twitter.com/servethehome)[Youtube](https://www.youtube.com/ServeTheHomeVideo?sub_confirmation=1)\n\nThursday, May 15, 2025\n\n- [News](https://www.servethehome.com/category/news/)\n- [Storage Reliability](https://www.servethehome.com/raid-calculator/)\n - [Raid Calculator](https://www.servethehome.com/raid-calculator/)\n - [RAID Reliability Calculator \\| Simple MTTDL Model](https://www.servethehome.com/raid-calculator/raid-reliability-calculator-simple-mttdl-model/)\n- [Contact](https://www.servethehome.com/contact/)\n- [About](https://www.servethehome.com/about/)\n - [Editorial and Copyright Policies](https://www.servethehome.com/about/editorial-copyright-policies/)\n- [AG Substack](https://axautikgroupllc.substack.com/)\n- [AG Reports](https://reports.axautikgroup.com/)\n- [Subscribe](https://eepurl.com/dryM09)\n- [Forums](https://forums.servethehome.com)\n\nSign in\n\nWelcome! Log into your account\n\nyour username\n\nyour password\n\n[Forgot your password? Get help](https://www.servethehome.com/amd-radeon-instinct-mi100-32gb-cdna-gpu-launched/amd-radeon-instinct-mi100-to-mi50-speedup/)\n\n[Privacy Policy](https://www.servethehome.com/about/data-processing-and-privacy-policy/)\n\nPassword recovery\n\nRecover your password\n\nyour email\n\nA password will be e-mailed to you.\n\n[![](https://www.servethehome.com/wp-content/uploads/2019/10/STH-Logo-96px.png)ServeTheHome](https://www.servethehome.com/)\n\nAdvertisement\n\n[![](https://www.servethehome.com/wp-content/uploads/2019/10/STH-Logo-48px.png)](https://www.servethehome.com/)[![](https://www.servethehome.com/wp-content/uploads/2019/10/STH-Logo-96px.png)](https://www.servethehome.com/)\n\n- [Forums](https://forums.servethehome.com)\n- [AI](https://www.servethehome.com/category/ai-deep-learning/)\n- [Server](https://www.servethehome.com/category/server-parts/)\n - [Server Systems](https://www.servethehome.com/category/server-parts/server-systems/)\n - [Server CPUs](https://www.servethehome.com/category/server-parts/cpus/)\n - [Accelerators](https://www.servethehome.com/category/server-parts/accelerators/)\n - [Server Motherboards](https://www.servethehome.com/category/server-parts/motherboards/)\n - [Server Chassis](https://www.servethehome.com/category/server-parts/server-chassis/)\n - [Other Components](https://www.servethehome.com/category/server-parts/other-components/)\n- [5G Edge](https://www.servethehome.com/category/edge/)\n- [Storage](https://www.servethehome.com/category/storage/)\n- [Networking](https://www.servethehome.com/category/networking/)\n- [Workstation](https://www.servethehome.com/category/workstations/)\n - [Workstation Processors](https://www.servethehome.com/category/workstations/workstation-processors/)\n - [Workstation Motherboards](https://www.servethehome.com/category/workstations/workstation-motherboards/)\n- [Software](https://www.servethehome.com/category/server-software/)\n - [Operating Systems](https://www.servethehome.com/category/server-software/operating-systems/)\n - [Server Applications](https://www.servethehome.com/category/server-software/server-applications/)\n - [Virtualization](https://www.servethehome.com/category/server-software/virtualization-other/)\n- [Guides](https://www.servethehome.com/buyers-guides/)\n - [Buyer’s Guides](https://www.servethehome.com/category/client-tips-and-tricks/buyers-guides/)\n - [Tips](https://www.servethehome.com/category/client-tips-and-tricks/)\n - [Top Hardware Components for TrueNAS / FreeNAS NAS Servers](https://www.servethehome.com/buyers-guides/top-hardware-components-for-truenas-freenas-nas-servers/)\n - [Top Hardware Components for pfSense Appliances](https://www.servethehome.com/buyers-guides/top-hardware-components-pfsense-appliances/)\n - [Top Hardware Components for napp-it and Solarish NAS Servers](https://www.servethehome.com/buyers-guides/top-hardware-components-napp-omnios-nas-servers/)\n - [Top Picks for Windows Server 2016 Essentials Hardware](https://www.servethehome.com/buyers-guides/top-picks-windows-server-2016-essentials-hardware/)\n - [The DIY WordPress Hosting Server Hardware Guide](https://www.servethehome.com/buyers-guides/the-diy-wordpress-hosting-server-hardware-guide/)\n\n[Home](https://www.servethehome.com/)[AMD Instinct MI100 32GB CDNA GPU Launched](https://www.servethehome.com/amd-radeon-instinct-mi100-32gb-cdna-gpu-launched/)AMD Radeon Instinct MI100 To MI50 Speedup\n\n# AMD Radeon Instinct MI100 To MI50 Speedup\n\n[![AMD Radeon Instinct CDNA Domain Specific Architecture](https://www.servethehome.com/wp-content/uploads/2020/11/AMD-Radeon-Instinct-MI100-to-MI50-Speedup.jpg)](https://www.servethehome.com/wp-content/uploads/2020/11/AMD-Radeon-Instinct-MI100-to-MI50-Speedup.jpg)\n\nAMD Radeon Instinct MI100 To MI50 Speedup\n\n[![AMD Radeon Instinct MI100 Enhanced Compute Unit With SIMD View](https://www.servethehome.com/wp-content/uploads/2020/11/AMD-Radeon-Instinct-MI100-Enhanced-Compute-Unit-with-SIMD-View-150x150.jpg)](https://www.servethehome.com/amd-radeon-instinct-mi100-32gb-cdna-gpu-launched/amd-radeon-instinct-mi100-enhanced-compute-unit-with-simd-view/)\n\n[![AMD Radeon Instinct MI100 Block Diagram](https://www.servethehome.com/wp-content/uploads/2020/11/AMD-Radeon-Instinct-MI100-Block-Diagram-150x150.jpg)](https://www.servethehome.com/amd-radeon-instinct-mi100-32gb-cdna-gpu-launched/amd-radeon-instinct-mi100-block-diagram/)\n\n\\- Advertisment -\n\n#### Most Read\n\n### [How to See if ECC is Working in Windows Quickly](https://www.servethehome.com/how-to-see-if-ecc-is-working-in-windows-quickly/)\n\nMay 14, 2025\n\n### [AMD EPYC 4005 Grado is Great and Intel is Exposed](https://www.servethehome.com/amd-epyc-4005-grado-is-great-and-intel-is-exposed/)\n\nMay 13, 2025\n\n### [ASRock Industrial 4X4 BOX-AI350 AMD Ryzen AI 350 Review](https://www.servethehome.com/asrock-industrial-4x4-box-ai350-amd-ryzen-ai-350-review/)\n\nMay 12, 2025\n\n### [This is the NVIDIA MGX 44RU Manifold for Liquid Cooling](https://www.servethehome.com/this-is-the-nvidia-mgx-44ru-manifold-for-liquid-cooling/)\n\nMay 11, 2025\n\n© 2009-2025 [Axautik Group LLC](https://www.axautikgroup.com/)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.servethehome.com%2Famd-radeon-instinct-mi100-32gb-cdna-gpu-launched%2Famd-radeon-instinct-mi100-to-mi50-speedup%2F","id":"https://www.servethehome.com/amd-radeon-instinct-mi100-32gb-cdna-gpu-launched/amd-radeon-instinct-mi100-to-mi50-speedup/","publishedDate":"2025-08-28T00:00:00.000Z","summary":"This webpage is a directory for ServeTheHome.com, a website focused on server hardware and related topics. It provides links to various sections, including AI, server parts, 5G edge, storage, networking, workstation components, software, and guides. It also includes links to the site's social media, contact information, and other resources.\n","title":"AMD Radeon Instinct MI100 To MI50 Speedup - ServeTheHome","url":"https://www.servethehome.com/amd-radeon-instinct-mi100-32gb-cdna-gpu-launched/amd-radeon-instinct-mi100-to-mi50-speedup/"},{"content":"Wednesday, August 27th 2025\n\n# [AMD's Next-Gen UDNA: Four Die Sizes, One Potential 96-CU Flagship](https://www.techpowerup.com/340372/amds-next-gen-udna-four-die-sizes-one-potential-96-cu-flagship)\n\nby[AleksandarK](https://www.techpowerup.com/forums/members/187454/)Aug 27th, 2025 04:40\n[Discuss (81 Comments)](https://www.techpowerup.com/340372/amds-next-gen-udna-four-die-sizes-one-potential-96-cu-flagship#comments)\n\nNew block diagrams from a well-known hardware leaker @Kepler\\_L2 sketch out a very modular AMD UDNA family that could include four die sizes, with the largest rumored to reach 96 compute units. The images show repeating shader arrays and shader engines, each engine holding a handful of compute units and its own render backend. Those engines feed into a central SoC block that contains the graphics command processor, the graphics engine, hardware schedulers, and a shared L2 cache. On the memory side, the flagship diagram shows sixteen unified memory controllers, each with 32 bits, which would add up to a 512-bit external interface. The leak also mentions the possibility of a much larger on-die Infinity Cache for the top part.\nIf you map the parts together, the flagship's numbers line up with the diagram. Eight shader arrays with two shader engines each give 16 shader engines, and with six compute units per engine, that reaches the 96 CU figure. The mid-tier design pares things back: four shader arrays and eight shader engines, with five compute units per engine, which is about 40 compute units and an estimated six memory controllers for a 192-bit bus. Below that are 24 CU and 12 CU designs built from smaller arrays and fewer controllers. The 24 CU design is shown with as many as eight memory controllers, which could mean different controller widths depending on whether the part uses a traditional GDDR style interface or an LPDDR5X-like arrangement.There are other hints in the diagrams that go beyond just compute counts and bus widths. One interesting detail is the suggestion that some data-center-focused parts could get much larger per-CU local caches. The documents also imply some convergence between Radeon and Instinct features under the UDNA label, which could let AMD reuse the same building blocks for both gaming and AI or accelerator roles. That kind of modular SoC layout would make it easier to mix and match engine counts, cache sizes, and memory controller counts across a single family of parts. All of this remains unofficial and speculative. The diagrams offer a plausible way for AMD to scale a single architecture up and down, but they do not prove that silicon has been taped out or that final specifications are fixed. AMD could still change CU counts, memory widths, cache sizes, or SKU names before production begins.\n\nSources:\n[Kepler\\_L2](https://forums.anandtech.com/threads/rdna-5-udna-cdna-next-speculation.2624468/page-40#post-41496872), [via VideoCardz](https://videocardz.com/newz/amd-rdna5-rumors-point-to-at0-flagship-gpu-with-512-bit-memory-bus-96-compute-units)\n\n### Related News\n\n- Tags:\n- [AI](https://www.techpowerup.com/news-tags/AI)\n- [AMD](https://www.techpowerup.com/news-tags/AMD)\n- [Die Size](https://www.techpowerup.com/news-tags/Die%20Size)\n- [flagship](https://www.techpowerup.com/news-tags/flagship)\n- [Infinity Cache](https://www.techpowerup.com/news-tags/Infinity%20Cache)\n- [Instinct](https://www.techpowerup.com/news-tags/Instinct)\n- [Radeon](https://www.techpowerup.com/news-tags/Radeon)\n- [SoC](https://www.techpowerup.com/news-tags/SoC)\n- [specification](https://www.techpowerup.com/news-tags/specification)\n- [UDNA](https://www.techpowerup.com/news-tags/UDNA)\n- [unofficial](https://www.techpowerup.com/news-tags/unofficial)\n\n- Jan 30th 2025 [AMD Radeon 9070 XT Rumored to Outpace RTX 5070 Ti by Almost 15%](https://www.techpowerup.com/331832/amd-radeon-9070-xt-rumored-to-outpace-rtx-5070-ti-by-almost-15) (305)\n- Jan 12th 2025 [AMD Radeon RX 9070 XT Tested in Cyberpunk 2077 and Black Myth: Wukong](https://www.techpowerup.com/331015/amd-radeon-rx-9070-xt-tested-in-cyberpunk-2077-and-black-myth-wukong) (177)\n- Jan 6th 2025 [AMD Debuts Radeon RX 9070 XT and RX 9070 Powered by RDNA 4, and FSR 4](https://www.techpowerup.com/330549/amd-debuts-radeon-rx-9070-xt-and-rx-9070-powered-by-rdna-4-and-fsr-4) (360)\n- Dec 24th 2024 [AMD Radeon RX 9070 XT Alleged Benchmark Leaks, Underwhelming Performance](https://www.techpowerup.com/330189/amd-radeon-rx-9070-xt-alleged-benchmark-leaks-underwhelming-performance) (205)\n- Feb 24th 2025 [AMD Radeon RX 9070 and 9070 XT Official Performance Metrics Leaked, +42% 4K Performance Over Radeon RX 7900 GRE](https://www.techpowerup.com/333025/amd-radeon-rx-9070-and-9070-xt-official-performance-metrics-leaked-42-4k-performance-over-radeon-rx-7900-gre) (195)\n- Jul 21st 2025 [AMD's Upcoming UDNA / RDNA 5 GPU Could Feature 96 CUs and 384-bit Memory Bus](https://www.techpowerup.com/339101/amds-upcoming-udna-rdna-5-gpu-could-feature-96-cus-and-384-bit-memory-bus) (199)\n- Feb 15th 2025 [AMD Radeon RX 9070 and 9070 XT Listed On Amazon - One Buyer Snags a Unit](https://www.techpowerup.com/332603/amd-radeon-rx-9070-and-9070-xt-listed-on-amazon-one-buyer-snags-a-unit) (261)\n- May 21st 2025 [AMD Announces Radeon RX 9060 XT Graphics Card, Claims \"Fastest Under $350\"](https://www.techpowerup.com/337066/amd-announces-radeon-rx-9060-xt-graphics-card-claims-fastest-under-usd-350) (141)\n- Feb 11th 2025 [AMD Plans Aggressive Price Competition with Radeon RX 9000 Series](https://www.techpowerup.com/332373/amd-plans-aggressive-price-competition-with-radeon-rx-9000-series) (277)\n- Jan 11th 2025 [AMD Radeon RX 9070 XT Leaked Listing Reveals Pricing Details](https://www.techpowerup.com/330995/amd-radeon-rx-9070-xt-leaked-listing-reveals-pricing-details) (109)\n\n[Add your own comment](https://www.techpowerup.com/forums/threads/340372/reply)\n\n## **81 Comments** on [AMD's Next-Gen UDNA: Four Die Sizes, One Potential 96-CU Flagship](https://www.techpowerup.com/forums/threads/340372/)\n\n[#1](https://www.techpowerup.com/forums/posts/5577626)\n\nDO IT AMD, NO BALLS, DO IT\nRELEASE THE BIG NAVI, MY WALLET IS- actually nvm, my wallet is not ready\n\n[#2](https://www.techpowerup.com/forums/posts/5577627)\n\nIf yeston makes gpu with this die, I will buy it :D\n\n[#3](https://www.techpowerup.com/forums/posts/5577633)\n\nLol, eight separate memory controllers...\n\n[#4](https://www.techpowerup.com/forums/posts/5577634)\n\nThanks so much for the article ❤️ I can't wait to learn more, it seems very promising that it's so scalable :)\n\n[#5](https://www.techpowerup.com/forums/posts/5577637)\n\nHaha no, that's a 256-bit bus. Even on the N4 96CU you can't have more than a 256-bit bus wired around the chip, it's just impossible.\n\n[#6](https://www.techpowerup.com/forums/posts/5577644)\n\n> dgianstefaniLol, eight separate memory controllers...\n\nGPU memory controllers are always 32 bits per controller.\n\n[#7](https://www.techpowerup.com/forums/posts/5577645)\n\n> N/AHaha no, that's a 256-bit bus. Even on the N4 96CU you can't have more than a 256-bit bus wired around the chip, it's just impossible.\n\nThe 7900XTX already uses a 384-bit bus with 96 CUs. Since there's talk of unifying RDNA with CDNA, I wouldn't be surprised to see a design targeting 512-bit with GDDR7. Ultimately, that's the only reason they're unifying the architectures, otherwise they would have stuck with RDNA5.\nP.S. Obviously, this is just speculation;\n\n[#8](https://www.techpowerup.com/forums/posts/5577651)\n\nWell, any Nvidia business practice that works, is adopted by AMD also.\nThe second die is less than half than the big die. So, I guess $1200 for the top model, $1000 for the cut down second, $900 for a GRE version latter. The $600 MSRP successor of the RX 9070XT will be probably offering better features and equivalent performance as that of the RX 9070XT. And I might be optimistic considering we are going from 64 CUs down to 40CUs. Hope those new CUs will be a miracle of some kind.\nBut again. This is a business practice - the second die much smaller than the first die - that is already and repeatedly approved by over 80% of consumers, so......\n\n> N/AHaha no, that's a 256-bit bus. Even on the N4 96CU you can't have more than a 256-bit bus wired around the chip, it's just impossible.\n\nThere where old GPUs with 512bit data bus. If they could do it 12+ years ago, why not today?\n[NVIDIA GeForce GTX 280 Specs \\| TechPowerUp GPU Database](https://www.techpowerup.com/gpu-specs/geforce-gtx-280.c216) [AMD Radeon R9 290X Specs \\| TechPowerUp GPU Database](https://www.techpowerup.com/gpu-specs/radeon-r9-290x.c2460)\nCosts? Obviously, that's why AMD came up with Infinity Cache while Nvidia rushes to use the latest and greatest VRAM tech. Still, not impossible.\n\n[#9](https://www.techpowerup.com/forums/posts/5577659)\n\n> N/AHaha no, that's a 256-bit bus. Even on the N4 96CU you can't have more than a 256-bit bus wired around the chip, it's just impossible.\n\nAMD Radeon R9 Fury X GPU had a 4096-bit memory interface (HBM) ten years ago...\n\n[#10](https://www.techpowerup.com/forums/posts/5577667)\n\n> eyenixUltimately, that's the only reason they're unifying the architectures, otherwise they would have stuck with RDNA5.\n\nYou can’t be stuck with something which doesn’t even exist, stop using the nonsensical name “RDNA5”, AMD said from the get go that the next arch is something new called UDNA.\nWhile 96 CU sounds strong, unless the IPC is much better than RDNA4 it’s not even as fast as a 5090. But the goal should be to compete with even more than that, 5090 won’t be same generation. So I hope this is just some early drafts and nothing final. They need 128 CUs.\n\n[#11](https://www.techpowerup.com/forums/posts/5577672)\n\nThis is gonna smack the 4090. Well played amd.\n\n[#12](https://www.techpowerup.com/forums/posts/5577673)\n\n> dgianstefaniLol, eight separate memory controllers...\n\nLol, this now how you read block diagrams.\nMemory controllers have a specific bus width, for modern ones its 3","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.techpowerup.com%2F340372%2Famds-next-gen-udna-four-die-sizes-one-potential-96-cu-flagship","id":"https://www.techpowerup.com/340372/amds-next-gen-udna-four-die-sizes-one-potential-96-cu-flagship","publishedDate":"2025-08-27T00:00:00.000Z","summary":"This article discusses leaked information about AMD's next-generation UDNA architecture, potentially featuring four die sizes. The flagship design is rumored to have 96 compute units and a 512-bit memory interface. The architecture appears modular, allowing for scaling with different engine counts, cache sizes, and memory controller configurations. The information is unofficial and speculative, with potential for changes before production.\n","title":"AMD's Next-Gen UDNA: Four Die Sizes, One Potential 96-CU Flagship","url":"https://www.techpowerup.com/340372/amds-next-gen-udna-four-die-sizes-one-potential-96-cu-flagship"}],"status":"completed","toolCallId":"call_C68EjBQf2cCOGU3txH0Qq2cD","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Here are good, vendor-sourced diagrams you can open that show the hierarchy layers for NVIDIA and AMD GPUs:\n\nNVIDIA (A100/Ampere)\n- GA100 full-chip block diagram (shows GPC → TPC → SM): NVIDIA A100 whitepaper, Figure 6\n  https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\n- GA10x (GA102) hierarchy overview (GPC, TPC, SM): GA102 whitepaper, “GPC, TPC, and SM High-Level Architecture” (Figure 2, Figure 3)\n  https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf\n- Optional (newer reference for hierarchy visuals): Blackwell GB202 GPC/TPC/SM diagrams (Figures 3–5)\n  https://images.nvidia.com/aem-dam/Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf\n\nAMD (CDNA/Instinct)\n- MI100 (CDNA1) block diagram (shows Shader Engines → CUs, L2, memory ctrls): AMD CDNA whitepaper, Figure 2\n  https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf\n- CDNA4 (MI350 series) XCD (compute die) conceptual block + package layout (SE/arrays/CUs at chiplet level): AMD CDNA 4 whitepaper, Figures 1–2\n  https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-4-architecture-whitepaper.pdf\n\nWhat to look for in each:\n- NVIDIA: GPU → GPC → TPC → SM → warp schedulers/cores; plus L2 slices, memory partitions/HBM, NVLink.\n- AMD CDNA: GPU/package → (for monolithic dies) Shader Engines → WGPs/CUs; for chiplet parts: package → XCDs (compute chiplets) → CU arrays; plus L2 slices, HBM stacks, Infinity Fabric/IODs.\n\nIf you want, tell me which exact models you’re targeting (e.g., A100 vs H100; MI250 vs MI300X), and I’ll extract the specific figure numbers/pages and annotate the hierarchy from those diagrams.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d41bbaa7e8819c9401e5d1871cb94801d9a0c64aead320"}},"resumableStreamId":"STREAM:1700e6f5-5a46-4003-b987-8558b4b49e6e","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":1.326,"tokens":538,"tokensPerSecond":18.461327293940016,"updated_at":1758731179972,"userId":"google:104458561188320708185","id":"j97emmsda414tq1rp69pd8pe0s7r6w7z"},{"_creationTime":1758731821520.6682,"_id":"j971zw19xkqcfzfqsdas5ybzth7r6zbz","attachmentIds":[],"content":"find a diagram of AMD's CU","created_at":1758731821520,"messageId":"45a61dd8-2ecf-49c1-9438-327fcfa9da20","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","updated_at":1758731821520,"userId":"google:104458561188320708185","id":"j971zw19xkqcfzfqsdas5ybzth7r6zbz","providerMetadata":{}},{"_creationTime":1758731821520.6685,"_id":"j977ss2x5ere0eakhywbp17ws57r709t","attachmentIds":[],"byok":false,"content":"Here are reliable, vendor (AMD) sources that include a Compute Unit (CU) block diagram. Open the PDFs and jump to the noted figures/pages:\n\n- AMD CDNA 3 whitepaper (Instinct MI300 series)\n  - Figure: “Conceptual block diagram of an enhanced compute unit (CU)”\n  - Link: https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf\n  - Tip: Search within the PDF for “Compute Unit” or “Figure 4” to find the CU diagram showing scheduler, scalar unit, vector/matrix cores, LDS, L1, etc.\n\n- AMD CDNA (MI100) whitepaper\n  - Section: “AMD CDNA Architecture Compute Units”\n  - Link: https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf\n  - Tip: Look for the CU-focused page in that section; it illustrates CU internals and memory (LDS) relationships.\n\n- AMD RDNA architecture whitepaper (gaming, but CU/WGP internals are diagrammed)\n  - Link (TechPowerUp mirror): https://www.techpowerup.com/gpu-specs/docs/amd-rdna-whitepaper.pdf\n  - What to find: “Dual Compute Unit Front End,” “SIMD Execution Units,” and per‑CU memories diagrams (shows scalar unit, SIMD arrays, LDS, caches).\n\n- AMD GPUOpen RDNA slides (clear per‑CU/WGP diagrams)\n  - Link: https://gpuopen.com/download/RDNA_Architecture_public.pdf\n  - Look at slides titled “GCN Compute Units” and “RDNA Workgroup Processors (WGP)” for CU/WGP block views; good for understanding CU vs WGP layout.\n\n- Older GCN CU anatomy (still useful to visualize a CU)\n  - Slides: “The AMD GCN Architecture – A Crash Course” (Layla Mah)\n  - Link: https://www.slideshare.net/DevCentralAMD/gs4106-the-amd-gcn-architecture-a-crash-course-by-layla-mah\n  - Look for the slide titled “GCN Compute Unit” showing scalar unit, 4× SIMD16, LDS, texture/filter/LS units.\n\nIf you want, tell me which family (CDNA MI2xx/MI3xx vs RDNA) you’re targeting and I’ll extract the exact page numbers and paste a cropped CU diagram.","created_at":1758731821521,"messageId":"9b584cc7-e7d5-4c78-b6f1-019536ee2e12","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"pdf","domains":null,"queries":["site:amd.com CU diagram compute unit AMD whitepaper","AMD GCN CU block diagram SIMD32 LDS register file","AMD CDNA compute unit diagram","AMD RDNA compute unit CU diagram whitepaper PDF"]},"result":[{"content":"INTRODUCING\nAMD CDNA™ 4 \nARCHITECTURE \nBreakthrough AI and HPC Acceleration with Enhanced AI \nCapabilities, Advanced Precisions, and High Efficiency\nWHITE PAPER | INTRODUCING AMD CDNA™ 4 ARCHITECTURE 2\nINTRODUCTION\nThe data center landscape has been fundamentally transformed by GPUs and accelerated computing. Accelerated \ncomputing was initially embraced in the scientific community to complement existing general-purpose CPUs for \nparticular workloads such as seismic analysis and molecular dynamics. With that foothold established, accelerators \nevolved to support increasingly more general programming languages such as C++ and Python and more diverse \napplications. Machine learning – especially in computer vision – was early to take advantage of new accelerators and \ngave rise to the field of deep learning that explicitly depends on accelerators such as GPUs to train and deploy neural \nnetworks. Between classic scientific computing and emerging machine learning and AI workloads, GPUs were in\u0002creasingly designed to operate at scale – powering the world’s largest supercomputers. The discovery of transformer\u0002based neural networks expanded the horizons for GPUs and ultimately led to the explosion of generative AI, which \nis one of the most demanding application areas to date and reaches well outside of scientific computing to both \nconsumer and business usage.\nThe demand for computational capabilities and efficiency is so great that architects can leave no stone unturned \nand are constantly revisiting basic assumptions in computing. Initially, accelerators adopted a new memory model \nbut retained familiar datatypes. Now, floating-point numerical representation is evolving alongside new algorithms \nto enable reduced precision thereby improving performance, memory footprint, and energy efficiency. At the same \ntime, GPUs have pushed the envelope in terms of integration – first adopting advanced packaging to tightly couple \nHBM to the accelerator and now using even more advanced packaging techniques to shift away from monolithic \nsilicon implementations.\nAMD has been at the forefront of this revolution, developing novel software and system architectures for GPUs to \nenable ever greater performance and capabilities. From the software side, the AMD CDNA™ 2 architecture unified \nCPU and GPU computing through cache coherency to unlock new applications and opportunities for the software \necosystem. The AMD CDNA 3 architecture revolutionized how accelerators are built, adopting advanced packaging to \nenable heterogeneous integration and repartitioning a processor across a dozen different chiplets. \nAs Figure 1 below shows, the AMD CDNA 4 architecture builds on the previous foundation of 3D packaging technolo\u0002gies and rebalances the elements of the processor across the heterogenous package. Each AMD Instinct™ MI350 \nSeries GPU integrates 8 vertically stacked accelerator complex dies (XCD) and 2 I/O Dies (IODs) containing system \ninfrastructure, tied together with the AMD on-package AMD Infinity Fabric™ technology and connecting to 8 stacks \nof 12-Hi high-bandwidth memory (HBM3E). The XCDs combine the latest process technology and support for new \nreduced precision datatypes to boost the throughput and efficiency for AI, while the repartitioned IOD helps improve \nlatency and efficiency for communication and memory traffic. Externally, the AMD Instinct MI350 Series family of \nproducts employ Infinity Fabric to directly connect 8 GPUs in a single node. \nWHITE PAPER | INTRODUCING AMD CDNA™ 4 ARCHITECTURE 3\nFigure 1. Advanced 3D Package and chiplet-based construction of the AMD Instinct MI350 Series GPU\nThis design provides the versatility to rapidly develop and deploy a family of AMD CDNA 4 architecture based \nproducts which address diverse customer needs – both ease-of-use and maximum performance. The AMD \nInstinct MI350 Series includes the air-cooled (AC) Instinct MI350X GPU that is 1000W offered in an OCP UBB8 \nbase board which was designed to be drop-in compatible with the previous generation AMD Instinct™ MI325X \nPlatforms and systems for the fast deployment into the existing ecosystem and quick time to value. For \nenhanced performance and efficiency with unparalleled density, the AMD Instinct MI355X GPU (1400W) offers \ndirect liquid-cooled (DLC) platforms, again offered in a on a UBB8 baseboard, for infrastructure that supports \ngreater power and cooling. The AMD Instinct MI355X GPU almost doubles the peak throughput for existing \nmachine learning focused 16-bit and 8-bit Matrix datatypes compared to the prior generation AMD Instinct \nMI325X GPU and introduces support for additional reduced precision numerical formats that offer 3.85x the \nperformance comparing 10TFLOP/s peak theoretical for FP6/FP4 to the 2.61 PFLOPS of FP8 peak theoretical \nperformance the MI325X.MI350-005 The MI355X also increases memory capacity generationally to 288GB HBM3E \nwith 8TB/s of bandwidth and over 1TB/s of communication bandwidth. The CDNA 4 architecture is designed \nto work with the open-source powered ROCm™ software ecosystem and provide excellent out-of-the-box \ncapabilities for scientific computing as well as enterprise orchestration through Kubernetes® and support for \nleading AI training and inference stacks and Day 0 support for popular generative AI models.\nAMD Infinity Cache™\nHBM3 Memory Stacks\nIODS\nIODs\nXCDs\nWHITE PAPER | INTRODUCING AMD CDNA™ 4 ARCHITECTURE 4\nThe AMD CDNA™ 3 architecture was a paradigm shift - adopting a modern, chiplet-based approach that fully \nleveraged heterogeneous integration and tied together myriad specialized chiplets with the Infinity Fabric \ninto a highly optimized computing platform. Compared to the monolithic approach that dominated the prior \nseveral decades, this was a fundamental rethinking of silicon design and architecture – one that will likely \nunlock performance and scalability for multiple generations.\nThe CDNA 4 architecture inherits this revolutionary foundation and demonstrates the advantages of this \nflexible strategy by carefully optimizing every component to deliver optimal performance, efficiency, and \nmanufacturability. The eight compute chiplets or XCDs benefit from the latest process technology and are \nimplemented in TSMC’s cutting-edge N3P process technology with a similar die size and footprint to the \nprior generation to achieve optimal performance and efficiency. The memory and communication functions \nin the IODs are dominated by the large AMD Infinity Cache™ and interconnects that do not effectively scale \nto fully take advantage of performance offered by the latest process technologies. Taking advantage of the \nindependent scaling in a heterogeneous architecture, these functions are kept on TSMC’s highly efficient \nand affordable N6 process but rebalanced across two massive IODs, rather than four, which optimizes the \nperformance and energy efficiency while maintaining an advantageous manufacturability profile. \nCHIPLET ARCHITECTURE\nWHITE PAPER | INTRODUCING AMD CDNA™ 4 ARCHITECTURE 5\nThe AMD CDNA™ 4 architecture highlights one of the major advantages of a chiplet-based, heterogeneous \napproach to building a compute platform – each chiplet can use the right process technology, which enables a \nmore efficient evolution over time. The accelerator complex dies (XCDs) house the computational portion of \nthe processor and the lowest levels of the cache hierarchy that are the most performance sensitive. The AMD \nCDNA 4 XCDs are built on the latest process technology, TSMC’s N3P, to take advantage of the improved logic \ndensity and performance compared to the N5 process used in the prior generation. \nAMD CDNA™ 4 COMPUTE\nThe AMD CDNA 4 architecture rebalances the XCDs as shown in Figure 2 to boost performance, especially \nfor the most demanding AI workloads, by increasing the capabilities of each Compute Unit (CU) – enabling \nhardware support for new datatypes and achieving dramatically higher computational throughput and \nefficiency for both vector and matrix workloads as seen in Table 1 below.\nIn the AMD CDNA 4 XCD, the number of CUs are slightly reduced compared to the prior generation, but \neach CU is more powerful through a combination of new features. Consequently, the global resources such \nas the scheduler, hardware queues and Asynchronous Compute Engines (ACE) that send compute shader \nworkgroups to the Compute Units (CUs) only need minor enhancements.\nEach XCD comprises 36 AMD CDNA 4 Compute Units organized as four arrays of 9 CUs, of which 32 are active, \nleaving four potentially disabled to enable high yield and efficiency operating frequencies. As with the prior \ngeneration, the L2 cache coalesces all the traffic within the XCD before fanning out to the Infinity Fabric that \nconnects to the rest of the system. The processor spans 8 XCDs, for up to 256 CUs – slightly less than the \nprior generation, but in some cases doubling the computational throughput compared to the AMD Instinct \nMI300 Series GPUs.MI350-005\nFigure 2. Conceptual block diagram of an AMD CDNA™ 4 architecture accelerator complex die (XCD)\nGLOBAL\nRESOURCES\n4MB L2 CACHE\nHWS \nHARDWARE\nSCHEDULER\nACE0\nCOMPUTE \nACCELERATOR\nUNIFIED COMPUTE SYSTEM\nHQDO-7 HQDO-7 HQDO-7 HQDO-7\nCU0 32KB L1 32KB L1 32KB L1\nACE1\nCOMPUTE \nACCELERATOR\nACE2\nCOMPUTE \nACCELERATOR\nACE3\nCOMPUTE \nACCELERATOR\nCU1 CU35\nWHITE PAPER | INTRODUCING AMD CDNA™ 4 ARCHITECTURE 6\nSince most workloads will span many CUs, the 64KB, 8-way set-associative instruction cache is shared \nbetween two adjacent CUs, making efficient use of the caches and area. \nThe greatest generational improvements in capabilities in the AMD CDNA 4 CUs are in the Matrix Cores and \nfocus on AI and machine learning, adding both hardware support for new standardized numerical formats \nand increasing the raw computational resources for existing data types. Lower precision numerical formats \nare one of the most efficient and powerful techniques to boost performance for AI. The smalle","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.amd.com%2Fcontent%2Fdam%2Famd%2Fen%2Fdocuments%2Finstinct-tech-docs%2Fwhite-papers%2Famd-cdna-4-architecture-whitepaper.pdf","id":"https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-4-architecture-whitepaper.pdf","publishedDate":"2025-06-12T00:00:00.000Z","summary":"This white paper introduces the AMD CDNA 4 architecture, designed for breakthrough AI and HPC acceleration. It builds on previous architectures with advanced 3D packaging and rebalances processor elements. The AMD Instinct MI350 Series GPU, based on CDNA 4, integrates 8 accelerator complex dies (XCD) and 2 I/O Dies, connected by AMD Infinity Fabric and high-bandwidth memory (HBM3E). This design supports diverse customer needs, including the air-cooled Instinct MI350X GPU.\n","title":"AMD CDNA™ 4 ARCHITECTURE","url":"https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-4-architecture-whitepaper.pdf"},{"content":"The All-New AMD GPU Architecture\nfor the Modern Era of HPC and Al\n\nAMDd1 AMD¢1 AMD¢l\nROCm INSTINCT together we advance_\n\nWHITE PAPER | INTRODUCING AMD CDNA™ 3 ARCHITECTURE 2\nOver the past three decades, GPUs have revolutionized the entire computing ecosystem, from smartphones \nand PCs to data centers. Some of the most exciting applications today, enabled by machine learning, would \nnot be possible without GPU acceleration. Over this time, the humble graphics card has transformed from a \nfixed-function offload engine to a general-purpose processor that accelerates programming languages such \nas Python, C++, and Fortran - and its evolution continues.\nAMD has pioneered the evolution of system architecture over the last decade to unify CPU and GPU computing \nat an unparalleled scale. AMD InstinctTM MI250X, at the heart of the first Exascale system, was enabled by \nthe AMD CDNA™ 2 architecture and advanced packaging, as well as AMD Infinity Fabric™, connecting the \nInstinct GPUs and AMD EPYC™ 7453s CPU with cache coherence.\nThe AMD Instinct MI200 accelerator family took initial steps towards advanced packaging, with two identical \ndies each incorporating the three essential elements of compute, memory, and communication. AMD \nCDNA 2 incorporated comprehensive improvements, particularly focusing on improving the communication \ninterfaces to scale to the largest systems.\nThe AMD CDNA™ 3 architecture continues to lead this evolution, as the AMD accelerated computing strategy \nembraces advanced packaging to enable heterogeneous integration. This strategy delivers outstanding \nperformance, changing the computing paradigm with a coherent programming model that tightly couples \nCPUs and GPUs together to tackle the most demanding problems of our era. The AMD CDNA™ 3 architecture \noffers significant advancements and delivers the highest performance, efficiency, and programmability to \ndate in the latest AMD Instinct MI300 Series products. \nAs Figure 1 shows, the architecture leverages the latest advancements in 3D packaging technologies and \nfundamentally repartitions the compute, memory, and communication elements of the processor across a \nheterogenous package. The Instinct MI300 Series products integrates up to 8 vertically stacked accelerator \ncomplex dies (XCD) and 4 I/O dies (IOD) containing system infrastructure, all tied together with the AMD \nInfinity Fabric™ technology and connecting to 8 stacks of high-bandwidth memory (HBM). At the lowest \nlevel, the computational throughput for vector and matrix data within the GPU cores is enhanced and \naugmented with native support for sparse data. At the macro level, this radical rethinking of the physical \nimplementation is paired with an entirely redesigned cache and memory hierarchy that scales gracefully with \nthe increased compute, and also integrates cache coherency as a first class citizen. \nINTRODUCTION\nWHITE PAPER | INTRODUCING AMD CDNA™ 3 ARCHITECTURE 3\nFigure 1. Advanced 3D Package and chiplet-based construction of the AMD Instinct™ MI300 Series processors\nThis design provides the versatility to construct AMD CDNA 3 variants, such as the Instinct MI300X and MI325X \ndiscrete GPUs or the Instinct MI300A APU, shown below in Figure 2. The Instinct MI300X discrete GPU focuses \nprimarily on accelerator compute and incorporates 8 accelerator complex dies (XCD). For reduced precision data \ncommon in machine learning, the Instinct MI300X and MI325X discrete GPUs offer significant generational \nperformance gains, with 3.4-6.8X the peak throughput and with peak theoretical FP8 performance of 2.6 \nPFLOP/s.MI300-11 For classic HPC workloads that use single- and double-precision, the computational throughput \nhas increased 1.7-3.4X offering 163.4 TFLOP/S FP64 Matrix for a single processor.MI300-11, MI325X-019 \nIn contrast, the Instinct MI300A APU is complete with CPU, GPU, and memory on one package. It reduces the \naccelerator computational capacity of the Instinct MI300X GPU by 25% to make room for three “Zen 4” x86-\nbased CPU dies that are tightly coupled with 6 GPU dies. The APU shares a single pool of virtual and physical \nmemory with extraordinarily low latency. The Instinct MI300A APU is the world’s first high-performance, data \ncenter APU, bringing tremendous ease-of-use to developers by eliminating host/device data copies and helping \nsave substantial power and area at the system level by eliminating components such as DIMMs and CPU-to\u0002GPU communication links.\nAMD Infinity Cache™\nHBM3 Memory Stacks\nIODs\nCCDs\nXCDs\nWHITE PAPER | INTRODUCING AMD CDNA™ 3 ARCHITECTURE 4\nFigure 2. Block diagram of the AMD Instinct™ MI300A APU, MI300X and MI325X discrete GPUs\nThe AMD CDNA 2 architecture harnessed advanced packaging to couple homogeneous dies into a dual\u0002processor package, connecting the two accelerator dies through a single high-bandwidth and low latency \ninterconnect formed over an interposer bridge. This simple approach enabled doubling the resources by \nreplicating compute, memory, and communication and using more silicon than would be feasible with a single \ndie. In contrast, AMD CDNA 3 firmly embraces the future of chiplets. It is a single logical processor with a \ndozen chiplets, each specialized in both design and fabrication for compute or memory and communication, \nand all tied together with the AMD Infinity Fabric network on-chip.\nCHIPLET ARCHITECTURE REPARTITIONING\nMI300X and MI325X \nWHITE PAPER | INTRODUCING AMD CDNA™ 3 ARCHITECTURE 5\nThe first set of chiplets is the accelerator complex dies (XCDs) which contain the computational elements of \nthe processor along with the lowest levels of the cache hierarchy, and are fabricated on TSMC’s 5nm process. \nAMD CDNA 3 COMPUTE\nFigure 3. Conceptual block diagram of an accelerator complex die (XCD)\nAs Figure 3 illustrates above, each XCD contains a shared set of global resources such as the scheduler, \nhardware queues, and four Asynchronous Compute Engines (ACE) that send compute shader workgroups \nto the Compute Units (CUs) that are the computational heart of the AMD CDNA™ 3 architecture. The four \nACEs are each associated with 40 CUs, although at the aggregate level there are only 38 CUs active, with 2 \ndisabled for yield management. The 38 CUs all share a 4MB L2 cache that serves to coalesce all the memory \ntraffic for the die. The AMD CDNA 3 XCD die is a smaller building block than the AMD Instinct™ MI200 Series \nGPU compute die, with under half the CUs, but using more advanced packaging, the processor includes 6-8 \nXCDs for as many as 304 CUs total, roughly 40% more than the Instinct MI250X GPU.MI300-15\nGLOBAL\nRESOURCES\n4MB L2 CACHE\nHWS \nHARDWARE\nSCHEDULER\nACE0\nCOMPUTE \nACCELERATOR\nUNIFIED COMPUTE SYSTEM\nHQDO-7 HQDO-7 HQDO-7 HQDO-7\nCU0 32KB L1 32KB L1 32KB L1\nACE1\nCOMPUTE \nACCELERATOR\nACE2\nCOMPUTE \nACCELERATOR\nACE3\nCOMPUTE \nACCELERATOR\nCU1 CU39\nWHITE PAPER | INTRODUCING AMD CDNA™ 3 ARCHITECTURE 6\nFigure 4. Conceptual block diagram of an enhanced compute unit (CU) of the AMD CDNA 3 architecture\nThe instruction cache is shared between two CUs and doubles the capacity from the prior generation to \na 64KB and 8-way set-associative data array. This arrangement takes advantage of the reality that the \noverwhelming majority of the time the same instruction stream will be executed by groups of CUs, so this \nhas the net effect of increasing the cacheable window and hit rate while keeping die area nearly constant. \nThe AMD CDNA 3 CU generationally improves the source caching to provide better re-use and bandwidth \namplification so that each vector register read can support more downstream vector or matrix operations.\nThe biggest improvements in the AMD CDNA 3 CUs are in the Matrix Cores and in particular emphasizing \nAI and machine learning, by enhancing throughput for existing data types commonly used for cutting edge \ntraining and inference as well as adding entirely new ones. One of the greatest levers for performance in \nmachine learning is employing more compact data types, which can save both memory and cache capacity \nand improve throughput and reduce power consumption. A decade ago, most machine learning applications \nrelied on FP32, but over time the community learned to use smaller and smaller data types. For optimal \ntraining performance, the AMD CDNA 2 Matrix Cores supported FP16 and BF16, while offering INT8 for \ninference. As Table 1 illustrates the AMD CDNA 3 Matrix Cores triples performance for FP16 and BF16, while \nproviding a 6.8x performance gain for INT8 compared to previous Gen AMD Instinct™ MI250X accelerators.\nMI300-11, MI325X-019\nScheduler\nLocal Data Share Matrix Core Unit Shader Core L1 Cache\nAs Figure 4 shows below, the AMD CDNA™ 3 compute units are complete, highly threaded and parallel \nprocessor cores including everything from instruction fetching and scheduling, execution units for scalar, \nvector and matrix data types, and load/store pipelines with an L1 cache and Local Data Share (LDS) that \nform the start of the memory hierarchy. While the compute units are architecturally similar to those in the \nAMD CDNA™ 2 architecture, they have been comprehensively improved with major changes throughout the \ncore to exploit greater parallelism at nearly every level and in many cases doubling or even quadrupling the \nperformance per CU for vector and matrix workloads.\nAMD CDNA™ 3 COMPUTE UNIT ARCHITECTURE\nWHITE PAPER | INTRODUCING AMD CDNA™ 3 ARCHITECTURE 7\nTable 1. Generational theoretical peak compute comparison for numerical formats \n and throughput between AMD Instinct™ MI300X, GPUs and Instinct MI250X GPUs.MI300-11, MI325-019\nComputation MI300X | MI325X \n(FLOPS/clock/CU)\nMI250X \n(FLOPS/clock/CU)\nMI300X | MI325X \nGPU \n(Peak TFLOP/s\nMI250X GPU (Peak \nTFLOP/s)\nMI300X | MI325X \nPeak Speedup\nMatrix FP64 256 256 163.4 95.7 1.7x\nVector FP64 128 128 81.7 47.9 1.7x\nMatrix FP32 256 256 163.4 95.7 1.7x\nVector FP32 256 128 163.4 47.9 3.4x\nMatrix TF32 1024 N/A 490.3 N/A N/A\nMatrix FP16 2048 1024 1307.4 383 3.4x\nMatri","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.amd.com%2Fcontent%2Fdam%2Famd%2Fen%2Fdocuments%2Finstinct-tech-docs%2Fwhite-papers%2Famd-cdna-3-white-paper.pdf","id":"https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf","publishedDate":null,"summary":"This white paper introduces the AMD CDNA™ 3 architecture, which powers the AMD Instinct MI300 Series products. This architecture leverages 3D packaging technologies to integrate compute, memory, and communication elements across a heterogeneous package. The Instinct MI300 Series includes variants like the MI300X and MI325X discrete GPUs, which offer significant performance gains for machine learning and HPC workloads, and the MI300A APU, which integrates CPU, GPU, and memory on one package.\n","title":"CDNA™ 3 architecture","url":"https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf"},{"content":"AMD«e\\\n\n\"AMD Instinct MI300\" Instruction Set\n\nArchitecture\nReference Guide\n\n15-July-2024\n\nSpecification Agreement\nThis Specification Agreement (\"Agreement\") is a legal agreement between Advanced Micro Devices, Inc. (\"AMD\") and \"You\" as the recipient of\nthe attached AMD Specification (\"Specification\"). If you are accessing the Specification as part of your performance of work for another party,\nyou acknowledge that you have authority to bind such party to the terms and conditions of this Agreement. If you accessed the Specification by\nany means or otherwise use or provide Feedback (defined below) on the Specification, You agree to the terms and conditions set forth in this\nAgreement. If You do not agree to the terms and conditions set forth in this Agreement, you are not licensed to use the Specification; do not use,\naccess, or provide Feedback about the Specification. In consideration of Your use or access of the Specification (in whole or in part), the receipt\nand sufficiency of which are acknowledged, You agree as follows:\n1. You may review the Specification only (a) as a reference to assist You in planning and designing Your product, service or technology\n(\"Product\") to interface with an AMD product in compliance with the requirements as set forth in the Specification and (b) to provide Feedback\nabout the information disclosed in the Specification to AMD.\n2. Except as expressly set forth in Paragraph 1, all rights in and to the Specification are retained by AMD. This Agreement does not give You any\nrights under any AMD patents, copyrights, trademarks, or other intellectual property rights. You may not (i) duplicate any part of the\nSpecification; (ii) remove this Agreement or any notices from the Specification, or (iii) give any part of the Specification, or assign or otherwise\nprovide Your rights under this Agreement, to anyone else.\n3. You agree that You shall not use nor procure others to use the contents of this Agreement for (i) modifying any existing patent or patent\napplication or creating any continuation, continuation in part or other extension of any patent or patent application, nor (ii) analyzing, assessing\nany patent or patent application (which shall include the creation or modification of any patent claim charts or infringement analyses).\n4. The Specification may contain preliminary information, errors, or inaccuracies, or may not include certain necessary information.\nAdditionally, AMD reserves the right to discontinue or make changes to the Specification and its products at any time without notice. The\nSpecification is provided entirely \"AS IS.\" AMD MAKES NO WARRANTY OF ANY KIND AND DISCLAIMS ALL EXPRESS, IMPLIED AND\nSTATUTORY WARRANTIES, INCLUDING BUT NOT LIMITED TO IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\nPARTICULAR PURPOSE, NONINFRINGEMENT, TITLE OR THOSE WARRANTIES ARISING AS A COURSE OF DEALING OR CUSTOM OF TRADE.\nAMD SHALL NOT BE LIABLE FOR DIRECT, INDIRECT, CONSEQUENTIAL, SPECIAL, INCIDENTAL, PUNITIVE OR EXEMPLARY DAMAGES OF\nANY KIND (INCLUDING LOSS OF BUSINESS, LOSS OF INFORMATION OR DATA, LOST PROFITS, LOSS OF CAPITAL, LOSS OF GOODWILL)\nREGARDLESS OF THE FORM OF ACTION WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE) AND STRICT PRODUCT LIABILITY OR\nOTHERWISE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n5. Furthermore, AMD’s products are not designed, intended, authorized or warranted for use as components in systems intended for surgical\nimplant into the body, or in other applications intended to support or sustain life, or in any other application in which the failure of AMD’s\nproduct could create a situation where personal injury, death, or severe property or environmental damage may occur.\n6. You have no obligation to give AMD any suggestions, comments, or feedback (\"Feedback\") relating to the Specification. However, any\nFeedback You voluntarily provide may be used by AMD without restriction, fee, or obligation of confidentiality. Accordingly, if You do give AMD\nFeedback on any version of the Specification, You agree AMD may freely use, reproduce, license, distribute, and otherwise commercialize Your\nFeedback in any product, as well as has the right to sublicense third parties to do the same. Further, You will not give AMD any Feedback that\nYou may have reason to believe is (i) subject to any patent, copyright or other intellectual property claim or right of any third party; or (ii)\nsubject to license terms which seek to require any product or intellectual property incorporating or derived from Feedback or any Product or\nother AMD intellectual property to be licensed to or otherwise provided to any third party.\n7. You shall adhere to all applicable U.S., European, and other export laws, including but not limited to the U.S. Export Administration\nRegulations (\"EAR\"), (15 C.F.R. Sections 730 through 774), and E.U. Council Regulation (EC) No 428/2009 of 5 May 2009. Further, pursuant to\nSection 740.6 of the EAR, You hereby certifies that, except pursuant to a license granted by the United States Department of Commerce Bureau\nof Industry and Security or as otherwise permitted pursuant to a License Exception under the U.S. Export Administration Regulations (\"EAR\"),\nYou will not (1) export, re-export or release to a national of a country in Country Groups D:1, E:1 or E:2 any restricted technology, software, or\nsource code You receive hereunder, or (2) export to Country Groups D:1, E:1 or E:2 the direct product of such technology or software, if such\nforeign produced direct product is subject to national security controls as identified on the Commerce Control List (currently found in\nSupplement 1 to Part 774 of EAR). For the most current Country Group listings, or for additional information about the EAR or Your obligations\nunder those regulations, please refer to the U.S. Bureau of Industry and Security’s website at http://www.bis.doc.gov/. This Section 7 is\napplicable solely to Specifications shall not apply to any Specifications that are released publicly.\n8. If You are a part of the U.S. Government, then the Specification is provided with \"RESTRICTED RIGHTS\" as set forth in subparagraphs (c) (1)\nand (2) of the Commercial Computer Software-Restricted Rights clause at FAR 52.227-14 or subparagraph (c) (1)(ii) of the Rights in Technical\nData and Computer Software clause at DFARS 252.277-7013, as applicable.\n9. This Agreement is governed by the laws of the State of California without regard to its choice of law principles. Any dispute involving it must\nbe brought in a court having jurisdiction of such dispute in Santa Clara County, California, and You waive any defenses and rights allowing the\ndispute to be litigated elsewhere. If any part of this agreement is unenforceable, it will be considered modified to the extent necessary to make it\n\"AMD Instinct MI300\" Instruction Set Architecture\nii of 546\nenforceable, and the remainder shall continue in effect. The failure of AMD to enforce any rights granted hereunder or to take action against\nYou in the event of any breach hereunder shall not be deemed a waiver by AMD as to subsequent enforcement of rights or subsequent actions in\nthe event of future breaches. This Agreement is the entire agreement between You and AMD concerning the Specification; it may be changed\nonly by a written document signed by both You and an authorized representative of AMD.\nDISCLAIMER\nThe information presented in this document is for informational purposes only and may contain technical inaccuracies, omissions, and\ntypographical errors. The information contained herein is subject to change and may be rendered inaccurate for many reasons,\nincluding but not limited to product and roadmap changes, component and motherboard version changes, new model and/or product\nreleases, product differences between differing manufacturers, software changes, BIOS flashes, firmware upgrades, or the like. Any\ncomputer system has risks of security vulnerabilities that cannot be completely prevented or mitigated. AMD assumes no obligation to\nupdate or otherwise correct or revise this information. However, AMD reserves the right to revise this information and to make\nchanges from time to time to the content hereof without obligation of AMD to notify any person of such revisions or changes. THIS\nINFORMATION IS PROVIDED \"AS IS.\" AMD MAKES NO REPRESENTATIONS OR WARRANTIES WITH RESPECT TO THE CONTENTS\nHEREOF AND ASSUMES NO RESPONSIBILITY FOR ANY INACCURACIES, ERRORS, OR OMISSIONS THAT MAY APPEAR IN THIS\nINFORMATION. AMD SPECIFICALLY DISCLAIMS ANY IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY, OR\nFITNESS FOR ANY PARTICULAR PURPOSE. IN NO EVENT WILL AMD BE LIABLE TO ANY PERSON FOR ANY RELIANCE, DIRECT,\nINDIRECT, SPECIAL, OR OTHER CONSEQUENTIAL DAMAGES ARISING FROM THE USE OF ANY INFORMATION CONTAINED\nHEREIN, EVEN IF AMD IS EXPRESSLY ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\nAMD, the AMD Arrow logo and combinations thereof are trademarks of Advanced Micro Devices, Inc. Other product names used in this\npublication are for identification purposes only and may be trademarks of their respective companies.\nCopyright © 2024 Advanced Micro Devices, Inc. All rights reserved.\nAdvanced Micro Devices, Inc.\n2485 Augustine Drive\nSanta Clara, CA, 95054\nwww.amd.com\n\"AMD Instinct MI300\" Instruction Set Architecture\niii of 546\nContents\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nAbout This Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nAudience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nOrganization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.amd.com%2Fcontent%2Fdam%2Famd%2Fen%2Fdocuments%2Finstinct-tech-docs%2Finstruction-set-architectures%2Famd-instinct-mi300-cdna3-instruction-set-architecture.pdf","id":"https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf","publishedDate":"2025-08-05T00:00:00.000Z","summary":"This document is a legal agreement between AMD and the recipient of the \"AMD Instinct MI300\" Instruction Set Architecture Reference Guide. It outlines the terms of use, including permitted uses (reviewing the specification for product planning and providing feedback), restrictions (no duplication, modification, or transfer of rights), and disclaimers. AMD retains all rights to the specification and provides it \"AS IS\" with no warranties. AMD is not liable for damages and its products are not intended for critical applications like medical implants. Any feedback provided to AMD can be used without restriction.\n","title":"\"AMD Instinct MI300\" Instruction Set Architecture","url":"https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/instruction-set-architectures/amd-instinct-mi300-cdna3-instruction-set-architecture.pdf"},{"content":"INTRODUCING\nAMD CDNA™ 2 \nARCHITECTURE \nPropelling humanity’s foremost research with the \nworld’s most powerful HPC and AI accelerator.\nAMD CDNA™ 2 Architecture | 2\nTable of Contents\nTable of Contents 2\nIntroduction 2\nAMD CDNA™ 2 Architecture Overview 2\nScaling the Memory Hierarchy 5\nCommunication and Scaling 5\nAMD CDNA™ 2 Architecture Shader Array 10\nAMD CDNA™ 2 Matrix Core Technology 11\nAMD CDNA™ 2 Packed FP32 12\nAMD ROCm™ Open Software Platform Enables AMD CDNA™ 2 13\nAMD Instinct™ MI200 Series Product Offering 14\nConclusion 15\nIntroduction\nThe history of GPUs is a story of evolution - from extremely specialized and hardwired designs to fully programmable accelerators. These \naccelerators are fundamentally optimized for massive throughput, and can run standard programming languages like C++ as part of a \nrich software ecosystem. This combination has proven incredibly compelling for scientific computing and machine learning, where the \ncomputational throughput has enabled tremendous innovation and discovery in a wide variety of applications. \nPrior accelerator architectures have consistently improved performance and efficiency while become increasingly programmable. \nHowever, the AMD CDNA™ 2 architecture takes this evolutionary path to the next level, achieving over 4x1\n performance boost over the \nprior generation AMD CDNA architecture, with 47.9 TFLOP/s peak double-precision vector FP64 throughput, to enable exascale levels of \nperformance with unrivalled programmability in heterogeneous systems. \nThe AMD CDNA 2 architecture represents a major leap forward compared to the prior generation by enhancing the Matrix Core technology \nfor HPC and AI, driving computational capabilities for double-precision floating-point data and a variety of matrix-multiply primitives. It \nalso focuses on improving accelerator communication and scaling, leveraging AMD’s unique Infinity Fabric™ to enable a multi-chip module \nthat provides 1.8x the compute density2\n over the previous generation and enables better connectivity within a single system. Lastly, the \nAMD CDNA 2 architecture enables accelerators such as the AMD Instinct™ MI250X to operate as a full peer within a computing system by \noffering cache coherency with select optimized 3rd Gen EPYC processors, offering a quick and simple on-ramp for CPU codes to tap the \npower of accelerators. These new capabilities are all seamlessly unlocked by AMD’s ROCm™ open software platform, which offers a rich set \nof tools for customers porting or developing cutting edge applications. \nAMD CDNATM 2 Architecture Overview\nThe AMD CDNA™ 2 architecture builds on the tremendous core strengths of the original AMD CDNA architecture to deliver a leap forward in \nsystem performance and usability while using a similar process technology. The AMD CDNA architecture is an excellent starting point for \na computational platform. However, to deliver exascale performance the architecture was overhauled with enhancements to nearly every \naspect from the compute units to the memory interface, with a particular emphasis on radically improving the communication interfaces \nfor full system scalability. \nThe AMD CDNA 2 architecture is designed first and foremost for the most taxing scientific computing and machine learning applications. \nIt powers the new AMD Instinct™ MI200 generation of products that target solutions ranging from compact single systems all the way to \nthe world’s largest exascale supercomputers with unique and highly differentiated programming models. Figure 1a illustrates the AMD \nInstinct MI200 Graphics Compute Die (GCD). The AMD Instinct™ MI200 is built on advanced packaging technologies, enabling two GCDs to \nbe integrated into a single package in the OAM (OCP Accelerator Module) form factor in the MI250 and MI250X products, as shown in figure \n1b. The AMD Instinct MI210 product enables one GCD in a traditional PCIe® form factor card for mainstream HPC and AI workloads in the \ndata center. Each GCD is built on the AMD CDNA 2 architecture. \nAMD CDNA™ 2 Architecture | 3\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCompute Engine\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCompute Engine\nMemory\nController\nVCN Infinity Fabric Link Infinity Fabric Link Infinity Fabric Link Infinity Fabric Link VCN\nInfinity Fabric Link Infinity Fabric Link Infinity Fabric Link Infinity Fabric or PCIe\nMemory Phy Memory Phy Memory Controller\nMemory\nController Memory Phy Memory Phy Memory Controller\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCompute Engine\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCompute Engine\nInfinity Fabric\nInfinity Fabric\nL2 Cache and Controllers\nINFINITY FABRIC\nFigure 1a –Block diagram of the AMD Instinct™ MI200 Graphics Compute Die (GCD)\nAMD CDNA™ 2 Architecture | 4\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCompute Engine\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCompute Engine\nVCN VCN Infinity Fabric Link Infinity Fabric Link Infinity Fabric Link Infinity Fabric Link\nInfinity Fabric Link Infinity Fabric Link Infinity Fabric Link Infinity Fabric or PCIe\nMemory\nController\nHBM2 HBM2\nHBM2 HBM2\nMemory Phy Memory Phy Memory\nController\nMemory\nController Memory Phy Memory Phy Memory\nController\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCompute Engine\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCompute Engine\nInfinity Fabric\nInfinity Fabric\nL2 Cache and Controllers\nL2 Cache and Controllers\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCompute Engine\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCompute Engine\nVCN VCN Infinity Fabric Link Infinity Fabric Link Infinity Fabric Link Infinity Fabric Link\nInfinity Fabric Link Infinity Fabric Link Infinity Fabric Link Infinity Fabric or PCIe\nMemory\nController\nHBM2 HBM2\nHBM2 HBM2\nMemory Phy Memory Phy Memory\nController\nMemory\nController Memory Phy Memory Phy Memory\nController\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCompute Engine\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCU\nCompute Engine\nInfinity Fabric\nInfinity Fabric\nINFINITY FABRIC\nFigure 1b –Block diagram of the AMD Instinct™ MI200 multi-chip module (AMD Instinct™ MI250/MI250X) in the OAM form \nfactor accelerator which comprise two Graphics Compute Dies (GCD) as illustrated.\nThe three critical functions of any processor are compute, memory, and communication. Each AMD CDNA™ 2 GCD has multiple blocks \ndedicated to these functions and are connected with a high-speed on-die fabric. However, to deliver exascale-performance this alone is \nnot enough. One of the crucial innovations in the AMD CDNA 2 architecture is employing AMD’s unique Infinity Fabric to extend the on-die \nfabric across the package so that each GCD appears as a GPU in one shared memory system. Connecting two GCDs together in this fashion \ndoubles the resources, creating a larger computational building block on top of the many other enhancements. \nThe AMD CDNA 2 architecture has several different incarnations offering both a custom implementation that uses AMD Infinity Fabric™ to \ninterface with an optimized 3rd generation AMD EPYC™ processor for a specific HPE/Cray supercomputer platform as well as a generally \navailable implementation that relies on PCI-Express® to interface with host processors. Each GCD includes a command processor that \ngets API-level commands from the host CPU and translates them into work that can be spawned on different parts of the AMD CDNA 2 \narchitecture.\nOne of the fundamental innovations in the prior generation AMD CDNA architecture was the introduction of the Matrix Core technology \nin the compute units (CUs) to boost computational throughput with a focus on datatypes used in machine learning. The Matrix Core \ntechnology in the AMD CDNA 2 architecture builds on this foundation and has been enhanced to support a wider range of datatypes and \napplications, with a particular emphasis on scientific computing with FP64 data. Additionally, similar to the prior generation, the CU array \nis partitioned into four shader engines that execute the compute kernels that are spawned by the command processor. The net result \nis that AMD Instinct MI200 series accelerators can provide up to a theoretical peak 47.9 TFLOP/s double-precision throughput, 4.2X the \nprevious generation1\n.\nEach GCD also features 2x Video Codec Next (VCN), a logic block that provides encode and decode capabilities on incoming and outgoing \ndata streams. This is a particularly useful logic for Machine Learning Training workloads for object detection that train on image and video \ndata. The VCN blocks support H.264/AVC, HEVC, VP9 and JPEG for decode, as well as H.264/AVC and HEVC for encode3\n.\nAMD CDNA™ 2 Architecture | 5\nScaling the Memory Hierarchy \nThe AMD CDNA™ 2 architecture boosts the computational throughput compared to the prior generation architecture. The extremely high \nbandwidth register files and local data storage are tailored to support this performance. Additionally, the shared memory hierarchy outside \nof the CUs is critical to delivering the bandwidth necessary for real-world applications which work on large scale datasets that reside in \nmemory. The AMD CDNA 2 memory controllers focus on efficiently accessing large working sets of data and bringing it on-die so that the \nL2 cache can provide amplified the bandwidth to feed the CUs. The AMD CDNA 2 architecture boosts many different dimensions of the \nmemory hierarchy, simultaneously improving bandwidth5\n generationally and capacity while enhancing synchronization. Additionally, for \nselect optimized system","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.amd.com%2Fcontent%2Fdam%2Famd%2Fen%2Fdocuments%2Finstinct-business-docs%2Fwhite-papers%2Famd-cdna2-white-paper.pdf","id":"https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna2-white-paper.pdf","publishedDate":null,"summary":"The AMD CDNA 2 architecture is designed for scientific computing and machine learning applications, offering a 4x performance boost over the previous generation. It enhances Matrix Core technology, improves accelerator communication, and enables cache coherency with select 3rd Gen EPYC processors. The architecture powers the AMD Instinct MI200 series, which includes the MI250X with two GCDs in an OAM form factor and the MI210 with one GCD in a PCIe form factor.\n","title":"AMD CDNA™ 2 ARCHITECTURE","url":"https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna2-white-paper.pdf"},{"content":"INTRODUCING\nAMD CDNA ARCHITECTURE \nThe All-New AMD GPU Architecture for \nthe Modern Era of HPC & AI \n2\nTable of Contents\nTable of Contents 2\nIntroduction 2\nAMD CDNA Architecture Overview 2\nL2 Cache and Memory 3\nCommunication and Scaling 3\nAMD CDNA Architecture Compute Units 4\nAMD ROCm™ Open Software Ecosystem 6\nConclusion 9\nFigure 1 - Domain specific architectures : optimal efficiency through domain-specific optimizations\nIntroduction\nOver the last thirty years, graphics processors have evolved from specialized, fixed-function architectures into fully programmable \naccelerators. While early designs focused on limited computation such as lighting transformations, the GPUs of today are general-purpose \nand deliver dramatic performance gains for computationally rich software written in standard languages like C++ and Fortran. The industry \nhas simultaneously evolved to take advantage of GPUs for critical fields like machine learning and scientific computing – that will drive \ninnovation for decades to come.\nAt a technical level, this translates into two major trends in hardware and software. First of all, GPUs have transformed from fixed\u0002function pipelines, to flexible computing engines like the GCN architecture that offer both scalar and vector processing. Simultaneously, \nthe software ecosystem has rapidly expanded to empower standard computing paradigms by providing critical tooling that encompasses \neverything from math libraries and compilers, to debuggers, to cluster management and deployment.\nAMD builds high performance computing solutions for the broad range of use cases, from PC gamers to data center platforms. While \ntraditional GPUs have been built to be general purpose in nature, designed to drive both graphics-oriented as well as compute-oriented \nworkloads, AMD recently introduced a different approach to fuel even higher efficiencies. There are now two architectures: AMD RDNA™ is \noptimized for gaming to maximize frames per second and AMD CDNA is optimized for computing to push the limits of flops per second.\n3\nAMD CDNA Architecture Overview\nGraphic processing units (GPUs) built on the AMD CDNA architecture target compute applications such as high performance computing \n(HPC) and AI & machine learning (ML) that run on everything from individual servers to the world’s largest Exascale supercomputers. \nThe overall system architecture is designed for extreme scalability and carefully boosts performance over the previous generations while \nadvancing programmability and productivity with the extremely robust AMD ROCm™ software stack. Figure 2 below illustrates the 7nm \nAMD Instinct™ MI100 accelerator, which is the first incarnation of the AMD CDNA architecture. \nFigure 2 – Block diagram of the AMD Instinct™ MI100 accelerator, the first GPUs powered by the AMD CDNA architecture.\nThe AMD Instinct MI100 GPU is organized into several main blocks that are all tied together with an on-die fabric. A PCI-Express® 4.0 \ninterface connects the GPU to a host processor such as 2nd Gen AMD EPYC™ CPUs using a 16GT/s link that delivers up to 32GB/s in each \ndirection. The command processor receives API-level commands and generates work for the various components of the GPU. At a \nhigh-level, the three main functions of any compute processor, whether CPU or GPU, are compute, memory, and communication and each \nfunction is associated with a different set of blocks.\nThe AMD Instinct™ MI100 accelerator is the world’s fastest HPC GPU, and a culmination of the AMD CDNA architecture, with all-new Matrix \nCore Technology, and AMD ROCm™ open ecosystem to deliver new levels of performance, portability, and productivity. AMD CDNA is an \nall-new GPU architecture from AMD to drive accelerated computing into the era of exascale computing. The new architecture augments \nscalar and vector processing with new Matrix Core Engines and adds Infinity Fabric™ technology to scale up to larger systems. The open \nROCm ecosystem puts customers in control and is a robust, mature platform that is easy to develop for and capable of running the most \ncritical applications. The overall result is that the MI100 is the first GPU to break the 10TFLOP/s FP64 barrier designed as the stepping \nstone to the next generation of Exascale systems that will deliver pioneering discoveries in machine learning and scientific computing.1\nShader Engine\nCU\nCU\nCU\nCU\nShader Engine\nCU\nCU\nCU\nCU\nShader Engine\nCU\nCU\nCU\nCU\nShader Engine\nCU\nCU\nCU\nCU\nDMA\nHWS\nACE\nACE\nACE\nACE\nMemory Controller Memory Controller\nMemory Controller Memory Controller\nCU\nCU\nCU\nCU\nShader Engine\nCU\nCU\nCU\nCU\nShader Engine\nCU\nCU\nCU\nCU\nShader Engine\nCU\nCU\nCU\nCU\nShader Engine\nPCIe® Gen 4 Multimedia Engine XGMI Links\nINFINITY FABRIC\n4\nL2 Cache and Memory\nThe lowest levels of the memory hierarchy for the AMD Instinct™ MI100 accelerator live inside the CUs, but most scientific or machine \nlearning data sets are measured in gigabytes or terabytes and will quickly spill into memory. The memory hierarchy is responsible for \nholding the working data and efficiently delivering it to the compute arrays where the data is consumed in computation.\nThe L2 cache is shared across the whole chip and physically partitioned into multiple slices. For the MI100, the cache is 16-way set \nassociative and comprises 32 slices (twice as many as in MI50) in total for an aggregate capacity of 8MB. Each slice can sustain 64B/cycle \nfor an aggregate bandwidth over 3TB/s across the GPU.\nThe AMD CDNA architecture is targeted for the most demanding workloads, and the memory controllers and interfaces are designed for \nmaximum bandwidth, power efficiency, and extreme reliability. The memory controllers drive 4- or 8-high stacks of co-packaged HBM2 at \n2.4GT/s for an aggregate theoretical throughput of 1.23TB/s – 20% faster than the prior generation AMD accelerators, while keeping the \nGPU power budget constant.2 The 32GB of HBM2 memory is protected by robust hardware ECC that enables mission critical applications to \noperate at a massive scale.6\nCommunication and Scaling\nThe last leg of the system architecture is communication to scale up across multiple GPUs. While systems running small workloads may \nuse a single AMD Instinct™ MI100 accelerator, larger and more demanding problems such as training neural networks require more powerful \nsystems. As an example, the BERT neural network which is used for language modeling scales to thousands of processors in the most \nrecent MLPerf benchmarks.\nThe AMD CDNA architecture uses standards based high-speed AMD Infinity Fabric technology to connect to other GPUs. The Infinity Fabric \nlinks operate at 23GT/s and are 16-bits wide similar to the previous generation, but the MI100 brings a third link for full connectivity in quad \nGPU configurations offering greater bi-section bandwidth and enabling highly scalable systems. Unlike PCIe®, the AMD Infinity Fabric links \nsupport coherent GPU memory, which enables multiple GPUs to share an address space and tightly cooperate on a single problem.\nAs Figure 3 illustrates, the additional AMD Infinity Fabric links enable a fully connected 4-GPU building block, whereas the Radeon Instinct™ \nMI50 GPU could use only a ring topology. The fully connected topology boosts performance for common communication patterns such as \nall-reduce, and scatter/gather. These communication primitives are widely used in HPC and ML, e.g., the weight update communication \nphase of training neural networks found in DLRM.\nFigure 3 – Fully connected 4-GPU Infinity Fabric™ technology hives with the AMD Instinct™ MI100 GPUs\nThe 120 enhanced compute units (CUs) are built on the foundations of the GCN architecture and organized into four compute engines, and \nare responsible for all the compute in the AMD Instinct MI100 GPU. While inspired by the prior-generation GCN architecture, each CU is \nrearchitected and enhanced with a Matrix Core Engine that significantly boosts computational throughput for many different numerical \nformats. In aggregate across all the CUs, the MI100 can achieve a theoretical double-precision floating-point throughput of up to 11.5 \nTFLOP/s – the the first processor to break the 10TFLOP/s barrier.1 Unlike the graphics-oriented AMD RDNA™ family, the AMD CDNA family \nremoves all of the fixed-function hardware that is designed to accelerate graphics tasks such as rasterization, tessellation, graphics caches, \nblending, and even the display engine. However, the AMD CDNA family retains dedicated logic for HEVC, H.264, and VP9 decoding that is \nsometimes used for compute workloads that operate on multimedia data, such as machine learning for object detection.7 Removing the \nfixed-function graphics hardware frees up area and power to invest in additional compute units, boosting performance and efficiency.\n5\nFigure 4 –HPC and AI performance improvements with AMD Infinity Fabric technology8,9,10\nPCIe® 4.0 Infinity Fabric PCIe® 4.0 Infinity Fabric PCIe® 4.0\nImage Processing\nResNext101 FP32, 8 GPUs\nLanguage Modeling\nBERT FP32, 8 GPUs\nRecommendation Network\nDLRM FP32, 8 GPUs\nRelative performance to PCIe systemRelative performance to PCIe systemRelative performance to PCIe system\nInfinity Fabric\n27% Faster 44% Faster 42% Faster\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nAMD CDNA Architecture Compute Units\nThe command processor and scheduling logic translate higher-level API commands into compute tasks. These compute tasks in turn \nare implemented as compute arrays and managed by the Asynchronous Compute Engines (ACE). Each of the four ACEs maintains an \nindependent stream of commands and can dispatch wavefronts to the compute units.\nThe impressive 120 CUs of the AMD CDNA architecture are organized into four arrays of CUs. The CUs are derived from the earlier GCN \narchitecture and execute wavefronts that contain 64 work-items. However, the CUs are enhanced with new Matrix Core Engines that are \noptimized for operating","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.amd.com%2Fcontent%2Fdam%2Famd%2Fen%2Fdocuments%2Finstinct-business-docs%2Fwhite-papers%2Famd-cdna-white-paper.pdf","id":"https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf","publishedDate":null,"summary":"AMD has introduced the CDNA architecture, a new GPU architecture optimized for computing applications like HPC and AI/ML. CDNA is designed for extreme scalability and boosts performance with the AMD ROCm software stack. The first incarnation of the CDNA architecture is the AMD Instinct MI100 accelerator, which features Matrix Core Technology and Infinity Fabric technology.\n","title":"AMD CDNA ARCHITECTURE","url":"https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf"},{"content":"Revision 1.4\nSouthern Islands Series Instruction Set Architecture\nReference Guide\nFebruary 2014\nii\n© 2014 Advanced Micro Devices, Inc. All rights reserved. AMD, the AMD Arrow logo, \nAMD Accelerated Parallel Processing, the AMD Accelerated Parallel Processing logo, ATI, \nthe ATI logo, Radeon, FireStream, FirePro, Catalyst, and combinations thereof are trade\u0002marks of Advanced Micro Devices, Inc. Microsoft, Visual Studio, Windows, and Windows \nVista are registered trademarks of Microsoft Corporation in the U.S. and/or other jurisdic\u0002tions. Other names are for informational purposes only and may be trademarks of their \nrespective owners. OpenCL and the OpenCL logo are trademarks of Apple Inc. used by \npermission by Khronos.\nThe contents of this document are provided in connection with Advanced Micro Devices, \nInc. (“AMD”) products. AMD makes no representations or warranties with respect to the \naccuracy or completeness of the contents of this publication and reserves the right to \nmake changes to specifications and product descriptions at any time without notice. The \ninformation contained herein may be of a preliminary or advance nature and is subject to \nchange without notice. No license, whether express, implied, arising by estoppel or other\u0002wise, to any intellectual property rights is granted by this publication. Except as set forth \nin AMD’s Standard Terms and Conditions of Sale, AMD assumes no liability whatsoever, \nand disclaims any express or implied warranty, relating to its products including, but not \nlimited to, the implied warranty of merchantability, fitness for a particular purpose, or \ninfringement of any intellectual property right.\nAMD’s products are not designed, intended, authorized or warranted for use as compo\u0002nents in systems intended for surgical implant into the body, or in other applications \nintended to support or sustain life, or in any other application in which the failure of AMD’s \nproduct could create a situation where personal injury, death, or severe property or envi\u0002ronmental damage may occur. AMD reserves the right to discontinue or make changes to \nits products at any time without notice.\nAdvanced Micro Devices, Inc.\nOne AMD Place\nP.O. Box 3453\nSunnyvale, CA 94088-3453\nwww.amd.com\nFor AMD Accelerated Parallel Processing:\nURL: developer.amd.com/appsdk\nDeveloping: developer.amd.com/\nSupport: developer.amd.com/appsdksupport\nForum: developer.amd.com/openclforum\nAMD S OUTHERN I SLANDS SERIES TECHNOLOGY\niii\nCopyright © 2014 Advanced Micro Devices, Inc. All rights reserved. \nContents\nContents\nPreface\nChapter 1 Introduction\nChapter 2 Kernel Organization\n2.1 Terminology ...................................................................................................................................... 2-1\n2.2 Kernel Capabilities........................................................................................................................... 2-2\n2.2.1 Scalar and Vector Operations.........................................................................................2-2\n2.2.2 Instruction Types..............................................................................................................2-2\n2.3 Memory Hierarchy............................................................................................................................ 2-3\n2.3.1 Work-item Private Memory..............................................................................................2-4\n2.3.2 Workgroup Private Memory ............................................................................................2-4\n2.3.3 Global memory .................................................................................................................2-5\n2.4 Program Example............................................................................................................................. 2-6\nChapter 3 Kernel State\n3.1 State Overview ................................................................................................................................. 3-1\n3.2 Program Counter (PC) .................................................................................................................... 3-2\n3.3 EXECute Mask.................................................................................................................................. 3-2\n3.4 Status Registers............................................................................................................................... 3-2\n3.5 Mode Register .................................................................................................................................. 3-4\n3.6 GPRs and LDS ................................................................................................................................. 3-4\n3.6.1 Out-of-Range Behavior....................................................................................................3-4\n3.6.2 SGPR Allocation and Storage ........................................................................................3-6\n3.6.3 SGPR Alignment...............................................................................................................3-6\n3.6.4 VGPR Allocation and Alignment ....................................................................................3-6\n3.6.5 LDS Allocation and Clamping ........................................................................................3-6\n3.7 M# Memory Descriptor .................................................................................................................... 3-7\n3.8 SCC: Scalar Condition Code........................................................................................................... 3-7\n3.9 Vector Compares: VCC and VCCZ.................................................................................................. 3-7\n3.10 Trap and Exception Registers........................................................................................................ 3-8\nAMD S OUTHERN I SLANDS SERIES TECHNOLOGY\niv\nCopyright © 2014 Advanced Micro Devices, Inc. All rights reserved. \nChapter 4 Program Flow Control\n4.1 Program Control............................................................................................................................... 4-1\n4.2 Branching.......................................................................................................................................... 4-1\n4.3 Work-Groups..................................................................................................................................... 4-2\n4.4 Data Dependency Resolution ......................................................................................................... 4-2\n4.5 Manually Inserted Wait States (NOPs) .......................................................................................... 4-3\n4.6 Arbitrary Divergent Control Flow................................................................................................... 4-4\nChapter 5 Scalar ALU Operations\n5.1 SALU Instruction Formats .............................................................................................................. 5-1\n5.2 Scalar ALU Operands...................................................................................................................... 5-2\n5.3 Scalar Condition Code (SCC) ......................................................................................................... 5-2\n5.4 Integer Arithmetic Instructions....................................................................................................... 5-4\n5.5 Conditional Instructions.................................................................................................................. 5-4\n5.6 Comparison Instructions................................................................................................................. 5-5\n5.7 Bit-Wise Instructions ....................................................................................................................... 5-5\n5.8 Special Instructions ......................................................................................................................... 5-7\nChapter 6 Vector ALU Operations\n6.1 Microcode Encodings...................................................................................................................... 6-1\n6.2 Operands........................................................................................................................................... 6-2\n6.2.1 Instruction Inputs .............................................................................................................6-2\n6.2.2 Instruction Outputs ..........................................................................................................6-3\n6.2.3 Out-of-Range GPRs..........................................................................................................6-4\n6.2.4 GPR Indexing ..................................................................................................................6-5\n6.3 Instructions....................................................................................................................................... 6-5\n6.4 Denormals and Rounding Modes .................................................................................................. 6-7\nChapter 7 Scalar Memory Operations\n7.1 Microcode Encoding........................................................................................................................ 7-1\n7.2 Operations ........................................................................................................................................ 7-2\n7.2.1 S_LOAD_DWORD .............................................................................................................7-2\n7.2.2 S_BUFFER_LOAD_DWORD.............................................................................................","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.amd.com%2Fcontent%2Fdam%2Famd%2Fen%2Fdocuments%2Fradeon-tech-docs%2Finstruction-set-architectures%2Fsouthern-islands-instruction-set-architecture.pdf","id":"https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/southern-islands-instruction-set-architecture.pdf","publishedDate":null,"summary":"This document is the \"Southern Islands Series Instruction Set Architecture Reference Guide\" from AMD, published in February 2014. It covers topics like kernel organization, including terminology, capabilities (scalar and vector operations, instruction types), and memory hierarchy (work-item private, workgroup private, and global memory). It also includes a program example and an overview of kernel state.\n","title":"[PDF] southern-islands-instruction-set-architecture.pdf - AMD","url":"https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/southern-islands-instruction-set-architecture.pdf"},{"content":"Copyright © 2016 Advanced Micro Devices, Inc. All rights reserved. \nReference Guide\nGraphics Core Next Architecture, Generation 3\nAugust 2016\nRevision 1.1\nii\nDISCLAIMER\nThe information contained herein is for informational purposes only, and is subject to \nchange without notice. While every precaution has been taken in the preparation of this \ndocument, it may contain technical inaccuracies, omissions and typographical errors, and \nAMD is under no obligation to update or otherwise correct this information. Advanced Mi\u0002cro Devices, Inc. makes no representations or warranties with respect to the accuracy or \ncompleteness of the contents of this document, and assumes no liability of any kind, in\u0002cluding the implied warranties of noninfringement, merchantability or fitness for particular \npurposes, with respect to the operation or use of AMD hardware, software or other prod\u0002ucts described herein. No license, including implied or arising by estoppel, to any intellec\u0002tual property rights is granted by this document. Terms and limitations applicable to the \npurchase or use of AMD’s products are as set forth in a signed agreement between the \nparties or in AMD's Standard Terms and Conditions of Sale.\n© 2016 Advanced Micro Devices, Inc. All rights reserved. AMD, the AMD Arrow logo, \nAMD Accelerated Parallel Processing, the AMD Accelerated Parallel Processing logo, and \ncombinations thereof are trademarks of Advanced Micro Devices, Inc. OpenCL and the \nOpenCL logo are trademarks of Apple Inc. used by permission by Khronos. Other names \nare for informational purposes only and may be trademarks of their respective owners. \nAMD’s products are not designed, intended, authorized or warranted for use as compo\u0002nents in systems intended for surgical implant into the body, or in other applications \nintended to support or sustain life, or in any other application in which the failure of AMD’s \nproduct could create a situation where personal injury, death, or severe property or envi\u0002ronmental damage may occur. AMD reserves the right to discontinue or make changes to \nits products at any time without notice.\nAdvanced Micro Devices, Inc.\nOne AMD Place\nP.O. Box 3453\nSunnyvale, CA 94088-3453\nwww.amd.com\nFor AMD Accelerated Parallel Processing:\nURL: developer.amd.com/appsdk\nDeveloping: developer.amd.com/\nForum: developer.amd.com/openclforum\nAMD GRAPHICS C ORE N EXT TECHNOLOGY\niii\nCopyright © 2016 Advanced Micro Devices, Inc. All rights reserved. \nContents\nContents\nPreface\nChapter 1 Introduction\nChapter 2 Program Organization\n2.1 The Compute Shader Program Type ............................................................................................. 2-1\n2.2 Instruction Terminology .................................................................................................................. 2-2\n2.3 Data Sharing..................................................................................................................................... 2-3\n2.3.1 Local Data Share (LDS)...................................................................................................2-4\n2.3.2 Global Data Share (GDS).................................................................................................2-5\n2.4 Device Memory................................................................................................................................. 2-5\nChapter 3 Kernel State\n3.1 State Overview ................................................................................................................................. 3-1\n3.2 Program Counter (PC) .................................................................................................................... 3-2\n3.3 EXECute Mask.................................................................................................................................. 3-2\n3.4 Status Registers............................................................................................................................... 3-2\n3.5 Mode Register .................................................................................................................................. 3-4\n3.6 GPRs and LDS ................................................................................................................................. 3-5\n3.6.1 Out-of-Range Behavior....................................................................................................3-5\n3.6.2 SGPR Allocation and Storage ........................................................................................3-6\n3.6.3 SGPR Alignment...............................................................................................................3-6\n3.6.4 VGPR Allocation and Alignment ....................................................................................3-6\n3.6.5 LDS Allocation and Clamping ........................................................................................3-6\n3.7 M# Memory Descriptor .................................................................................................................... 3-7\n3.8 SCC: Scalar Condition Code........................................................................................................... 3-7\n3.9 Vector Compares: VCC and VCCZ.................................................................................................. 3-8\n3.10 Trap and Exception Registers........................................................................................................ 3-8\n3.11 Memory Violations ......................................................................................................................... 3-10\nChapter 4 Program Flow Control\n4.1 Program Control............................................................................................................................... 4-1\n4.2 Branching.......................................................................................................................................... 4-1\n4.3 Work-Groups..................................................................................................................................... 4-2\nAMD GRAPHICS C ORE N EXT TECHNOLOGY\niv\nCopyright © 2016 Advanced Micro Devices, Inc. All rights reserved. \n4.4 Data Dependency Resolution ......................................................................................................... 4-2\n4.5 Manually Inserted Wait States (NOPs) .......................................................................................... 4-4\n4.6 Arbitrary Divergent Control Flow................................................................................................... 4-5\nChapter 5 Scalar ALU Operations\n5.1 SALU Instruction Formats .............................................................................................................. 5-1\n5.2 Scalar ALU Operands...................................................................................................................... 5-2\n5.3 Scalar Condition Code (SCC) ......................................................................................................... 5-2\n5.4 Integer Arithmetic Instructions....................................................................................................... 5-5\n5.5 Conditional Instructions.................................................................................................................. 5-5\n5.6 Comparison Instructions................................................................................................................. 5-6\n5.7 Bit-Wise Instructions ....................................................................................................................... 5-6\n5.8 Special Instructions ......................................................................................................................... 5-8\nChapter 6 Vector ALU Operations\n6.1 Microcode Encodings...................................................................................................................... 6-1\n6.2 Operands........................................................................................................................................... 6-2\n6.2.1 Instruction Inputs .............................................................................................................6-2\n6.2.2 Instruction Outputs ..........................................................................................................6-3\n6.2.3 Out-of-Range GPRs..........................................................................................................6-4\n6.2.4 GPR Indexing ..................................................................................................................6-5\n6.3 Instructions....................................................................................................................................... 6-5\n6.4 Denormalized and Rounding Modes ............................................................................................. 6-7\n6.5 ALU CLAMP Bit Usage.................................................................................................................... 6-7\n6.6 VGPR Indexing ................................................................................................................................. 6-7\n6.6.1 Indexing Instructions.......................................................................................................6-8\n6.6.2 Special Cases ...................................................................................................................6-8\nChapter 7 Scalar Memory Operations\n7.1 Microcode Encoding........................................................................................................................ 7-1\n7.2 Operations ........................................................................................................................................ 7-2\n7.2.1 S_LOAD_DWORD ............","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.amd.com%2Fcontent%2Fdam%2Famd%2Fen%2Fdocuments%2Fradeon-tech-docs%2Finstruction-set-architectures%2Fgcn3-instruction-set-architecture.pdf","id":"https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/gcn3-instruction-set-architecture.pdf","publishedDate":"2016-08-03T00:00:00.000Z","summary":"This document is a reference guide for the Graphics Core Next Architecture, Generation 3, published by Advanced Micro Devices, Inc. (AMD) in August 2016 (Revision 1.1). It includes disclaimers about the information's accuracy and limitations of liability. The guide covers topics such as program organization, instruction terminology, data sharing (Local Data Share and Global Data Share), device memory, and kernel state. It also provides contact information for AMD and links to resources for AMD Accelerated Parallel Processing.\n","title":"[PDF] gcn3-instruction-set-architecture.pdf - AMD","url":"https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/gcn3-instruction-set-architecture.pdf"},{"content":"1. THE AMD GCN ARCHITECTURE\nA CRASH COURSE\n@MissQuickstep\nLAYLA MAH – LAYLA.MAH@AMD.COM\nDEVELOPER TECHNOLOGY ENGINEER\n 2. AGENDA\n Part 1: A Brief History of GPU Evolution\n Part 2: Introduction to Graphics Core Next (GCN)\n Part 3: Anatomy of a GCN Compute Unit (CU)\n Part 4: GCN Shader: Arbitration, Examples &amp; Tips\n Part 5: GCN Memory Hierarchy\n Part 6: GCN Compute Architecture (ACE)\n Part 7: GCN Fixed Function Units\n(CP, GeometryEngine, Rasterizer, RBE, …)\n Part 8: Main Takeaways &amp; Conclusion\n Bonus Slides: Tiled Resources, Partially Resident Textures (PRT)\n22\n| A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\n 3. GPU EVOLUTION\n1ST ERA:\nFixed Function\n3D Geometry Transformation\n2ND ERA:\nSimple Shaders\n3RD ERA:\nGraphics Parallel Core\nVLIW5\nFMAD+\nSpecial\nFunctions\nBranch Unit\nStream\nProcessing Units\nGeneral Purpose\nRegisters\nLighting\nVLIW4\nGeneral Purpose\nRegisters\n3 | A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\nBranch Unit\nStream\nProcessing Units\n 4. GPU EVOLUTION\n1ST ERA:\nFixed Function\n3D Geometry Transformation\n2ND ERA:\nSimple Shaders\n3RD ERA:\nGraphics Parallel Core\nVLIW5\nFMAD+\nSpecial\nFunctions\nBranch Unit\nStream\nProcessing Units\nGeneral Purpose\nRegisters\nLighting\nVLIW4\nGeneral Purpose\nRegisters\n4 | A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\nBranch Unit\nStream\nProcessing Units\n 5. GPU EVOLUTION\n1ST ERA:\nFixed Function\n3D Geometry Transformation\n2ND ERA:\nSimple Shaders\nPrior to 2002\n Graphics-specific hardware\n3RD ERA:\nGraphics Parallel Core\nVLIW5\n‒ Texture mapping/filtering\n‒ Transform &amp; Lighting (T&amp;L) Engines\n‒ Geometry processing\n‒ Rasterization\n‒ Fixed function lighting equations\nLighting\nStream\nProcessing Units\nFMAD+\nSpecial\nFunctions\nBranch Unit\n‒ Multi-texturing\nGeneral Purpose\nRegisters\n‒ Dedicated texture and pixel caches VLIW4\n‒ Sufficient for basic graphics tasks\nProcessing Units\n‒ No general purpose compute capability General Purpose\nStream\nRegisters\n5 | A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\nBranch Unit\n Dot product and scalar multiply-add\n 6. GPU EVOLUTION\n1ST ERA:\nFixed Function\n3D Geometry Transformation\n2ND ERA:\nSimple Shaders\nMemory Interface\n3RD ERA:\nGraphics Parallel Core\nVLIW5\nStream\nProcessing Units\nGeneral Purpose\nRegisters\nSetup Engine\nLighting\nPixel Shader Core\nVLIW4\nStream\nProcessing Units\nGeneral Purpose\nRegisters\n6 | A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\nBranch Unit\n16 Pixel Pipes\nFMAD+\nSpecial\nFunctions\nBranch Unit\n8 Vertex Pipes\n 7. GPU EVOLUTION\n1ST ERA:\nFixed Function\n2002-2006\n3D Geometry Transformation\nGraphics Programmability\n– Direct3D 8/9, OpenGL 2.0\nIEEE not required\nMemory Interface\n8 Vertex Pipes\nSetup Engine\n– NV 16-bit full-speed\n–\nLighting NV 32-bit half-speed\nPixel Shader Core\n7 | A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\n Shader Models 1.0 - 2.0\nVLIW5\n‒ VS and PS are distinct\n‒ Minimal Instruction Sets\n‒ Limited Instruction Slots\n‒ LimitedGeneral Purpose Lengths\nShader\nRegisters\n‒ No DYNAMIC Flow Control\n‒ No Looping Constructs\n‒ VLIW4\nNo Vertex Texture Fetch\n‒ No Bitwise Operators\n‒ No Native Integer ALU\nStream\nProcessing Units\n‒ […]\nFMAD+\nSpecial\nFunctions\nBranch Unit\n– Specialized shader units for\nvertex &amp; pixel processing\n Added dedicated caches\nThe Rise of Shaders\nStream\nProcessing Units\nDifferent precision per IHV\n– ATI 24-bit full-speed\n3RD ERA:\nGraphics Parallel Core\nBranch Unit\n– Floating point processing\n2ND ERA:\nSimple Shaders\n16 Pixel Pipes\nGeneral Purpose\nRegisters\n 8. GPU EVOLUTION\n1ST ERA:\nFixed Function\n3D Geometry Transformation\n2ND ERA:\nSimple Shaders\n3RD ERA:\nGraphics Parallel Core\nVLIW5\nFMAD+\nSpecial\nFunctions\nBranch Unit\nStream\nProcessing Units\nGeneral Purpose\nRegisters\nLighting\nVLIW4\nGeneral Purpose\nRegisters\n8 | A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\nBranch Unit\nStream\nProcessing Units\n 9. GPU EVOLUTION\n1ST ERA:\nFixed Function\n2ND ERA:\nSimple Shaders\nThe Rise of The Unified Shader (VLIW-5)\n 5-Element Very-Long-Instruction-Word (XYZWT)\n3D Geometry Transformation\n3RD ERA:\nGraphics Parallel Core\nVLIW5\n‒ Ideal for 4-element Vector and 4x4 Matrix Operations\n‒ Vector/Vector math in a single instruction\nStream\nProcessing Units\n‒ Plus One Transcendental-Unit function per Instruction\nFMAD+\nSpecial\nFunctions\nBranch Unit\n‒ Began with XENOS and utilized from R600 until “Cayman”\n‒ Flexible and optimized for Graphics workloads\nGeneral Purpose\nRegisters\n More advanced caching\n‒ Instruction, constant, multi-level texture/data, &amp; later: LDS/GDS\nLighting\nVLIW4\nStream\nProcessing Units\n More flexible: Unified ALU, Branch Unit, Dynamic Flow\nControl, Vertex Texture, Geometry Shader, Tessellation\nEngines, etc.\n9 | A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\nGeneral Purpose\nRegisters\nBranch Unit\n Single Precision 32-bit IEEE-Compliant Floating Point ALUs\n 10. GPU EVOLUTION\n1ST ERA:\nFixed Function\n3D Geometry Transformation\n2ND ERA:\nSimple Shaders\n3RD ERA:\nGraphics Parallel Core\nVLIW5\nFMAD+\nSpecial\nFunctions\nBranch Unit\nStream\nProcessing Units\nGeneral Purpose\nRegisters\nLighting\nVLIW4\nGeneral Purpose\nRegisters\n10 | A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\nBranch Unit\nStream\nProcessing Units\n 11. GPU EVOLUTION\n1ST ERA:\nFixed Function\n2ND ERA:\nSimple Shaders\nOptimized For Die Area Efficiency (VLIW-4)\n3D 4-Element Very-Long-Instruction-Word (XYZW)\n Geometry Transformation\n3RD ERA:\nGraphics Parallel Core\nVLIW5\n‒ Profiling showed average VLIW utilization was &lt; 3.4/5\n‒ Each ALU has a smaller LUT\n‒ Combined using 3-term Lagrange polynomial interpolation across multiple ALU\nStream\nProcessing Units\n‒ Better optimized for combination of Graphics &amp; Compute\n‒ Graphics is still the primary focus, but compute is gaining attention\n‒ Still ideal for 4-element Vector and 4x4 Matrix Operations\n‒ Fewer ALU bubbles in transcendental-light code, better utilization\nLighting\nFMAD+\nSpecial\nFunctions\nGeneral Purpose\nRegisters\nVLIW4\n‒ Multiple dispatch processors &amp; separate command queues\n11 | A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\nStream\nProcessing Units\nGeneral Purpose\nRegisters\nBranch Unit\n‒ Simplified programming and optimization relative to VLIW-5\n Improved support for DirectCompute™ and OpenCL™\nBranch Unit\n‒ Removed dedicated T-Unit – Optimized die area usage\n 12. GPU EVOLUTION\nLANE 0\nLANE 1\nLANE 2\nLANE 15\nSIMD\nVLIW4 SIMD\nLANE\n0 1 2\nSIMD 0\n15\nLANE\n0 1 2\n15\nSIMD 1\nLANE\n0 1 2\n15\nSIMD 2\nLANE\n0 1 2\n15\nSIMD 3\nGCN Quad SIMD-16\n 64 Single Precision multiply-add (per-clock)\n 64 Single Precision multiply-add (per-clock)\n 16 SIMDs × ( 1 VLIW inst × 4 ALU ops )\n 4 SIMDs × ( 1 ALU op × 16 threads )\n 1 VLIW inst containing 4 ALU ops (per-clock)\n 4 ALU ops (from different wavefronts) / clock\n Needs 4 parallel ALU ops to fill each VLIW inst\n Needs 4+ wavefronts to keep SIMD lanes full\n Compiler manages register port conflicts\n No register port conflicts\n Specialized, complex compiler scheduling\n Standard compiler scheduling &amp; optimizations\n Difficult assembly creation, analysis, and debug\n Simplified assembly creation, analysis, &amp; debug\n Complicated tool chain support\n Simplified tool chain development and support\n Careful optimization req. for peak performance\n Stable and predictable performance\n12 | A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\n 13. AMD\nGRAPHICS\nCORE\nNEXT\n13 | A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\n 14.  GS-4112 – Mantle: Empowering 3D Graphics Innovation\n Keynote – Johan Andersson, Technical Director, EA\n GS-4145 – Oxide on Mantle Adoption (Wed 5:00-5:45)\n New low level programming\ninterface for PCs\n Designed in collaboration with\ntop game developers\n Lightweight driver that allows\ndirect access to GPU hardware\n Compatible with DirectX® HLSL\nfor simplified porting\nMANTLE\nGraphics Applications\nMantle API\nMantle Driver\nGCN\nWorks with all\nGraphics Core\nNext GPUs\n 15. AMD GRAPHICS CORE NEXT ARCHITECTURE\nA NEW GPU DESIGN FOR A NEW ERA OF COMPUTING\nFaster performance\nHigher efficiency\nNew graphics features\nNew compute features\nGRAPHICS CORE NEXT\n 16. AMD GRAPHICS CORE NEXT ARCHITECTURE\nA NEW GPU DESIGN FOR A NEW ERA OF COMPUTING\n Cutting-edge graphics performance and features\n High compute density with multi-tasking\n Built for power efficiency\n Optimized for heterogeneous computing\n Enabling the Heterogeneous System Architecture (HSA)\n Amazing scalability and flexibility\n16 | A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\nGRAPHICS CORE NEXT\n 17. AMD GRAPHICS CORE NEXT ARCHITECTURE\nA NEW GPU DESIGN FOR A NEW ERA OF COMPUTING\n Unlimited Resources &amp; Samplers\n All UAV formats can be read/write\n Simpler Assembly Language\n Simpler Shader Code\n Ability to support C/C++ (like)\n Architectural support for traps, exceptions &amp; debugging\n Ability to share virtual x86-64 address space with CPU cores\n17 | A CRASH COURSE: THE AMD GCN ARCHITECTURE | JANUARY 31, 2014 | APU13\nGRAPHICS CORE NEXT\n 18. AMD GRAPHICS CORE NEXT ARCHITECTURE\nA NEW GPU DESIGN FOR A NEW ERA OF COMPUTING\n AMD TECHNOLOGY POWERS NEXT-GEN CONSOLES\nNEW NEXT-GEN GAME CONSOLES\nRAISE THE BAR FOR GRAPHICS PERFORMANCE\nPERFORMANCE\nTFLOPS-CLASS\nCOMPUTE POWER\nMEMORY\n16X\nMORE\nMEMORY\n*\n* Based on PlayStation 3 512MB vs.\nPlayStation 4 8192MB GDDR5.\nGRAPHICS CORE NEXT\n 19. GCN COMPUTE UNIT\nA NEW GPU DESIGN FOR A NEW ERA OF COMPUTING\nGRAPHICS CORE NEXT\n CU = Basic Building Block of GPU Computational Power\nBranch &amp;\nMessage Unit\nVector Units\n(4x SIMD-16)\nTexture Filter\nUnits (4)\nTexture Fetch\nLoad / Store\nUnits (16)\n\nNew Instruction Set Architecture\nScalar Unit\n‒\nNon-VLIW\n‒\nVector unit + scalar co-processor\n‒\nScheduler\nDistributed programmable scheduler\n\nEach CU can execute instructions from\nmultiple kernels at once\n\nIncreased instructions per clock per mm2\n‒ High utilization\n‒ High throughput\nVector Regis","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.slideshare.net%2FDevCentralAMD%2Fgs4106-the-amd-gcn-architecture-a-crash-course-by-layla-mah","id":"https://www.slideshare.net/DevCentralAMD/gs4106-the-amd-gcn-architecture-a-crash-course-by-layla-mah","publishedDate":"2023-02-06T00:00:00.000Z","summary":"This is a crash course on the AMD GCN (Graphics Core Next) architecture, presented by Layla Mah. The presentation covers the evolution of GPUs, from fixed function to graphics parallel cores, and then dives into the specifics of the GCN architecture. The agenda includes: a brief history of GPU evolution, an introduction to GCN, the anatomy of a GCN Compute Unit (CU), GCN shader arbitration, examples & tips, GCN memory hierarchy, GCN compute architecture (ACE), and GCN fixed function units.\n","title":"GS-4106 The AMD GCN Architecture - A Crash Course, by Layla Mah","url":"https://www.slideshare.net/DevCentralAMD/gs4106-the-amd-gcn-architecture-a-crash-course-by-layla-mah"},{"content":"reserved 5_6_5 no no yes yes yes 1_5_5_5 no no yes yes yes 5_5_5_1 no no yes yes yes 4_4_4_4 no no yes yes yes 8_24 no no yes no yes 24_8 no no yes no yes X24_8_32 no no yes no yes 23-31 reserved GB_GR no no yes no no BG_RG no no yes no no 5_9_9_9 no no yes no no BC1 no no yes no no BC2 no no yes no no BC3 no no yes no no BC4 no no yes no no BC5 no no yes no no\n\nAuthors:\nG R A P H I C S C O R E N E X T T E C H N O L O G Y\nG R A P H I C S C O R E N E X T T E C H N O L O G Y Viii\nG R A P H I C S C O R E N E X T T E C H N O L O G Y Xi\nField Name\n\nAbstract\n\nGraphics Core Next Architecture, Generation 3 M a r c h 2 0 1 5 Revision 1.0 ii DISCLAIMER The information contained herein is for informational purposes only, and is subject to change without notice. While every precaution has been taken in the preparation of this document, it may contain technical inaccuracies, omissions and typographical errors, and AMD is under no obligation to update or otherwise correct this information. Advanced Micro Devices, Inc. makes no representations or warranties with respect to the accuracy or completeness of the contents of this document, and assumes no liability of any kind, including the implied warranties of noninfringement, merchantability or fitness for particular purposes, with respect to the operation or use of AMD hardware, software or other products described herein. No license, including implied or arising by estoppel, to any intellectual property rights is granted by this document. Terms and limitations applicable to the purchase or use of AMD's products are as set forth in a signed agreement between the parties or in AMD's Standard Terms and Conditions of Sale.\n\nAudience\n\nThis document is intended for programmers writing application and system software, including operating systems, compilers, loaders, linkers, device drivers, and system utilities. It assumes that programmers are writing compute-intensive parallel applications (streaming applications) and assumes an understanding of requisite programming practices.\n\nOrganization\n\nThis document begins with an overview of the AMD GCN processors' hardware and programming environment (Chapter 1). Chapter 2 describes the organization of GCN programs. Chapter 3 describes the program state that is maintained. Chapter 4 describes the program flow. Chapter 5 describes the scalar ALU operations. Chapter 6 describes the vector ALU operations. Chapter 7 describes the scalar memory operations. Chapter 8 describes the vector memory operations. Chapter 9 provides information about the flat memory instructions. Chapter 10 describes the data share operations. Chapter 11 describes exporting the parameters of pixel color and vertex shaders. Chapter 12 describes instruction details, first by the microcode format to which they belong, then in alphabetic order.\n\nFinally, Chapter 13 provides a detailed specification of each microcode format.\n\nConventions\n\nThe following conventions are used in this document.\n\nmono-spaced font\n\nA filename, file path, or code. * Any number of alphanumeric characters in the name of a code format, parameter, or instruction.\n\n< > Angle brackets denote streams.\n\n[1,2) A range that includes the left-most value (in this case, 1) but excludes the right-most value (in this case, 2).\n\n[\n\n1,2]\n\nA range that includes both the left-most and right-most values (in this case, 1 and 2).\n\n{x | y} One of the multiple options listed. In this case, x or y. 0.0 A single-precision (32-bit) floating-point value.\n\n1011b A binary value, in this example a 4-bit value.\n\n7:4 A bit range, from bit 7 to 4, inclusive. The high-order bit is shown first.\n\nitalicized word or phrase The first use of a term or concept basic to the understanding of stream computing.\n\nRelated Documents\n\n Intermediate Language (IL) Reference Manual. Published by AMD.\n\n AMD Accelerated Parallel Processing OpenCL Programming Guide.\n\nPublished by AMD.\n\n \n\nContact Information\n\nFor information concerning AMD Accelerated Parallel Processing developing, please see: developer.amd.com/.\n\nFor information about developing with AMD Accelerated Parallel Processing, please see: developer.amd.com/appsdk.\n\nWe also have a growing community of AMD Accelerated Parallel Processing users. Come visit us at the AMD Accelerated Parallel Processing Developer Forum (developer.amd.com/openclforum) to find out what applications other users are trying on their AMD Accelerated Parallel Processing products. Introduction AMD GCN processors implements a parallel microarchitecture that provides an excellent platform not only for computer graphics applications but also for general-purpose data parallel applications. Any data-intensive application that exhibits high bandwidth needs or significant computational requirements is a candidate for running on an AMD GCN processor.\n\nFigure 1 .1 shows a block diagram of the AMD GCN Generation 3 series processors.\n\nFigure 1.1 AMD GCN Generation 3 Series Block Diagram\n\nIt includes a data-parallel processor (DPP) array, a command processor, a memory controller, and other logic (not shown). The GCN command processor reads commands that the host has written to memory-mapped GCN registers in the system-memory address space. The command processor sends hardwaregenerated interrupts to the host when the command is completed. The GCN memory controller has direct access to all GCN device memory and the hostspecified areas of system memory. To satisfy read and write requests, the memory controller performs the functions of a direct-memory access (DMA) controller, including computing memory-address offsets based on the format of the requested data in memory.\n\nA host application cannot write to the GCN device memory directly, but it can command the GCN device to copy programs and data between system memory and device memory. For the CPU to write to GPU memory, there are two ways:\n\n Request the GPU's DMA engine to write data by pointing to the location of the source data on CPU memory, then pointing at the offset in the GPU memory.\n\n Upload a kernel to run on the shaders that access the memory through the PCIe link, then process it and store it in the GPU memory.\n\nIn the GCN environment, a complete application includes two parts:\n\n a program running on the host processor, and  programs, called kernels, running on the GCN processor.\n\nThe GCN programs are controlled by host commands, which  set GCN internal base-address and other configuration registers,\n\n specify the data domain on which the GCN GPU is to operate,  invalidate and flush caches on the GCN GPU, and  cause the GCN GPU to begin execution of a program.\n\nThe GCN driver program runs on the host.\n\nThe DPP array is the heart of the GCN processor. The array is organized as a set of compute unit pipelines, each independent from the others, that operate in parallel on streams of floating-point or integer data. The compute unit pipelines can process data or, through the memory controller, transfer data to, or from, memory. Computation in a compute unit pipeline can be made conditional.\n\nOutputs written to memory can also be made conditional.\n\nHost commands request a compute unit pipeline to execute a kernel by passing it:\n\n an identifier pair (x, y),  a conditional value, and  the location in memory of the kernel code.\n\nWhen it receives a request, the compute unit pipeline loads instructions and data from memory, begins execution, and continues until the end of the kernel. As kernels are running, the GCN hardware automatically fetches instructions and data from memory into on-chip caches; GCN software plays no role in this. GCN software also can load data from off-chip memory into on-chip general-purpose registers (GPRs) and caches.\n\nConceptually, each compute unit pipeline maintains a separate interface to memory, consisting of index pairs and a field identifying the type of request (program instruction, floating-point constant, integer constant, boolean constant, input read, or output write). The index pairs for inputs, outputs, and constants are specified by the requesting GCN instructions from the hardware-maintained program state in the pipelines.\n\nThe AMD GCN devices can detect floating point exceptions and can generate interrupts. In particular, it detects IEEE floating-point exceptions in hardware; these can be recorded for post-execution analysis. The software interrupts shown in Figure 1 .1 from the command processor to the host represent hardwaregenerated interrupts for signalling command-completion and related management functions.\n\nFigure 1 .2 shows the dataflow for a GCN application. For general-purpose applications, only one processing block performs all computation.\n\nFigure 1.2 GCN Generation 3 Dataflow\n\nThe GCN processor hides memory latency by keeping track of potentially hundreds of work-items in different stages of execution, and by overlapping compute operations with memory-access operations.\n\n2.1 The Compute Shader Program Type\n\nThe program type commonly run on the GCN GPU (see Figure 1 .2, on page 1-3) is the Compute Shader (CS), which is a generic program (compute kernel) that uses an input work-item ID as an index to perform:\n\n-gather reads on one or more sets of input data, -arithmetic computation, and -scatter writes to one or more set of output data to memory.\n\nCompute shaders can write to multiple (up to eight) surfaces, which can be a mix of multiple render targets (MRTs), unordered access views (UAVs), and flat address space.\n\nAll program types accept the same instruction types, and all of the program types can run on any of the available DPP-array pipelines that support these programs; however, each kernel type has certain restrictions, which are described with that type.\n\n2.2 Instruction Terminology\n\nTable 2 .1 summarizes some of the instruction-related terms used in this document. The instructions themselves are described in the remaining chapters. Details on each instruction are given in Chapter 13, \"Microcode Formats\".  ","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fgithub.com%2Folvaffe%2Fgpu-docs%2Fraw%2Fmaster%2Famd-open-gpu-docs%2FAMD_GCN3_Instruction_Set_Architecture.pdf","id":"https://github.com/olvaffe/gpu-docs/raw/master/amd-open-gpu-docs/AMD_GCN3_Instruction_Set_Architecture.pdf","publishedDate":"2023-01-07T00:00:00.000Z","summary":"This document provides information about the AMD GCN processors' hardware and programming environment, intended for programmers writing application and system software. It covers topics such as program organization, state, flow, ALU operations, memory operations, and instruction details. The document uses specific conventions and includes references to related documents.\n","title":"reserved 5_6_5 no no yes yes yes 1_5_5_5 no no yes yes yes 5_5_5_1 no no yes yes yes 4_4_4_4 no no yes yes yes 8_24 no no yes no yes 24_8 no no yes no yes X24_8_32 no no yes no yes 23-31 reserved GB_GR no no yes no no BG_RG no no yes no no 5_9_9_9 no no yes no no BC1 no no yes no no BC2 no no yes no no BC3 no no yes no no BC4 no no yes no no BC5 no no yes no no","url":"https://github.com/olvaffe/gpu-docs/raw/master/amd-open-gpu-docs/AMD_GCN3_Instruction_Set_Architecture.pdf"},{"content":"\n Get SlideShare without ads \n Enjoy access to millions of presentations, documents, ebooks, audiobooks, magazines, and more ad-free. \n ||||I|||| ×\n ×\n * Upload\n * Home\n * Explore\n * Login\n * Signup\n * Home\n * Explore\n Submit Search\n * Upload\n * Login\n * Signup\n\n We've updated our privacy policy. Click here to review the details. Tap here to review the details.\n\n ×\n Successfully reported this slideshow.\n Your SlideShare is downloading. ×\n\n Gcn performance ftw by stephan hodes\n\n Report\n AMD Developer Central\n •\n Jun. 09, 2014\n • 1 like • 3,543 views\n Skip to next slide You can skip to the next slide in 3\n Ad\n Skip to next slide You can skip to the next slide in 3\n Ad\n Skip to next slide You can skip to the next slide in 3\n Ad\n Skip to next slide You can skip to the next slide in 3\n Ad\n Skip to next slide You can skip to the next slide in 3\n Ad\n Skip to next slide You can skip to the next slide in 3\n Ad\n Skip to next slide You can skip to the next slide in 3\n Ad\n Skip to next slide You can skip to the next slide in 3\n Ad\n Skip to next slide You can skip to the next slide in 3\n Ad\n Skip to next slide You can skip to the next slide in 3\n Ad\n Skip to next slide You can skip to the next slide in 3\n Ad\n ×\n Your browser does not support the video tag.\n Skip to next slide\n Ad\n ×\n Next SlideShares\n Upcoming SlideShare\n Rendering Battlefield 4 with Mantle by Johan Andersson - AMD at GDC14\n Loading in … 3\n ×\n\n Check these out next\n\n Introduction to Direct 3D 12 by Ivan Nevraev\n AMD Developer Central\n HC-4021, Efficient scheduling of OpenMP and OpenCL™ workloads on Accelerated ...\n AMD Developer Central\n DX12 & Vulkan: Dawn of a New Generation of Graphics APIs\n AMD Developer Central\n PT-4054, \"OpenCL™ Accelerated Compute Libraries\" by John Melonakos\n AMD Developer Central\n Vertex Shader Tricks by Bill Bilodeau - AMD at GDC14\n AMD Developer Central\n GS-4108, Direct Compute in Gaming, by Bill Bilodeau\n AMD Developer Central\n Keynote (Mike Muller) - Is There Anything New in Heterogeneous Computing - by...\n AMD Developer Central\n CC-4005, Performance analysis of 3D Finite Difference computational stencils ...\n AMD Developer Central\n 1 of 21 Ad\n 1 of 21 Ad\n\n Gcn performance ftw by stephan hodes\n\n Jun. 09, 2014\n • 1 like • 3,543 views\n Report\n Download Now Download\n\n Download to read offline\n\n Technology Design\n AMD Developer Central\n Follow\n Technology Design\n Advertisement\n\n Recommended\n\n Rendering Battlefield 4 with Mantle by Johan Andersson - AMD at GDC14\n AMD Developer Central\n 4.4k views\n •\n 53 slides\n Direct3D12 and the Future of Graphics APIs by Dave Oldcorn\n AMD Developer Central\n 12.8k views\n •\n 24 slides\n Rendering Battlefield 4 with Mantle by Yuriy ODonnell\n AMD Developer Central\n 3.1k views\n •\n 56 slides\n Media SDK Webinar 2014\n AMD Developer Central\n 3k views\n •\n 37 slides\n DirectGMA on AMD’S FirePro™ GPUS\n AMD Developer Central\n 3.4k views\n •\n 17 slides\n An Introduction to OpenCL™ Programming with AMD GPUs - AMD & Acceleware Webinar\n AMD Developer Central\n 4.8k views\n •\n 52 slides\n Leverage the Speed of OpenCL™ with AMD Math Libraries\n AMD Developer Central\n 4.7k views\n •\n 26 slides\n TressFX The Fast and The Furry by Nicolas Thibieroz\n AMD Developer Central\n 6.4k views\n •\n 23 slides\n Advertisement\n Advertisement\n\n Recommended\n\n Rendering Battlefield 4 with Mantle by Johan Andersson - AMD at GDC14\n AMD Developer Central\n 4.4k views\n •\n 53 slides\n Direct3D12 and the Future of Graphics APIs by Dave Oldcorn\n AMD Developer Central\n 12.8k views\n •\n 24 slides\n Rendering Battlefield 4 with Mantle by Yuriy ODonnell\n AMD Developer Central\n 3.1k views\n •\n 56 slides\n Media SDK Webinar 2014\n AMD Developer Central\n 3k views\n •\n 37 slides\n DirectGMA on AMD’S FirePro™ GPUS\n AMD Developer Central\n 3.4k views\n •\n 17 slides\n An Introduction to OpenCL™ Programming with AMD GPUs - AMD & Acceleware Webinar\n AMD Developer Central\n 4.8k views\n •\n 52 slides\n Leverage the Speed of OpenCL™ with AMD Math Libraries\n AMD Developer Central\n 4.7k views\n •\n 26 slides\n TressFX The Fast and The Furry by Nicolas Thibieroz\n AMD Developer Central\n 6.4k views\n •\n 23 slides\n\n More Related Content\n\n Slideshows for you (20)\n\n Introduction to Direct 3D 12 by Ivan Nevraev\n AMD Developer Central\n •\n 8.4k views\n HC-4021, Efficient scheduling of OpenMP and OpenCL™ workloads on Accelerated ...\n AMD Developer Central\n •\n 12.2k views\n DX12 & Vulkan: Dawn of a New Generation of Graphics APIs\n AMD Developer Central\n •\n 10k views\n PT-4054, \"OpenCL™ Accelerated Compute Libraries\" by John Melonakos\n AMD Developer Central\n •\n 1.6k views\n Vertex Shader Tricks by Bill Bilodeau - AMD at GDC14\n AMD Developer Central\n •\n 30.3k views\n GS-4108, Direct Compute in Gaming, by Bill Bilodeau\n AMD Developer Central\n •\n 3.8k views\n Keynote (Mike Muller) - Is There Anything New in Heterogeneous Computing - by...\n AMD Developer Central\n •\n 2.3k views\n CC-4005, Performance analysis of 3D Finite Difference computational stencils ...\n AMD Developer Central\n •\n 2.1k views\n PostgreSQL with OpenCL\n Muhaza Liebenlito\n •\n 5.3k views\n CC-4000, Characterizing APU Performance in HadoopCL on Heterogeneous Distribu...\n AMD Developer Central\n •\n 1.5k views\n GS-4152, AMD’s Radeon R9-290X, One Big dGPU, by Michael Mantor\n AMD Developer Central\n •\n 5.9k views\n Siggraph 2016 - Vulkan and nvidia : the essentials\n Tristan Lorach\n •\n 8.1k views\n Shader model 5 0 and compute shader\n zaywalker\n •\n 5.4k views\n GS-4147, TressFX 2.0, by Bill-Bilodeau\n AMD Developer Central\n •\n 4.5k views\n Can FPGAs Compete with GPUs?\n inside-BigData.com\n •\n 1.1k views\n Computer Vision Powered by Heterogeneous System Architecture (HSA) by Dr. Ha...\n AMD Developer Central\n •\n 4.7k views\n MM-4105, Realtime 4K HDR Decoding with GPU ACES, by Gary Demos\n AMD Developer Central\n •\n 1.9k views\n PT-4102, Simulation, Compilation and Debugging of OpenCL on the AMD Southern ...\n AMD Developer Central\n •\n 1.8k views\n MM-4092, Optimizing FFMPEG and Handbrake Using OpenCL and Other AMD HW Capabi...\n AMD Developer Central\n •\n 5.2k views\n Final lisa opening_keynote_draft_-_v12.1tb\n r Skip\n •\n 593 views\n Introduction to Direct 3D 12 by Ivan Nevraev\n AMD Developer Central\n 8.4k views\n •\n 43 slides\n HC-4021, Efficient scheduling of OpenMP and OpenCL™ workloads on Accelerated ...\n AMD Developer Central\n 12.2k views\n •\n 29 slides\n DX12 & Vulkan: Dawn of a New Generation of Graphics APIs\n AMD Developer Central\n 10k views\n •\n 38 slides\n PT-4054, \"OpenCL™ Accelerated Compute Libraries\" by John Melonakos\n AMD Developer Central\n 1.6k views\n •\n 64 slides\n Vertex Shader Tricks by Bill Bilodeau - AMD at GDC14\n AMD Developer Central\n 30.3k views\n •\n 33 slides\n GS-4108, Direct Compute in Gaming, by Bill Bilodeau\n AMD Developer Central\n 3.8k views\n •\n 48 slides\n\n Viewers also liked (12)\n\n GS-4106 The AMD GCN Architecture - A Crash Course, by Layla Mah\n AMD Developer Central\n •\n 39.7k views\n \"Enabling Efficient Heterogeneous Processing Through Coherency,\" a Presentati...\n Edge AI and Vision Alliance\n •\n 506 views\n Webinar: Whats New in Java 8 with Develop Intelligence\n AMD Developer Central\n •\n 1.8k views\n Inside XBox- One, by Martin Fuller\n AMD Developer Central\n •\n 3.5k views\n The Small Batch (and other) solutions in Mantle API, by Guennadi Riguer, Mant...\n AMD Developer Central\n •\n 2.5k views\n Low-level Shader Optimization for Next-Gen and DX11 by Emil Persson\n AMD Developer Central\n •\n 19.2k views\n Introduction to Node.js\n AMD Developer Central\n •\n 4.1k views\n Productive OpenCL Programming An Introduction to OpenCL Libraries with Array...\n AMD Developer Central\n •\n 3k views\n Holy smoke! Faster Particle Rendering using Direct Compute by Gareth Thomas\n AMD Developer Central\n •\n 14.3k views\n Optimizing the Graphics Pipeline with Compute, GDC 2016\n Graham Wihlidal\n •\n 134.3k views\n Inside XBOX ONE by Martin Fuller\n AMD Developer Central\n •\n 13.6k views\n AMD and the new “Zen” High Performance x86 Core at Hot Chips 28\n AMD\n •\n 1.2M views\n GS-4106 The AMD GCN Architecture - A Crash Course, by Layla Mah\n AMD Developer Central\n 39.7k views\n •\n 98 slides\n \"Enabling Efficient Heterogeneous Processing Through Coherency,\" a Presentati...\n Edge AI and Vision Alliance\n 506 views\n •\n 47 slides\n Webinar: Whats New in Java 8 with Develop Intelligence\n AMD Developer Central\n 1.8k views\n •\n 18 slides\n Inside XBox- One, by Martin Fuller\n AMD Developer Central\n 3.5k views\n •\n 26 slides\n The Small Batch (and other) solutions in Mantle API, by Guennadi Riguer, Mant...\n AMD Developer Central\n 2.5k views\n •\n 27 slides\n Low-level Shader Optimization for Next-Gen and DX11 by Emil Persson\n AMD Developer Central\n 19.2k views\n •\n 57 slides\n Advertisement\n\n Similar to Gcn performance ftw by stephan hodes (20)\n\n Do Theoretical Flo Ps Matter For Real Application’S Performance Kaust 2012\n Joshua Mora\n •\n 1.5k views\n [Unite Seoul 2019] Mali GPU Architecture and Mobile Studio\n Owen Wu\n •\n 832 views\n Newbie’s guide to_the_gpgpu_universe\n Ofer Rosenberg\n •\n 1.7k views\n 7nm \"Navi\" GPU - A GPU Built For Performance\n AMD\n •\n 2.7k views\n new_age_graphics_android_x86\n Droidcon Berlin\n •\n 1.3k views\n FPGA on the Cloud\n jtsagata\n •\n 329 views\n Final_Report\n Connor Delaosa\n •\n 631 views\n Exploring Compiler Optimization Opportunities for the OpenMP 4.x Accelerator...\n Akihiro Hayashi\n •\n 290 views\n Amd epyc update_gdep_xilinx_ai_web_seminar_20201028\n ssuser5b12d1\n •\n 168 views\n A beginner’s guide to programming GPUs with CUDA\n Piyush Mittal\n •\n 5k views\n LEGaTO Integration\n LEGATO project\n •\n 63 views\n GMSL in Linux\n Kieran Bingham\n •\n 3.6k views\n [TGDF 2019] Mali GPU Architecture and Mobile Studio\n Owen Wu\n •\n 848 views\n Vpu technology &gpgpu computing\n Arka Ghosh\n •\n 1 view\n Vpu technology &gpgpu computing\n Arka Ghosh\n •\n 1.1k views\n Vpu technology &gpgpu computing\n Arka Ghosh\n •\n 437 views\n Experiences with Power 9 at A*STAR CRC\n Ganesan Narayanasamy\n •\n 293 views\n PT-4055, Optimizing Raytracing on GCN with AMD Development Tools, by Tzachi C...\n AMD Developer Central\n •\n 2k views\n Direct3D and the Fu","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.slideshare.net%2FDevCentralAMD%2Fgcn-performance-ftw-by-stephan-hodes","id":"https://www.slideshare.net/DevCentralAMD/gcn-performance-ftw-by-stephan-hodes","publishedDate":"2023-02-27T00:00:00.000Z","summary":"This webpage is a SlideShare presentation titled \"Gcn performance ftw\" by Stephan Hodes, published on June 9, 2014, by AMD Developer Central. It has 1 like and 3,543 views. The page includes links to other presentations related to AMD technologies, such as \"Rendering Battlefield 4 with Mantle\" and \"Direct3D12 and the Future of Graphics APIs.\"\n","title":"Gcn performance ftw by stephan hodes","url":"https://www.slideshare.net/DevCentralAMD/gcn-performance-ftw-by-stephan-hodes"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.scribd.com%2Fdocument%2F720336336%2FAMD-s-CDNA-3-Compute-Architecture-Chips-and-Cheese","id":"https://www.scribd.com/document/720336336/AMD-s-CDNA-3-Compute-Architecture-Chips-and-Cheese","publishedDate":null,"summary":"Failed to fetch content","title":"AMD's CDNA 3 Compute Architecture - Chips and Cheese - Scribd","url":"https://www.scribd.com/document/720336336/AMD-s-CDNA-3-Compute-Architecture-Chips-and-Cheese"},{"content":"RDNA Architecture | 1\nINTRODUCING \nDNA \nARCHITECTURE\nThe all new RadeonTM gaming architecture powering “Navi”\nRDNA Architecture | 2\nTable of Contents\nTable of Contents .........................................................................................................................2\nIntroduction..................................................................................................................................3\nRDNA Architecture Overview and Philosophy ..............................................................................4\nRDNA System Architecture ..........................................................................................................6\nRDNA Shader Array and Graphics Functions ................................................................................8\nDual Compute Unit Front End.......................................................................................................9\nSIMD Execution Units ..................................................................................................................11\nScalar Execution and Control Flow ..............................................................................................11\nVector Execution..........................................................................................................................12\nVector ALUs .................................................................................................................................13\nDual Compute Unit Memories .................................................................................................... 14\nLocal Data Share and Atomics.....................................................................................................15\nVector Caches ..............................................................................................................................15\nExport and GDS............................................................................................................................17\nShared Graphics L1 Cache ............................................................................................................17\nL2 Cache and Memory................................................................................................................. 18\nRadeon Multimedia and Display Engines ................................................................................... 18\nTrue Audio Next..........................................................................................................................20\nAdvanced Visual Effects.............................................................................................................20\nRadeon RX 5700 Family ..............................................................................................................21\nConclusion .................................................................................................................................. 23\nLegal Disclaimer and Attributions ..............................................................................................24\nRDNA Architecture | 3\nIntroduction\nThe world of graphics has fundamentally evolved and shifted over the course of the last three \ndecades towards greater programmability. The earliest graphics systems were implemented \npurely in software and ran on CPUs, but could not offer the performance needed for more than \nthe most basic visual effects. The first specialized graphics architectures were almost purely \nfixed-function and could only accelerate a very limited range of specific 2D or 3D computations \nsuch as geometry or lighting transformations. The next wave of architectures introduced \ngraphics shaders that gave programmers a taste of flexibility, but with stringent limitations. \nMore recently, graphics processors evolved towards programmability – offering programmable \ngraphics shaders and fully general-purpose computing.\nFigure 1 – Eras of Graphics.\nAMD’s TeraScale was designed for the era of programmable graphics and ushered in general\u0002purpose compute with DirectX® 11’s DirectCompute API and a VLIW-based architecture. The \nGraphics Core Next (GCN) architecture moved to a more programmable interleaved vector\ncompute model and introduced asynchronous computing, enabling traditional graphics and \ngeneral-compute to work together efficiently. The GCN architecture is at the heart of over 400 \nmillion systems, spanning from notebook PCs, to extreme gaming desktops, cutting-edge \ngame consoles, and cloud gaming services that can reach any consumer on a network.\nLooking to the future, the challenge for the next era of graphics is to shift away from the \nconventional graphics pipeline and its limitations to a compute-first world where the only limit \non visual effects is the imagination of developers. To meet the challenges of modern graphics, \nAMD’s RDNA architecture is a scalar architecture, designed from the ground up for efficient \nand flexible computing, that can scale across a variety of gaming platforms. The 7nm “Navi”\nfamily of GPUs is the first instantiation of the RDNA architecture and includes the Radeon RX \n5700 series. \nRDNA Architecture | 4\nRDNA Architecture Overview and Philosophy\nThe new RDNA architecture is optimized for efficiency and programmability while offering \nbackwards compatibility with the GCN architecture. It still uses the same seven basic \ninstruction types: scalar compute, scalar memory, vector compute, vector memory, branches, \nexport, and messages. However, the new architecture fundamentally reorganizes the data flow \nwithin the processor, boosting performance and improving efficiency.\nIn all AMD graphics architectures, a kernel is a single stream of instructions that operate on a \nlarge number of data parallel work-items. The work-items are organized into architecturally \nvisible work-groups that can communicate through an explicit local data share (LDS). The \nshader compiler further subdivides work-groups into microarchitectural wavefronts that are \nscheduled and executed in parallel on a given hardware implementation. \nFor the GCN architecture, the shader compiler creates wavefronts that contain 64 work-items. \nWhen every work-item in a wavefront is executing the same instruction, this organization is \nhighly efficient. Each GCN compute unit (CU) includes four SIMD units that consist of 16 ALUs; \neach SIMD executes a full wavefront instruction over four clock cycles. The main challenge then \nbecomes maintaining enough active wavefronts to saturate the four SIMD units in a CU.\nThe RDNA architecture is natively designed for a new narrower wavefront with 32 work-items,\nintuitively called wave32, that is optimized for efficient compute. Wave32 offers several \ncritical advantages for compute and complements the existing graphics-focused wave64 \nmode. \nOne of the defining features of modern compute workloads is complex control flow: loops, \nfunction calls, and other branches are essential for more sophisticated algorithms. However, \nwhen a branch forces portions of a wavefront to diverge and execute different instructions, the \noverall efficiency suffers since each instruction will execute a partial wavefront and disable the \nother portions. The new narrower wave32 mode improves efficiency for more complex compute \nworkloads by reducing the cost of control flow and divergence. \nSecond, a narrower wavefront completes faster and uses fewer resources for accessing data. \nEach wavefront requires control logic, registers, and cache while active. As one example, the \nnew wave32 mode uses half the number of registers. Since the wavefront will complete \nquicker, the registers free up faster, enabling more active wavefronts. Ultimately, wave32 \nenables delivering throughput and hiding latency much more efficient.\nThird, splitting a workload into smaller wave32 dataflows increases the total number of \nwavefronts. This subdivision of work items boosts parallelism and allows the GPU to use more \ncores to execute a given workload, improving both performance and efficiency.\nRDNA Architecture | 5\nFigure 2 – Adjacent Compute Unit Cooperation in RDNA architecture. \nThe new dual compute unit design is the essence of the RDNA architecture and replaces the \nGCN compute unit as the fundamental computational building block of the GPU. As Figure 2 \nillustrates, the dual compute unit still comprises four SIMDs that operate independently. \nHowever, this dual compute unit was specifically designed for wave32 mode; the RDNA SIMDs \ninclude 32 ALUs, twice as wide as the vector ALUS in the prior generation, boosting\nperformance by executing a wavefront up to 2X faster. The new SIMDs were built for mixed\u0002precision operation and efficiently compute with a variety of datatypes to enable scientific \ncomputing and machine learning. Figure 3 below illustrates how the new dual compute unit \nexploits instruction-level parallelism within a simple example shader to execute on a SIMD \nwith half the latency in wave32 mode and a 44% reduction in latency for wave64 mode \ncompared to the previous generation GCN SIMD.\n1\nINSTRUCTION ISSUE EXAMPLE\nFigure 3 – Wave32 and wave64 execution on an example snippet of code. \nThe RDNA architecture also redefines the cache and memory hierarchy to increase bandwidth \nfor graphics and compute, reduce power consumption, and enable greater scaling for the \nfuture. Earlier architectures employed a two-level cache hierarchy. Generally, the first level of \ncaching was private to each GCN compute unit and focused on compute. The second level of \ncaching was the globally shared L2 that resided alongside the memory controllers and would \ndeliver data both to compute units and graphics functions such as the geometry engines and \npixel pipelines.\nRDNA Architecture | 6\nTo satisfy the more powerful dual compute units, the L0 caches for scalar and vector data have \nscaled up as well. The new architecture introduces a specialized intermed","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.techpowerup.com%2Fgpu-specs%2Fdocs%2Famd-rdna-whitepaper.pdf","id":"https://www.techpowerup.com/gpu-specs/docs/amd-rdna-whitepaper.pdf","publishedDate":null,"summary":"This webpage introduces AMD's RDNA architecture, the foundation for their \"Navi\" graphics cards. It covers the architecture's overview, system components, shader array, compute units, memory, multimedia and display engines, and advanced visual effects. The document also touches on the evolution of graphics from early software-based systems to the current programmable processors.\n","title":"[PDF] amd-rdna-whitepaper.pdf - TechPowerUp","url":"https://www.techpowerup.com/gpu-specs/docs/amd-rdna-whitepaper.pdf"},{"content":"AMD¢1\n\nRDNA Architecture\n\n\nForward-looking statement\nThis presentation contains forward-looking statements concerning Advanced Micro Devices, Inc. (AMD) including, but not \nlimited to, the features, functionality, performance, availability, timing, pricing, expectations and expected benefits of \nAMD’s current and future products, which are made pursuant to the Safe Harbor provisions of the Private Securities \nLitigation Reform Act of 1995. Forward-looking statements are commonly identified by words such as \"would,\" \"may,\" \n\"expects,\" \"believes,\" \"plans,\" \"intends,\" \"projects\" and other terms with similar meaning. Investors are cautioned that the \nforward-looking statements in this presentation are based on current beliefs, assumptions and expectations, speak only as \nof the date of this presentation and involve risks and uncertainties that could cause actual results to differ materially from \ncurrent expectations. Such statements are subject to certain known and unknown risks and uncertainties, many of which \nare difficult to predict and generally beyond AMD's control, that could cause actual results and other future events to differ \nmaterially from those expressed in, or implied or projected by, the forward-looking information and statements. Investors \nare urged to review in detail the risks and uncertainties in AMD's Securities and Exchange Commission filings, including but \nnot limited to AMD's Quarterly Report on Form 10-Q for the quarter ended March 30, 2019\n2\nHighlights of the RDNA Workgroup Processor (WGP)\n▪ Designed for lower latency and higher effective IPC\n▪ Native Wave32 with support for Wave64 via dual-issue\n▪ Single-cycle instruction issue\n▪ Co-execution of transcendental arithmetic operations\n▪ Resources of two Compute Units available to a single workgroup\n▪ 2x scalar execution resources\n▪ Vector memory improvements\n3\nGCN Compute Units\n▪ 4 Compute Units:\n▪ RX590 has 36 CU, RX Vega64 has 64 CU\nL1$\nTexture \nAddresser\nTexture Data\nI$\nK$\nSALU SIMD16 SIMD16 SIMD16 SIMD16 LDS\nL1$\nTexture \nAddresser\nTexture Data\nSALU SIMD16 SIMD16 SIMD16 SIMD16 LDS\nL1$\nTexture \nAddresser\nTexture Data\nSALU SIMD16 SIMD16 SIMD16 SIMD16 LDS\nL1$\nTexture \nAddresser\nTexture Data\nSALU SIMD16 SIMD16 SIMD16 SIMD16 LDS\n4\n▪ 2 Workgroup Processors:\n▪ “Navi” has 20 WGP, corresponding to 40 CU\nLDS\nSALU SIMD32 SALU SIMD32\nSALU SIMD32 SALU SIMD32\nSALU SIMD32 SALU SIMD32\nSIMD32\nRDNA Workgroup Processors (WGP)\nL0$\nTexture \nAddresser\nTexture Data I$\nK$ LDS\nSALU\nL0$\nTexture \nAddresser\nTexture Data\nL0$\nTexture \nAddresser\nTexture Data\nL0$\nTexture \nAddresser\nTexture Data\nI$\nK$\nSALU SIMD32\n5\n4-cycle instruction issue on GCN\n▪ Each wave is assigned to one SIMD16, up to 10 waves per SIMD16\n▪ Each SIMD16 issues 1 instruction every 4 cycles\n▪ Vector instructions throughput is 1 every 4 cycles\nSALU SIMD16 SIMD16 SIMD16 SIMD16\nSIMD0 SIMD1\n0-15\nSIMD2 SIMD3\n16-31 32-47 48-63\n0-15 16-31 32-47 48-63\n0-15 16-31 32-47 48-63\n0-15 16-31 32-47 48-63\nSIMD0 SIMD1\n0-15\nSIMD2 SIMD3\n16-31 32-47 48-63\n0-15 16-31 32-47 48-63\n0-15 16-31 32-47 48-63\n0-15 16-31 32-47 48-63\nSALU\nSIMD0\nSIMD1\nSIMD2\nSIMD3\nCycle 0 1 2 3 4 5 6 7\n6\nSingle-cycle instruction issue on RDNA\n▪ Each wave is assigned to one SIMD32, up to 20 waves per SIMD32\n▪ Each SIMD32 issues 1 instruction every cycle\n▪ Vector instruction throughput is 1 every cycle (for Wave32)\n▪ 5 cycles of latency are exposed (automatic dependency check in hardware)\n⁃ Dependency stalls can be filled by other waves\nSALU SIMD32\n7\nSingle-cycle instruction issue: ILP and scheduling matters\nv_fma_f32 v4, v0, s0, s3\nv_fmac_f32 v4, v1, s1\nv_fma_f32 v5, v0, s4, s7\nv_fmac_f32 v5, v1, s5\nv_fma_f32 v6, v0, s8, s11\nCycle\nv_fmac_f32 v6, v1, s9\n0 1 2 3 4 5 6 7 8 9 10 11\nv_fma_f32 v4, v0, s0, s3\nv_fma_f32 v5, v0, s4, s7\nv_fma_f32 v6, v0, s8, s11\nv_fmac_f32 v4, v1, s1\nv_fmac_f32 v5, v1, s5\nCycle\nv_fmac_f32 v6, v1, s9\n0 1 2 3 4 5 6 7 8 9 10 11\n12 13 14 15 16 17 18\n8\nTranscendental math co-execution\n▪ rcp/rsq/sqrt/log/exp/sin/cos\n▪ Transcendental instructions are ¼ rate (like GCN)\n▪ Non-transcendental instructions can execute in parallel\nv_rcp_f32 v14, v14\nv_fma_f32 v8, v0, s4, v2\nv_fma_f32 v9, v1, s4, v3\nv_rcp_f32 v15, v15\nv_fma_f32 v10, v4, s4, v5\nCycle 0\nv_fma_f32 v11, v6, s4, v7\nv_mul_f32 v8, v8, v14\nv_mul_f32 v9, v9, v14\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n9\n▪ Wave64 on GCN\n▪ 2x Wave32 on RDNA – one per SIMD32\n▪ With small dispatches, RDNA behaves significantly better than GCN\nInstruction latency: GCN vs. RDNA\nv_rcp_f32 v14, v14\nv_fma_f32 v8, v0, s4, v2\nv_fma_f32 v9, v1, s4, v3\nv_rcp_f32 v15, v15\nv_fma_f32 v10, v4, s4, v5\nv_fma_f32 v11, v6, s4, v7\nv_mul_f32 v8, v8, v14\nv_mul_f32 v9, v9, v14\nv_rcp_f32 v14, v14\nv_fma_f32 v8, v0, s4, v2\nv_fma_f32 v9, v1, s4, v3\nv_rcp_f32 v15, v15\nv_fma_f32 v10, v4, s4, v5\nv_fma_f32 v11, v6, s4, v7\nv_mul_f32 v8, v8, v14\nv_mul_f32 v9, v9, v14\n10\nVector register file (VGPRs)\n▪ Each SIMD32 has 1024 physical registers\n▪ Divided among waves, up to 256 each\n▪ Wave64 \"counts double\"\n▪ Examples:\n⁃ 4x Wave32 with 256 VGPRs\n⁃ 2x Wave64 with 256 VGPRs\n⁃ 16x Wave32 with 64 VGPRs\n⁃ 8x Wave64 with 64 VGPRs\n▪ Occupancy in \"# of threads per SIMD lane\" is unchanged from GCN\n⁃ Occupancy 4 on GCN = 16 threads per lane\n⁃ RDNA equivalent: 16x Wave32 or 8x Wave64\n▪ Call to action:\n⁃ Think about occupancy in terms of \"# of threads per SIMD lane\"\n11\n▪ GCN: Wave64, 128 VGPR allocation\n▪ RDNA: Wave32, 128 VGPR allocation\nVector register-based occupancy illustrated\nSIMD16 SIMD16\n0\n256\n16 DW 16 DW 16 DW 16 DW 16 DW 16 DW 16 DW 16 DW\n0\n256\n32 DW 32 DW 32 DW 32DW\n128\n128\n32\nSIMD32\n12\nKeeping the SIMD busy: GCN vs. RDNA\n▪ Example: Small dispatch, 64 threads only\nL1$\nTexture \nAddresser\nTexture Data\nSALU SIMD16 SIMD16 SIMD16 SIMD16 LDS\nSALU SIMD32 SALU SIMD32\nSIMD32 L0$\nTexture \nAddresser\nTexture Data\nLDS\nSALU\nL0$\nTexture \nAddresser\nTexture Data\nSALU SIMD32\nL1$\nTexture \nAddresser\nTexture Data\nSALU SIMD16 SIMD16 SIMD16 SIMD16 LDS\n13\nKeeping the SIMD busy: GCN vs. RDNA\n▪ RDNA requires much fewer threads for all blocks to light up:\nL1$\nTexture \nAddresser\nTexture Data\nSALU SIMD16 SIMD16 SIMD16 SIMD16 LDS\nSALU SIMD32 SALU SIMD32\nSIMD32 L0$\nTexture \nAddresser\nTexture Data\nLDS\nSALU\nL0$\nTexture \nAddresser\nTexture Data\nSALU SIMD32\nL1$\nTexture \nAddresser\nTexture Data\nSALU SIMD16 SIMD16 SIMD16 SIMD16 LDS\n14\nKeeping the SIMD busy: GCN vs. RDNA\n▪ 2 CU require 2*4*64 = 512 threads to be able to reach 100% ALU utilization\n▪ WGP requires 4*32 = 128 threads to be able to reach 100% ALU utilization\n⁃ Only achieved with high instruction level parallelism (ILP)\n⁃ Graphics workloads often have 3 independent streams (RGB / XYZ)\n⁃ 256 threads / WGP often reach >90% ALU utilization in practice1\n▪ Additional threads are needed on both GCN and RDNA to hide memory latency\n⁃ Unless you can fill the wait with ALU (this is extremely rare)\n⁃ # of threads required for memory latency hiding has reduced as well, but not as much\n▪ Fewer threads required overall to keep the machine busy\n⁃ Utilization ramps up more quickly after barriers\n⁃ High VGPR counts hurt slightly less\n▪ Call to action:\n⁃ Keep ILP in mind when writing shader code\n15\n1 Observed in pixel shaders in a trace of a typical game rendering workload\nWhat (not) to do for ILP\n▪ Naïve idea: run multiple work items in a single thread\n▪ Problems:\n⁃ Code bloat (mind the I$ size!)\n⁃ Higher VGPR count\n⁃ Increases the effective dispatch granularity; more waste along the edges\n▪Don't panic: extra waves are a really good source of parallelism\nimage_load v[0:1], …\ns_waitcnt vmcnt(0)\nv_mul_f32 v0, v0, s0\nv_mul_f32 v1, v1, s0\nimage_store v[0:1], ...\nimage_load v[0:1], …\nimage_load v[2:3], …\ns_waitcnt vmcnt(1)\nv_mul_f32 v0, v0, s0\nv_mul_f32 v1, v1, s0\ns_waitcnt vmcnt(0)\nv_mul_f32 v2, v2, s0\nv_mul_f32 v3, v3, s0\nimage_store v[0:1], ...\nimage_store v[2:3], ...\n16\n▪ Vector instructions of Wave64 execute as 2x Wave32\n▪ Same instructions, no code bloat\n▪ When low or high half of EXEC is 0, that half skips execution\nWave64 via dual-issue\nv_fma_f32 v4, v0, s0, s3\n(high half)\nv_fma_f32 v5, v0, s4, s7\n(high half)\nv_fma_f32 v6, v0, s8, s11\nCycle\n(high half)\n0 1 2 3 4 5 6 7 8 9 10 11\nv_fmac_f32 v4, v1, s1\n(high half)\n17\nWave32 vs. Wave64\n▪ Compiler makes the decision\n⁃ Compute and vertex shaders usually as Wave32, pixel shaders usually as Wave64\n⁃ Heuristics will continue to be tuned for the foreseeable future\n▪ Call to action:\n⁃ Make sure barriers in shaders are semantically correct!\n⁃ The compiler removes unnecessary barriers\n⁃ Enable variable subgroup size, especially when using wave/subgroup intrinsics (Vulkan extension pending…)\n⁃ Workgroup size: keep it a multiple of 64\nWave32 Wave64\nLower latency / wave lifetime\nQuicker ramp-up of WGPs after barriers\nMore efficient for partially filled waves\nTighter memory access patterns\nAllows higher occupancy (# threads per lane)\nMore efficient for attribute interpolation\n18\nLDS per workgroup processor\n▪ 128 kB per workgroup processor\n▪ Shared memory for compute, attributes for pixel shaders\n▪ Up to 64 kB per workgroup\n▪ Read / write / atomic throughput of up to 32 dwords per cycle (doubled relative to GCN)\n▪ 32 banks\n⁃ Same as “Vega”\n⁃ Mind the bank conflicts!\nSALU SIMD32 SALU SIMD32\nSIMD32 L0$\nTexture \nAddresser\nTexture Data I$\nK$ LDS\nSALU\nL0$\nTexture \nAddresser\nTexture Data\nSALU SIMD32\n19\nAdditional IPC improvements\n▪ Overall goal: fewer move instructions\n▪ Dual scalar source\n▪ All VALU instructions support immediates (96-bit instructions)\n▪ Optional NSA (non-sequential address) encoding for image instructions\n⁃ Avoids moves\n⁃ Simplified register allocation helps reduce VGPR pressure\ns_buffer_load_dwordx2 s[0:1], …\ns_waitcnt lgkmcnt(0)\nv_mov_b32 v1, s0\nv_fma_f32 v0, v0, v1, s1\ns_buffer_load_dwordx2 s[0:1], …\ns_waitcnt lgkmcnt(0)\nv_fma_f32 v0, v0, s0, s1\ns_mov_b32 s0, 0x3f490fdb ; pi/4\nv_mul_f32 v0, |v0|, s0\nv_mul_f32 v0, |v0|, 0x3f490fdb\nv_mov_b32 v7, v14\nimage_sample v[0:1], v[7:8], ...\nimage_sample v[0:1], [v14, v8], ...\n20\nWorkgroup Processor summary\n▪ Wave32 and single-cycle issue for bet","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fgpuopen.com%2Fdownload%2FRDNA_Architecture_public.pdf","id":"https://gpuopen.com/download/RDNA_Architecture_public.pdf","publishedDate":null,"summary":"This presentation discusses the RDNA architecture by AMD. Key highlights of the RDNA Workgroup Processor (WGP) include lower latency, higher effective IPC, native Wave32 support, single-cycle instruction issue, co-execution of transcendental arithmetic operations, and improved vector memory. The document compares the GCN Compute Units with RDNA Workgroup Processors, highlighting differences in instruction issue cycles and SIMD configurations.\n","title":"[PDF] RDNA Architecture - AMD GPUOpen","url":"https://gpuopen.com/download/RDNA_Architecture_public.pdf"},{"content":"AMD«e\\\n\n\"RDNA4\" Instruction Set Architecture\n\nReference Guide\n\n7-April-2025\n\nSpecification Agreement\nThis Specification Agreement (this \"Agreement\") is a legal agreement between Advanced Micro Devices, Inc. (\"AMD\") and \"You\" as the recipient\nof the attached AMD Specification (the \"Specification\"). If you are accessing the Specification as part of your performance of work for another\nparty, you acknowledge that you have authority to bind such party to the terms and conditions of this Agreement. If you accessed the\nSpecification by any means or otherwise use or provide Feedback (defined below) on the Specification, You agree to the terms and conditions\nset forth in this Agreement. If You do not agree to the terms and conditions set forth in this Agreement, you are not licensed to use the\nSpecification; do not use, access or provide Feedback about the Specification. In consideration of Your use or access of the Specification (in\nwhole or in part), the receipt and sufficiency of which are acknowledged, You agree as follows:\n1. You may review the Specification only (a) as a reference to assist You in planning and designing Your product, service or technology\n(\"Product\") to interface with an AMD product in compliance with the requirements as set forth in the Specification and (b) to provide\nFeedback about the information disclosed in the Specification to AMD.\n2. Except as expressly set forth in Paragraph 1, all rights in and to the Specification are retained by AMD. This Agreement does not give You\nany rights under any AMD patents, copyrights, trademarks or other intellectual property rights. You may not (i) duplicate any part of the\nSpecification; (ii) remove this Agreement or any notices from the Specification, or (iii) give any part of the Specification, or assign or\notherwise provide Your rights under this Agreement, to anyone else.\n3. The Specification may contain preliminary information, errors, or inaccuracies, or may not include certain necessary information.\nAdditionally, AMD reserves the right to discontinue or make changes to the Specification and its products at any time without notice. The\nSpecification is provided entirely \"AS IS.\" AMD MAKES NO WARRANTY OF ANY KIND AND DISCLAIMS ALL EXPRESS, IMPLIED AND\nSTATUTORY WARRANTIES, INCLUDING BUT NOT LIMITED TO IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\nPARTICULAR PURPOSE, NONINFRINGEMENT, TITLE OR THOSE WARRANTIES ARISING AS A COURSE OF DEALING OR CUSTOM OF\nTRADE. AMD SHALL NOT BE LIABLE FOR DIRECT, INDIRECT, CONSEQUENTIAL, SPECIAL, INCIDENTAL, PUNITIVE OR EXEMPLARY\nDAMAGES OF ANY KIND (INCLUDING LOSS OF BUSINESS, LOSS OF INFORMATION OR DATA, LOST PROFITS, LOSS OF CAPITAL, LOSS\nOF GOODWILL) REGARDLESS OF THE FORM OF ACTION WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE) AND STRICT\nPRODUCT LIABILITY OR OTHERWISE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n4. Furthermore, AMD’s products are not designed, intended, authorized or warranted for use as components in systems intended for surgical\nimplant into the body, or in other applications intended to support or sustain life, or in any other application in which the failure of AMD’s\nproduct could create a situation where personal injury, death, or severe property or environmental damage may occur.\n5. You have no obligation to give AMD any suggestions, comments or feedback (\"Feedback\") relating to the Specification. However, any\nFeedback You voluntarily provide may be used by AMD without restriction, fee or obligation of confidentiality. Accordingly, if You do give\nAMD Feedback on any version of the Specification, You agree AMD may freely use, reproduce, license, distribute, and otherwise\ncommercialize Your Feedback in any product, as well as has the right to sublicense third parties to do the same. Further, You will not give\nAMD any Feedback that You may have reason to believe is (i) subject to any patent, copyright or other intellectual property claim or right\nof any third party; or (ii) subject to license terms which seek to require any product or intellectual property incorporating or derived from\nFeedback or any Product or other AMD intellectual property to be licensed to or otherwise provided to any third party.\n6. You shall adhere to all applicable U.S., European, and other export laws, including but not limited to the U.S. Export Administration\nRegulations (\"EAR\"), (15 C.F.R. Sections 730 through 774), and E.U. Council Regulation (EC) No 428/2009 of 5 May 2009. Further, pursuant to\nSection 740.6 of the EAR, You hereby certifies that, except pursuant to a license granted by the United States Department of Commerce\nBureau of Industry and Security or as otherwise permitted pursuant to a License Exception under the U.S. Export Administration\nRegulations (\"EAR\"), You will not (1) export, re-export or release to a national of a country in Country Groups D:1, E:1 or E:2 any restricted\ntechnology, software, or source code You receive hereunder, or (2) export to Country Groups D:1, E:1 or E:2 the direct product of such\ntechnology or software, if such foreign produced direct product is subject to national security controls as identified on the Commerce\nControl List (currently found in Supplement 1 to Part 774 of EAR). For the most current Country Group listings, or for additional\ninformation about the EAR or Your obligations under those regulations, please refer to the U.S. Bureau of Industry and Security’s website\nat http://www.bis.doc.gov/.\n7. If You are a part of the U.S. Government, then the Specification is provided with \"RESTRICTED RIGHTS\" as set forth in subparagraphs (c)\n(1) and (2) of the Commercial Computer Software-Restricted Rights clause at FAR 52.227-14 or subparagraph (c) (1)(ii) of the Rights in\nTechnical Data and Computer Software clause at DFARS 252.277-7013, as applicable.\n8. This Agreement is governed by the laws of the State of California without regard to its choice of law principles. Any dispute involving it\nmust be brought in a court having jurisdiction of such dispute in Santa Clara County, California, and You waive any defenses and rights\n\"RDNA4\" Instruction Set Architecture\nii of 697\nallowing the dispute to be litigated elsewhere. If any part of this agreement is unenforceable, it will be considered modified to the extent\nnecessary to make it enforceable, and the remainder shall continue in effect. The failure of AMD to enforce any rights granted hereunder\nor to take action against You in the event of any breach hereunder shall not be deemed a waiver by AMD as to subsequent enforcement of\nrights or subsequent actions in the event of future breaches. This Agreement is the entire agreement between You and AMD concerning\nthe Specification; it may be changed only by a written document signed by both You and an authorized representative of AMD.\nDISCLAIMER\nThe information contained herein is for informational purposes only, and is subject to change without notice. This document may\ncontain technical inaccuracies, omissions and typographical errors, and AMD is under no obligation to update or otherwise correct this\ninformation. Advanced Micro Devices, Inc. makes no representations or warranties with respect to the accuracy or completeness of the\ncontents of this document, and assumes no liability of any kind, including the implied warranties of noninfringement, merchantability\nor fitness for particular purposes, with respect to the operation or use of AMD hardware, software or other products described herein.\nNo license, including implied or arising by estoppel, to any intellectual property rights is granted by this document. Terms and\nlimitations applicable to the purchase or use of AMD’s products or technology are as set forth in a signed agreement between the\nparties or in AMD’s Standard Terms and Conditions of Sale.\nAMD, the AMD Arrow logo, and combinations thereof are trademarks of Advanced Micro Devices, Inc. OpenCL is a trademark of Apple\nInc. used by permission by Khronos Group, Inc. DirectX is a registered trademark of Microsoft Corporation in the US and other\njurisdictions. Other product names used in this publication are for identification purposes only and may be trademarks of their\nrespective companies.\n© 2018-2024 Advanced Micro Devices, Inc. All rights reserved.\nAdvanced Micro Devices, Inc.\n2485 Augustine Drive\nSanta Clara, CA, 95054\nwww.amd.com\n\"RDNA4\" Instruction Set Architecture\niii of 697\nContents\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nAbout This Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nAudience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nOrganization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nRelated Documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\nAdditional Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.1. Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2. Hardware Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2.1. Work-Group Processor . . . . . . . . . . . . . . . . .","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.amd.com%2Fcontent%2Fdam%2Famd%2Fen%2Fdocuments%2Fradeon-tech-docs%2Finstruction-set-architectures%2Frdna4-instruction-set-architecture.pdf","id":"https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/rdna4-instruction-set-architecture.pdf","publishedDate":"2025-04-07T00:00:00.000Z","summary":"This document is the \"RDNA4\" Instruction Set Architecture Reference Guide from AMD, dated April 7, 2025. It's a legal agreement between AMD and the recipient of the specification. The agreement allows the recipient to review the specification for planning and designing products that interface with AMD products and to provide feedback. AMD retains all rights to the specification and provides it \"AS IS\" with no warranties. AMD is not liable for any damages. Furthermore, AMD's products are not designed for critical applications like surgical implants. Any feedback provided to AMD can be used without restriction.\n","title":"[PDF] \"RDNA4\" Instruction Set Architecture: Reference Guide - AMD","url":"https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/rdna4-instruction-set-architecture.pdf"},{"content":"AMD«e\\\n\n\"RDNA3\" Instruction Set Architecture\n\nReference Guide\n\n15-August-2023\n\nSpecification Agreement\nThis Specification Agreement (this \"Agreement\") is a legal agreement between Advanced Micro Devices, Inc. (\"AMD\") and \"You\" as the recipient\nof the attached AMD Specification (the \"Specification\"). If you are accessing the Specification as part of your performance of work for another\nparty, you acknowledge that you have authority to bind such party to the terms and conditions of this Agreement. If you accessed the\nSpecification by any means or otherwise use or provide Feedback (defined below) on the Specification, You agree to the terms and conditions\nset forth in this Agreement. If You do not agree to the terms and conditions set forth in this Agreement, you are not licensed to use the\nSpecification; do not use, access or provide Feedback about the Specification. In consideration of Your use or access of the Specification (in\nwhole or in part), the receipt and sufficiency of which are acknowledged, You agree as follows:\n1. You may review the Specification only (a) as a reference to assist You in planning and designing Your product, service or technology\n(\"Product\") to interface with an AMD product in compliance with the requirements as set forth in the Specification and (b) to provide\nFeedback about the information disclosed in the Specification to AMD.\n2. Except as expressly set forth in Paragraph 1, all rights in and to the Specification are retained by AMD. This Agreement does not give You\nany rights under any AMD patents, copyrights, trademarks or other intellectual property rights. You may not (i) duplicate any part of the\nSpecification; (ii) remove this Agreement or any notices from the Specification, or (iii) give any part of the Specification, or assign or\notherwise provide Your rights under this Agreement, to anyone else.\n3. The Specification may contain preliminary information, errors, or inaccuracies, or may not include certain necessary information.\nAdditionally, AMD reserves the right to discontinue or make changes to the Specification and its products at any time without notice. The\nSpecification is provided entirely \"AS IS.\" AMD MAKES NO WARRANTY OF ANY KIND AND DISCLAIMS ALL EXPRESS, IMPLIED AND\nSTATUTORY WARRANTIES, INCLUDING BUT NOT LIMITED TO IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\nPARTICULAR PURPOSE, NONINFRINGEMENT, TITLE OR THOSE WARRANTIES ARISING AS A COURSE OF DEALING OR CUSTOM OF\nTRADE. AMD SHALL NOT BE LIABLE FOR DIRECT, INDIRECT, CONSEQUENTIAL, SPECIAL, INCIDENTAL, PUNITIVE OR EXEMPLARY\nDAMAGES OF ANY KIND (INCLUDING LOSS OF BUSINESS, LOSS OF INFORMATION OR DATA, LOST PROFITS, LOSS OF CAPITAL, LOSS\nOF GOODWILL) REGARDLESS OF THE FORM OF ACTION WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE) AND STRICT\nPRODUCT LIABILITY OR OTHERWISE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n4. Furthermore, AMD’s products are not designed, intended, authorized or warranted for use as components in systems intended for surgical\nimplant into the body, or in other applications intended to support or sustain life, or in any other application in which the failure of AMD’s\nproduct could create a situation where personal injury, death, or severe property or environmental damage may occur.\n5. You have no obligation to give AMD any suggestions, comments or feedback (\"Feedback\") relating to the Specification. However, any\nFeedback You voluntarily provide may be used by AMD without restriction, fee or obligation of confidentiality. Accordingly, if You do give\nAMD Feedback on any version of the Specification, You agree AMD may freely use, reproduce, license, distribute, and otherwise\ncommercialize Your Feedback in any product, as well as has the right to sublicense third parties to do the same. Further, You will not give\nAMD any Feedback that You may have reason to believe is (i) subject to any patent, copyright or other intellectual property claim or right\nof any third party; or (ii) subject to license terms which seek to require any product or intellectual property incorporating or derived from\nFeedback or any Product or other AMD intellectual property to be licensed to or otherwise provided to any third party.\n6. You shall adhere to all applicable U.S., European, and other export laws, including but not limited to the U.S. Export Administration\nRegulations (\"EAR\"), (15 C.F.R. Sections 730 through 774), and E.U. Council Regulation (EC) No 428/2009 of 5 May 2009. Further, pursuant to\nSection 740.6 of the EAR, You hereby certifies that, except pursuant to a license granted by the United States Department of Commerce\nBureau of Industry and Security or as otherwise permitted pursuant to a License Exception under the U.S. Export Administration\nRegulations (\"EAR\"), You will not (1) export, re-export or release to a national of a country in Country Groups D:1, E:1 or E:2 any restricted\ntechnology, software, or source code You receive hereunder, or (2) export to Country Groups D:1, E:1 or E:2 the direct product of such\ntechnology or software, if such foreign produced direct product is subject to national security controls as identified on the Commerce\nControl List (currently found in Supplement 1 to Part 774 of EAR). For the most current Country Group listings, or for additional\ninformation about the EAR or Your obligations under those regulations, please refer to the U.S. Bureau of Industry and Security’s website\nat http://www.bis.doc.gov/.\n7. If You are a part of the U.S. Government, then the Specification is provided with \"RESTRICTED RIGHTS\" as set forth in subparagraphs (c)\n(1) and (2) of the Commercial Computer Software-Restricted Rights clause at FAR 52.227-14 or subparagraph (c) (1)(ii) of the Rights in\nTechnical Data and Computer Software clause at DFARS 252.277-7013, as applicable.\n8. This Agreement is governed by the laws of the State of California without regard to its choice of law principles. Any dispute involving it\nmust be brought in a court having jurisdiction of such dispute in Santa Clara County, California, and You waive any defenses and rights\n\"RDNA3\" Instruction Set Architecture\nii of 600\nallowing the dispute to be litigated elsewhere. If any part of this agreement is unenforceable, it will be considered modified to the extent\nnecessary to make it enforceable, and the remainder shall continue in effect. The failure of AMD to enforce any rights granted hereunder\nor to take action against You in the event of any breach hereunder shall not be deemed a waiver by AMD as to subsequent enforcement of\nrights or subsequent actions in the event of future breaches. This Agreement is the entire agreement between You and AMD concerning\nthe Specification; it may be changed only by a written document signed by both You and an authorized representative of AMD.\nDISCLAIMER\nThe information contained herein is for informational purposes only, and is subject to change without notice. This document may\ncontain technical inaccuracies, omissions and typographical errors, and AMD is under no obligation to update or otherwise correct this\ninformation. Advanced Micro Devices, Inc. makes no representations or warranties with respect to the accuracy or completeness of the\ncontents of this document, and assumes no liability of any kind, including the implied warranties of noninfringement, merchantability\nor fitness for particular purposes, with respect to the operation or use of AMD hardware, software or other products described herein.\nNo license, including implied or arising by estoppel, to any intellectual property rights is granted by this document. Terms and\nlimitations applicable to the purchase or use of AMD’s products or technology are as set forth in a signed agreement between the\nparties or in AMD’s Standard Terms and Conditions of Sale.\nAMD, the AMD Arrow logo, and combinations thereof are trademarks of Advanced Micro Devices, Inc. OpenCL is a trademark of Apple\nInc. used by permission by Khronos Group, Inc. DirectX is a registered trademark of Microsoft Corporation in the US and other\njurisdictions. Other product names used in this publication are for identification purposes only and may be trademarks of their\nrespective companies.\n© 2018-2022 Advanced Micro Devices, Inc. All rights reserved.\nAdvanced Micro Devices, Inc.\n2485 Augustine Drive\nSanta Clara, CA, 95054\nwww.amd.com\n\"RDNA3\" Instruction Set Architecture\niii of 600\nContents\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nAbout This Document . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nAudience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nOrganization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nRelated Documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\nAdditional Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.1. Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2. Hardware Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2.1. Work-group Processor (WGP) . . . . . . . . . . . . .","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.amd.com%2Fcontent%2Fdam%2Famd%2Fen%2Fdocuments%2Fradeon-tech-docs%2Finstruction-set-architectures%2Frdna3-shader-instruction-set-architecture-feb-2023_0.pdf","id":"https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf","publishedDate":"2023-08-15T00:00:00.000Z","summary":"This document is a legal agreement between AMD and the recipient of the \"RDNA3\" Instruction Set Architecture Reference Guide. It outlines the terms of use, including that the recipient may only use the specification for planning and designing products to interface with AMD products and to provide feedback. AMD retains all rights to the specification and disclaims all warranties. The document also states that AMD's products are not intended for use in life-sustaining systems and that any feedback provided to AMD can be used without restriction.\n","title":"[PDF] \"RDNA3\" Instruction Set Architecture: Reference Guide - AMD","url":"https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf"}],"status":"completed","toolCallId":"call_a3JjqLSdEeulTBYjgGpwWHsl","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Here are reliable, vendor (AMD) sources that include a Compute Unit (CU) block diagram. Open the PDFs and jump to the noted figures/pages:\n\n- AMD CDNA 3 whitepaper (Instinct MI300 series)\n  - Figure: “Conceptual block diagram of an enhanced compute unit (CU)”\n  - Link: https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/white-papers/amd-cdna-3-white-paper.pdf\n  - Tip: Search within the PDF for “Compute Unit” or “Figure 4” to find the CU diagram showing scheduler, scalar unit, vector/matrix cores, LDS, L1, etc.\n\n- AMD CDNA (MI100) whitepaper\n  - Section: “AMD CDNA Architecture Compute Units”\n  - Link: https://www.amd.com/content/dam/amd/en/documents/instinct-business-docs/white-papers/amd-cdna-white-paper.pdf\n  - Tip: Look for the CU-focused page in that section; it illustrates CU internals and memory (LDS) relationships.\n\n- AMD RDNA architecture whitepaper (gaming, but CU/WGP internals are diagrammed)\n  - Link (TechPowerUp mirror): https://www.techpowerup.com/gpu-specs/docs/amd-rdna-whitepaper.pdf\n  - What to find: “Dual Compute Unit Front End,” “SIMD Execution Units,” and per‑CU memories diagrams (shows scalar unit, SIMD arrays, LDS, caches).\n\n- AMD GPUOpen RDNA slides (clear per‑CU/WGP diagrams)\n  - Link: https://gpuopen.com/download/RDNA_Architecture_public.pdf\n  - Look at slides titled “GCN Compute Units” and “RDNA Workgroup Processors (WGP)” for CU/WGP block views; good for understanding CU vs WGP layout.\n\n- Older GCN CU anatomy (still useful to visualize a CU)\n  - Slides: “The AMD GCN Architecture – A Crash Course” (Layla Mah)\n  - Link: https://www.slideshare.net/DevCentralAMD/gs4106-the-amd-gcn-architecture-a-crash-course-by-layla-mah\n  - Look for the slide titled “GCN Compute Unit” showing scalar unit, 4× SIMD16, LDS, texture/filter/LS units.\n\nIf you want, tell me which family (CDNA MI2xx/MI3xx vs RDNA) you’re targeting and I’ll extract the exact page numbers and paste a cropped CU diagram.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d41e3c87e481a28d60299965e127f20c1380ff1911721e"}},"resumableStreamId":"STREAM:66dfe749-f5bf-46f9-b146-b38b4d7e125d","role":"assistant","status":"done","threadId":"020ae027-8a02-4dc0-9799-45305ca05185","timeToFirstToken":1.094,"tokens":537,"tokensPerSecond":21.962291930800376,"updated_at":1758731821521,"userId":"google:104458561188320708185","id":"j977ss2x5ere0eakhywbp17ws57r709t"},{"_creationTime":1758732660365.6528,"_id":"j976p90jhqnwv1rfk2hc7cedz17r78bn","attachmentIds":[],"content":"in the ECLIP paper ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs\nhttps://arxiv.org/abs/2506.12598\nwhat do they mean by \"Unlike\n Nvidia’s MPS or MIG, which applies spatial partitions at the process\n level, AMD’s CU Masking feature within the ROCm runtime enables\n spatial partitioning at a finer granularity of streams by specifying\n which CUs to execute kernels on. \"","created_at":1758684871844,"messageId":"6473ea8d-4ba9-4a55-d8e0-eb6194b68086","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","updated_at":1758684871844,"userId":"google:104458561188320708185","id":"j976p90jhqnwv1rfk2hc7cedz17r78bn","providerMetadata":{}},{"_creationTime":1758732660365.653,"_id":"j975b5b2byfrhd02hs3789eq297r744g","attachmentIds":[],"byok":false,"content":"They’re contrasting what is being partitioned and at what control granularity:\n\n- NVIDIA MPS/MIG (process-level spatial partitioning)\n  - MIG (Multi-Instance GPU) carves a GPU into fixed “slices” (sets of SMs, memory slices, etc.). Each slice is then bound to a process. All CUDA work launched by that process runs only on that slice. Changing the partitioning means reconfiguring MIG instances, which is relatively heavyweight, and the unit of control is the process.\n  - MPS allows multiple processes to share an SM pool with lower context-switch overhead, but you still don’t select specific SMs per kernel/stream; isolation and partitioning decisions are effectively per process or global, not per individual stream.\n\n- AMD ROCm CU Masking (stream-level spatial partitioning)\n  - On AMD GPUs, “compute units” (CUs) are the execution blocks roughly analogous to NVIDIA SMs.\n  - ROCm exposes a “CU Mask” you can attach to a stream/queue that says “kernels launched on this stream may execute only on these specific CUs.”\n  - Because the mask is applied at the stream (command queue) level, you can:\n    - Partition the GPU among multiple co-located workloads at finer granularity than processes.\n    - Change which CUs a particular subset of kernels (those launched on a given stream) can use without reconfiguring the whole GPU or rebinding processes.\n    - Even within a single process, assign different kernels or model stages to different CU subsets by launching them on different masked streams.\n\nIn short: NVIDIA’s common mechanisms (MIG/MPS) assign hardware slices or sharing policies to entire processes, while AMD’s CU Masking lets you pin kernels to specific CUs via per-stream masks. That finer granularity enables ECLIP to do “kernel-wise” or “stage-wise” resource allocation and quickly reshuffle CU assignments with low overhead, rather than tearing down and recreating process-level partitions.","created_at":1758684871845,"messageId":"938f7c71-c592-4767-1c9e-8d0338120953","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":["arxiv.org","rocm.docs.amd.com","docs.amd.com","nvidia.com","developer.nvidia.com"],"queries":["ECLIP Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs paper 2506.12598 CU Masking ROCm streams explanation","AMD CU Masking ROCm stream granularity vs Nvidia MPS MIG process level spatial partitioning explanation"]},"result":[{"content":"# Electrical Engineering and Systems Science > Systems and Control\n\n**arXiv:2506.12598** (eess)\n\n\\[Submitted on 14 Jun 2025\\]\n\n# Title:ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs\n\nAuthors: [Ryan Quach](https://arxiv.org/search/eess?searchtype=author&query=Quach,+R), [Yidi Wang](https://arxiv.org/search/eess?searchtype=author&query=Wang,+Y), [Ali Jahanshahi](https://arxiv.org/search/eess?searchtype=author&query=Jahanshahi,+A), [Daniel Wong](https://arxiv.org/search/eess?searchtype=author&query=Wong,+D), [Hyoseung Kim](https://arxiv.org/search/eess?searchtype=author&query=Kim,+H)\n\nView a PDF of the paper titled ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs, by Ryan Quach and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2506.12598) [HTML (experimental)](https://arxiv.org/html/2506.12598v1)\n\n> Abstract:As AI inference becomes mainstream, research has begun to focus on improving the energy consumption of inference servers. Inference kernels commonly underutilize a GPU's compute resources and waste power from idling components. To improve utilization and energy efficiency, multiple models can co-locate and share the GPU. However, typical GPU spatial partitioning techniques often experience significant overheads when reconfiguring spatial partitions, which can waste additional energy through repartitioning overheads or non-optimal partition configurations. In this paper, we present ECLIP, a framework to enable low-overhead energy-efficient kernel-wise resource partitioning between co-located inference kernels. ECLIP minimizes repartitioning overheads by pre-allocating pools of CU masked streams and assigns optimal CU assignments to groups of kernels through our resource allocation optimizer. Overall, ECLIP achieves an average of 13% improvement to throughput and 25% improvement to energy efficiency.\n\n| | |\n| --- | --- |\n| Comments: | Accepted to ISLPED 2025 |\n| Subjects: | Systems and Control (eess.SY) |\n| Cite as: | [arXiv:2506.12598](https://arxiv.org/abs/2506.12598) \\[eess.SY\\] |\n| (or [arXiv:2506.12598v1](https://arxiv.org/abs/2506.12598v1) \\[eess.SY\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2506.12598](https://doi.org/10.48550/arXiv.2506.12598) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Ryan Quach \\[ [view email](https://arxiv.org/show-email/05675750/2506.12598)\\] **\\[v1\\]**\nSat, 14 Jun 2025 18:34:48 UTC (458 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs, by Ryan Quach and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2506.12598)\n- [HTML (experimental)](https://arxiv.org/html/2506.12598v1)\n- [TeX Source](https://arxiv.org/src/2506.12598)\n- [Other Formats](https://arxiv.org/format/2506.12598)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\neess.SY\n\n[< prev](https://arxiv.org/prevnext?id=2506.12598&function=prev&context=eess.SY)  \\|  [next >](https://arxiv.org/prevnext?id=2506.12598&function=next&context=eess.SY)\n\n[new](https://arxiv.org/list/eess.SY/new) \\| [recent](https://arxiv.org/list/eess.SY/recent) \\| [2025-06](https://arxiv.org/list/eess.SY/2025-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2506.12598?context=cs) [cs.SY](https://arxiv.org/abs/2506.12598?context=cs.SY) [eess](https://arxiv.org/abs/2506.12598?context=eess)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2506.12598)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2506.12598)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2506.12598)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2506.12598) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2506.12598","id":"https://arxiv.org/abs/2506.12598","publishedDate":"2025-06-14T00:00:00.000Z","summary":"This webpage is an arXiv entry for the paper \"ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs\" (arXiv:2506.12598). The paper, submitted on June 14, 2025, by Ryan Quach and others, presents ECLIP, a framework designed to improve the energy efficiency of AI inference servers by enabling low-overhead kernel-wise resource partitioning on GPUs. ECLIP achieves an average of 13% improvement to throughput and 25% improvement to energy efficiency. The paper was accepted to ISLPED 2025.\n","title":"ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs","url":"https://arxiv.org/abs/2506.12598"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2508.18556","id":"https://arxiv.org/abs/2508.18556","publishedDate":"2025-08-25T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"Managing Multi Instance GPUs for High Throughput and Energy Savings","url":"https://arxiv.org/abs/2508.18556"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2109.01611.pdf","id":"https://arxiv.org/pdf/2109.01611.pdf","publishedDate":"2023-10-26T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"Multi-model Machine Learning Inference Serving with GPU Spatial Partitioning","url":"https://arxiv.org/pdf/2109.01611.pdf"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2409.06646v1","id":"https://arxiv.org/html/2409.06646v1","publishedDate":"2024-07-12T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"Optimal Workload Placement on Multi-Instance GPUs","url":"https://arxiv.org/html/2409.06646v1"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2509.17357","id":"https://arxiv.org/abs/2509.17357","publishedDate":"2025-09-22T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via Partially Disaggregated Prefill","url":"https://arxiv.org/abs/2509.17357"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2105.10312v1.pdf","id":"https://arxiv.org/pdf/2105.10312v1.pdf","publishedDate":"2021-05-21T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"Contention-Aware GPU Partitioning and Task-to-Partition Allocation for Real-Time Workloads","url":"https://arxiv.org/pdf/2105.10312v1.pdf"}],"status":"completed","toolCallId":"call_Ugk0Gyx47banwSJHpfUM4Otj","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"They’re contrasting what is being partitioned and at what control granularity:\n\n- NVIDIA MPS/MIG (process-level spatial partitioning)\n  - MIG (Multi-Instance GPU) carves a GPU into fixed “slices” (sets of SMs, memory slices, etc.). Each slice is then bound to a process. All CUDA work launched by that process runs only on that slice. Changing the partitioning means reconfiguring MIG instances, which is relatively heavyweight, and the unit of control is the process.\n  - MPS allows multiple processes to share an SM pool with lower context-switch overhead, but you still don’t select specific SMs per kernel/stream; isolation and partitioning decisions are effectively per process or global, not per individual stream.\n\n- AMD ROCm CU Masking (stream-level spatial partitioning)\n  - On AMD GPUs, “compute units” (CUs) are the execution blocks roughly analogous to NVIDIA SMs.\n  - ROCm exposes a “CU Mask” you can attach to a stream/queue that says “kernels launched on this stream may execute only on these specific CUs.”\n  - Because the mask is applied at the stream (command queue) level, you can:\n    - Partition the GPU among multiple co-located workloads at finer granularity than processes.\n    - Change which CUs a particular subset of kernels (those launched on a given stream) can use without reconfiguring the whole GPU or rebinding processes.\n    - Even within a single process, assign different kernels or model stages to different CU subsets by launching them on different masked streams.\n\nIn short: NVIDIA’s common mechanisms (MIG/MPS) assign hardware slices or sharing policies to entire processes, while AMD’s CU Masking lets you pin kernels to specific CUs via per-stream masks. That finer granularity enables ECLIP to do “kernel-wise” or “stage-wise” resource allocation and quickly reshuffle CU assignments with low overhead, rather than tearing down and recreating process-level partitions.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d366d08d2c81a19ceab23ad73c351408f28b28fc7b22db"}},"resumableStreamId":"STREAM:3c9efb14-45c8-4768-9127-ebaedd0b9aa5","role":"assistant","status":"done","threadId":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","timeToFirstToken":0.769,"tokens":415,"tokensPerSecond":24.031501534541665,"updated_at":1758684871845,"userId":"google:104458561188320708185","id":"j975b5b2byfrhd02hs3789eq297r744g"},{"_creationTime":1758732690851.129,"_id":"j978n2rf9m51exmqg4q0e4dk8h7r63ym","attachmentIds":[],"content":"when you mentioned a process, do you mean a kernel or can multiple kernels from the same model run in parallel for NVIDIA  MIG","created_at":1758732690851,"messageId":"e0e6c51f-ce7d-4b03-b4f4-f929fa90cbee","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","updated_at":1758732690851,"userId":"google:104458561188320708185","id":"j978n2rf9m51exmqg4q0e4dk8h7r63ym","providerMetadata":{}},{"_creationTime":1758732690851.1292,"_id":"j97fzfpe618anek7np28e3hwax7r7rdk","attachmentIds":[],"byok":false,"content":"By “process” I mean an OS process (e.g., a CUDA context owned by a Unix process), not a single kernel.\n\nHow this relates to MIG and kernel parallelism:\n\n- NVIDIA MIG\n  - A GPU is carved into one or more MIG instances (each with a fixed set of SMs, memory slices, etc.).\n  - Each MIG instance is exposed as a separate logical GPU device.\n  - A process attaches to one MIG device; all kernels launched by that process run only on that instance.\n  - Within a MIG instance, multiple kernels from the same process can overlap/parallelize if:\n    - There are sufficient resources (SMs, registers, shared memory, etc.).\n    - You use multiple CUDA streams or concurrent kernels are enabled.\n  - Kernels from different processes run on different MIG instances (true spatial isolation across processes), unless you run NVIDIA MPS within a MIG slice to share that slice among multiple processes.\n\n- NVIDIA MPS (without MIG)\n  - Multiple processes can submit work concurrently to the same physical GPU with lower context-switch overhead.\n  - You don’t control which specific SMs a given kernel uses; the scheduler shares the SM pool among processes.\n  - Parallelism across kernels can occur both within a process (multiple streams) and across processes.\n\nSo: “process” ≠ “kernel.” With MIG, multiple kernels from the same model/process can run concurrently inside the same MIG slice (subject to resources), but they cannot spill into other slices. The granularity of assignment is per process (device/context), not per kernel/stream, which is what the paper contrasts with AMD’s per-stream CU masking.","created_at":1758732690852,"messageId":"b84cad9d-0dcd-4122-a9a0-7958179daafc","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"By “process” I mean an OS process (e.g., a CUDA context owned by a Unix process), not a single kernel.\n\nHow this relates to MIG and kernel parallelism:\n\n- NVIDIA MIG\n  - A GPU is carved into one or more MIG instances (each with a fixed set of SMs, memory slices, etc.).\n  - Each MIG instance is exposed as a separate logical GPU device.\n  - A process attaches to one MIG device; all kernels launched by that process run only on that instance.\n  - Within a MIG instance, multiple kernels from the same process can overlap/parallelize if:\n    - There are sufficient resources (SMs, registers, shared memory, etc.).\n    - You use multiple CUDA streams or concurrent kernels are enabled.\n  - Kernels from different processes run on different MIG instances (true spatial isolation across processes), unless you run NVIDIA MPS within a MIG slice to share that slice among multiple processes.\n\n- NVIDIA MPS (without MIG)\n  - Multiple processes can submit work concurrently to the same physical GPU with lower context-switch overhead.\n  - You don’t control which specific SMs a given kernel uses; the scheduler shares the SM pool among processes.\n  - Parallelism across kernels can occur both within a process (multiple streams) and across processes.\n\nSo: “process” ≠ “kernel.” With MIG, multiple kernels from the same model/process can run concurrently inside the same MIG slice (subject to resources), but they cannot spill into other slices. The granularity of assignment is per process (device/context), not per kernel/stream, which is what the paper contrasts with AMD’s per-stream CU masking.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d4219387a881a19db27336e18831c20dfde4614987f9d4"}},"resumableStreamId":"STREAM:83f721f9-5dc1-4632-93cd-cea8187ceccb","role":"assistant","status":"done","threadId":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","timeToFirstToken":1.158,"tokens":341,"tokensPerSecond":64.25475786696816,"updated_at":1758732690852,"userId":"google:104458561188320708185","id":"j97fzfpe618anek7np28e3hwax7r7rdk"},{"_creationTime":1758732802572.7908,"_id":"j976cjmnpjrc52rvs8dx93rszn7r68dn","attachmentIds":[],"content":"can one process use more than one MIG?","created_at":1758732802573,"messageId":"10e1cf20-ed03-43b5-9eb7-95e7a68dbada","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","updated_at":1758732802573,"userId":"google:104458561188320708185","id":"j976cjmnpjrc52rvs8dx93rszn7r68dn","providerMetadata":{}},{"_creationTime":1758732802572.791,"_id":"j979ntgwdtab8jzme7bgj75vm97r649j","attachmentIds":[],"byok":false,"content":"Short answer: not in the typical/official workflow. A single CUDA context (the usual “one process → one CUDA context”) is bound to one device, and each MIG instance appears as a separate CUDA device. So a normal process uses exactly one MIG instance at a time.\n\nDetails and options:\n- Default behavior: Each MIG slice is a separate logical GPU (e.g., CUDA device 0, 1, …). A single CUDA context can’t span multiple devices, so your process doesn’t “use multiple MIGs” with one context.\n- Using multiple devices from one process: A single process can create multiple CUDA contexts, one per device, and manage them explicitly (multi-GPU programming). In that case, the process can open two or more MIG instances and orchestrate work across them. This is similar to using two different physical GPUs from one process; you’ll handle:\n  - Multiple CUDA contexts (one per MIG device)\n  - Memory allocations per device and explicit transfers between them (no unified SM space)\n  - Cross-device synchronization if needed\n- With MPS: MPS doesn’t merge MIG devices. It can let multiple processes share one MIG slice, but it won’t let one context span slices.\n\nSo:\n- One context → one MIG device.\n- One process can attach to multiple MIG devices by creating multiple contexts and targeting each device explicitly.\n- You cannot have a single kernel automatically use SMs from multiple MIG instances.","created_at":1758732802574,"messageId":"8409f862-f5a6-405f-af86-5c982d31eb74","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Short answer: not in the typical/official workflow. A single CUDA context (the usual “one process → one CUDA context”) is bound to one device, and each MIG instance appears as a separate CUDA device. So a normal process uses exactly one MIG instance at a time.\n\nDetails and options:\n- Default behavior: Each MIG slice is a separate logical GPU (e.g., CUDA device 0, 1, …). A single CUDA context can’t span multiple devices, so your process doesn’t “use multiple MIGs” with one context.\n- Using multiple devices from one process: A single process can create multiple CUDA contexts, one per device, and manage them explicitly (multi-GPU programming). In that case, the process can open two or more MIG instances and orchestrate work across them. This is similar to using two different physical GPUs from one process; you’ll handle:\n  - Multiple CUDA contexts (one per MIG device)\n  - Memory allocations per device and explicit transfers between them (no unified SM space)\n  - Cross-device synchronization if needed\n- With MPS: MPS doesn’t merge MIG devices. It can let multiple processes share one MIG slice, but it won’t let one context span slices.\n\nSo:\n- One context → one MIG device.\n- One process can attach to multiple MIG devices by creating multiple contexts and targeting each device explicitly.\n- You cannot have a single kernel automatically use SMs from multiple MIG instances.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d422034580819fa001aab70a3117930d7a3627c62a2470"}},"resumableStreamId":"STREAM:3e0b5750-6761-46d6-aafe-ac36ccc43ba6","role":"assistant","status":"done","threadId":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","timeToFirstToken":0.962,"tokens":299,"tokensPerSecond":45.39243965386367,"updated_at":1758732802574,"userId":"google:104458561188320708185","id":"j979ntgwdtab8jzme7bgj75vm97r649j"},{"_creationTime":1758732818201.4272,"_id":"j97ecfy9xc92vewkjqeq5rnd4x7r7cp3","attachmentIds":[],"content":"Is the size of MIG fixed?","created_at":1758732818201,"messageId":"4a6ba69e-6fac-40b7-9de1-a173ce690511","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","updated_at":1758732818201,"userId":"google:104458561188320708185","id":"j97ecfy9xc92vewkjqeq5rnd4x7r7cp3","providerMetadata":{}},{"_creationTime":1758732818201.4275,"_id":"j977018n6hqgxfwbhr4079abhh7r6k4j","attachmentIds":[],"byok":false,"content":"MIG sizes are configurable, but only in a set of supported “profiles” per GPU. You don’t get arbitrary sizes.\n\nKey points:\n- Each MIG-capable GPU (e.g., A100, H100, etc.) supports a fixed menu of instance profiles that define how many SMs (or GPCs/TPCs), memory slices, L2, and bandwidth are assigned. Examples on A100 include profiles like 1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb (numbers vary by memory capacity and SKU). H100 has its own profile set.\n- You can create multiple MIG instances whose profiles fit within the GPU’s total resources, up to the device-specific maximum number of instances.\n- You can reconfigure (destroy/recreate) MIG instances to change sizes, but this is a disruptive operation for running workloads on those instances.\n- A single instance’s size is fixed to the chosen profile while it exists; it doesn’t auto-expand or shrink at runtime.\n\nSo, MIG is flexible within predefined discrete sizes, not dynamically resizable or arbitrary.","created_at":1758732818202,"messageId":"98b38517-0807-4473-9366-d92f225edd50","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"MIG sizes are configurable, but only in a set of supported “profiles” per GPU. You don’t get arbitrary sizes.\n\nKey points:\n- Each MIG-capable GPU (e.g., A100, H100, etc.) supports a fixed menu of instance profiles that define how many SMs (or GPCs/TPCs), memory slices, L2, and bandwidth are assigned. Examples on A100 include profiles like 1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb (numbers vary by memory capacity and SKU). H100 has its own profile set.\n- You can create multiple MIG instances whose profiles fit within the GPU’s total resources, up to the device-specific maximum number of instances.\n- You can reconfigure (destroy/recreate) MIG instances to change sizes, but this is a disruptive operation for running workloads on those instances.\n- A single instance’s size is fixed to the chosen profile while it exists; it doesn’t auto-expand or shrink at runtime.\n\nSo, MIG is flexible within predefined discrete sizes, not dynamically resizable or arbitrary.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d42212bdfc8190b7730dd9b61cb639035e1ceba17b3968"}},"resumableStreamId":"STREAM:bf746e0f-09a8-4b11-8e22-fdbcf9156758","role":"assistant","status":"done","threadId":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","timeToFirstToken":1.576,"tokens":243,"tokensPerSecond":42.23883191378412,"updated_at":1758732818202,"userId":"google:104458561188320708185","id":"j977018n6hqgxfwbhr4079abhh7r6k4j"},{"_creationTime":1758732862489.8213,"_id":"j97ca31wxnts0x0hzkfd1q4bd17r7bsq","attachmentIds":[],"content":"now explain to me MPS","created_at":1758732862490,"messageId":"f052dc5b-00cb-40f0-a519-f465cd8f3ef9","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","updated_at":1758732862490,"userId":"google:104458561188320708185","id":"j97ca31wxnts0x0hzkfd1q4bd17r7bsq","providerMetadata":{}},{"_creationTime":1758732862489.8215,"_id":"j977zmpxqpn6ymjbh0ajebcpzh7r7nqy","attachmentIds":[],"byok":false,"content":"Here’s a concise, practical overview of NVIDIA MPS (Multi-Process Service):\n\nWhat MPS is\n- A runtime service that lets multiple processes submit CUDA work concurrently to the same GPU with much lower context-switch overhead and better hardware utilization than naïve time-slicing.\n- It multiplexes multiple client processes through a single shared server context on the GPU, enabling finer-grained interleaving of kernels and better overlap.\n\nWhat problems it solves\n- Reduces context-switch overhead when many small kernels come from different processes.\n- Improves SM occupancy by allowing kernels from different processes to co-reside on SMs when resources permit.\n- Smooths performance for multi-tenant or multi-job environments without having to partition the GPU spatially (as with MIG).\n\nHow it works conceptually\n- Without MPS: each process has its own CUDA context; the driver schedules kernels from different contexts more coarsely, often leading to serialization and expensive context switches.\n- With MPS: client processes connect to an MPS server. The server owns a single GPU context and streams from different clients are funneled into it. The GPU scheduler can then place thread blocks from different clients’ kernels on the same SMs concurrently if resources fit.\n\nGranularity and control\n- Scheduling granularity: block/warp level within SMs. You cannot pin specific SMs to a process or kernel.\n- Isolation: No hard spatial isolation. Memory address spaces are still separate per client process, but execution resources (SMs, caches, memory bandwidth) are shared.\n- Fairness/limits: You can set per-client active thread block limits or use CUDA_VISIBLE_DEVICES to control device access. Newer MPS versions also support per-client “active thread percentage” caps on some architectures, but this is still not strict isolation like MIG.\n\nMPS vs MIG\n- MPS: sharing on one physical GPU; soft partitioning; best for maximizing utilization and reducing latency when tenants can share. Multiple processes can run concurrently on the same SM pool.\n- MIG: hard partitions; each instance appears as a separate GPU device; strong isolation of SMs/L2/DRAM slices; no sharing across instances.\n- You can run MPS inside a single MIG instance to share that slice among multiple processes.\n\nConcurrency behavior\n- Multiple processes’ kernels can overlap if combined resource usage fits (registers, shared memory, blocks per SM).\n- Within a single process, you still use CUDA streams for overlap; MPS extends similar overlap across processes.\n\nSetup and usage (typical)\n- Start the MPS control daemon, then run client processes with MPS enabled. Environment variables like CUDA_MPS_ACTIVE_THREAD_PERCENTAGE can tune per-client limits. Each process still uses regular CUDA APIs; MPS is mostly transparent.\n\nWhen to use\n- Many small/short kernels from multiple processes.\n- Inference serving (multi-model, multi-tenant) when you don’t need strict isolation.\n- Batch environments where you want better packing and throughput without reconfiguring MIG.\n\nWhen not ideal\n- Strong isolation or guaranteed QoS is required → prefer MIG.\n- Highly memory-bandwidth-bound jobs where interference harms performance.\n- Need to pin work to specific SMs or do per-stream spatial partitioning → not supported by MPS.\n\nLimitations and caveats\n- No per-kernel/stream SM affinity.\n- Potential interference in caches and memory bandwidth.\n- Some features (e.g., debugging/profiling) can be trickier under MPS.\n- Not all CUDA features/architectures behave identically; consult your GPU’s MPS support notes.\n\nBottom line\nMPS is a lightweight, software-level sharing mechanism that lets multiple processes efficiently co-execute on the same GPU by pooling them into one context, boosting utilization without the rigidity of MIG—but it does not provide hard isolation or spatial control.","created_at":1758732862491,"messageId":"b903eb23-5a0f-4543-87bd-b7ebc661e7e5","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Here’s a concise, practical overview of NVIDIA MPS (Multi-Process Service):\n\nWhat MPS is\n- A runtime service that lets multiple processes submit CUDA work concurrently to the same GPU with much lower context-switch overhead and better hardware utilization than naïve time-slicing.\n- It multiplexes multiple client processes through a single shared server context on the GPU, enabling finer-grained interleaving of kernels and better overlap.\n\nWhat problems it solves\n- Reduces context-switch overhead when many small kernels come from different processes.\n- Improves SM occupancy by allowing kernels from different processes to co-reside on SMs when resources permit.\n- Smooths performance for multi-tenant or multi-job environments without having to partition the GPU spatially (as with MIG).\n\nHow it works conceptually\n- Without MPS: each process has its own CUDA context; the driver schedules kernels from different contexts more coarsely, often leading to serialization and expensive context switches.\n- With MPS: client processes connect to an MPS server. The server owns a single GPU context and streams from different clients are funneled into it. The GPU scheduler can then place thread blocks from different clients’ kernels on the same SMs concurrently if resources fit.\n\nGranularity and control\n- Scheduling granularity: block/warp level within SMs. You cannot pin specific SMs to a process or kernel.\n- Isolation: No hard spatial isolation. Memory address spaces are still separate per client process, but execution resources (SMs, caches, memory bandwidth) are shared.\n- Fairness/limits: You can set per-client active thread block limits or use CUDA_VISIBLE_DEVICES to control device access. Newer MPS versions also support per-client “active thread percentage” caps on some architectures, but this is still not strict isolation like MIG.\n\nMPS vs MIG\n- MPS: sharing on one physical GPU; soft partitioning; best for maximizing utilization and reducing latency when tenants can share. Multiple processes can run concurrently on the same SM pool.\n- MIG: hard partitions; each instance appears as a separate GPU device; strong isolation of SMs/L2/DRAM slices; no sharing across instances.\n- You can run MPS inside a single MIG instance to share that slice among multiple processes.\n\nConcurrency behavior\n- Multiple processes’ kernels can overlap if combined resource usage fits (registers, shared memory, blocks per SM).\n- Within a single process, you still use CUDA streams for overlap; MPS extends similar overlap across processes.\n\nSetup and usage (typical)\n- Start the MPS control daemon, then run client processes with MPS enabled. Environment variables like CUDA_MPS_ACTIVE_THREAD_PERCENTAGE can tune per-client limits. Each process still uses regular CUDA APIs; MPS is mostly transparent.\n\nWhen to use\n- Many small/short kernels from multiple processes.\n- Inference serving (multi-model, multi-tenant) when you don’t need strict isolation.\n- Batch environments where you want better packing and throughput without reconfiguring MIG.\n\nWhen not ideal\n- Strong isolation or guaranteed QoS is required → prefer MIG.\n- Highly memory-bandwidth-bound jobs where interference harms performance.\n- Need to pin work to specific SMs or do per-stream spatial partitioning → not supported by MPS.\n\nLimitations and caveats\n- No per-kernel/stream SM affinity.\n- Potential interference in caches and memory bandwidth.\n- Some features (e.g., debugging/profiling) can be trickier under MPS.\n- Not all CUDA features/architectures behave identically; consult your GPU’s MPS support notes.\n\nBottom line\nMPS is a lightweight, software-level sharing mechanism that lets multiple processes efficiently co-execute on the same GPU by pooling them into one context, boosting utilization without the rigidity of MIG—but it does not provide hard isolation or spatial control.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d4223f041081a0acbde80115ba25ee04d27064b1984fa2"}},"resumableStreamId":"STREAM:9d0f3b3d-d3d1-4192-b218-419d0803e1cf","role":"assistant","status":"done","threadId":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","timeToFirstToken":0.925,"tokens":785,"tokensPerSecond":46.77352082464398,"updated_at":1758732862491,"userId":"google:104458561188320708185","id":"j977zmpxqpn6ymjbh0ajebcpzh7r7nqy"},{"_creationTime":1758733286895.5007,"_id":"j9737m2r6zynj23ptj25knm6r17r7scb","attachmentIds":[],"content":"In the paper, how does AMD demonstrate more granularity of control?","created_at":1758733286895,"messageId":"3caecb17-1f22-4072-b141-144fbd0e493d","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","updated_at":1758733286895,"userId":"google:104458561188320708185","id":"j9737m2r6zynj23ptj25knm6r17r7scb","providerMetadata":{}},{"_creationTime":1758733286895.501,"_id":"j9716cad1t2krdfb6be8c4kh757r70hm","attachmentIds":[],"byok":false,"content":"They show finer-grained spatial control by using ROCm’s CU masking at the stream (queue) level, and then exploit that to do per-stream, per-kernel stage partitioning and fast reconfiguration. Concretely:\n\n- Unit of control: They attach a CU mask to a HIP stream/queue so that “kernels launched on this stream run only on these specified CUs.” That’s finer than process/device-level (MIG/MPS). Multiple streams in the same process can each have different CU subsets.\n\n- Intra-process partitioning: Within a single model/process, they launch different kernels or pipeline stages on different masked streams to pin stages to disjoint CU sets, enabling concurrent co-location and stage-wise resource allocation.\n\n- Dynamic resizing/reassignment: Because the mask is just a stream attribute, they can quickly change CU assignments (e.g., expand/shrink a tenant’s CU set) without tearing down contexts or reconfiguring device partitions.\n\n- Selective throttling: They can give latency-critical streams larger CU masks and background streams smaller ones, achieving QoS-like control without hard partitions.\n\n- Kernel-level isolation experiments: They demonstrate co-running models where specific kernels (e.g., attention vs. MLP) are bound to particular CU groups, measuring throughput/latency/energy effects, something not directly achievable with per-process mechanisms.\n\nIn short, AMD’s per-stream CU masking lets them pin and reshuffle execution resources at the granularity of streams/kernels inside a single process, whereas NVIDIA’s common mechanisms (MIG/MPS) operate at process/device level and don’t let you choose specific SMs per stream.","created_at":1758733286896,"messageId":"37390d82-781c-445a-ad1a-84cc8a66f7ae","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":["arxiv.org","rocm.docs.amd.com","docs.amd.com","github.com"],"queries":["ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs CU Masking streams granularity AMD ROCm paper 2506.12598","ROCm CU Masking stream-level control set_cu_mask hipStreamSetAttribute HIP attribute cu_mask"]},"result":[{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2506.12598","id":"https://arxiv.org/abs/2506.12598","publishedDate":"2025-06-14T00:00:00.000Z","summary":"This webpage is an academic paper on energy-efficient and practical co-location of ML inference on spatially partitioned GPUs. It acknowledges support from the Simons Foundation and member institutions.\n","title":"ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs","url":"https://arxiv.org/abs/2506.12598"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2508.18556","id":"https://arxiv.org/abs/2508.18556","publishedDate":"2025-08-25T00:00:00.000Z","summary":"This webpage discusses managing Multi-Instance GPUs (MIGs) for improved throughput and energy savings. It acknowledges support from the Simons Foundation and member institutions.\n","title":"Managing Multi Instance GPUs for High Throughput and Energy Savings","url":"https://arxiv.org/abs/2508.18556"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2501.17752v1","id":"https://arxiv.org/html/2501.17752v1","publishedDate":"2024-12-05T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"On the Partitioning of GPU Power among Multi-Instances","url":"https://arxiv.org/html/2501.17752v1"},{"content":"[We gratefully acknowledge support from\\\n\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2304.11745.pdf","id":"https://arxiv.org/pdf/2304.11745.pdf","publishedDate":"2023-10-26T00:00:00.000Z","summary":"This webpage is a PDF titled \"GACER: Granularity-Aware ConcurrEncy Regulation for Multi-Tenant Deep Learning\" and acknowledges support from the Simons Foundation and member institutions.\n","title":"GACER: Granularity-Aware ConcurrEncy Regulation for Multi-Tenant Deep Learning","url":"https://arxiv.org/pdf/2304.11745.pdf"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2409.06646v1","id":"https://arxiv.org/html/2409.06646v1","publishedDate":"2024-07-12T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"Optimal Workload Placement on Multi-Instance GPUs","url":"https://arxiv.org/html/2409.06646v1"},{"content":"Exploring GPU Stream-Aware Message Passing using Triggered Operations Mark Pagel\n\n\nNaveen Namashivayam \nNick Radcliffe nick.radcliffe@hpe.com \nKrishna Kandalla krishnachaitanya.kandalla@hpe.com \nLarry Kaplan larry.kaplan@hpe.com \nTrey White trey.white@hpe.com \nEnterprise, USAHewlett Packard \n\nHewlett Packard Enterprise\nUSA\n\n\nHewlett Packard Enterprise\nUSA\n\n\nHewlett Packard Enterprise\nUSA\n\n\nHewlett Packard Enterprise\nUSA\n\n\nHewlett Packard Enterprise\nUSA\n\nExploring GPU Stream-Aware Message Passing using Triggered Operations Mark Pagel\nIndex Terms-heterogeneous supercomputing systemsCPUGPUMPIGPU-NIC AsyncGPU StreamsGPU Control Pro- cessorsControl PathData Path\nModern heterogeneous supercomputing systems are comprised of compute blades that offer CPUs and GPUs. On such systems, it is essential to move data efficiently between these different compute engines across a high-speed network. While current generation scientific applications and systems software stacks are GPU-aware, CPU threads are still required to orchestrate data moving communication operations and interprocess synchronization operations.A new GPU stream-aware MPI communication strategy called stream-triggered (ST) communication is explored to allow offloading both computation and communication control paths to the GPU. The proposed ST communication strategy is implemented on HPE Slingshot Interconnects over a new proprietary HPE Slingshot NIC (Slingshot 11) using the supported triggered operations feature. Performance of the proposed new communication strategy is evaluated using a microbenchmark kernel called Faces, based on the nearest-neighbor communication pattern in the CORAL-2 Nekbone benchmark, over a heterogeneous node architecture consisting of AMD CPUs and GPUs.\n Fig. 1\n. Illustrating sequence of events on a typical GPU-aware parallel application that relies on MPI for inter-process communication and synchronization operations.\n\nA GPU stream [12] is a queue of device operations. GPU compute kernel concurrency is achieved through creating multiple concurrent streams. Operations issued on a stream get executed in the order in which these operations were issued. Also, the execution of these operations is asynchronous with respect to operations in other streams. A new GPU streamaware MPI communication strategy called stream-triggered (ST) communication is proposed and explored to offload both the computation and communication control paths to the GPU.\n\nThe new proposed communication scheme allows the CPU to create network command descriptors with deferred execution semantics and append them to the NIC command queue. These command descriptors have special attributes that allow them to get triggered [16] at a later point in time when certain conditions are reached. In addition, the CPU also creates This work is planned for possible publication to the IEEE. Copyright may be transferred without notice, after which this version may no longer accessible.\n\ncontrol operations and appends them to the GPU stream. These operations will be executed by the GPU control processor in sequential order, relative to other operations enqueued in the GPU stream. When these control operations are executed by the GPU control processor, they act as triggers that initiate the execution of the previously appended network command descriptors in the NIC's command queue. This approach also allows the GPU control processor to synchronize with the NIC to determine the successful completion of communication operations. This synchronization step involves the use of hardware counters in the NIC. Thus, ST tries to minimize the need for synchronization between the CPU and GPU.\n\n\nA. Contributions of This Work\n\nThe following are the major contributions.\n\n1) Propose a GPU stream-aware MPI communication strategy to offload both computation and communication control paths to the GPU; 2) Implement the proposed communication strategy on HPE Slingshot Interconnects exploiting the triggered operations feature [16] available in the new HPE Slingshot NIC (Slingshot 11) [14,10,18]; 3) To demonstrate the effectiveness of the new proposed communication strategy, create a microbenchmark kernel called Faces based on the nearest-neighbor communication pattern in the CORAL-2 [1] Nekbone benchmark [6]; and 4) Experiment with proposed solutions on a heterogeneous system architecture with support for AMD-based CPU and GPU processors.\n\n\nII. BACKGROUND\n\nThis section provides an overview of the ST communication strategy in MPI, along with the deferred execution features provided by the HPE Slingshot NIC (Slingshot 11) to support the proposed ST interface.\n\n\nA. MPI Control and Data Paths\n\nMPI communication operations in GPU-aware applications are typically comprised of control paths and data paths. Control paths correspond to coordination operations that occur between the application process running on the CPU, the control processor on the GPU device, application compute kernels running on the GPU device, and the NIC. Data paths refer to those operations that involve moving data between CPU-attached and GPU-attached memory regions. These data movement operations can occur within the same compute node or between different compute nodes across a high-speed network. The data paths are typically handled by the NIC (for inter-node data transfer operations) and the GPU DMA engines (for intra-node peer-to-peer data transfer operations).\n\nAs detailed in Fig. 1, applications typically experience expensive synchronization points, and an application process running on the CPU is closely involved in the progress of control paths. Fig. 2 includes a high-level description of the proposed new ST communication strategy. This strategy enables a GPU-aware application to offload control paths to underlying implementation and hardware components.\n\n\nB. Stream Triggered Communication Overview\n\nA parallel application using the ST strategy continues to manage compute kernels on the GPU via existing mechanisms. In addition, the ST strategy allows an application process running on the CPU to define a set of GPU stream based MPI communication operations. The new proposed MPI operations are detailed in Section III.\n\nThese communication operations can be scheduled for execution at a later point in time. More importantly, in addition to offering a deferred execution model, the ST strategy enables the GPU control processor (GPU CP) to be closely involved in the control paths associated with MPI communication operations. Along with managing compute kernels defined by the application, the GPU CP coordinates with the NIC to manage the control paths of MPI communication operations. This design minimizes the need for the application process running on the CPU to drive the control paths of MPI communication operations.\n\nAs shown in the Fig. 2, an application process running on the CPU enqueues a GPU kernel K1 to the GPU stream, b triggered ST-based MPI operations to the NIC, c the corresponding trigger operations to the GPU stream, and d GPU kernel K2 to the same stream. In Fig. 2, the enqueue operations from the CPU are represented via dashed arrows, and the actual executions of various operations from the GPU Control Processor are represented as solid arrows. tear-down operations. The CPU also does not directly wait for MPI communication operations to complete. The GPU CP manages the control paths, and this potentially eliminates the expensive synchronization points between an application process and its GPU device.\n\n\nC. Slingshot 11 Triggered Operations using Libfabric Deferred Work Queues\n\nTriggered operations [16] supported by Slingshot 11 are the key hardware features required for implementing the proposed ST MPI communication scheme. They are exposed using the deferred work queue (DWQ) [3] features in the Libfabric [23] SW stack. DWQ allows an application to enqueue a list of operation with deferred execution semantics. Each DWQ descriptor is comprised of a DMA descriptor, a trigger counter object, a completion counter object, and a trigger threshold value.\n\nWhen the MPI implementation submits a DWQ send operation to the NIC command queue, this operation is not executed by the NIC immediately. The operation gets executed when the associated trigger counter object in the DWQ operation reaches a required threshold value. The GPU CP is responsible for performing this update. Similarly, the completion of the DWQ operation is monitored by the GPU CP using the completion counter object which can be used for performing synchronizations or execute other enqueued DWQ operations.\n\nThe current generation Slingshot 11 NIC provides support for the following DWQ operations: (1) tagged and untagged sends, (2) one-sided RMA operations, and (3) fetching and non-fetching atomic operations. Missing DWQ features required by the MPI ST semantics, like the tagged and untagged receive operations, are emulated with an internal asynchronous progress thread per MPI process.\n\nThe trigger and completion counter NIC objects can be mapped to GPU CP accessible memory pointers for the GPU CP to directly access these NIC counter objects.\n\n\nD. Stream Memory Operations\n\nGPU stream memory operations are used to support the proposed ST communication operations. Specifically, they are used to update and monitor the trigger and completion counter objects specified in Section. II-C. There are two common stream memory operations supported by different GPU vendors: (1) writeValue and (2) waitValue.\n\nA writeValue operation enqueues a write command to the GPU stream, and the write operation is performed only after all earlier commands to this stream have completed execution. A waitValue operation enqueues a wait command to the GPU stream. All operations enqueued on this stream after the wait command will not be executed until the defined wait condition is satisfied.\n\nThe function prototypes of the hipStreamWriteValue64 and hipStreamWaitValue64 operations on the AMD HIP runtime","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2208.04817.pdf","id":"https://arxiv.org/pdf/2208.04817.pdf","publishedDate":"2023-10-26T00:00:00.000Z","summary":"This paper explores a new GPU stream-aware MPI communication strategy called stream-triggered (ST) communication to offload both computation and communication control paths to the GPU. The ST communication strategy is implemented on HPE Slingshot Interconnects using the triggered operations feature. The performance of the proposed communication strategy is evaluated using a microbenchmark kernel called Faces, based on the nearest-neighbor communication pattern in the CORAL-2 Nekbone benchmark, over a heterogeneous node architecture consisting of AMD CPUs and GPUs. The new communication scheme allows the CPU to create network command descriptors with deferred execution semantics and append them to the NIC command queue, which are triggered at a later point in time when certain conditions are reached.\n","title":"Exploring GPU Stream-Aware Message Passing using Triggered Operations","url":"https://arxiv.org/pdf/2208.04817.pdf"},{"content":"Exploring Fully Offloaded GPU Stream-Aware Message Passing\n\n\nNaveen Namashivayam \nUSA, USA, USA\n\nHewlett Packard Enterprise \nUSA, USA, USA\n\nKrishna Kandalla krishnachaitanya.kandalla@hpe.com \nUSA, USA, USA\n\nHewlett Packard Enterprise \nUSA, USA, USA\n\nLarry Kaplan larry.kaplan@hpe.com \nUSA, USA, USA\n\nHewlett Packard Enterprise \nUSA, USA, USA\n\nMark Pagel mark.pagel@hpe.com \nUSA, USA, USA\n\nExploring Fully Offloaded GPU Stream-Aware Message Passing\nHewlett Packard Enterprise USA James B White III Hewlett Packard Enterprise USA\nModern heterogeneous supercomputing systems are comprised of CPUs, GPUs, and high-speed network interconnects. Communication libraries supporting efficient data transfers involving memory buffers from the GPU memory typically require the CPU to orchestrate the data transfer operations. A new offload-friendly communication strategy, stream-triggered (ST) communication, was explored to allow offloading the synchronization and data movement operations from the CPU to the GPU. A Message Passing Interface (MPI) one-sided active target synchronization based implementation was used as an exemplar to illustrate the proposed strategy. A latencysensitive nearest-neighbor microbenchmark was used to explore the various performance aspects of the implementation. The offloaded implementation shows significant on-node performance advantages over standard MPI active RMA (36%) and point-to-point (61%) communication. The current multi-node improvement is less (23% faster than standard active RMA but 11% slower than pointto-point), but plans are in progress to purse further improvements.INTRODUCTIONTo accommodate modern heterogeneous supercomputing systems comprised of CPUs and GPUs[16,28,36,45], current-generation scientific applications and systems-software stacks use GPU-aware communication libraries[50]. GPU-aware libraries support performing inter-process communication operations involving GPU buffers without having to stage them through a CPU-attached memory. Remote direct memory access (RDMA)[43,49]and vendorspecific peer-to-peer[38,44]data transfer mechanisms are used to implement GPU-aware inter-node and intra-node data movement operations.Even with GPU-awareness in the communication stack, CPU threads are still required to orchestrate data-moving communication and inter-process synchronization operations.Figure 1demonstrates this issue using a typical GPU-aware parallel application based on the Message Passing Interface (MPI)[22]as an example for the inter-process communication.An MPI process running on the CPU launches a device kernel (K1) and waits for its execution to complete. a It synchronizes with the local GPU to ensure completion of compute kernel (K1). Next, the CPU b launches and c waits for completion of the inter-process communication operation involving application GPU buffers previously updated by the kernel (K1). To reuse GPU buffers on subsequent compute kernel (K2), d K2 is launched only after the inter-process communication operations have completed.This results in all communication and synchronization operations occurring at GPU kernel boundaries, and creates potentially expensive synchronization points that require the CPU to synchronize with the GPU and Network Interface Controller (NIC) devices.A communication strategy called stream-triggered (ST) communication is explored in this work. Along with managing compute kernels defined by the application, the proposed ST solution allows the GPU to coordinate with the NIC to manage MPI communication operations. This design goal avoids using the CPU cycles to orchestrate and drive the MPI communication operations. This eliminates any potential synchronization points between an application process and its GPU device.PROPOSED COMMUNICATION SCHEMECommunication operations in GPU-aware applications are typically comprised of data paths and control paths. Data paths refer to those operations that involve moving data between the CPU-attached and the GPU-attached memory regions. These data movement operations can occur within the same compute node or between different compute nodes across a high-speed network. Control paths correspond to coordination operations that occur between (1) the application process running on the CPU, (2) application compute kernels running on the GPU, and (3) the network interface controller (NIC). The proposed ST strategy enables a GPU-aware application to offload the control paths to underlying implementation and hardware components.\n\nContributions of This Work\n\nThe following are the major contributions of this work.\n\n(1) Propose a new communication strategy to fully offload the communication control paths from the CPU to the GPU; (2) Use an MPI one-sided active-target-synchronization [32] based solution to demonstrate the proposed strategy; (3) Implement the proposed strategy on a modern NIC, HPE Slingshot NIC [1, 7,18], exploiting the supported triggered operations feature [13,42]; (4) Discuss various implementation design options to fully offload the communication control path to the GPU; and (5) Analyze the performance of the proposed ST strategy using a latency-sensitive microbenchmark kernel called Faces, inspired by the nearest-neighbor communication pattern from the CORAL-2 [3,28] Nekbone benchmark [6]. \n\n\nGPU Streams\n\nA GPU stream [2] is a queue of device operations. GPU compute kernel concurrency is achieved by creating multiple concurrent streams. Operations issued on a stream typically run asynchronously with respect to the CPU and operations enqueued in other GPU streams. Operations in a given stream are guaranteed to be executed in FIFO order. In this work, the GPU component that provides these execution guarantees to schedule and control the execution of the enqueued operation is referred to as the GPU Stream Execution Controller (GPU SEC). Depending on the GPU vendor, GPU SEC can be a software, hardware, or kernel component associated with the GPU.\n\n\nStream-Triggered Communication\n\nA parallel application using the ST strategy continues to manage compute kernels on the GPU via existing mechanisms. In addition, the new strategy allows an application process running on the CPU to define a set of ST communication operations. These communication operations can be scheduled for execution at a later point in time. More importantly, in addition to offering a deferred execution model, ST enables the GPU to be closely involved in the control paths of the MPI communication operations. Figure 2 illustrates a sequence of events involved in a parallel application using MPI-based ST inter-process communication and synchronization operations. An application process running on the CPU enqueues a GPU kernel K1 to the GPU stream, b triggered ST-based MPI operations to the NIC, c the corresponding trigger event to the GPU stream, and d GPU kernel K2 to the same stream. The CPU returns immediately after the operations are enqueued and is not blocked on the completion of the enqueued operations. It is the GPU SEC responsibility to 1 launch K1 and 2 wait for its completion. Once K1 completes, 3 GPU SEC triggers the execution of MPI operations and 4 waits for these operations to finish. Next, 5 the GPU SEC launches K2.\n\nWith the proposed solution, an application process running on the CPU enqueues operations to the NIC command queue and the GPU stream, but does not get directly involved in the control paths of MPI communication operations, subsequent kernel launch, and tear-down operations. The CPU does not directly wait for MPI communication operations to complete. The GPU manages the control paths and eliminates potential synchronization points in the application.\n\n\nTRIGGERED OPERATIONS BACKGROUND\n\nThis section describes the core components required in implementing GPU ST communications. Triggered operations [13,15,30,42,48] originally introduced in QsNet [41] and Portals [24] are currently supported in several modern network interconnects like HPE Slingshot [1, 7,18]. This feature allows the application process from the software layer like MPI to enqueue NIC command descriptors ahead of time in the NIC command queue. Unlike regular NIC command descriptors, triggered operations offer deferred execution semantics, where the execution of these operations can be triggered at a later point in time. A modern HPC interconnect with support for triggered operations is used to implement and explore the proposed ST scheme. This hardware feature from the NIC is exposed to MPI through network middleware libraries like Libfabric [10,26].\n\n\nTrigger Events\n\nThis section describes the events required to trigger execution of an enqueued triggered operation from the GPU. The NIC command descriptor for the triggered operations takes three parameters [10] trigger counter, completion counter, and trigger threshold value. Following are the options available to trigger the execution of an enqueued triggered operation.\n\n(1) Update the associated trigger counter to a previously defined threshold value; or (2) Associate a NIC counter with a Memory Mapped I/O (MMIO) register. When a local store operation is performed to this MMIO register, it increments the register value. When the value reaches a previously defined threshold, it triggers the execution of the enqueued operation.\n\nFor ST, a GPU triggers the execution of these NIC descriptors through the execution of a GPU kernel. The GPU kernel does not have access to the NIC HW counters; instead it performs a store to the MMIO register to trigger the execution of the enqueued operation. Figure 3 represents a sequence of events involved in offloading the trigger event to the GPU SEC, and the GPU SEC launching the GPU kernel to update the MMIO register associated with an enqueued triggered operation. \n\n\nCompletion Events\n\nCompletions of the executed triggered operations are identified by polling the associated completion counters from the NIC command descriptors. Section 3.1 describes using a lightwe","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2306.15773.pdf","id":"https://arxiv.org/pdf/2306.15773.pdf","publishedDate":"2023-10-26T00:00:00.000Z","summary":"This paper explores a new communication strategy called stream-triggered (ST) communication for modern heterogeneous supercomputing systems comprised of CPUs, GPUs, and high-speed network interconnects. The ST strategy allows offloading synchronization and data movement operations from the CPU to the GPU, avoiding the CPU's involvement in orchestrating MPI communication operations. An MPI one-sided active target synchronization based implementation was used to illustrate the proposed strategy. The offloaded implementation shows significant on-node performance advantages over standard MPI active RMA (36%) and point-to-point (61%) communication.\n","title":"Exploring Fully Offloaded GPU Stream-Aware Message Passing","url":"https://arxiv.org/pdf/2306.15773.pdf"},{"content":"# HiCCL: A Hierarchical Collective Communication Library\n\nMert Hidayetoglu\nStanford UniversityCA, USA, Simon Garcia de Gonzalo\nSandia National LaboratoriesNM, USA, Elliott Slaughter\nSLAC National Accelerator LaboratoryCA, USA, Pinku Surana\nSLAC National Accelerator LaboratoryCA, USA, Wen-mei Hwu\nNvidia / University of Illinois at Urbana-ChampaignIL, USA, William Gropp\nUniversity of Illinois at Urbana-ChampaignIL, USA and Alex Aiken\nStanford UniversityCA, USA\n\n###### Abstract.\n\nHiCCL (Hierarchical Collective Communication Library) addresses the growing complexity and diversity in high-performance network architectures. As GPU systems have envolved into networks of GPUs with different multilevel communication hierarchies, optimizing each collective function for a specific system has become a challenging task. Consequently, many collective libraries struggle to adapt to different hardware and software, especially across systems from different vendors. HiCCL’s library design decouples the collective communication logic from network-specific optimizations through a compositional API. The communication logic is composed using multicast, reduction, and fence primitives, which are then factorized for a specified network hieararchy using only point-to-point operations within a level. Finally, striping and pipelining optimizations applied as specified for streamlining the execution.\n\nPerformance evaluation of HiCCL across four different machines—two with Nvidia GPUs, one with AMD GPUs, and one with Intel GPUs—demonstrates an average 17×\\\\times× higher throughput than the collectives of highly specialized GPU-aware MPI implementations, and competitive throughput with those of vendor-specific libraries (NCCL, RCCL, and OneCCL), while providing portability across all four machines.\n\n## 1\\. Introduction\n\nCommunication libraries in high-performance computing (HPC) for scientific applications and machine learning offer collectives—standard functions such as Scatter, Reduce, and All-Reduce ( [MPICH,](https://arxiv.org/html/2408.05962v1#bib.bib15); [OpenMPI,](https://arxiv.org/html/2408.05962v1#bib.bib12); [thakur2005optimization,](https://arxiv.org/html/2408.05962v1#bib.bib32)) that involve coordinated communication across processors. Our evaluation on leadership-class GPU systems revealed that the throughput of available library collectives is either not optimized in GPU-aware MPI implementations or have limited cross-vendor portability in NCCL, RCCL, and OneCCL. These performance and portability issues motivated us to develop a library design capable of producing throughput-optimized collectives portable across diverse network architectures and GPU programming models. However, there are many collective functions, each requiring a unique optimization strategy depending on the network architecture ( [wilkins2023generalized,](https://arxiv.org/html/2408.05962v1#bib.bib33)), making it unproductive to hand optimize a collective for one system and then repeat the process when porting to another system.\n\nModern systems typically feature a network comprised of multiple compute nodes each housing multiple GPUs ( [Frontier,](https://arxiv.org/html/2408.05962v1#bib.bib10); [Sunspot,](https://arxiv.org/html/2408.05962v1#bib.bib9); [Perlmutter,](https://arxiv.org/html/2408.05962v1#bib.bib5); [Delta,](https://arxiv.org/html/2408.05962v1#bib.bib11)), which results in a nonuniform interconnect architecture among the GPU endpoints. Specifically, the network is hierarchical: GPUs within the same node communicate via a high-throughput intranode network, while GPUs on different nodes communicate through a lower-throughput network that links the nodes. Consequently, the bandwidth available for GPU communication is influenced by their placement within the hierarchical network architecture ( [siefert2023latency,](https://arxiv.org/html/2408.05962v1#bib.bib30)).\nThis hierarchy can extend to multiple levels, such as GPU dies in a device, devices across NUMA nodes, or nodes within racks, where communication bandwidth decreases at higher levels of the hierarchy, making data transfer between more “distant” GPUs costlier.\nCommunication bandwidth decreases at higher levels of the hierarchy, making data transfer between more “distant” GPUs costlier. Efficient hierarchical communication relies on having lower volume at higher levels of the hierarchy ( [hidayetouglu2019memxct,](https://arxiv.org/html/2408.05962v1#bib.bib19); [bienz2021modeling,](https://arxiv.org/html/2408.05962v1#bib.bib3); [lockhart2023characterizing,](https://arxiv.org/html/2408.05962v1#bib.bib25)), which is\ncurrently achieved with carefully hand-designed and optimized implementations of collectives.\n\nOur aim is to provide both high throughput\nand performance portability for collective communication operations across diverse hierarchical networks with GPUs of different vendors111Today’s major GPU vendors are Nvidia, AMD, and Intel. while also automating much of the process of building optimized collectives. We present a method for constructing collective communication operations that achieves these goals in two steps:\nFirst, we specify the communication pattern for a collective operation as a composition of multicast, reduction, and fence primitives (Section [3](https://arxiv.org/html/2408.05962v1#S3)).\nSecond, we identify and implement a number of optimizations that can be mechanically applied to the given composition (Section [4](https://arxiv.org/html/2408.05962v1#S4)).\nThese optimizations include striping communication across multiple network interface cards (NICs), pipelining communication in multiple stages, and whether to use a customized tree, ring, or hybrid virtual topology to best match the target network hierarchy.\nCombining these optimizations with a machine-specific description that fills in important constants (e.g., the number of levels in the hierarchy and the base communication library to use for each level), users can create highly optimized collective operations tuned to a specific network.\nWhen porting between machines, only the machine description\nneeds to change; the specification of the logic of the collective operation can be automatically optimized for the target network using the new machine description.\n\nWe implement our optimized collectives relying only on point-to-point communication functions of standard communication libraries and, when\navailable, vendor-provided capabilities for a specific machine.\nThis implementation strategy gives us both maximum flexibility to use the best point-to-point primitives for a specific\nlevel of a network hierarchy while also ensuring maximum\nportability (Section [5](https://arxiv.org/html/2408.05962v1#S5)).\n\nWe have implemented these ideas in a hierarchical collective communication library, HiCCL, which provides an API to build collective functions and apply hierarchical optimizations. Moreover HiCCL provides performance portability across systems of different shapes and sizes and of different vendors. We make the following contributions:\n\n- •\n\n\nWe introduce a machine-agnostic specification of collective functions using multicast, reduction, and fence primitives. We show these primitives are sufficient to express all collective functions in the MPI standard and their alternative implementations.\n\n- •\n\n\nWe identify a set of unified hierarchical optimizations that are applicable to any collective function composed with the proposed primitives.\nWe show how the optimizations adapt to different, modern GPU systems, and are sufficient to saturate the throughput of the various networks.\n\n- •\n\n\nWe introduce HiCCL222https://github.com/merthidayetoglu/HiCCL: a hierarchical communication library which integrates multiple communication capabilities without relying on existing collective functions. We demonstrate HiCCL’s performance portability by matching or outperforming available MPI and vendor-provided libraries on different systems with Nvidia, AMD, and Intel GPUs.\n\n\nWe evaluate HiCCL on four current HPC systems—Delta ( [Delta,](https://arxiv.org/html/2408.05962v1#bib.bib11)) and Perlmutter ( [Perlmutter,](https://arxiv.org/html/2408.05962v1#bib.bib5)) with Nvidia GPUs, Frontier ( [Frontier,](https://arxiv.org/html/2408.05962v1#bib.bib10)) with AMD GPUs, and Aurora ( [Sunspot,](https://arxiv.org/html/2408.05962v1#bib.bib9)) with Intel GPUs—and on eight collective functions that are listed in Table [1](https://arxiv.org/html/2408.05962v1#S2.T1). HiCCL achieves 17×\\\\times× geomean speedup over the native MPI implementations across all platforms. Furthermore, it delivers competitive (1.27×\\\\times× against NCCL/RCCL), and in some cases superior (12.1×\\\\times× against OneCCL), throughput compared to the libraries provided by the GPU vendors.\n\n## 2\\. Background\n\nThis section provides an overview of collective functions and hierarchical communication on multi-GPU, multi-NIC node architetures.\n\nTable 1. Collectives of MPI, NCCL, RCCL and OneCCL.\n\n| Collective | MPI | NCCL / RCCL | OneCCL |\n| --- | --- | --- | --- |\n| Scatter | MPI\\_Scatter |\n| Broadcast | MPI\\_Bcast | ncclBcast | ccl::broadcast |\n| Gather | MPI\\_Gather |\n| Reduce | MPI\\_Reduce | ncclReduce | ccl::reduce |\n| All-to-all | MPI\\_Alltoall | ccl::alltoall |\n| All-gather | MPI\\_Allgather | ncclAllgather | ccl::allgatherv |\n| Reduce-scatter | MPI\\_Reduce\\_sc. | ncclReduceSc. | ccl::reduce\\_sc. |\n| All-reduce | MPI\\_Allreduce | ncclAllreduce | ccl::allreduce |\n\n### 2.1. Conventional Libraries and Collective Functions\n\nMPI ( [snir1998mpi,](https://arxiv.org/html/2408.05962v1#bib.bib31)) is a well-established standard for message passing in distributed computing, with numerous implementations ( [MPICH,](https://arxiv.org/html/2408.05962v1#bib.bib15); [OpenMPI,](https://arxiv.org/html/2408.05962v1#bib.bib12); [SpectrumMPI,](https://arxiv.org/html/2408.05962v1#bib.bib20); [panda2021mvapich,](https://arxiv.org/html/2408.05962v1#bib.bib27)). GPU-aware implementations of MPI are availabl","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2408.05962v1","id":"https://arxiv.org/html/2408.05962v1","publishedDate":"2024-03-29T00:00:00.000Z","summary":"HiCCL is a new library designed to improve collective communication in high-performance computing, especially for GPU systems with complex network architectures. It addresses the challenges of optimizing collective functions across diverse hardware and software, including those from different vendors. HiCCL achieves this by decoupling communication logic from network-specific optimizations through a compositional API, using primitives like multicast and reduction. Performance evaluations across various machines (Nvidia, AMD, and Intel GPUs) show that HiCCL outperforms GPU-aware MPI implementations and is competitive with vendor-specific libraries (NCCL, RCCL, and OneCCL) while offering portability.\n","title":"HiCCL: A Hierarchical Collective Communication Library","url":"https://arxiv.org/html/2408.05962v1"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2509.13694","id":"https://arxiv.org/abs/2509.13694","publishedDate":"2025-09-17T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"StreamTensor: Make Tensors Stream in Dataflow Accelerators for LLMs","url":"https://arxiv.org/abs/2509.13694"}],"status":"completed","toolCallId":"call_UIwnYQHOTlLwBjKEpzS3WRAF","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"They show finer-grained spatial control by using ROCm’s CU masking at the stream (queue) level, and then exploit that to do per-stream, per-kernel stage partitioning and fast reconfiguration. Concretely:\n\n- Unit of control: They attach a CU mask to a HIP stream/queue so that “kernels launched on this stream run only on these specified CUs.” That’s finer than process/device-level (MIG/MPS). Multiple streams in the same process can each have different CU subsets.\n\n- Intra-process partitioning: Within a single model/process, they launch different kernels or pipeline stages on different masked streams to pin stages to disjoint CU sets, enabling concurrent co-location and stage-wise resource allocation.\n\n- Dynamic resizing/reassignment: Because the mask is just a stream attribute, they can quickly change CU assignments (e.g., expand/shrink a tenant’s CU set) without tearing down contexts or reconfiguring device partitions.\n\n- Selective throttling: They can give latency-critical streams larger CU masks and background streams smaller ones, achieving QoS-like control without hard partitions.\n\n- Kernel-level isolation experiments: They demonstrate co-running models where specific kernels (e.g., attention vs. MLP) are bound to particular CU groups, measuring throughput/latency/energy effects, something not directly achievable with per-process mechanisms.\n\nIn short, AMD’s per-stream CU masking lets them pin and reshuffle execution resources at the granularity of streams/kernels inside a single process, whereas NVIDIA’s common mechanisms (MIG/MPS) operate at process/device level and don’t let you choose specific SMs per stream.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d423f24ba8819f8052c14f4c0e13c5029365713186b63b"}},"resumableStreamId":"STREAM:441c3608-8c3e-4454-a2a3-87ecf8dea14c","role":"assistant","status":"done","threadId":"102dcb6d-c8ac-40bf-abb5-31590131c4ce","timeToFirstToken":1.536,"tokens":336,"tokensPerSecond":17.473607571896615,"updated_at":1758733286896,"userId":"google:104458561188320708185","id":"j9716cad1t2krdfb6be8c4kh757r70hm"}],"version":"6.0.2"}