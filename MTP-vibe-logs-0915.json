{"threads":[{"_creationTime":1757965380432.0127,"_id":"jd77wnc5jp2ppt5czfxm68xj1s7qnds7","createdAt":1757965380432,"generationStatus":"completed","lastMessageAt":1757965679919,"model":"gpt-5-chat","pinned":false,"threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","title":"MTP Meaning-Typed Language Abstraction for AI-Integrated Programming","updatedAt":1757965679919,"userId":"google:104458561188320708185","userSetTitle":false,"visibility":"visible","id":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","last_message_at":1757965679919,"created_at":1757965380432,"updated_at":1757965679919,"status":"completed","user_edited_title":false}],"messages":[{"_creationTime":1757965380432.013,"_id":"j97cx64303sqgwfv0x9xh6drqd7qn9z0","attachmentIds":[],"content":"https://arxiv.org/abs/2405.08965\nthis paper is called MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\n\ngenerate a summary of this paper and explain to me what problem it is tackling","created_at":1757965380432,"messageId":"6a10037d-f965-49ad-b9c5-4ea0f7c1c8f5","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","updated_at":1757965380432,"userId":"google:104458561188320708185","id":"j97cx64303sqgwfv0x9xh6drqd7qn9z0","providerMetadata":{}},{"_creationTime":1757965380432.0132,"_id":"j97ft93grcetqbfzp4e4fz9zcn7qm9qx","attachmentIds":[],"byok":false,"content":"Hereâ€™s a concise, practical summary and the problem it tackles.\n\nWhat problem is it tackling?\n- Todayâ€™s â€œAI-integratedâ€ apps call LLMs at runtime for core functionality. Developers have to:\n  - Manually design prompts, pick context, and tweak wording\n  - Learn and wire up new frameworks/languages for prompting\n  - Parse free-form LLM output back into program objects and types\n- This makes systems fragile, hard to maintain, and costly to evolve. Even with tools like LangChain, LMQL, DSPy, and SGLang, devs still shoulder prompt design, IO conversions, and framework complexity.\n\nCore idea\n- Code already encodes a lot of meaning: function names, signatures, variable names, docstrings, and types. Modern LLMs can infer intent from this semantic context.\n- Meaning-Typed Programming (MTP) treats LLMs as first-class, meaning-typed code constructs, so the language and runtime extract semantics from code and automatically generate/promote the right prompts and parse structured outputsâ€”without manual prompt engineering.\n\nWhat MTP proposes\n1) â€œbyâ€ operator (language construct)\n- Replace a function body with a clause like by model_name to delegate the function to an LLM at runtime.\n- The system uses the function signature, names, comments, and types as implicit intent and context.\n\n2) MT-IR (Meaning-Typed Intermediate Representation)\n- A compiler-level IR that captures semantically rich info about the codebase: names, types, signatures, comments, relationships.\n- At runtime, it binds actual values to this semantic graph to form the LLM input.\n\n3) MT-Runtime\n- A runtime engine that triggers at by call sites, assembles context-aware prompts from MT-IR plus runtime values, calls the selected model, validates/parses outputs back into the declared return types, and handles errors and retries.\n\nImplementation and results\n- Implemented in Jac (a production-grade Python superset) as part of the Jaseci open-source project (module: byLLM).\n- Empirical findings (OOPSLA 2025 version):\n  - Developers completed tasks 3.2x faster with 45% fewer lines of code compared to existing frameworks.\n  - Lower development complexity and costs; improved runtime performance while maintaining or improving accuracy.\n  - Robust even when up to 50% of naming conventions are degraded (i.e., it still extracts enough meaning to work well).\n  - Observes a â€œless is moreâ€ trend in prompt engineering as model capabilities improveâ€”leaning on code semantics reduces the need for lengthy, brittle prompts.\n\nHow it compares to existing tools\n- Unlike LangChain/LMQL/DSPy/SGLang, which help you craft and manage prompts, MTP makes prompt construction an automated language/runtime responsibility, using the programâ€™s own semantics as the source of truth.\n- It also standardizes output parsing via declared types and function signatures, reducing brittle custom parsing.\n\nWhy this matters\n- Makes AI integration feel like normal programming: declare intent via code, let the language/runtime translate to/from LLMs.\n- Reduces maintenance burden and onboarding cost, and scales better across teams and codebases as LLMs improve.\n- Bridges symbolic code and neural components with a principled, type- and meaning-aware abstraction layer.\n\nIf you want, I can show a small before/after code sketch (with and without MTP) to illustrate how the by operator and MT-IR-driven prompting change developer workflow.","created_at":1757965380433,"messageId":"adb6cb58-d193-48f6-b8d8-f8072617fe8d","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":null,"queries":["arXiv 2405.08965 MTP Meaning-Typed Language Abstraction for AI-Integrated Programming","\"MTP\" Meaning-Typed Programming arXiv 2405.08965 pdf","MTP AI-integrated programming meaning-typed language abstraction 2024"]},"result":[{"content":"# Computer Science > Programming Languages\n\n**arXiv:2405.08965** (cs)\n\n\\[Submitted on 14 May 2024 ( [v1](https://arxiv.org/abs/2405.08965v1)), last revised 3 Sep 2025 (this version, v5)\\]\n\n# Title:MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\n\nAuthors: [Jayanaka L. Dantanarayana](https://arxiv.org/search/cs?searchtype=author&query=Dantanarayana,+J+L), [Yiping Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang,+Y), [Kugesan Sivasothynathan](https://arxiv.org/search/cs?searchtype=author&query=Sivasothynathan,+K), [Christopher Clarke](https://arxiv.org/search/cs?searchtype=author&query=Clarke,+C), [Baichuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+B), [Savini Kashmira](https://arxiv.org/search/cs?searchtype=author&query=Kashmira,+S), [Krisztian Flautner](https://arxiv.org/search/cs?searchtype=author&query=Flautner,+K), [Lingjia Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+L), [Jason Mars](https://arxiv.org/search/cs?searchtype=author&query=Mars,+J)\n\nView a PDF of the paper titled MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming, by Jayanaka L. Dantanarayana and 8 other authors\n\n[View PDF](https://arxiv.org/pdf/2405.08965) [HTML (experimental)](https://arxiv.org/html/2405.08965v5)\n\n> Abstract:Software development is shifting from traditional programming to AI-integrated applications that leverage generative AI and large language models (LLMs) during runtime. However, integrating LLMs remains complex, requiring developers to manually craft prompts and process outputs. Existing tools attempt to assist with prompt engineering, but often introduce additional complexity. This paper presents Meaning-Typed Programming (MTP), a novel paradigm that abstracts LLM integration through intuitive language-level constructs. By leveraging the inherent semantic richness of code, MTP automates prompt generation and response handling without additional developer effort. We introduce the (1) by operator for seamless LLM invocation, (2) MT-IR, a meaning-based intermediate representation for semantic extraction, and (3) MT-Runtime, an automated system for managing LLM interactions. We implement MTP in Jac, a programming language that supersets Python, and find that MTP significantly reduces coding complexity while maintaining accuracy and efficiency. MTP significantly reduces development complexity, lines of code modifications needed, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches. Our user study shows that developers using MTP completed tasks 3.2x faster with 45% fewer lines of code compared to existing frameworks. Moreover, MTP demonstrates resilience even when up to 50% of naming conventions are degraded, demonstrating robustness to suboptimal code. MTP is developed as part of the Jaseci open-source project, and is available under the module byLLM.\n\n| | |\n| --- | --- |\n| Comments: | OOPSLA 2025 |\n| Subjects: | Programming Languages (cs.PL); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2405.08965](https://arxiv.org/abs/2405.08965) \\[cs.PL\\] |\n| (or [arXiv:2405.08965v5](https://arxiv.org/abs/2405.08965v5) \\[cs.PL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2405.08965](https://doi.org/10.48550/arXiv.2405.08965) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jayanaka L. Dantanarayana \\[ [view email](https://arxiv.org/show-email/13cb6fa0/2405.08965)\\] **[\\[v1\\]](https://arxiv.org/abs/2405.08965v1)**\nTue, 14 May 2024 21:12:01 UTC (623 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2405.08965v2)**\nMon, 14 Oct 2024 21:20:40 UTC (4,649 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2405.08965v3)**\nThu, 16 Jan 2025 18:56:27 UTC (2,918 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2405.08965v4)**\nWed, 4 Jun 2025 00:51:13 UTC (3,247 KB)\n**\\[v5\\]**\nWed, 3 Sep 2025 21:19:19 UTC (3,661 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming, by Jayanaka L. Dantanarayana and 8 other authors\n\n- [View PDF](https://arxiv.org/pdf/2405.08965)\n- [HTML (experimental)](https://arxiv.org/html/2405.08965v5)\n- [TeX Source](https://arxiv.org/src/2405.08965)\n- [Other Formats](https://arxiv.org/format/2405.08965)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.PL\n\n[<Â prev](https://arxiv.org/prevnext?id=2405.08965&function=prev&context=cs.PL)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2405.08965&function=next&context=cs.PL)\n\n[new](https://arxiv.org/list/cs.PL/new) \\| [recent](https://arxiv.org/list/cs.PL/recent) \\| [2024-05](https://arxiv.org/list/cs.PL/2024-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2405.08965?context=cs) [cs.AI](https://arxiv.org/abs/2405.08965?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2405.08965)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2405.08965)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2405.08965)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2405.08965) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2405.08965","id":"https://arxiv.org/abs/2405.08965","publishedDate":null,"summary":"The paper introduces Meaning-Typed Programming (MTP), a new paradigm designed to simplify AI-integrated software development, particularly when working with large language models (LLMs). MTP abstracts the complexities of prompt engineering by providing language-level constructs that leverage the semantic richness of code, automating prompt creation and response handling. Key features include the 'by' operator for seamless LLM invocation, MT-IR for semantic extraction, and MT-Runtime for managing LLM interactions. Implemented in Jac, an extension of Python, MTP significantly reduces coding effort, improves runtime performance, and maintains or exceeds existing accuracy levels. User studies show developers are 3.2 times faster with 45% fewer lines of code, and the approach remains robust even with degraded naming conventions. MTP is part of the open-source Jaseci project, aiming to make AI integration more intuitive and efficient in software development.","title":"MTP: A Meaning-Typed Language Abstraction for AI ...","url":"https://arxiv.org/abs/2405.08965"},{"content":"\\\\setcctype\n\nby\n\n# MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\n\nJayanaka L. Dantanarayana\n[0009-0000-4320-8280](https://orcid.org/0009-0000-4320-8280)University of MichiganAnn ArborUSA[jayanaka@umich.edu](mailto:jayanaka@umich.edu),Â Yiping Kang\n[0000-0002-5964-3655](https://orcid.org/0000-0002-5964-3655)University of MichiganAnn ArborUSA[ypkang@umich.edu](mailto:ypkang@umich.edu),Â Kugesan Sivasothynathan\n[0009-0004-4657-4947](https://orcid.org/0009-0004-4657-4947)Jaseci LabsAnn ArborUSA[kugesan.sivasothynathan@jaseci.org](mailto:kugesan.sivasothynathan@jaseci.org),Â Christopher Clarke\n[0000-0001-8741-3155](https://orcid.org/0000-0001-8741-3155)University of MichiganAnn ArborUSA[csclarke@umich.edu](mailto:csclarke@umich.edu),Â Baichuan Li\n[0009-0008-4812-6303](https://orcid.org/0009-0008-4812-6303)University of MichiganAnn ArborUSA[patrli@umich.edu](mailto:patrli@umich.edu),Â Savini Kashmira\n[0009-0005-4911-7597](https://orcid.org/0009-0005-4911-7597)University of MichiganAnn ArborUSA[savinik@umich.edu](mailto:savinik@umich.edu),Â Krisztian Flautner\n[0009-0002-8347-1811](https://orcid.org/0009-0002-8347-1811)University of MichiganAnn ArborUSA[manowar@umich.edu](mailto:manowar@umich.edu),Â Lingjia Tang\n[0000-0002-5609-7775](https://orcid.org/0000-0002-5609-7775)University of MichiganAnn ArborUSA[lingjia@umich.edu](mailto:lingjia@umich.edu)Â andÂ Jason Mars\n[0000-0002-7029-5292](https://orcid.org/0000-0002-7029-5292)University of MichiganAnn ArborUSA[profmars@umich.edu](mailto:profmars@umich.edu)\n\n(2025-03-25; 2025-08-12)\n\n###### Abstract.\n\nSoftware development is shifting from traditional programming to AI-integrated applications that leverage generative AI and large language models (LLMs) during runtime. However, integrating LLMs remains complex, requiring developers to manually craft prompts and process outputs. Existing tools attempt to assist with prompt engineering, but often introduce additional complexity.\n\nThis paper presents Meaning-Typed Programming (MTP), a novel paradigm that abstracts LLM integration through intuitive language-level constructs. By leveraging the inherent semantic richness of code, MTP automates prompt generation and response handling without additional developer effort. We introduce the (1) by operator for seamless LLM invocation, (2) MT-IR, a meaning-based intermediate representation for semantic extraction, and (3) MT-Runtime, an automated system for managing LLM interactions. We implement MTP in Jac, a programming language that supersets Python, and find that MTP significantly reduces coding complexity while maintaining accuracy and efficiency. MTP significantly reduces development complexity, lines of code modifications needed,\nand costs while improving run-time performance and maintaining or exceeding the accuracy\nof existing approaches. Our user study shows that developers using MTP completed tasks 3.2Ã— faster with 45% fewer lines of code compared to existing frameworks. Moreover, MTP demonstrates resilience even when up to 50% of naming conventions are degraded, demonstrating robustness to suboptimal code. MTP is developed as part of the Jaseci open-source project, and is available under the module [byLLM](https://github.com/jaseci-labs/jaseci/tree/main).\n\nprogramming languages for AI, prompt engineering, generative AI\n\nâ€ â€ copyright: ccâ€ â€ doi: 10.1145/3763092â€ â€ journalyear: 2025â€ â€ journal: PACMPLâ€ â€ journalvolume: 9â€ â€ journalnumber: OOPSLA2â€ â€ article: 314â€ â€ publicationmonth: 10â€ â€ ccs: Computing methodologiesÂ Artificial intelligence\n\n## 1\\. INTRODUCTION\n\nWith the advent of generative AI and Large Language Models (LLMs), the way we develop software is rapidly evolving. While much work has focused on using LLMs in code generation(Wang and Chen, [2023](https://arxiv.org/html/2405.08965v5#bib.bib44); Levin etÂ al., [2025](https://arxiv.org/html/2405.08965v5#bib.bib24)), an emerging trend is the integration of LLMs as components that provide essential functionalityÂ (Sharma etÂ al., [2025](https://arxiv.org/html/2405.08965v5#bib.bib41); Beurer-Kellner etÂ al., [2023](https://arxiv.org/html/2405.08965v5#bib.bib5); Li etÂ al., [2023](https://arxiv.org/html/2405.08965v5#bib.bib25); Weber, [2024](https://arxiv.org/html/2405.08965v5#bib.bib46)). These _AI-integrated applications_ leverage generative AI at runtime to perform critical program functionality, merging conventional programming with AI-driven capabilities. This shift fundamentally transforms software development, redefining how applications are designed and built.\n\nFigure 1. High level view of what traditional program functionality consists of (Left) vs. AI-integrated programming using prompt engineering (Middle) and Meaning-Typed Programming(MTP) paradigm (Right).\n\n\\\\Description\n\nBuilding AI-integrated applications remains a complex challenge due to the fundamental differences between operating assumptions. In conventional programming, code describes precise operations performed on explicitly defined variables such as objects (FigureÂ [1](https://arxiv.org/html/2405.08965v5#S1.F1) left). In contrast, LLMs process natural language text as input and produce text-based outputs. To integrate LLMs into programs, developers must manually construct textual input, a process known as _prompt engineering_.\nPrompt engineering is the primary method of realizing AI-integrated programming and can be tedious, complex, and imprecise. Developers must craft descriptive instructions, manage textual context, and handle the inherent variability in LLM output. This process includes a significant amount of trial-and-error, making it time-consuming and difficult to maintain.\n\nSeveral efforts have been made to address these challenges by developing frameworks and tools to facilitate prompt engineering. Frameworks such as LangChainÂ (Langchain, [2024](https://arxiv.org/html/2405.08965v5#bib.bib22)), Language Model Query Language (LMQL)Â (Beurer-Kellner etÂ al., [2023](https://arxiv.org/html/2405.08965v5#bib.bib5)), DSPyÂ (Khattab etÂ al., [2024](https://arxiv.org/html/2405.08965v5#bib.bib18)), and SGLangÂ (Zheng etÂ al., [2025](https://arxiv.org/html/2405.08965v5#bib.bib55)) have been introduced to assist developers in generating and managing prompts. However, while these frameworks provide more sophisticated tooling to help bridge the gap between conventional programming and the incorporation of AI models, in practice, they introduce additional layers of complexity for developers:\n\n1. (1)\n\n\n_Prompt design complexity:_ Developers remain responsible for the manual construction of prompts, including determining the appropriate prompt language and selecting relevant information to include.\n\n2. (2)\n\n\n_Steep learning curve:_ These systems require developers to familiarize themselves with new query languages, frameworks, or specialized syntax, complicating the integration process.\n\n3. (3)\n\n\n_Input/output conversion complexity:_ Parsing LLM output and converting them back to objects remains a challenge, especially given the variability in output across different LLMs.\n\n\nIn this paper, we introduce _meaning-typed programming_ (MTP), a novel approach to simplify the creation of AI-integrated applications by (i) introducing new high level language abstractions that hide the complexities of _AI-integration_, (ii) a novel technique for automating prompt generation using inherent code semantics, and (iii) a runtime system that automates LLM inference and output handling.\n\nThe key insights of this work are: _(1) In practice, programs are written to be readable by various developers to comprehend the intentions and semantics conveyed by the code. This semantic richness can be leveraged to automatically translate intent embodied in the code to prompts for the LLM. (2) As state-of-the-art LLMs continue to advance, AI becomes increasingly feasible to infer code intentions without requiring explicit descriptions or prompt construction by developers. This trend is not only supported by our empirical findings, but we surprisingly find less can perhaps be more in prompt engineering (see Â§Â [5](https://arxiv.org/html/2405.08965v5#S5))._\n\nThere are four primary challenges that must be overcome in the design and implementation of MTP. Firstly, the introduced language abstraction should be simple, intuitive, and flexible. Secondly, developing an automated approach that provides LLMs with sufficient semantic information for accurate task performance is challenging. Thirdly, since AI-integrated applications rely on dynamic values, prompts must be generated at runtime, requiring a lightweight system to handle prompt generation and LLM inference efficiently. Lastly, because LLMs generate non-deterministic outputs, the outputs must be carefully processed to ensure correct type conversion. To address these challenges, we introduce three key components to enable the seamless creation of AI-integrated programs. These include\n\n1. (1)\n\n\nThe â€˜byâ€™ operator, that enables the seamless integration of LLM functionality into code. It acts as a bridge between traditional and LLM operations, allowing developers to replace function bodies with a simple bymodel\\_name clause.\n\n\n\n\n\nAs shown later in our evaluation, meticulously extracted relevant semantic information is sufficient for the LLM to produce the accurate result, significantly simplifying LLM integration into existing codebases.\n\n2. (2)\n\n\nMT-IR, a meaning-based intermediate representation at the compiler level that collects and organizes semantically rich information, preserving the meaning behind variable names, function signatures, and types, during runtime.\n\n3. (3)\n\n\nMT-Runtime, an automated runtime engine within the languageâ€™s virtual machine, triggered at by call sites. It binds runtime values to MT-IR dynamically, manages LLM interactions, generates context-aware prompts, and handles responses and errors. Through these three components, MTP allows developers to focus on application logic while automatically managing LLM integration, e","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2405.08965v5","id":"https://arxiv.org/html/2405.08965v5","publishedDate":null,"summary":"The paper introduces Meaning-Typed Programming (MTP), a new paradigm designed to simplify the integration of Large Language Models (LLMs) into software development. As AI-driven applications become more prevalent, developers face challenges in managing prompt engineering and response handling. MTP addresses this by providing language-level constructsâ€”such as the 'by' operator for seamless LLM invocation, a meaning-based intermediate representation (MT-IR), and an automated runtime system (MT-Runtime)â€”that abstract these complexities. Implemented in Jac, an extension of Python, MTP reduces coding effort, improves runtime efficiency, and maintains high accuracy, even under suboptimal naming conventions. User studies show that developers using MTP complete tasks over three times faster with fewer lines of code and at lower costs. Developed as part of the open-source Jaseci project, MTP aims to facilitate more intuitive and robust AI-integrated programming.","title":"MTP: A Meaning-Typed Language Abstraction for AI- ...","url":"https://arxiv.org/html/2405.08965v5"},{"content":"Now on home page\n\n## ADS\n\n## MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\n\n- [Dantanarayana, Jayanaka L.](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Dantanarayana%2C+Jayanaka+L.%22);\n- [Kang, Yiping](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Kang%2C+Yiping%22);\n- [Sivasothynathan, Kugesan](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Sivasothynathan%2C+Kugesan%22);\n- [Clarke, Christopher](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Clarke%2C+Christopher%22);\n- [Li, Baichuan](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Li%2C+Baichuan%22);\n- [Kashmira, Savini](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Kashmira%2C+Savini%22);\n- [Flautner, Krisztian](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Flautner%2C+Krisztian%22);\n- [Tang, Lingjia](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Tang%2C+Lingjia%22);\n- [Mars, Jason](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Mars%2C+Jason%22)\n\n#### Abstract\n\nSoftware development is shifting from traditional programming to AI-integrated applications that leverage generative AI and large language models (LLMs) during runtime. However, integrating LLMs remains complex, requiring developers to manually craft prompts and process outputs. Existing tools attempt to assist with prompt engineering, but often introduce additional complexity. This paper presents Meaning-Typed Programming (MTP), a novel paradigm that abstracts LLM integration through intuitive language-level constructs. By leveraging the inherent semantic richness of code, MTP automates prompt generation and response handling without additional developer effort. We introduce the (1) by operator for seamless LLM invocation, (2) MT-IR, a meaning-based intermediate representation for semantic extraction, and (3) MT-Runtime, an automated system for managing LLM interactions. We implement MTP in Jac, a programming language that supersets Python, and find that MTP significantly reduces coding complexity while maintaining accuracy and efficiency. MTP significantly reduces development complexity, lines of code modifications needed, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches. Our user study shows that developers using MTP completed tasks 3.2x faster with 45% fewer lines of code compared to existing frameworks. Moreover, MTP demonstrates resilience even when up to 50% of naming conventions are degraded, demonstrating robustness to suboptimal code. MTP is developed as part of the Jaseci open-source project, and is available under the module byLLM.\n\nPublication:\n\narXiv e-prints\n\nPub Date:May 2024DOI:\n\n[10.48550/arXiv.2405.08965](https://ui.adsabs.harvard.edu/link_gateway/2024arXiv240508965D/doi:10.48550/arXiv.2405.08965)\n\narXiv:[arXiv:2405.08965](https://ui.adsabs.harvard.edu/link_gateway/2024arXiv240508965D/arXiv:2405.08965)Bibcode:[2024arXiv240508965D](https://ui.adsabs.harvard.edu/abs/2024arXiv240508965D/abstract)Keywords:\n\n- Programming Languages;\n- Artificial Intelligence\n\nE-Print:OOPSLA 2025\n\nfull text sources\n\nPreprint\n\n\\|\n\nðŸŒ“","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fui.adsabs.harvard.edu%2Fabs%2F2024arXiv240508965M%2Fabstract","id":"https://ui.adsabs.harvard.edu/abs/2024arXiv240508965M/abstract","publishedDate":null,"summary":"The paper introduces Meaning-Typed Programming (MTP), a novel approach to simplify AI-integrated software development by abstracting large language model (LLM) interactions through language-level constructs. Implemented in Jac, a superset of Python, MTP automates prompt generation and response handling using semantic-rich representations, notably the \"by\" operator, MT-IR (meaning-based intermediate representation), and MT-Runtime (automation system). Results show that MTP reduces coding complexity, development time (tasks completed 3.2x faster), and costs while maintaining or improving accuracy and efficiency. It also exhibits robustness even when naming conventions are degraded up to 50%. Developed as part of the open-source Jaseci project, MTP aims to streamline AI-augmented programming, making LLM integration more intuitive and less error-prone.","title":"Meaning-Typed Programming: Language-level ...","url":"https://ui.adsabs.harvard.edu/abs/2024arXiv240508965M/abstract"},{"content":"Now on home page\n\n## ADS\n\n## MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\n\n- [Dantanarayana, Jayanaka L.](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Dantanarayana%2C+Jayanaka+L.%22);\n- [Kang, Yiping](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Kang%2C+Yiping%22);\n- [Sivasothynathan, Kugesan](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Sivasothynathan%2C+Kugesan%22);\n- [Clarke, Christopher](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Clarke%2C+Christopher%22);\n- [Li, Baichuan](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Li%2C+Baichuan%22);\n- [Kashmira, Savini](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Kashmira%2C+Savini%22);\n- [Flautner, Krisztian](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Flautner%2C+Krisztian%22);\n- [Tang, Lingjia](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Tang%2C+Lingjia%22);\n- [Mars, Jason](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Mars%2C+Jason%22)\n\n#### Abstract\n\nSoftware development is shifting from traditional programming to AI-integrated applications that leverage generative AI and large language models (LLMs) during runtime. However, integrating LLMs remains complex, requiring developers to manually craft prompts and process outputs. Existing tools attempt to assist with prompt engineering, but often introduce additional complexity. This paper presents Meaning-Typed Programming (MTP), a novel paradigm that abstracts LLM integration through intuitive language-level constructs. By leveraging the inherent semantic richness of code, MTP automates prompt generation and response handling without additional developer effort. We introduce the (1) by operator for seamless LLM invocation, (2) MT-IR, a meaning-based intermediate representation for semantic extraction, and (3) MT-Runtime, an automated system for managing LLM interactions. We implement MTP in Jac, a programming language that supersets Python, and find that MTP significantly reduces coding complexity while maintaining accuracy and efficiency. MTP significantly reduces development complexity, lines of code modifications needed, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches. Our user study shows that developers using MTP completed tasks 3.2x faster with 45% fewer lines of code compared to existing frameworks. Moreover, MTP demonstrates resilience even when up to 50% of naming conventions are degraded, demonstrating robustness to suboptimal code. MTP is developed as part of the Jaseci open-source project, and is available under the module byLLM.\n\nPublication:\n\narXiv e-prints\n\nPub Date:May 2024DOI:\n\n[10.48550/arXiv.2405.08965](https://ui.adsabs.harvard.edu/link_gateway/2024arXiv240508965D/doi:10.48550/arXiv.2405.08965)\n\narXiv:[arXiv:2405.08965](https://ui.adsabs.harvard.edu/link_gateway/2024arXiv240508965D/arXiv:2405.08965)Bibcode:[2024arXiv240508965D](https://ui.adsabs.harvard.edu/abs/2024arXiv240508965D/abstract)Keywords:\n\n- Programming Languages;\n- Artificial Intelligence\n\nE-Print:OOPSLA 2025\n\nfull text sources\n\nPreprint\n\n\\|\n\nðŸŒ“","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fui.adsabs.harvard.edu%2Fabs%2F2024arXiv240508965D%2Fabstract","id":"https://ui.adsabs.harvard.edu/abs/2024arXiv240508965D/abstract","publishedDate":null,"summary":"The paper introduces Meaning-Typed Programming (MTP), a new paradigm designed to simplify AI-integrated software development by abstracting large language model (LLM) interactions through semantic-rich language constructs. Implemented in Jac, an extension of Python, MTP automates prompt creation, response processing, and LLM invocation via intuitive operators like the \"by\" operator, and employs a meaning-based intermediate representation (MT-IR) and runtime system (MT-Runtime). Experimental results show that MTP reduces coding complexity, development time (tasks completed 3.2x faster), and costs while maintaining or improving accuracy and efficiency. The approach demonstrates robustness even with degraded naming conventions and is available as open source within the Jaseci project. Overall, MTP advances seamless, efficient AI-embedded programming by leveraging semantic abstractions.","title":"Meaning-Typed Programming: Language Abstraction and ...","url":"https://ui.adsabs.harvard.edu/abs/2024arXiv240508965D/abstract"},{"content":"\\\\UseRawInputEncoding\n\n# Meaning-Typed Programming: Language-level Abstractions and Runtime for GenAI Applications\n\nJason Mars\nUniversity of MichiganUnited States[profmars@umich.edu](mailto:profmars@umich.edu),Â Yiping Kang\nUniversity of MichiganUnited States[ypkang@umich.edu](mailto:ypkang@umich.edu),Â Jayanaka L. Dantanarayana\nUniversity of MichiganUnited States[jayanaka@umich.edu](mailto:jayanaka@umich.edu),Â Kugesan Sivasothynathan\nUniversity of Moratuwa\nJaseci LabsSri Lanka[kugesan.sivasothynathan@jaseci.org](mailto:kugesan.sivasothynathan@jaseci.org),Â Christopher Clarke\nUniversity of MichiganUnited States[csclarke@umich.edu](mailto:csclarke@umich.edu),Â Baichuan Li\nUniversity of MichiganUnited States[patrli@umich.edu](mailto:patrli@umich.edu),Â Krisztian Flautner\nUniversity of MichiganUnited States[manowar@umich.edu](mailto:manowar@umich.edu)Â andÂ Lingjia Tang\nUniversity of MichiganUnited States[lingjia@umich.edu](mailto:lingjia@umich.edu)\n\n###### Abstract.\n\nSoftware is rapidly evolving from being programmed with traditional logical code, to _neuro-integrated_ applications that leverage generative AI and large language models (LLMs) for application functionality. This shift increases the complexity of building applications, as developers now must reasoning about, program, and prompt LLMs. Despite efforts to create tools to assist with prompt engineering, these solutions often introduce additional layers of complexity to the development of neuro-integrated applications.\n\nThis paper proposes _meaning-typed programming_ (MTP), a novel approach to simplify the creation of neuro-integrated applications by introducing new language-level abstractions that hide the complexities of LLM integration. _Our key insight is that typical conventional code already possesses a high level of semantic richness that can be automatically reasoned about, as it is designed to be readable and maintainable by humans._ Leveraging this insight, we conceptualize LLMs as meaning-typed code constructs and introduce a by abstraction at the language level, MT-IR, a new meaning-based intermediate representation at the compiler level, and MT-Runtime, an automated run-time engine for LLM integration and operations. We implement MTP in a production-grade Python super-set language called Jac and perform an extensive evaluation. Our results demonstrate that MTP not only simplifies the development process but also meets or exceeds the efficacy of state-of-the-art manual and tool-assisted prompt engineering techniques in terms of accuracy and usability.\n\n## 1\\. Introduction\n\nSoftware development is rapidly evolving towards _neuro-integrated_ applications that leverage generative AI and large language models (LLMs) for critical functionality. This shift represents a convergence of conventional programming with the powerful capabilities of AI, promising to revolutionize how we build software.\nHowever, integrating LLMs into traditional programming remains a complex challenge due to the fundamental differences between operating assumptions. In conventional programming, code describes operations performed on explicitly defined objects (FigureÂ [1](https://arxiv.org/html/2405.08965v3#S1.F1) left). In contrast, LLMs process natural language text as input and produce text-based outputs. To integrate LLMs into programs, developers must manually construct textual input, a process known as _prompt engineering_.\n\nFigure 1. Comparison of LLM as Meaning Typed Code construct abstraction (Right) with the present-day LLM abstraction (Middle) and Symbolic Programming Abstraction (Left) of a programming function.\n\n\\\\Description\n\nPrompt engineering, the primary means of LLM programming, can be tedious, complex, and time-consuming. Developers must craft precise instructions, manage textual context, and handle the inherent variability in LLM output. This process lacks the structure, consistency, and debugging capabilities developers are accustomed to in traditional programming environments.\n\nSeveral efforts have been made to address these challenges by developing infrastructure and tools to facilitate prompt engineering. Frameworks such as LangChain, Language Model Query Language (LMQL)Â ( [Beurer2023LMQL,](https://arxiv.org/html/2405.08965v3#bib.bib4)), DSPyÂ ( [khattab2023dspy,](https://arxiv.org/html/2405.08965v3#bib.bib16)), and SGLangÂ ( [zheng2023SGLang,](https://arxiv.org/html/2405.08965v3#bib.bib46)) have been introduced to assist developers in generating and managing prompts. However, while these frameworks provide more sophisticated tooling to help bridge the gap between conventional programming and the incorporation of AI models, in practice, they introduce additional layers of complexity for developers:\n\n1. (1)\n\n\n_Prompt design complexity:_ Developers remain responsible for the manual construction of prompts, including determining the appropriate prompt language and selecting relevant information to include.\n\n2. (2)\n\n\n_Steep learning curve:_ These systems require developers to familiarize themselves with new query languages, frameworks, or specialized syntax, complicating the integration process.\n\n3. (3)\n\n\n_Input/output conversion complexity:_ Parsing LLM output and converting them back to objects remains a challenge, especially given the variability in output across different LLMs.\n\n\nThe key insights of this work are: _(1) In practice, programs are written to be readable by various developers to comprehend the intentions and semantics conveyed by the code. This semantic richness can be leveraged to automatically translate intent embodied in the code to prompts for the LLM. (2) As state-of-the-art LLMs continue to advance, AI is increasingly feasible to infer code intentions without requiring explicit descriptions or prompt construction by developers. This trend is not only supported by our empirical findings, but we surprisingly find less can be more in prompt engineering._\n\nBased on this insight, this paper proposes _meaning-typed programming_, a novel approach to simplify the creation of neuro-integrated applications by introducing new language-level abstractions that hide the complexities of LLM integration. There are three key components to enable meaning-typed programs. These include (1) The â€™byâ€™ operator that enables the seamless integration of LLM functionality into code. It acts as a bridge between traditional and LLM operations, allowing developers to replace function bodies with a simple by model\\_name clause:\n\n\\\\Description\n\nThis declaration, along with the function signature and available semantic information, is sufficient for the LLM to produce the correct return value at runtime, significantly simplifying LLM integration in existing codebases. (2) MT-IR, a meaning-based intermediate representation at the compiler level that collects and organizes semantically rich information, preserving the meaning behind variable names, function signatures, and comments. MT-IR is also used dynamically to bind run-time values to semantic representations, enhancing the modelâ€™s understanding of program context and enabling effective, context-aware LLM integration.\n(3) MT-Runtime, an automated runtime engine within the languageâ€™s virtual machine, triggered at by call sites. It binds runtime values to MT-IR, manages LLM interactions, generates context-aware prompts, and handles responses and errors. MT-Runtime allows developers to focus on application logic while automatically managing LLM integration, ensuring seamless and contextualized operations.\n\nWe implement _meaning-typed programming (MTP)_ in a production-grade Python superset language called Jac and perform an extensive evaluation. Specifically, we make the following contributions:\n\n- â€¢\n\n\nWe introduce the concept of meaning-typed programming, a novel paradigm that treats LLMs as meaning-typed code constructs, enabling seamless integration of LLMs into conventional programs without explicit prompt engineering.\n\n- â€¢\n\n\nWe present the design and implementation of the MTP framework: the â€˜byâ€™ operator for intuitive LLM integration; MT-IR, a semantically rich intermediate representation for implied meaning; and MT-Runtime, an automated runtime engine for LLM operations\n\n- â€¢\n\n\nWe implement MTP in Jac, a production-grade Python superset language, conduct extensive evaluation and show that MTP significantly reduces development complexity, lines of code, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches.\n\n- â€¢\n\n\nWe provide an empirical analysis of LLM evolution trends, demonstrating how MTP takes advantage of advances in LLM capabilities to simplify neuro-integrated programming over time.\n\n- â€¢\n\n\nWe present multiple case studies demonstrating the application of MTP in complex problem domains, analyzing its impact on code structure, development workflow, and complexity reduction compared to traditional LLM integration methods.\n\n\nOur results show that MTP simplifies the development process and meets or exceeds the efficacy of state-of-the-art manual and tool-assisted prompt engineering techniques. This approach significantly reduces development complexity and lines of code while improving run-time performance and costs associated with LLM integration.\n\n## 2\\. Problem, Background, and Insights\n\nConventional Prompt Engineering\n\n[â¬‡](data:;base64,CmdwdCA9IE9wZW5BSSgpCgpkZWYgcmVzcG9uZF93aXRoX2NvbmZpZGVuY2VfbGV2ZWwocXVlcnk6IHN0ciwKICAgIGNoYXRfaGlzdG9yeTogZGljdFtzdHIsc3RyXSwKICAgIHVzZXJfZGF0YTpkaWN0KSAtPiB0dXBsZVtzdHIsZmxvYXRdOgoKICAgIHN5c3RlbV9tZXNzYWdlID0gKAogICAgICAgICJZb3UgYXJlIGEgY2hhdGJvdC4gUmVzcG9uZCB0byB0aGUgY2hhdCB1c2luZyB0aGUgYXZhaWxhYmxlIGluZm9ybWF0aW9uLiBBZGRpdGlvbmFsbHkgZ2VuZXJhdGUgYSBjb25maWRlbmNlIGxldmVsIG9mIHRoZSByZXNwb25zZSBhcyBhIHBlcmNlbnRhZ2UiICkKCiAgICBwcm9tcHQgPSAgc3lzdGVtX21lc3NhZ2UKICAgIHByb21wdCArPSBmJ1xuUXVlcnkgZm9ybSB0aGUgdXNlcjpcbntxdWVyeX0nCiAgICBwcm9tcHQgKz0gJ1xuQ2hhdCBIaXN0b3J5OlxuJwogICAgZm9yIHVzZXIsY29udGVudCBpbiBjaGF0X2hpc3Rvcnk6CiAgICA","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2405.08965v3","id":"https://arxiv.org/html/2405.08965v3","publishedDate":"2025-01-16T00:00:00.000Z","summary":"The paper introduces Meaning-Typed Programming (MTP), a novel approach to simplify the development of neuro-integrated applications that leverage large language models (LLMs) and generative AI. Traditional methods rely heavily on prompt engineering, which is complex and error-prone. MTP addresses this by introducing language-level abstractions that treat LLM interactions as meaning-typed code constructs, effectively hiding their complexity. Key components include a meaning-based intermediate representation (MT-IR) and a runtime engine (MT-Runtime) that automate LLM integration. Implemented in a Python extension called Jac, MTP enhances usability and accuracy, making the development of AI-augmented applications more efficient and maintainable. The approach aims to bridge the gap between conventional programming and AI-driven functionalities, reducing developer burden and improving robustness in neuro-integrated software development.","title":"Language-level Abstractions and Runtime for GenAI ...","url":"https://arxiv.org/html/2405.08965v3"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F385216269_Meaning_Typed_Prompting_A_Technique_for_Efficient_Reliable_Structured_Output_Generation","id":"https://www.researchgate.net/publication/385216269_Meaning_Typed_Prompting_A_Technique_for_Efficient_Reliable_Structured_Output_Generation","publishedDate":"2024-10-22T00:00:00.000Z","summary":"Failed to fetch content","title":"A Technique for Efficient, Reliable Structured Output ...","url":"https://www.researchgate.net/publication/385216269_Meaning_Typed_Prompting_A_Technique_for_Efficient_Reliable_Structured_Output_Generation"},{"content":"[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\n[We gratefully acknowledge support from\\\n\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=http%3A%2F%2Farxiv.org%2Flist%2Fcs%2F2024-05%3Fskip%3D2980%26show%3D2000","id":"http://arxiv.org/list/cs/2024-05?skip=2980&show=2000","publishedDate":null,"summary":"The webpage lists recent computer science research papers from arXiv for May 2024, specifically from the section starting at entry 2980. It features Cornell Universityâ€™s involvement and acknowledges support from the Simons Foundation and associated institutions. The page provides access to a collection of scholarly articles in computer science, highlighting ongoing research updates for that period.","title":"Computer Science May 2024","url":"http://arxiv.org/list/cs/2024-05?skip=2980&show=2000"},{"content":"Meaning Typed Prompting: A Technique for Efficient, Reliable Structured\nOutput Generation\nChandra Irugalbandara\nASCII Corp\nirugalbandara@ascii.ai\nAbstract\nExtending Large Language Models (LLMs) to\nadvanced applications requires reliable struc\u0002tured output generation. Existing methods\nwhich often rely on rigid JSON schemas, can\nlead to unreliable outputs, diminished reason\u0002ing capabilities, and increased computational\noverhead, limiting LLMsâ€™ adaptability for com\u0002plex tasks. We introduce Meaning Typed\nPrompting (MTP), a technique for efficient\nstructured output generation that integrates\ntypes, meanings, and abstractions, such as\nvariables and classes, into the prompting pro\u0002cess. By utilizing expressive type definitions,\nMTP enhances output clarity and reduces de\u0002pendence on complex abstractions, simplify\u0002ing development, and improving implemen\u0002tation efficiency. This enables LLMs to un\u0002derstand relationships and generate structured\ndata more effectively. Empirical evaluations\non multiple benchmarks demonstrate that MTP\noutperforms existing frameworks in accuracy,\nreliability, consistency, and token efficiency.\nWe present Semantix, a framework that imple\u0002ments MTP, providing practical insights into its\napplication1.\n1 Introduction\nLarge Language Models (LLMs) have enabled the\ndevelopment of sophisticated applications, includ\u0002ing complex task automation and domain-specific\nsolutions (Xia et al., 2024) where strict adherence\nto formatting standards is crucial. Structured out\u0002puts improve AI integration into development tools\nby offering consistent output structures, simplify\u0002ing error handling, and making LLM-generated\nresponses more reliable for real-world use. With\nthe rise of agentic frameworks (Wu et al., 2023;\nMitra et al., 2024), having structured outputs is\nparticularly important for seamless data parsing\nwithout an external output validator.\n1\nSource codes for Semantix is available here\nCurrent approaches for generating structured\noutputs typically employ zero-shot (Kojima et al.,\n2022) or few-shot prompting (Brown et al., 2020),\nwhere developers provide instructions alongside\nJSON templates or Schemas to define the de\u0002sired output format (Khattab et al., 2023; Beurer\u0002Kellner et al., 2022; Willard and Louf, 2023). Post\u0002processing techniques are then used to extract and\nvalidate the generated data. Although OpenAIâ€™s\nconstrained decoding strategy (Pokrass et al., 2024)\nensures reliability, it limits reasoning capabilities\ndue to its reliance on restriction on JSON Schemas\n(Tam et al., 2024). Additionally, detailed JSON\nSchemas increase token consumption and intro\u0002duce challenges in maintaining syntactic correct\u0002ness, often resulting in invalid outputs. Past at\u0002tempts have been made to develop frameworks for\nstructured output generation in LLMs. However,\nthey either require full access to the language mod\u0002els (Willard and Louf, 2023), use proprietary ser\u0002vices such as Function Calling (OpenAI, 2023a;\nVertexAI, 2023) or JSON Mode (OpenAI, 2023b)\ninternally or suffer from issues of reliability and\nprecision, frequently consuming excessive tokens\n(Liu, 2023; Olivier, 2024; Liu, 2022; Khattab et al.,\n2023). Furthermore, these methods often depend\non hard-coded prompt templates or require manual\nprompting reducing adaptability.\nTo address these limitations, we introduce Mean\u0002ing Typed Prompting (MTP), a novel approach for\nstructured output generation. Building on MTP,\nwe present Semantix, a framework that uses MTP\nat its core to embed semantic information directly\ninto type definitions with expressive meaning types,\neliminating the need for additional abstractions.\nThis approach reduces dependency on prompt con\u0002figurations and simplifies the frameworkâ€™s learning\ncurve. Inspired by MTLLM (Mars et al., 2024),\nwhich introduces meaning-typed transformations,\nSemantix extends this paradigm by specifically tar\u0002geting structured output generation through MTP.\n1\narXiv:2410.18146v1 [cs.CL] 22 Oct 2024\n{\"full_name\":\"Albert\nEinstein\",\"yod\":1955,\"personality\":\"introvert\",\"life_works\":\n[{\"work\":\"Theory of Relativity\",\"year\":1915},\n{\"work\":\"Photoelectric Effect\",\"year\":1905},\n{\"work\":\"E=mcÂ²\",\"year\":1905}]}\nModel Output\nOpenAI Structured Output\nSystem Message\n# Goal:Â  (get_person_info)\n## Output Type Definiti\n- Person\n## Type Definitio\n- LifeWork (class) -> LifeWork(work: str, year: int)\n- Person (class) -> Person(full_name: str, yod: int - year of death\", personality: Personality,\nlife_works: list[LifeWork] - Three major works in life\")\n- Personality (Enum) -> Personality.INTROVERT, Personality.EXTROVERT\nUser Message\n## Inputs\n- name (str) = \"Albert Einstein\"\nFollow the following template to provide the answer.\n```output\nProvide the output in the desired output type.\n```\nModel Input\n```output\nPerson(full_name=\"Albert Einstein\", yod=1955, personality=Personality.INTROVERT,\nlife_works=[\nÂ  Â  LifeWork(work=\"Theory of Relativity\", year=1915),\nÂ  Â  LifeWork(work=\"Photoelectric Effect\", year=1905),\nÂ  Â  LifeWork(work=\"Unified Field Theory\", year=195\n])\n```\nModel Output\nSemantix with MTP\nPerson(full_name='Albert Einstein', yod=1955, personality=\n,Â [LifeWork(work=\"Theory of Relativity\",\nyear=1915),Â LifeWork(work=\"Photoelectric Effect\",\nyear=1905),Â LifeWork(work=\"E=mc\", year=1905)])\nParsed Output\nPerson(full_name='Albert Einstein', yod=1955, personality=\n, life_works=\n[LifeWork(work=\"Theory of Relativity\",\nyear=1915), LifeWork(work=\"Photoelectric Effect\",\nyear=1905), LifeWork(work=\"Unified Field Theory\", year=1950)])\nJSON Schema is used within the model input\nParsed Output\nToken Usage: 180\nToken Usage: 61\nToken Usage: 141\nToken Usage: 73\nSystem Message\nYour input elds ar\n1. `name` (str): Name of the Person\nYour output elds ar\n1. `person` (Person): ${person}. Respond with a single JSON object. JSON Schema:\n{\"$defs\": {\"LifeWork\": {\"properties\": {\"work\": {\"title\": \"Work\", \"type\": \"string\"}, \"year\": {\"title\":\n\"Year\", \"type\": \"integer\"}}, \"required\": [\"work\", \"year\"], \"title\": \"LifeWork\", \"type\": \"object\"},\n\"Personality\": {\"enum\": [\"introvert\", \"extrovert\"], \"title\": \"Personality\", \"type\": \"string\"}},\n\"properties\": {\"full_name\": {\"title\": \"Full Name\", \"type\": \"string\"}, \"yod\": {\"title\": \"year of\ndeath\", \"type\": \"integer\"}, \"personality\": {\"$ref\": \"#/$defs/Personality\"}, \"life_works\":\n{\"items\": {\"$ref\": \"#/$defs/LifeWork\"}, \"title\": \"Three major works in life\", \"type\": \"array\"}},\n\"required\": [\"full_name\", \"yod\", \"personality\", \"life_works\"], \"title\": \"Person\", \"type\":\n\"object\"}\nAll interactions will be structured in the following way, with the appropriate values lled i\n[[ ## name ## ]]\n{name}\n[[ ## person ## ]]\n{person}\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is:Â \nÂ  Â  Â  Â  Given the elds `name`, produce the elds `pers\nUser Message\n[[ ## name ## ]]\nAlbert Einstein\nRespond with the corresponding output elds, starting with the eld `person`, and \nending with the marker for `completed`.\nModel Input\n[[ ## person ## ]]\n{\nÂ  \"full_name\": \"Albert Einstein\",\nÂ  \"yod\": 1955,\nÂ  \"personality\": \"introvert\",\nÂ  \"life_works\": [\nÂ  Â  {\nÂ  Â  Â  \"work\": \"Theory of Relativity\",\nÂ  Â  Â  \"year\": 1915\nÂ  Â  },\nÂ  Â  {\nÂ  Â  Â  \"work\": \"Photoelectric Effect\",\nÂ  Â  Â  \"year\": 1905\nÂ  Â  },\nÂ  Â  {\nÂ  Â  Â  \"work\": \"Brownian Motion\",\nÂ  Â  Â  \"year\": 1905\nÂ  Â  }\nÂ  ]\n}\n[[ ## completed ## ]]\nModel Output\nDSPy\nPerson(full_name='Albert\nEinstein' yod=1955\npersonality=\n life_works=\n[LifeWork(work='Theory of\nRelativity', year=1915),\nLifeWork(work='Photoelectric\nEffect', year=1905),\nLifeWork(work='Brownian\nMotion', year=1905)])\nParsed Output\nToken Usage: 364\nToken Usage: 112\nPersonÂ (Â \nfull_name = \"Albert Einstein\",Â \nyod=1955,\npersonality=Personality.INTROVERT,\nlife_works=[\nLifeWork(\nwork=\"Theory of Relativity\",Â \nyear=1915\n),\n....\n]\n)\nExpected Output\nString name = \"Albert\nEinstein\"\nInput\nEnum Personality\nINTROVERT = \"extrovert\"\nEXTROVERT = \"introvert\"\nClass LifeWork\nwork: string\nyear: integer\nClass Person\nfull_name: string\nyod: integer\npersonality: Personality\nlife_works: list[LifeWork]\nCode Constructs\nSystem Message\nGet Information of the Person.\nUser Message\nName of the Person : Albert Einstein\nModel Input\nFigure 1: Comparison of structured output methodsâ€”OpenAI, Semantix, and DSPyâ€”using identical code constructs\nand the same LLM (gpt-4o-mini). OpenAI and DSPy convert types into JSON Schema and output JSON; Semantix\nuses natural language representations and outputs object representations. Semantix has the lowest input token usage\nand, for longer outputs, consistently uses fewer tokens due to less frequent use of {}\".\nThis advancement effectively addresses the limi\u0002tations of JSON Schema based methods, function\ncalling, and constrained decoding.\nWe evaluate Semantix on benchmarks for struc\u0002tured outputs, including multi-label classification,\nnamed entity recognition, and synthetic data gen\u0002eration (Leo, 2024). To assess its impact on rea\u0002soning capabilities, we also conduct evaluations\non GSM8k (Cobbe et al., 2021) and MMMU (Yue\net al., 2024). Our empirical results demonstrate that\nSemantix significantly enhances output reliability,\nclarity, and token efficiency compared to existing\nmethods.\n2 Related works\nStructured output generation from LLMs is essen\u0002tial for ensuring consistency and efficiency (Parnin\net al., 2023; Fan et al., 2023). Several frameworks\ntackle this challenge using different approaches.\nOpenAIâ€™s structured outputs (Pokrass et al., 2024)\nemploy JSON Schema-based constrained decoding\nto ensure outputs align with predefined formats.\nHowever, this method is restricted to closed-source\nAPIs, limiting transparency, flexibility, and adapt\u0002ability in diverse environments. Outline (Willard\nand Louf, 2023) extends this concept to open\u0002source models by incorporating regular expres\u0002sions, making it more accessible for developers.\nMarvin (Perfect, 2024), ModelSmith (Olivier,\n2024), Instructor (Liu, 2023), and Fructose (Ba\u0002nanaML, 2023) rely on JSON schemas, leverag\u0002ing JSON Mode or function-calling APIs, which\nare typically available only in closed-source LLM\nAPIs. Fruc","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2410.18146","id":"https://arxiv.org/pdf/2410.18146","publishedDate":null,"summary":"The paper introduces Meaning Typed Prompting (MTP), a novel technique designed to improve the efficiency, reliability, and accuracy of structured output generation by Large Language Models (LLMs). Unlike traditional methods that rely heavily on rigid JSON schemasâ€”often leading to increased token consumption and unreliable outputsâ€”MTP integrates semantic types, meanings, variables, and abstractions directly into prompts. This enhances clarity, reduces complexity, and allows LLMs to better understand relationships within data. The authors also present Semantix, a framework implementing MTP, demonstrating superior performance over existing approaches in benchmarks related to structured data generation. Overall, MTP simplifies development, improves output quality, and makes structured output generation more adaptable for complex, real-world applications.","title":"arXiv:2410.18146v1 [cs.CL] 22 Oct 2024","url":"https://arxiv.org/pdf/2410.18146"},{"content":"# Computer Science > Programming Languages\n\n**arXiv:2411.07452** (cs)\n\n\\[Submitted on 12 Nov 2024\\]\n\n# Title:Top-Down or Bottom-Up? Complexity Analyses of Synchronous Multiparty Session Types\n\nAuthors: [Thien Udomsrirungruang](https://arxiv.org/search/cs?searchtype=author&query=Udomsrirungruang,+T), [Nobuko Yoshida](https://arxiv.org/search/cs?searchtype=author&query=Yoshida,+N)\n\nView a PDF of the paper titled Top-Down or Bottom-Up? Complexity Analyses of Synchronous Multiparty Session Types, by Thien Udomsrirungruang and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2411.07452)\n\n> Abstract:Multiparty session types provide a type discipline for ensuring communication safety, deadlock-freedom and liveness for multiple concurrently running participants. The original formulation of MPST takes the top-down approach, where a global type specifies a bird's eye view of the intended interactions between participants, and each distributed process is locally type-checked against its end-point projection. A more recent one takes the bottom-up approach, where a desired property $\\\\varphi$ of a set of participants is ensured if the same property $\\\\varphi$ is true for an ensemble of end-point types (a typing context) inferred from each participant.\n>\n> This paper compares these two main procedures of MPST, giving their detailed complexity analyses. To this aim, we build several new algorithms missing from the bottom-up or top-down workflows by using graph representation of session types. We first propose a subtyping system based on type graphs, offering more efficient subtype-checking than the existing (exponential) inductive algorithm. Next for the top-down, we measure complexity of the four end-point projections from the literature. For bottom-up, we develop a novel type inference system from MPST processes which generates minimum type graphs, succinctly capturing covariance of internal choice and contravariance of external choice. For property-checking of typing contexts, we achieve PSPACE-hardness by reducing it from the quantified Boolean formula problem, and prove membership in PSPACE. Finally, we calculate the total complexity of the top-down and the bottom-up approaches. Our analyses reveal that the top-down based on global types is more efficient than the bottom-up in many realistic cases; liveness checking for typing contexts in the bottom-up has the highest complexity; and the type inference costs exponential against the size of a process, which impacts the total complexity.\n\n| | |\n| --- | --- |\n| Comments: | 65 pages, 13 figures. Full version of a paper to appear in POPL 2025 |\n| Subjects: | Programming Languages (cs.PL) |\n| Cite as: | [arXiv:2411.07452](https://arxiv.org/abs/2411.07452) \\[cs.PL\\] |\n| | (or [arXiv:2411.07452v1](https://arxiv.org/abs/2411.07452v1) \\[cs.PL\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2411.07452](https://doi.org/10.48550/arXiv.2411.07452) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Thien Udomsrirungruang \\[ [view email](https://arxiv.org/show-email/51cdde79/2411.07452)\\]\n\n**\\[v1\\]**\nTue, 12 Nov 2024 00:28:06 UTC (253 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Top-Down or Bottom-Up? Complexity Analyses of Synchronous Multiparty Session Types, by Thien Udomsrirungruang and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2411.07452)\n- [TeX Source](https://arxiv.org/src/2411.07452)\n- [Other Formats](https://arxiv.org/format/2411.07452)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.PL\n\n[<Â prev](https://arxiv.org/prevnext?id=2411.07452&function=prev&context=cs.PL)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2411.07452&function=next&context=cs.PL)\n\n[new](https://arxiv.org/list/cs.PL/new) \\| [recent](https://arxiv.org/list/cs.PL/recent) \\| [2024-11](https://arxiv.org/list/cs.PL/2024-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2411.07452?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2411.07452)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2411.07452)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2411.07452)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2411.07452&description=Top-Down or Bottom-Up? Complexity Analyses of Synchronous Multiparty Session Types) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2411.07452&title=Top-Down or Bottom-Up? Complexity Analyses of Synchronous Multiparty Session Types)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2411.07452) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2411.07452","id":"https://arxiv.org/abs/2411.07452","publishedDate":"2024-11-12T00:00:00.000Z","summary":"This paper analyzes the complexity of two main approaches in Multiparty Session Types (MPST): the top-down (global type-based) and bottom-up (end-point type inference) methods. MPST ensures safe, deadlock-free communication among multiple participants. The authors develop new algorithms, including a graph-based subtyping system that improves efficiency over existing exponential algorithms. They evaluate the complexity of traditional projections and introduce a novel type inference system for bottom-up approaches, capturing type covariance and contravariance. Their results show that the top-down approach generally offers better efficiency, especially for liveness checking, which is PSPACE-hard, and that bottom-up type inference can be exponentially costly relative to process size. Overall, the study provides a detailed complexity comparison, highlighting the practical advantages of the global, top-down method in MPST.","title":"Top-Down or Bottom-Up? Complexity Analyses of Synchronous Multiparty Session Types","url":"https://arxiv.org/abs/2411.07452"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F380607996_Meaning-Typed_Programming_Language-level_Abstractions_and_Runtime_for_GenAI_Applications%2Fdownload","id":"https://www.researchgate.net/publication/380607996_Meaning-Typed_Programming_Language-level_Abstractions_and_Runtime_for_GenAI_Applications/download","publishedDate":"2024-05-14T00:00:00.000Z","summary":"Failed to fetch content","title":"Language-level Abstractions and Runtime for GenAI Applications","url":"https://www.researchgate.net/publication/380607996_Meaning-Typed_Programming_Language-level_Abstractions_and_Runtime_for_GenAI_Applications/download"}],"status":"completed","toolCallId":"call_WvYHleOgEFPv0t8n9jP6ThOJ","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Hereâ€™s a concise, practical summary and the problem it tackles.\n\nWhat problem is it tackling?\n- Todayâ€™s â€œAI-integratedâ€ apps call LLMs at runtime for core functionality. Developers have to:\n  - Manually design prompts, pick context, and tweak wording\n  - Learn and wire up new frameworks/languages for prompting\n  - Parse free-form LLM output back into program objects and types\n- This makes systems fragile, hard to maintain, and costly to evolve. Even with tools like LangChain, LMQL, DSPy, and SGLang, devs still shoulder prompt design, IO conversions, and framework complexity.\n\nCore idea\n- Code already encodes a lot of meaning: function names, signatures, variable names, docstrings, and types. Modern LLMs can infer intent from this semantic context.\n- Meaning-Typed Programming (MTP) treats LLMs as first-class, meaning-typed code constructs, so the language and runtime extract semantics from code and automatically generate/promote the right prompts and parse structured outputsâ€”without manual prompt engineering.\n\nWhat MTP proposes\n1) â€œbyâ€ operator (language construct)\n- Replace a function body with a clause like by model_name to delegate the function to an LLM at runtime.\n- The system uses the function signature, names, comments, and types as implicit intent and context.\n\n2) MT-IR (Meaning-Typed Intermediate Representation)\n- A compiler-level IR that captures semantically rich info about the codebase: names, types, signatures, comments, relationships.\n- At runtime, it binds actual values to this semantic graph to form the LLM input.\n\n3) MT-Runtime\n- A runtime engine that triggers at by call sites, assembles context-aware prompts from MT-IR plus runtime values, calls the selected model, validates/parses outputs back into the declared return types, and handles errors and retries.\n\nImplementation and results\n- Implemented in Jac (a production-grade Python superset) as part of the Jaseci open-source project (module: byLLM).\n- Empirical findings (OOPSLA 2025 version):\n  - Developers completed tasks 3.2x faster with 45% fewer lines of code compared to existing frameworks.\n  - Lower development complexity and costs; improved runtime performance while maintaining or improving accuracy.\n  - Robust even when up to 50% of naming conventions are degraded (i.e., it still extracts enough meaning to work well).\n  - Observes a â€œless is moreâ€ trend in prompt engineering as model capabilities improveâ€”leaning on code semantics reduces the need for lengthy, brittle prompts.\n\nHow it compares to existing tools\n- Unlike LangChain/LMQL/DSPy/SGLang, which help you craft and manage prompts, MTP makes prompt construction an automated language/runtime responsibility, using the programâ€™s own semantics as the source of truth.\n- It also standardizes output parsing via declared types and function signatures, reducing brittle custom parsing.\n\nWhy this matters\n- Makes AI integration feel like normal programming: declare intent via code, let the language/runtime translate to/from LLMs.\n- Reduces maintenance burden and onboarding cost, and scales better across teams and codebases as LLMs improve.\n- Bridges symbolic code and neural components with a principled, type- and meaning-aware abstraction layer.\n\nIf you want, I can show a small before/after code sketch (with and without MTP) to illustrate how the by operator and MT-IR-driven prompting change developer workflow.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_091516b5e84b3ce10068c86c4fa6d481a197ff5c96495b618d"}},"resumableStreamId":"STREAM:09b97986-ab02-446f-8867-c17ab9d988dd","role":"assistant","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","timeToFirstToken":1.807,"tokens":722,"tokensPerSecond":30.53757983335448,"updated_at":1757965380433,"userId":"google:104458561188320708185","id":"j97ft93grcetqbfzp4e4fz9zcn7qm9qx"},{"_creationTime":1757965416452.3137,"_id":"j97fb49wr3bhy64tre8qwcx9157qn2q5","attachmentIds":[],"content":"explain the \"by\" opeartor in detail","created_at":1757965416452,"messageId":"b00b61bb-a78e-4da8-b825-264534061bf4","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","updated_at":1757965416452,"userId":"google:104458561188320708185","id":"j97fb49wr3bhy64tre8qwcx9157qn2q5","providerMetadata":{}},{"_creationTime":1757965416452.314,"_id":"j979xmyv4ftxwdkwy4wefh1ed97qm2aa","attachmentIds":[],"byok":false,"content":"Hereâ€™s a focused deep dive on the by operator from the MTP paper.\n\nWhat it is\n- A language-level construct that replaces a functionâ€™s body with a declarative clause delegating its implementation to an LLM at runtime.\n- Syntax pattern (in Jac, the reference implementation): define a normal function with name, params, return type, docstring/comments, then specify by <model/adapter/options> instead of writing a body.\n\nIntended developer experience\n- You declare intent via ordinary code semantics (names, types, docstrings) rather than writing prompts.\n- The compiler and runtime use your functionâ€™s â€œmeaningâ€ to auto-generate an effective prompt and to parse/validate the result back into the declared return type.\n\nHow it works end-to-end\n1) Declaration time (compile-time)\n- You write a function signature and annotate types. Optionally include docstrings/comments and meaningful parameter names.\n- You attach a by clause indicating the model (e.g., by gpt-4o) and possibly extra options (temperature, tools, system hints).\n- The compiler lifts the functionâ€™s semantics into MT-IR (Meaning-Typed Intermediate Representation), capturing:\n  - Function name and purpose\n  - Parameter names, types, and defaults\n  - Return type and shape (primitives, enums, records/structs, lists, unions)\n  - Docstrings and inline comments\n  - Related symbols in scope that may be relevant context\n\n2) Call time (runtime)\n- When the function is called, MT-Runtime binds actual argument values to the MT-IR and assembles a context-aware prompt automatically.\n- It invokes the specified model and enforces output shaping:\n  - Prefer structured decoding (e.g., JSON/object mode) aligned to the return type\n  - Validate and coerce results to the declared type; if invalid, perform guided repairs/retries\n  - Surface typed errors if recovery fails\n\n3) Post-processing and control\n- The runtime can:\n  - Cache results keyed by inputs (memoization)\n  - Log prompts/traces for observability\n  - Apply guardrails (regex/schema validation, safe tool calls)\n  - Fall back to alternative models or deterministic code if configured\n\nWhy it works\n- Modern LLMs can infer task intent from code semantics. The by operator standardizes how that intent is extracted and presented.\n- Types provide a contract that tames free-form outputs, making LLM calls predictable, testable, and composable.\n\nKey semantics and guarantees\n- Fidelity to signature: Inputs are passed as explicit, labeled arguments; outputs must conform to the return type or an error is raised.\n- Meaning-first prompting: The primary prompt content is derived from code meaning (names/types/docs), not ad hoc strings.\n- Determinism envelope: While the LLM is nondeterministic, the operator narrows variability via schema-constrained decoding, retries, and temperature settings.\n- Backwards compatibility: You can replace a by function with a hand-written body later without changing call sites.\n\nTypical usage patterns\n- Pure inference helpers: classify, extract, summarize with typed outputs.\n- Schema-constrained generation: return well-typed objects, lists, unions.\n- Tool-augmented behaviors: allow the model to call registered tools/APIs (if enabled in options).\n- Incremental migration: swap brittle prompt-engineered functions for by functions, keeping the same signatures.\n\nConfiguration knobs (conceptual)\n- Model and endpoint: model name/provider; can be overridden centrally.\n- Decoding: temperature, max tokens, top-p, etc.\n- Output mode: JSON/schema-first, function-call mode, or free text with post-parse.\n- Guardrails: validation strategies, retry limits, safety policies.\n- Context scope: what symbols/comments to include, depth limits.\n- Cost/perf: caching, batching, parallelism.\n\nError handling and robustness\n- Schema mismatch: runtime prompts the model to repair to the required type, with a bounded number of retries.\n- Ambiguity: the runtime can request clarification from the model (self-ask) or escalate an error.\n- Underspecified names: the paper reports resilience even when up to ~50% of naming quality is degraded; meaning is still recovered from remaining semantics and types.\n\nHow it differs from â€œprompt functionsâ€ in frameworks\n- You do not write the prompt template; the operator delegates prompt assembly to the language/runtime using MT-IR.\n- Output typing is not an add-on; it is the source of truth that drives structured decoding and validation.\n- It is a first-class construct of the language, not a library convention.\n\nMental model\n- Think of by as â€œthis functionâ€™s body is implemented by a model, but the contract (signature, types, doc) is mine; the language will translate between my code world and the model world.â€\n\nA small, concrete sketch (illustrative, not exact)\n- Without by:\n  - You handcraft a prompt, bind variables, call an SDK, parse JSON, validate types, handle retries.\n- With by:\n  - You declare:\n    - function extract_items(text: str) -> list[Item] by gpt-4o\n    - where Item is a typed record with fields you defined\n  - Call extract_items(doc) as normal; runtime handles prompt, parsing, validation, retries.\n\nTesting and debugging\n- Unit tests call the function like any other. You can:\n  - Inject a mock model or recorded traces\n  - Inspect generated prompts/traces in logs\n  - Verify typed outputs without brittle string asserts\n\nPerformance and cost\n- Fewer tokens: prompts are concise and structured; runtime can elide redundant context.\n- Fewer round-trips: schema-first decoding reduces back-and-forth; validation-guided retries are targeted.\n- Centralized controls let teams swap models or adjust temperatures without touching call sites.\n\nWhen not to use it\n- Deterministic or compute-heavy code you can write normally.\n- Tight latency SLOs where LLM variability is unacceptable.\n- Tasks lacking stable type contracts or where free-form prose is the actual required output (unless you still want minimal constraints like str with style hints).\n\nBottom line\n- The by operator turns â€œprompted function programmingâ€ into â€œtyped, meaning-driven function delegation.â€ You keep familiar language constructs; the compiler/runtime shoulder prompt construction, context assembly, and robust typed IO with LLMs.","created_at":1757965416453,"messageId":"6f71108d-cc00-4e83-9d3b-f13e38f8f21f","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":null,"queries":["\"MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\" by operator details MT-IR MT-Runtime Jac byLLM arXiv:2405.08965","site:arxiv.org 2405.08965 MTP by operator","Jaseci byLLM operator \"by\" language construct meaning-typed programming"]},"result":[{"content":"# Computer Science > Computation and Language\n\n**arXiv:2503.14985** (cs)\n\n\\[Submitted on 19 Mar 2025 ( [v1](https://arxiv.org/abs/2503.14985v1)), last revised 26 Mar 2025 (this version, v2)\\]\n\n# Title:ML-Triton, A Multi-Level Compilation and Language Extension to Triton GPU Programming\n\nAuthors: [Dewei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+D), [Wei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu,+W), [Liyang Ling](https://arxiv.org/search/cs?searchtype=author&query=Ling,+L), [Ettore Tiotto](https://arxiv.org/search/cs?searchtype=author&query=Tiotto,+E), [Quintin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+Q), [Whitney Tsang](https://arxiv.org/search/cs?searchtype=author&query=Tsang,+W), [Julian Opperman](https://arxiv.org/search/cs?searchtype=author&query=Opperman,+J), [Jacky Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng,+J)\n\nView a PDF of the paper titled ML-Triton, A Multi-Level Compilation and Language Extension to Triton GPU Programming, by Dewei Wang and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.14985)\n\n> Abstract:In the era of LLMs, dense operations such as GEMM and MHA are critical components. These operations are well-suited for parallel execution using a tilebased approach. While traditional GPU programming often relies on low level interfaces like CUDA or SYCL, Triton has emerged as a DSL that offers a more user-friendly and portable alternative by programming at a higher level. The current Triton starts at the workgroup (aka threadblock) level, and directly lowers to per-thread level. And then attempt to coalesce and amend through a series of passes, promoting information from low-level representation. We believe this is pre-mature lowering based on the below observations. 1. GPU has a hierarchical structure both physically and logically. Modern GPUs often feature SIMD units capable of directly operating on tiles on a warp or warpgroup basis, such as blocked load and blocked MMA. 2. Multi-level gradual lowering can make compiler decoupled and clean by separating considerations inter and intra a logical layer. 3. Kernel developers often need fine control to get good performance on the latest hardware. FlashAttention2 advocates explicit data partition between warps to make a performance boost. In this context, we propose ML-Triton which features multi-level compilation flow and programming interface. Our approach begins at the workgroup level and progressively lowers to the warp and intrinsic level, implementing a multilevel lowering align with the hierarchical nature of GPU. Additionally, we extend triton language to support user-set compiler hint and warp level programming, enabling researchers to get good out-of-the box performance without awaiting compiler updates. Experimental results demonstrate that our approach achieves performance above 95% of expert-written kernels on Intel GPU, as measured by the geometric mean.\n\n| | |\n| --- | --- |\n| Subjects: | Computation and Language (cs.CL) |\n| Cite as: | [arXiv:2503.14985](https://arxiv.org/abs/2503.14985) \\[cs.CL\\] |\n| (or [arXiv:2503.14985v2](https://arxiv.org/abs/2503.14985v2) \\[cs.CL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.14985](https://doi.org/10.48550/arXiv.2503.14985) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Dewei Wang \\[ [view email](https://arxiv.org/show-email/c5ca3da8/2503.14985)\\] **[\\[v1\\]](https://arxiv.org/abs/2503.14985v1)**\nWed, 19 Mar 2025 08:31:39 UTC (2,148 KB)\n**\\[v2\\]**\nWed, 26 Mar 2025 04:06:37 UTC (2,148 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled ML-Triton, A Multi-Level Compilation and Language Extension to Triton GPU Programming, by Dewei Wang and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.14985)\n- [TeX Source](https://arxiv.org/src/2503.14985)\n- [Other Formats](https://arxiv.org/format/2503.14985)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[<Â prev](https://arxiv.org/prevnext?id=2503.14985&function=prev&context=cs.CL)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2503.14985&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2025-03](https://arxiv.org/list/cs.CL/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.14985?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.14985)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.14985)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.14985)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.14985) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.14985","id":"https://arxiv.org/abs/2503.14985","publishedDate":"2025-03-19T00:00:00.000Z","summary":"The paper introduces ML-Triton, an extension and multi-level compilation framework for Triton GPU programming aimed at optimizing dense operations like GEMM and MHA crucial for large language models. Unlike traditional GPU programming models that operate at a single level, ML-Triton leverages the hierarchical structure of modern GPUs by progressively lowering from workgroup to warp and intrinsic levels. This approach enables more fine-grained control, better performance, and easier compiler design by separating considerations across different logical levels. The extension includes user-set compiler hints and warp-level programming features, allowing developers to achieve near-expert kernel performanceâ€”over 95%â€”on Intel GPUs. Overall, ML-Triton enhances portability, performance, and developer control in GPU programming for AI workloads.","title":"ML-Triton, A Multi-Level Compilation and Language Extension to Triton GPU Programming","url":"https://arxiv.org/abs/2503.14985"},{"content":"# Computer Science > Programming Languages\n\n**arXiv:2411.07443** (cs)\n\n\\[Submitted on 11 Nov 2024\\]\n\n# Title:MimIR: An Extensible and Type-Safe Intermediate Representation for the DSL Age\n\nAuthors: [Roland LeiÃŸa](https://arxiv.org/search/cs?searchtype=author&query=Lei%C3%9Fa,+R), [Marcel Ulrich](https://arxiv.org/search/cs?searchtype=author&query=Ulrich,+M), [Joachim Meyer](https://arxiv.org/search/cs?searchtype=author&query=Meyer,+J), [Sebastian Hack](https://arxiv.org/search/cs?searchtype=author&query=Hack,+S)\n\nView a PDF of the paper titled MimIR: An Extensible and Type-Safe Intermediate Representation for the DSL Age, by Roland Lei{\\\\ss}a and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2411.07443)\n\n> Abstract:Traditional compilers, designed for optimizing low-level code, fall short when dealing with modern, computation-heavy applications like image processing, machine learning, or numerical simulations. Optimizations should understand the primitive operations of the specific application domain and thus happen on that level.\n>\n> Domain-specific languages (DSLs) fulfill these requirements. However, DSL compilers reinvent the wheel over and over again as standard optimizations, code generators, and general infrastructure & boilerplate code must be reimplemented for each DSL compiler.\n>\n> This paper presents MimIR, an extensible, higher-order intermediate representation. At its core, MimIR is a pure type system and, hence, a form of a typed lambda calculus. Developers can declare the signatures of new (domain-specific) operations, called \"axioms\". An axiom can be the declaration of a function, a type operator, or any other entity with a possibly polymorphic, polytypic, and/or dependent type. This way, developers can extend MimIR at any low or high level and bundle them in a plugin. Plugins extend the compiler and take care of optimizing and lowering the plugins' axioms.\n>\n> We show the expressiveness and effectiveness of MimIR in three case studies: Low-level plugins that operate at the same level of abstraction as LLVM, a regular-expression matching plugin, and plugins for linear algebra and automatic differentiation. We show that in all three studies, MimIR produces code that has state-of-the-art performance.\n\n| | |\n| --- | --- |\n| Comments: | will appear on POPL'25 |\n| Subjects: | Programming Languages (cs.PL) |\n| Cite as: | [arXiv:2411.07443](https://arxiv.org/abs/2411.07443) \\[cs.PL\\] |\n| | (or [arXiv:2411.07443v1](https://arxiv.org/abs/2411.07443v1) \\[cs.PL\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2411.07443](https://doi.org/10.48550/arXiv.2411.07443) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Roland LeiÃŸa \\[ [view email](https://arxiv.org/show-email/76658ff9/2411.07443)\\]\n\n**\\[v1\\]**\nMon, 11 Nov 2024 23:54:04 UTC (197 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MimIR: An Extensible and Type-Safe Intermediate Representation for the DSL Age, by Roland Lei{\\\\ss}a and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2411.07443)\n- [TeX Source](https://arxiv.org/src/2411.07443)\n- [Other Formats](https://arxiv.org/format/2411.07443)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.PL\n\n[<Â prev](https://arxiv.org/prevnext?id=2411.07443&function=prev&context=cs.PL)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2411.07443&function=next&context=cs.PL)\n\n[new](https://arxiv.org/list/cs.PL/new) \\| [recent](https://arxiv.org/list/cs.PL/recent) \\| [2024-11](https://arxiv.org/list/cs.PL/2024-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2411.07443?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2411.07443)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2411.07443)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2411.07443)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2411.07443&description=MimIR: An Extensible and Type-Safe Intermediate Representation for the DSL Age) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2411.07443&title=MimIR: An Extensible and Type-Safe Intermediate Representation for the DSL Age)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2411.07443) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2411.07443","id":"https://arxiv.org/abs/2411.07443","publishedDate":"2024-11-11T00:00:00.000Z","summary":"The paper introduces MimIR, an extensible, type-safe intermediate representation (IR) designed for domain-specific language (DSL) compilers. Unlike traditional low-level compiler IRs, MimIR is based on a pure type system resembling a typed lambda calculus, allowing developers to define new domain-specific operations (\"axioms\") with polymorphic, polytypic, or dependent types. This modular approach enables easy extension and plugin development, facilitating optimizations and code lowering at various abstraction levels. The authors demonstrate MimIR's effectiveness through three case studies: low-level plugins comparable to LLVM, regex matching, and plugins for linear algebra and automatic differentiation. Results show MimIR achieves state-of-the-art performance, making it a promising framework for optimizing computation-heavy applications in modern DSLs.","title":"MimIR: An Extensible and Type-Safe Intermediate Representation for the DSL Age","url":"https://arxiv.org/abs/2411.07443"},{"content":"# Computer Science > Programming Languages\n\n**arXiv:2412.13398** (cs)\n\n\\[Submitted on 18 Dec 2024\\]\n\n# Title:Pattern Matching in AI Compilers and its Formalization (Extended Version)\n\nAuthors: [Joseph W. Cutler](https://arxiv.org/search/cs?searchtype=author&query=Cutler,+J+W), [Alex Collins](https://arxiv.org/search/cs?searchtype=author&query=Collins,+A), [Bin Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan,+B), [Mahesh Ravishankar](https://arxiv.org/search/cs?searchtype=author&query=Ravishankar,+M), [Vinod Grover](https://arxiv.org/search/cs?searchtype=author&query=Grover,+V)\n\nView a PDF of the paper titled Pattern Matching in AI Compilers and its Formalization (Extended Version), by Joseph W. Cutler and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2412.13398)\n\n> Abstract:PyPM is a Python-based domain specific language (DSL) for building rewrite-based optimization passes on machine learning computation graphs. Users define individual optimizations by writing (a) patterns that match subgraphs of a computation graph and (b) corresponding rules which replace a matched subgraph with an optimized kernel. PyPM is distinguished from the many other DSLs for defining rewriting passes by its complex and novel pattern language which borrows concepts from logic programming. PyPM patterns can be recursive, nondeterminstic, and can require checking domain-specific constraints such as the shapes of tensors. The PyPM implementation is thus similarly complicated, consisting of thousands of lines of C++ code. In this paper, we present our work on building PyPM, as well as formalizing and distilling and this complexity to an understandable mathematical core. We have developed a formal core calculus expressing the main operations of the PyPM pattern language. We define both a declarative semantics - describing which patterns match which terms - and an algorithmic semantics - an idealized version of the PyPM pattern interpreter - and prove their equivalence. The development is fully mechanized in the Coq proof assistant.\n\n| | |\n| --- | --- |\n| Comments: | To appear at CGO'25 |\n| Subjects: | Programming Languages (cs.PL); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2412.13398](https://arxiv.org/abs/2412.13398) \\[cs.PL\\] |\n| (or [arXiv:2412.13398v1](https://arxiv.org/abs/2412.13398v1) \\[cs.PL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2412.13398](https://doi.org/10.48550/arXiv.2412.13398) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Joseph W. Cutler \\[ [view email](https://arxiv.org/show-email/94147ab4/2412.13398)\\] **\\[v1\\]**\nWed, 18 Dec 2024 00:29:09 UTC (174 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Pattern Matching in AI Compilers and its Formalization (Extended Version), by Joseph W. Cutler and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2412.13398)\n- [TeX Source](https://arxiv.org/src/2412.13398)\n- [Other Formats](https://arxiv.org/format/2412.13398)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.PL\n\n[<Â prev](https://arxiv.org/prevnext?id=2412.13398&function=prev&context=cs.PL)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2412.13398&function=next&context=cs.PL)\n\n[new](https://arxiv.org/list/cs.PL/new) \\| [recent](https://arxiv.org/list/cs.PL/recent) \\| [2024-12](https://arxiv.org/list/cs.PL/2024-12)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2412.13398?context=cs) [cs.LG](https://arxiv.org/abs/2412.13398?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2412.13398)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2412.13398)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2412.13398)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2412.13398) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2412.13398","id":"https://arxiv.org/abs/2412.13398","publishedDate":"2024-12-18T00:00:00.000Z","summary":"This paper introduces PyPM, a Python-based domain-specific language (DSL) designed for creating rewrite-based optimization passes on machine learning computation graphs. PyPM allows users to specify complex patternsâ€”often recursive, nondeterministic, and constrained by domain-specific factors like tensor shapesâ€”to identify subgraphs for optimization. Its pattern language draws from logic programming concepts, making it notably expressive but also complex, with thousands of lines of C++ code underpinning its implementation.\n\nThe authors focus on formalizing the core operations of PyPM by developing a mathematical calculus that captures its pattern matching semantics. They define both a declarative semanticsâ€”detailing which patterns match which graph componentsâ€”and an algorithmic semanticsâ€”representing an idealized interpreter for the pattern language. Crucially, they prove the equivalence of these semantics, ensuring correctness and consistency. The formalization is mechanized within the Coq proof assistant, providing a rigorous foundation for understanding and verifying pattern matching in AI compiler optimizations.\n\nThis work aims to clarify the complex pattern matching mechanisms in PyPM, contributing to the broader field of programming languages and machine learning compiler optimization by providing a formal, mathematically sound framework. The paper is set to appear at CGO'25.","title":"Pattern Matching in AI Compilers and its Formalization (Extended Version)","url":"https://arxiv.org/abs/2412.13398"},{"content":"# Computer Science > Programming Languages\n\n**arXiv:2503.21937** (cs)\n\n\\[Submitted on 27 Mar 2025\\]\n\n# Title:Lobster: A GPU-Accelerated Framework for Neurosymbolic Programming\n\nAuthors: [Paul Biberstein](https://arxiv.org/search/cs?searchtype=author&query=Biberstein,+P), [Ziyang Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+Z), [Joseph Devietti](https://arxiv.org/search/cs?searchtype=author&query=Devietti,+J), [Mayur Naik](https://arxiv.org/search/cs?searchtype=author&query=Naik,+M)\n\nView a PDF of the paper titled Lobster: A GPU-Accelerated Framework for Neurosymbolic Programming, by Paul Biberstein and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.21937) [HTML (experimental)](https://arxiv.org/html/2503.21937v1)\n\n> Abstract:Neurosymbolic programs combine deep learning with symbolic reasoning to achieve better data efficiency, interpretability, and generalizability compared to standalone deep learning approaches. However, existing neurosymbolic learning frameworks implement an uneasy marriage between a highly scalable, GPU-accelerated neural component with a slower symbolic component that runs on CPUs. We propose Lobster, a unified framework for harnessing GPUs in an end-to-end manner for neurosymbolic learning. Lobster maps a general neurosymbolic language based on Datalog to the GPU programming paradigm. This mapping is implemented via compilation to a new intermediate language called APM. The extra abstraction provided by APM allows Lobster to be both flexible, supporting discrete, probabilistic, and differentiable modes of reasoning on GPU hardware with a library of provenance semirings, and performant, implementing new optimization passes. We demonstrate that Lobster programs can solve interesting problems spanning the domains of natural language processing, image processing, program reasoning, bioinformatics, and planning. On a suite of 8 applications, Lobster achieves an average speedup of 5.3x over Scallop, a state-of-the-art neurosymbolic framework, and enables scaling of neurosymbolic solutions to previously infeasible tasks.\n\n| | |\n| --- | --- |\n| Subjects: | Programming Languages (cs.PL); Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2503.21937](https://arxiv.org/abs/2503.21937) \\[cs.PL\\] |\n| (or [arXiv:2503.21937v1](https://arxiv.org/abs/2503.21937v1) \\[cs.PL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.21937](https://doi.org/10.48550/arXiv.2503.21937) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Paul Biberstein \\[ [view email](https://arxiv.org/show-email/a6a29b2c/2503.21937)\\] **\\[v1\\]**\nThu, 27 Mar 2025 19:32:58 UTC (337 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Lobster: A GPU-Accelerated Framework for Neurosymbolic Programming, by Paul Biberstein and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.21937)\n- [HTML (experimental)](https://arxiv.org/html/2503.21937v1)\n- [TeX Source](https://arxiv.org/src/2503.21937)\n- [Other Formats](https://arxiv.org/format/2503.21937)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.PL\n\n[<Â prev](https://arxiv.org/prevnext?id=2503.21937&function=prev&context=cs.PL)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2503.21937&function=next&context=cs.PL)\n\n[new](https://arxiv.org/list/cs.PL/new) \\| [recent](https://arxiv.org/list/cs.PL/recent) \\| [2025-03](https://arxiv.org/list/cs.PL/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.21937?context=cs) [cs.AI](https://arxiv.org/abs/2503.21937?context=cs.AI) [cs.DC](https://arxiv.org/abs/2503.21937?context=cs.DC) [cs.LG](https://arxiv.org/abs/2503.21937?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.21937)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.21937)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.21937)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.21937) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.21937","id":"https://arxiv.org/abs/2503.21937","publishedDate":"2025-03-27T00:00:00.000Z","summary":"The paper introduces Lobster, a GPU-accelerated framework designed for neurosymbolic programming, which combines deep learning with symbolic reasoning to enhance data efficiency, interpretability, and generalization. Traditional neurosymbolic systems often rely on CPU-based symbolic components, limiting performance and scalability. Lobster addresses this by mapping a general neurosymbolic language based on Datalog to GPU programming through a new intermediate language called APM. This enables flexible reasoning modesâ€”discrete, probabilistic, and differentiableâ€”on GPU hardware, supported by a library of provenance semirings and optimization passes. Experimental results demonstrate that Lobster significantly outperforms existing frameworks like Scallop, achieving an average speedup of 5.3x across eight applications spanning NLP, image processing, bioinformatics, and planning, and enabling scaling to previously infeasible tasks.","title":"Lobster: A GPU-Accelerated Framework for Neurosymbolic Programming","url":"https://arxiv.org/abs/2503.21937"},{"content":"# Computer Science > Cryptography and Security\n\n**arXiv:2503.23278** (cs)\n\n\\[Submitted on 30 Mar 2025 ( [v1](https://arxiv.org/abs/2503.23278v1)), last revised 6 Apr 2025 (this version, v2)\\]\n\n# Title:Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions\n\nAuthors: [Xinyi Hou](https://arxiv.org/search/cs?searchtype=author&query=Hou,+X), [Yanjie Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao,+Y), [Shenao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+S), [Haoyu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+H)\n\nView a PDF of the paper titled Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions, by Xinyi Hou and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.23278) [HTML (experimental)](https://arxiv.org/html/2503.23278v2)\n\n> Abstract:The Model Context Protocol (MCP) is a standardized interface designed to enable seamless interaction between AI models and external tools and resources, breaking down data silos and facilitating interoperability across diverse systems. This paper provides a comprehensive overview of MCP, focusing on its core components, workflow, and the lifecycle of MCP servers, which consists of three key phases: creation, operation, and update. We analyze the security and privacy risks associated with each phase and propose strategies to mitigate potential threats. The paper also examines the current MCP landscape, including its adoption by industry leaders and various use cases, as well as the tools and platforms supporting its integration. We explore future directions for MCP, highlighting the challenges and opportunities that will influence its adoption and evolution within the broader AI ecosystem. Finally, we offer recommendations for MCP stakeholders to ensure its secure and sustainable development as the AI landscape continues to evolve.\n\n| | |\n| --- | --- |\n| Subjects: | Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2503.23278](https://arxiv.org/abs/2503.23278) \\[cs.CR\\] |\n| (or [arXiv:2503.23278v2](https://arxiv.org/abs/2503.23278v2) \\[cs.CR\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.23278](https://doi.org/10.48550/arXiv.2503.23278) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Xinyi Hou \\[ [view email](https://arxiv.org/show-email/ba9a13f1/2503.23278)\\] **[\\[v1\\]](https://arxiv.org/abs/2503.23278v1)**\nSun, 30 Mar 2025 01:58:22 UTC (438 KB)\n**\\[v2\\]**\nSun, 6 Apr 2025 13:32:33 UTC (476 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions, by Xinyi Hou and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.23278)\n- [HTML (experimental)](https://arxiv.org/html/2503.23278v2)\n- [TeX Source](https://arxiv.org/src/2503.23278)\n- [Other Formats](https://arxiv.org/format/2503.23278)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CR\n\n[<Â prev](https://arxiv.org/prevnext?id=2503.23278&function=prev&context=cs.CR)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2503.23278&function=next&context=cs.CR)\n\n[new](https://arxiv.org/list/cs.CR/new) \\| [recent](https://arxiv.org/list/cs.CR/recent) \\| [2025-03](https://arxiv.org/list/cs.CR/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.23278?context=cs) [cs.AI](https://arxiv.org/abs/2503.23278?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.23278)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.23278)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.23278)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.23278) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.23278","id":"https://arxiv.org/abs/2503.23278","publishedDate":"2025-03-30T00:00:00.000Z","summary":"The paper \"Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions\" provides a comprehensive overview of MCP, a standardized interface facilitating interaction between AI models and external tools. MCP aims to improve interoperability across diverse systems by breaking data silos. The authors detail MCP's core components, workflow, and server lifecycle phasesâ€”creation, operation, and updatesâ€”highlighting associated security and privacy risks at each stage and proposing mitigation strategies. The paper surveys the current MCP landscape, including industry adoption, use cases, and supporting platforms. It also discusses future challenges and opportunities for MCP's integration into the broader AI ecosystem, emphasizing the importance of secure and sustainable development as AI continues to evolve.","title":"Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions","url":"https://arxiv.org/abs/2503.23278"},{"content":"# Computer Science > Programming Languages\n\n**arXiv:2405.08965** (cs)\n\n\\[Submitted on 14 May 2024 ( [v1](https://arxiv.org/abs/2405.08965v1)), last revised 3 Sep 2025 (this version, v5)\\]\n\n# Title:MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\n\nAuthors: [Jayanaka L. Dantanarayana](https://arxiv.org/search/cs?searchtype=author&query=Dantanarayana,+J+L), [Yiping Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang,+Y), [Kugesan Sivasothynathan](https://arxiv.org/search/cs?searchtype=author&query=Sivasothynathan,+K), [Christopher Clarke](https://arxiv.org/search/cs?searchtype=author&query=Clarke,+C), [Baichuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+B), [Savini Kashmira](https://arxiv.org/search/cs?searchtype=author&query=Kashmira,+S), [Krisztian Flautner](https://arxiv.org/search/cs?searchtype=author&query=Flautner,+K), [Lingjia Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+L), [Jason Mars](https://arxiv.org/search/cs?searchtype=author&query=Mars,+J)\n\nView a PDF of the paper titled MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming, by Jayanaka L. Dantanarayana and 8 other authors\n\n[View PDF](https://arxiv.org/pdf/2405.08965) [HTML (experimental)](https://arxiv.org/html/2405.08965v5)\n\n> Abstract:Software development is shifting from traditional programming to AI-integrated applications that leverage generative AI and large language models (LLMs) during runtime. However, integrating LLMs remains complex, requiring developers to manually craft prompts and process outputs. Existing tools attempt to assist with prompt engineering, but often introduce additional complexity. This paper presents Meaning-Typed Programming (MTP), a novel paradigm that abstracts LLM integration through intuitive language-level constructs. By leveraging the inherent semantic richness of code, MTP automates prompt generation and response handling without additional developer effort. We introduce the (1) by operator for seamless LLM invocation, (2) MT-IR, a meaning-based intermediate representation for semantic extraction, and (3) MT-Runtime, an automated system for managing LLM interactions. We implement MTP in Jac, a programming language that supersets Python, and find that MTP significantly reduces coding complexity while maintaining accuracy and efficiency. MTP significantly reduces development complexity, lines of code modifications needed, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches. Our user study shows that developers using MTP completed tasks 3.2x faster with 45% fewer lines of code compared to existing frameworks. Moreover, MTP demonstrates resilience even when up to 50% of naming conventions are degraded, demonstrating robustness to suboptimal code. MTP is developed as part of the Jaseci open-source project, and is available under the module byLLM.\n\n| | |\n| --- | --- |\n| Comments: | OOPSLA 2025 |\n| Subjects: | Programming Languages (cs.PL); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2405.08965](https://arxiv.org/abs/2405.08965) \\[cs.PL\\] |\n| (or [arXiv:2405.08965v5](https://arxiv.org/abs/2405.08965v5) \\[cs.PL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2405.08965](https://doi.org/10.48550/arXiv.2405.08965) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jayanaka L. Dantanarayana \\[ [view email](https://arxiv.org/show-email/13cb6fa0/2405.08965)\\] **[\\[v1\\]](https://arxiv.org/abs/2405.08965v1)**\nTue, 14 May 2024 21:12:01 UTC (623 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2405.08965v2)**\nMon, 14 Oct 2024 21:20:40 UTC (4,649 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2405.08965v3)**\nThu, 16 Jan 2025 18:56:27 UTC (2,918 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2405.08965v4)**\nWed, 4 Jun 2025 00:51:13 UTC (3,247 KB)\n**\\[v5\\]**\nWed, 3 Sep 2025 21:19:19 UTC (3,661 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming, by Jayanaka L. Dantanarayana and 8 other authors\n\n- [View PDF](https://arxiv.org/pdf/2405.08965)\n- [HTML (experimental)](https://arxiv.org/html/2405.08965v5)\n- [TeX Source](https://arxiv.org/src/2405.08965)\n- [Other Formats](https://arxiv.org/format/2405.08965)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.PL\n\n[<Â prev](https://arxiv.org/prevnext?id=2405.08965&function=prev&context=cs.PL)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2405.08965&function=next&context=cs.PL)\n\n[new](https://arxiv.org/list/cs.PL/new) \\| [recent](https://arxiv.org/list/cs.PL/recent) \\| [2024-05](https://arxiv.org/list/cs.PL/2024-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2405.08965?context=cs) [cs.AI](https://arxiv.org/abs/2405.08965?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2405.08965)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2405.08965)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2405.08965)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2405.08965) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2405.08965","id":"https://arxiv.org/abs/2405.08965","publishedDate":null,"summary":"The paper introduces Meaning-Typed Programming (MTP), a novel language abstraction designed to simplify the integration of large language models (LLMs) and generative AI into software development. Traditional AI-driven applications require complex prompt engineering and manual handling of LLM outputs, increasing development effort and complexity. MTP addresses this by providing intuitive language-level constructs that leverage the semantic richness of code to automate prompt generation and response processing.\n\nKey features of MTP include:\n- The **\"by\" operator** for seamless LLM invocation.\n- **MT-IR (Meaning-Based Intermediate Representation)** for semantic extraction.\n- **MT-Runtime**, an automated system managing LLM interactions.\n\nImplemented in Jac, a superset of Python, MTP significantly reduces lines of code, development time, and costs while maintaining or improving accuracy and runtime performance. User studies show developers can complete tasks 3.2 times faster with 45% fewer lines of code. The approach is robust even with degraded naming conventions, demonstrating resilience to suboptimal code.\n\nMTP is part of the open-source Jaseci project and aims to streamline AI integration in programming, making it more accessible and efficient for developers working on AI-enhanced applications. The research was presented at OOPSLA 2025.","title":"MTP: A Meaning-Typed Language Abstraction for AI ...","url":"https://arxiv.org/abs/2405.08965"},{"content":"# Quantum Physics\n\n**arXiv:2412.17209** (quant-ph)\n\n\\[Submitted on 23 Dec 2024 ( [v1](https://arxiv.org/abs/2412.17209v1)), last revised 26 Aug 2025 (this version, v2)\\]\n\n# Title:Classical simulability of Clifford+T circuits with Clifford-augmented matrix product states\n\nAuthors: [Zejun Liu](https://arxiv.org/search/quant-ph?searchtype=author&query=Liu,+Z), [Bryan K. Clark](https://arxiv.org/search/quant-ph?searchtype=author&query=Clark,+B+K)\n\nView a PDF of the paper titled Classical simulability of Clifford+T circuits with Clifford-augmented matrix product states, by Zejun Liu and Bryan K. Clark\n\n[View PDF](https://arxiv.org/pdf/2412.17209) [HTML (experimental)](https://arxiv.org/html/2412.17209v2)\n\n> Abstract:Determining the quantum-classical boundary between quantum circuits which can be efficiently simulated classically and those which cannot remains a fundamental question. One approach to classical simulation is to represent the output of a quantum circuit as a Clifford-augmented Matrix Product State (CAMPS) which, via a disentangling algorithm, decomposes the wave function into Clifford and MPS components and from which Pauli expectation values can be computed in time polynomial in the MPS bond-dimension. In this work, we develop an optimization-free disentangling (OFD) algorithm for Clifford circuits either doped with multi-qubit gates of the form $\\\\alpha I+\\\\beta P$. We give a simple algebraic criterion which characterizes the individual quantum circuits for which OFD generates an efficient CAMPS - the bond-dimension is exponential in the null space of a GF(2) matrix induced by a tableau of the twisted Pauli strings $P$. This significantly increases the number of circuits with rigorous polynomial time classical simulations. We also give evidence that the typical $N$ qubit random Clifford circuit doped with $N$ uniformly distributed $T$ gates of poly-logarithmic depth or greater has a CAMPS with polynomial bond-dimension. In addition, we compare OFD against disentangling by optimization. We further explore the representability of CAMPS for random Clifford circuits doped with more than $N$ $T$-gates. We also propose algorithms for sampling, probability and amplitude estimation of bitstrings, and evaluation of entanglement RÃ©nyi entropy from CAMPS, which, though still having exponential complexity, are more efficient than standard MPS simulations. This work establishes a versatile framework for understanding classical simulatability of Clifford+$T$ circuits and explores the interplay between quantum entanglement and quantum magic in quantum systems.\n\n| | |\n| --- | --- |\n| Subjects: | Quantum Physics (quant-ph) |\n| Cite as: | [arXiv:2412.17209](https://arxiv.org/abs/2412.17209) \\[quant-ph\\] |\n| (or [arXiv:2412.17209v2](https://arxiv.org/abs/2412.17209v2) \\[quant-ph\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2412.17209](https://doi.org/10.48550/arXiv.2412.17209) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Zejun Liu \\[ [view email](https://arxiv.org/show-email/58483649/2412.17209)\\] **[\\[v1\\]](https://arxiv.org/abs/2412.17209v1)**\nMon, 23 Dec 2024 01:26:40 UTC (666 KB)\n**\\[v2\\]**\nTue, 26 Aug 2025 05:12:51 UTC (730 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Classical simulability of Clifford+T circuits with Clifford-augmented matrix product states, by Zejun Liu and Bryan K. Clark\n\n- [View PDF](https://arxiv.org/pdf/2412.17209)\n- [HTML (experimental)](https://arxiv.org/html/2412.17209v2)\n- [TeX Source](https://arxiv.org/src/2412.17209)\n- [Other Formats](https://arxiv.org/format/2412.17209)\n\n[view license](http://creativecommons.org/licenses/by-nc-sa/4.0/)\n\nCurrent browse context:\n\nquant-ph\n\n[<Â prev](https://arxiv.org/prevnext?id=2412.17209&function=prev&context=quant-ph)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2412.17209&function=next&context=quant-ph)\n\n[new](https://arxiv.org/list/quant-ph/new) \\| [recent](https://arxiv.org/list/quant-ph/recent) \\| [2024-12](https://arxiv.org/list/quant-ph/2024-12)\n\n### References & Citations\n\n- [INSPIRE HEP](https://inspirehep.net/arxiv/2412.17209)\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2412.17209)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2412.17209)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2412.17209)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2412.17209) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2412.17209","id":"https://arxiv.org/abs/2412.17209","publishedDate":"2024-12-23T00:00:00.000Z","summary":"This paper investigates the classical simulability of quantum circuits, specifically Clifford+T circuits, using a novel approach called Clifford-augmented Matrix Product States (CAMPS). The authors develop an optimization-free disentangling (OFD) algorithm tailored for Clifford circuits with certain multi-qubit gates, enabling efficient classical simulation by characterizing when the bond-dimension of CAMPS remains manageable. They identify algebraic criteria linking circuit structure to simulation complexity, significantly broadening the class of circuits that can be simulated efficiently. Additionally, the work provides evidence that random Clifford circuits with a number of T-gates proportional to the number of qubits and sufficient depth likely admit polynomial-bond-dimension CAMPS. The authors also propose algorithms for sampling and estimating properties like entanglement entropy from CAMPS, which, despite exponential complexity in general, are more efficient than traditional methods. Overall, the research advances understanding of the boundary between classically simulatable quantum circuits and those exhibiting quantum advantage, highlighting the interplay between entanglement and quantum magic.","title":"Classical simulability of Clifford+T circuits with Clifford-augmented matrix product states","url":"https://arxiv.org/abs/2412.17209"},{"content":"# Mathematics > Functional Analysis\n\n**arXiv:2503.10128** (math)\n\n\\[Submitted on 13 Mar 2025\\]\n\n# Title:Comparison of tuples of operators and its components\n\nAuthors: [Arpita Mal](https://arxiv.org/search/math?searchtype=author&query=Mal,+A)\n\nView a PDF of the paper titled Comparison of tuples of operators and its components, by Arpita Mal\n\n[View PDF](https://arxiv.org/pdf/2503.10128) [HTML (experimental)](https://arxiv.org/html/2503.10128v1)\n\n> Abstract:For tuples of compact operators $\\\\mathcal{T}=(T\\_1,\\\\ldots, T\\_d)$ and $\\\\mathcal{S}=(S\\_1,$ $\\\\ldots,S\\_d)$ on Banach spaces over a field $\\\\mathbb{F}$, considering the joint $p$-operator norms on the tuples, we study $dist(\\\\mathcal{T},\\\\mathbb{F}^d\\\\mathcal{S}),$ the distance of $\\\\mathcal{T}$ from the $d$-dimensional subspace $\\\\mathcal{F}^d\\\\mathcal{S}:=\\\\{\\\\textbf{z}\\\\mathcal{S}:\\\\textbf{z}\\\\in \\\\mathbb{F}^d\\\\}.$ We obtain a relation between $dist(\\\\mathcal{T},\\\\mathbb{F}^d\\\\mathcal{S})$ and $dist(T\\_i,\\\\mathbb{F}S\\_i),$ for $1\\\\leq i\\\\leq d.$ We prove that if $p=\\\\infty,$ then $dist(\\\\mathcal{T},\\\\mathbb{F}^d\\\\mathcal{S})=\\\\underset{1\\\\leq i\\\\leq d}{\\\\max}dist(T\\_i,\\\\mathbb{F}S\\_i),$ and for $1\\\\leq p<\\\\infty,$ under a sufficient condition, $dist(\\\\mathcal{T},\\\\mathbb{F}^d\\\\mathcal{S})^p=\\\\underset{1\\\\leq i\\\\leq d}{\\\\sum}dist(T\\_i,\\\\mathbb{F}S\\_i)^p.$ As a consequence, we deduce the equivalence of Birkhoff-James orthogonality, $\\\\mathcal{T}\\\\perp\\_B \\\\mathbb{F}^d\\\\mathcal{S} \\\\Leftrightarrow T\\_i\\\\perp\\_B S\\_i,$ under a sufficient condition. Furthermore, we explore the relation of one sided Gateaux derivatives of $\\\\mathcal{T}$ in the direction of $\\\\mathcal{S}$ with that of $T\\_i$ in the direction of $S\\_i.$ Applying this, we explore the relation between the smoothness of $\\\\mathcal{T}$ and $T\\_i.$ By identifying an operator, whose range is $\\\\ell\\_\\\\infty^d,$ as a tuple of functionals, we effectively use the results obtained here for operators whose range is $\\\\ell\\_\\\\infty^d$ and deduce nice results involving functionals. It provides another perspective of studying tuples of operators.\n\n| | |\n| --- | --- |\n| Subjects: | Functional Analysis (math.FA) |\n| Cite as: | [arXiv:2503.10128](https://arxiv.org/abs/2503.10128) \\[math.FA\\] |\n| | (or [arXiv:2503.10128v1](https://arxiv.org/abs/2503.10128v1) \\[math.FA\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2503.10128](https://doi.org/10.48550/arXiv.2503.10128) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Arpita Mal \\[ [view email](https://arxiv.org/show-email/5d23ee75/2503.10128)\\]\n\n**\\[v1\\]**\nThu, 13 Mar 2025 07:39:01 UTC (13 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Comparison of tuples of operators and its components, by Arpita Mal\n\n- [View PDF](https://arxiv.org/pdf/2503.10128)\n- [HTML (experimental)](https://arxiv.org/html/2503.10128v1)\n- [TeX Source](https://arxiv.org/src/2503.10128)\n- [Other Formats](https://arxiv.org/format/2503.10128)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.FA\n\n[<Â prev](https://arxiv.org/prevnext?id=2503.10128&function=prev&context=math.FA)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2503.10128&function=next&context=math.FA)\n\n[new](https://arxiv.org/list/math.FA/new) \\| [recent](https://arxiv.org/list/math.FA/recent) \\| [2025-03](https://arxiv.org/list/math.FA/2025-03)\n\nChange to browse by:\n\n[math](https://arxiv.org/abs/2503.10128?context=math)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.10128)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.10128)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.10128)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2503.10128&description=Comparison of tuples of operators and its components) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2503.10128&title=Comparison of tuples of operators and its components)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.10128) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.10128","id":"https://arxiv.org/abs/2503.10128","publishedDate":"2025-03-13T00:00:00.000Z","summary":"This paper investigates properties of tuples of compact operators on Banach spaces, focusing on their joint p-operator norms and related geometric concepts such as distances, orthogonality, and smoothness. It establishes relationships between the distance of a tuple from a subspace and the distances of individual operators within the tuple, revealing that for p=âˆž, the tuple's distance equals the maximum of individual distances, while for finite p, under certain conditions, it relates to the p-sum of these distances. The study also explores Birkhoff-James orthogonality and one-sided Gateaux derivatives, connecting the properties of operator tuples to their components. By representing operator ranges as tuples of functionals, the results extend to functionals and offer new perspectives in the analysis of operator tuples, contributing to the understanding of their geometric and functional structure in the context of functional analysis.","title":"Comparison of tuples of operators and its components","url":"https://arxiv.org/abs/2503.10128"},{"content":"# Quantum Physics\n\n**arXiv:2502.12641** (quant-ph)\n\n\\[Submitted on 18 Feb 2025\\]\n\n# Title:Matrix Product States as Observations of Entangled Hidden Markov Models\n\nAuthors: [Abdessatar Souissi](https://arxiv.org/search/quant-ph?searchtype=author&query=Souissi,+A)\n\nView a PDF of the paper titled Matrix Product States as Observations of Entangled Hidden Markov Models, by Abdessatar Souissi\n\n[View PDF](https://arxiv.org/pdf/2502.12641) [HTML (experimental)](https://arxiv.org/html/2502.12641v1)\n\n> Abstract:This paper reveals the intrinsic structure of Matrix Product States (MPS) by establishing their deep connection to entangled hidden Markov models (EHMMs). It is demonstrated that a significant class of MPS can be derived as the outcomes of EHMMs, showcasing their underlying quantum correlations. Additionally, a lower bound is derived for the relative entropy between the EHMM-observation process and the corresponding MPS, providing a quantitative measure of their informational divergence. Conversely, it is shown that every MPS is naturally associated with an EHMM, further highlighting the interplay between these frameworks. These results are supported by illustrative examples from quantum information, emphasizing their importance in understanding entanglement, quantum correlations, and tensor network representations.\n\n| | |\n| --- | --- |\n| Subjects: | Quantum Physics (quant-ph); Mathematical Physics (math-ph) |\n| Cite as: | [arXiv:2502.12641](https://arxiv.org/abs/2502.12641) \\[quant-ph\\] |\n| | (or [arXiv:2502.12641v1](https://arxiv.org/abs/2502.12641v1) \\[quant-ph\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2502.12641](https://doi.org/10.48550/arXiv.2502.12641) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Abdessatar Souissi \\[ [view email](https://arxiv.org/show-email/29028a31/2502.12641)\\]\n\n**\\[v1\\]**\nTue, 18 Feb 2025 08:44:17 UTC (19 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Matrix Product States as Observations of Entangled Hidden Markov Models, by Abdessatar Souissi\n\n- [View PDF](https://arxiv.org/pdf/2502.12641)\n- [HTML (experimental)](https://arxiv.org/html/2502.12641v1)\n- [TeX Source](https://arxiv.org/src/2502.12641)\n- [Other Formats](https://arxiv.org/format/2502.12641)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\nquant-ph\n\n[<Â prev](https://arxiv.org/prevnext?id=2502.12641&function=prev&context=quant-ph)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2502.12641&function=next&context=quant-ph)\n\n[new](https://arxiv.org/list/quant-ph/new) \\| [recent](https://arxiv.org/list/quant-ph/recent) \\| [2025-02](https://arxiv.org/list/quant-ph/2025-02)\n\nChange to browse by:\n\n[math](https://arxiv.org/abs/2502.12641?context=math)\n\n[math-ph](https://arxiv.org/abs/2502.12641?context=math-ph)\n\n[math.MP](https://arxiv.org/abs/2502.12641?context=math.MP)\n\n### References & Citations\n\n- [INSPIRE HEP](https://inspirehep.net/arxiv/2502.12641)\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2502.12641)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2502.12641)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2502.12641)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2502.12641&description=Matrix Product States as Observations of Entangled Hidden Markov Models) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2502.12641&title=Matrix Product States as Observations of Entangled Hidden Markov Models)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2502.12641) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2502.12641","id":"https://arxiv.org/abs/2502.12641","publishedDate":"2025-02-18T00:00:00.000Z","summary":"This paper explores the deep connection between Matrix Product States (MPS) and entangled hidden Markov models (EHMMs) in quantum physics. It demonstrates that many MPS can be viewed as outcomes of EHMMs, highlighting their quantum correlations and entanglement structures. The study establishes a quantitative lower bound on the relative entropy between the observation processes of EHMMs and the corresponding MPS, providing insight into their informational divergence. Conversely, it shows that every MPS can be associated with an EHMM, emphasizing a bidirectional relationship. These findings deepen our understanding of tensor network representations, quantum entanglement, and correlations, with illustrative examples from quantum information theory.","title":"Matrix Product States as Observations of Entangled Hidden Markov Models","url":"https://arxiv.org/abs/2502.12641"},{"content":"# Quantum Physics\n\n**arXiv:2412.14261** (quant-ph)\n\n\\[Submitted on 18 Dec 2024\\]\n\n# Title:Correlations, Spectra and Entaglement Transitions in Ensembles of Matrix Product States\n\nAuthors: [Hugo LÃ³io](https://arxiv.org/search/quant-ph?searchtype=author&query=L%C3%B3io,+H), [Guillaume Cecile](https://arxiv.org/search/quant-ph?searchtype=author&query=Cecile,+G), [Sarang Gopalakrishnan](https://arxiv.org/search/quant-ph?searchtype=author&query=Gopalakrishnan,+S), [Guglielmo Lami](https://arxiv.org/search/quant-ph?searchtype=author&query=Lami,+G), [Jacopo De Nardis](https://arxiv.org/search/quant-ph?searchtype=author&query=De+Nardis,+J)\n\nView a PDF of the paper titled Correlations, Spectra and Entaglement Transitions in Ensembles of Matrix Product States, by Hugo L\\\\'oio and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2412.14261) [HTML (experimental)](https://arxiv.org/html/2412.14261v1)\n\n> Abstract:We investigate ensembles of Matrix Product States (MPSs) generated by quantum circuit evolution followed by projection onto MPSs with a fixed bond dimension $\\\\chi$. Specifically, we consider ensembles produced by: (i) random sequential unitary circuits, (ii) random brickwork unitary circuits, and (iii) circuits involving both unitaries and projective measurements. In all cases, we characterize the spectra of the MPS transfer matrix and show that, for the first two cases in the thermodynamic limit, they exhibit a finite universal value of the spectral gap in the limit of large $\\\\chi$, albeit with different spectral densities. We show that a finite gap in this limit does not imply a finite correlation length, as the mutual information between two large subsystems increases with $\\\\chi$ in a manner determined by the entire shape of the spectral density. The latter differs for different types of circuits, indicating that these ensembles of MPS retain relevant physical information about the underlying microscopic dynamics. In particular, in the presence of monitoring, we demonstrate the existence of a measurement-induced entanglement transition (MIPT) in MPS ensembles, with the averaged dimension of the transfer matrix's null space serving as the effective order parameter.\n\n| | |\n| --- | --- |\n| Subjects: | Quantum Physics (quant-ph); Statistical Mechanics (cond-mat.stat-mech) |\n| Cite as: | [arXiv:2412.14261](https://arxiv.org/abs/2412.14261) \\[quant-ph\\] |\n| | (or [arXiv:2412.14261v1](https://arxiv.org/abs/2412.14261v1) \\[quant-ph\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2412.14261](https://doi.org/10.48550/arXiv.2412.14261) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Hugo LÃ³io \\[ [view email](https://arxiv.org/show-email/30651cad/2412.14261)\\]\n\n**\\[v1\\]**\nWed, 18 Dec 2024 19:00:33 UTC (1,715 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Correlations, Spectra and Entaglement Transitions in Ensembles of Matrix Product States, by Hugo L\\\\'oio and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2412.14261)\n- [HTML (experimental)](https://arxiv.org/html/2412.14261v1)\n- [TeX Source](https://arxiv.org/src/2412.14261)\n- [Other Formats](https://arxiv.org/format/2412.14261)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\nquant-ph\n\n[<Â prev](https://arxiv.org/prevnext?id=2412.14261&function=prev&context=quant-ph)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2412.14261&function=next&context=quant-ph)\n\n[new](https://arxiv.org/list/quant-ph/new) \\| [recent](https://arxiv.org/list/quant-ph/recent) \\| [2024-12](https://arxiv.org/list/quant-ph/2024-12)\n\nChange to browse by:\n\n[cond-mat](https://arxiv.org/abs/2412.14261?context=cond-mat)\n\n[cond-mat.stat-mech](https://arxiv.org/abs/2412.14261?context=cond-mat.stat-mech)\n\n### References & Citations\n\n- [INSPIRE HEP](https://inspirehep.net/arxiv/2412.14261)\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2412.14261)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2412.14261)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2412.14261)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2412.14261&description=Correlations, Spectra and Entaglement Transitions in Ensembles of Matrix Product States) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2412.14261&title=Correlations, Spectra and Entaglement Transitions in Ensembles of Matrix Product States)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2412.14261) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2412.14261","id":"https://arxiv.org/abs/2412.14261","publishedDate":"2024-12-18T00:00:00.000Z","summary":"This paper explores the statistical and entanglement properties of ensembles of Matrix Product States (MPS) generated through various quantum circuit models, including random unitary circuits, brickwork circuits, and circuits with measurements. The authors analyze the spectral characteristics of the transfer matrix associated with these MPS ensembles, finding that in the thermodynamic limit, the spectral gap approaches a finite universal value for certain circuit types, though the spectral density varies. Importantly, a finite spectral gap does not necessarily imply a finite correlation length; mutual information between large subsystems increases with bond dimension, reflecting the influence of the entire spectral density shape. The study also identifies a measurement-induced entanglement transition (MIPT) in circuits involving monitoring, with the null space dimension of the transfer matrix serving as an order parameter. Overall, the work highlights how spectral properties of MPS ensembles encode key physical information about the underlying quantum dynamics and phase transitions.","title":"Correlations, Spectra and Entaglement Transitions in Ensembles of Matrix Product States","url":"https://arxiv.org/abs/2412.14261"},{"content":"\\\\UseRawInputEncoding\n\n# Meaning-Typed Programming: Language-level Abstractions and Runtime for GenAI Applications\n\nJason Mars\nUniversity of MichiganUnited States[profmars@umich.edu](mailto:profmars@umich.edu),Â Yiping Kang\nUniversity of MichiganUnited States[ypkang@umich.edu](mailto:ypkang@umich.edu),Â Jayanaka L. Dantanarayana\nUniversity of MichiganUnited States[jayanaka@umich.edu](mailto:jayanaka@umich.edu),Â Kugesan Sivasothynathan\nUniversity of Moratuwa\nJaseci LabsSri Lanka[kugesan.sivasothynathan@jaseci.org](mailto:kugesan.sivasothynathan@jaseci.org),Â Christopher Clarke\nUniversity of MichiganUnited States[csclarke@umich.edu](mailto:csclarke@umich.edu),Â Baichuan Li\nUniversity of MichiganUnited States[patrli@umich.edu](mailto:patrli@umich.edu),Â Krisztian Flautner\nUniversity of MichiganUnited States[manowar@umich.edu](mailto:manowar@umich.edu)Â andÂ Lingjia Tang\nUniversity of MichiganUnited States[lingjia@umich.edu](mailto:lingjia@umich.edu)\n\n###### Abstract.\n\nSoftware is rapidly evolving from being programmed with traditional logical code, to _neuro-integrated_ applications that leverage generative AI and large language models (LLMs) for application functionality. This shift increases the complexity of building applications, as developers now must reasoning about, program, and prompt LLMs. Despite efforts to create tools to assist with prompt engineering, these solutions often introduce additional layers of complexity to the development of neuro-integrated applications.\n\nThis paper proposes _meaning-typed programming_ (MTP), a novel approach to simplify the creation of neuro-integrated applications by introducing new language-level abstractions that hide the complexities of LLM integration. _Our key insight is that typical conventional code already possesses a high level of semantic richness that can be automatically reasoned about, as it is designed to be readable and maintainable by humans._ Leveraging this insight, we conceptualize LLMs as meaning-typed code constructs and introduce a by abstraction at the language level, MT-IR, a new meaning-based intermediate representation at the compiler level, and MT-Runtime, an automated run-time engine for LLM integration and operations. We implement MTP in a production-grade Python super-set language called Jac and perform an extensive evaluation. Our results demonstrate that MTP not only simplifies the development process but also meets or exceeds the efficacy of state-of-the-art manual and tool-assisted prompt engineering techniques in terms of accuracy and usability.\n\n## 1\\. Introduction\n\nSoftware development is rapidly evolving towards _neuro-integrated_ applications that leverage generative AI and large language models (LLMs) for critical functionality. This shift represents a convergence of conventional programming with the powerful capabilities of AI, promising to revolutionize how we build software.\nHowever, integrating LLMs into traditional programming remains a complex challenge due to the fundamental differences between operating assumptions. In conventional programming, code describes operations performed on explicitly defined objects (FigureÂ [1](https://arxiv.org/html/2405.08965v3#S1.F1) left). In contrast, LLMs process natural language text as input and produce text-based outputs. To integrate LLMs into programs, developers must manually construct textual input, a process known as _prompt engineering_.\n\nFigure 1. Comparison of LLM as Meaning Typed Code construct abstraction (Right) with the present-day LLM abstraction (Middle) and Symbolic Programming Abstraction (Left) of a programming function.\n\n\\\\Description\n\nPrompt engineering, the primary means of LLM programming, can be tedious, complex, and time-consuming. Developers must craft precise instructions, manage textual context, and handle the inherent variability in LLM output. This process lacks the structure, consistency, and debugging capabilities developers are accustomed to in traditional programming environments.\n\nSeveral efforts have been made to address these challenges by developing infrastructure and tools to facilitate prompt engineering. Frameworks such as LangChain, Language Model Query Language (LMQL)Â ( [Beurer2023LMQL,](https://arxiv.org/html/2405.08965v3#bib.bib4)), DSPyÂ ( [khattab2023dspy,](https://arxiv.org/html/2405.08965v3#bib.bib16)), and SGLangÂ ( [zheng2023SGLang,](https://arxiv.org/html/2405.08965v3#bib.bib46)) have been introduced to assist developers in generating and managing prompts. However, while these frameworks provide more sophisticated tooling to help bridge the gap between conventional programming and the incorporation of AI models, in practice, they introduce additional layers of complexity for developers:\n\n1. (1)\n\n\n_Prompt design complexity:_ Developers remain responsible for the manual construction of prompts, including determining the appropriate prompt language and selecting relevant information to include.\n\n2. (2)\n\n\n_Steep learning curve:_ These systems require developers to familiarize themselves with new query languages, frameworks, or specialized syntax, complicating the integration process.\n\n3. (3)\n\n\n_Input/output conversion complexity:_ Parsing LLM output and converting them back to objects remains a challenge, especially given the variability in output across different LLMs.\n\n\nThe key insights of this work are: _(1) In practice, programs are written to be readable by various developers to comprehend the intentions and semantics conveyed by the code. This semantic richness can be leveraged to automatically translate intent embodied in the code to prompts for the LLM. (2) As state-of-the-art LLMs continue to advance, AI is increasingly feasible to infer code intentions without requiring explicit descriptions or prompt construction by developers. This trend is not only supported by our empirical findings, but we surprisingly find less can be more in prompt engineering._\n\nBased on this insight, this paper proposes _meaning-typed programming_, a novel approach to simplify the creation of neuro-integrated applications by introducing new language-level abstractions that hide the complexities of LLM integration. There are three key components to enable meaning-typed programs. These include (1) The â€™byâ€™ operator that enables the seamless integration of LLM functionality into code. It acts as a bridge between traditional and LLM operations, allowing developers to replace function bodies with a simple by model\\_name clause:\n\n\\\\Description\n\nThis declaration, along with the function signature and available semantic information, is sufficient for the LLM to produce the correct return value at runtime, significantly simplifying LLM integration in existing codebases. (2) MT-IR, a meaning-based intermediate representation at the compiler level that collects and organizes semantically rich information, preserving the meaning behind variable names, function signatures, and comments. MT-IR is also used dynamically to bind run-time values to semantic representations, enhancing the modelâ€™s understanding of program context and enabling effective, context-aware LLM integration.\n(3) MT-Runtime, an automated runtime engine within the languageâ€™s virtual machine, triggered at by call sites. It binds runtime values to MT-IR, manages LLM interactions, generates context-aware prompts, and handles responses and errors. MT-Runtime allows developers to focus on application logic while automatically managing LLM integration, ensuring seamless and contextualized operations.\n\nWe implement _meaning-typed programming (MTP)_ in a production-grade Python superset language called Jac and perform an extensive evaluation. Specifically, we make the following contributions:\n\n- â€¢\n\n\nWe introduce the concept of meaning-typed programming, a novel paradigm that treats LLMs as meaning-typed code constructs, enabling seamless integration of LLMs into conventional programs without explicit prompt engineering.\n\n- â€¢\n\n\nWe present the design and implementation of the MTP framework: the â€˜byâ€™ operator for intuitive LLM integration; MT-IR, a semantically rich intermediate representation for implied meaning; and MT-Runtime, an automated runtime engine for LLM operations\n\n- â€¢\n\n\nWe implement MTP in Jac, a production-grade Python superset language, conduct extensive evaluation and show that MTP significantly reduces development complexity, lines of code, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches.\n\n- â€¢\n\n\nWe provide an empirical analysis of LLM evolution trends, demonstrating how MTP takes advantage of advances in LLM capabilities to simplify neuro-integrated programming over time.\n\n- â€¢\n\n\nWe present multiple case studies demonstrating the application of MTP in complex problem domains, analyzing its impact on code structure, development workflow, and complexity reduction compared to traditional LLM integration methods.\n\n\nOur results show that MTP simplifies the development process and meets or exceeds the efficacy of state-of-the-art manual and tool-assisted prompt engineering techniques. This approach significantly reduces development complexity and lines of code while improving run-time performance and costs associated with LLM integration.\n\n## 2\\. Problem, Background, and Insights\n\nConventional Prompt Engineering\n\n[â¬‡](data:;base64,CmdwdCA9IE9wZW5BSSgpCgpkZWYgcmVzcG9uZF93aXRoX2NvbmZpZGVuY2VfbGV2ZWwocXVlcnk6IHN0ciwKICAgIGNoYXRfaGlzdG9yeTogZGljdFtzdHIsc3RyXSwKICAgIHVzZXJfZGF0YTpkaWN0KSAtPiB0dXBsZVtzdHIsZmxvYXRdOgoKICAgIHN5c3RlbV9tZXNzYWdlID0gKAogICAgICAgICJZb3UgYXJlIGEgY2hhdGJvdC4gUmVzcG9uZCB0byB0aGUgY2hhdCB1c2luZyB0aGUgYXZhaWxhYmxlIGluZm9ybWF0aW9uLiBBZGRpdGlvbmFsbHkgZ2VuZXJhdGUgYSBjb25maWRlbmNlIGxldmVsIG9mIHRoZSByZXNwb25zZSBhcyBhIHBlcmNlbnRhZ2UiICkKCiAgICBwcm9tcHQgPSAgc3lzdGVtX21lc3NhZ2UKICAgIHByb21wdCArPSBmJ1xuUXVlcnkgZm9ybSB0aGUgdXNlcjpcbntxdWVyeX0nCiAgICBwcm9tcHQgKz0gJ1xuQ2hhdCBIaXN0b3J5OlxuJwogICAgZm9yIHVzZXIsY29udGVudCBpbiBjaGF0X2hpc3Rvcnk6CiAgICA","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2405.08965v3","id":"https://arxiv.org/html/2405.08965v3","publishedDate":"2024-10-19T00:00:00.000Z","summary":"The paper introduces Meaning-Typed Programming (MTP), a novel approach to simplify the development of neuro-integrated applications that leverage large language models (LLMs) and generative AI. Traditional methods require complex prompt engineering, which is tedious and error-prone. MTP addresses this by introducing language-level abstractions that treat LLM interactions as meaning-typed code constructs, effectively hiding the complexity of prompt crafting and LLM integration. The authors implement MTP in a Python-based language called Jac, featuring an intermediate representation (MT-IR) and a runtime engine (MT-Runtime) to facilitate seamless LLM operations. Evaluation shows that MTP not only streamlines development but also achieves comparable or better accuracy and usability than existing prompt engineering techniques, paving the way for more accessible and maintainable neuro-integrated applications.","title":"Meaning-Typed Programming: Language-level Abstractions and Runtime for GenAI Applications","url":"https://arxiv.org/html/2405.08965v3"},{"content":"Luca Beurer-Kellner\nMarc Fischer\nMartin Vechev\nEth Zurich\nSwitzerland\nPrompting Is Programming: A Query Language for Large Language Models Prompting Is Programming: A Query Language for Large Language Models\n10.1145/35913001CCS Concepts: â€¢ Software and its engineering â†’ Context specific languagesâ€¢ Computing method- ologies â†’ Natural language processingMachine learning Additional Key Words and Phrases: language model programming, prompt programming\nLarge language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task-and model-specific programs have to be implemented, which may still require ad-hoc interaction.Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the language model output. This enables easy adaption to many tasks while abstracting language model internals and providing high-level semantics.To enable LMP, we implement LMQL (short for Language Model Query Language), which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model. We show that LMQL can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level APIs. Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use APIs (26-85% cost savings).\nINTRODUCTION\nLarge Language Models (Large LMs -LLMs) [4,9,19,26] have proven successful at various languagebased tasks such as machine translation, text summarization, question answering, reasoning, code generation from text and many more. Due to these results, LMs have become popular beyond the machine learning community and are slowly being integrated into many applications.\n(Large) Language Models. Internally, language models operate on tokens, which are different from how humans perceive language. Given the tokenized version of some input, called the prompt, a large language model predicts the next token. That is, over a large vocabulary of tokens it assigns each a score or probability. A decoding procedure is then used, which by invoking the LM multiple times, computes a completion of the prompt. Commonly, the goal is to determine (or approximate) the highest probability continuation, however, as producing a particular token might lower the probability, before a subsequent token increases it, decoding sometimes requires expensive search or backtracking strategies. Nonetheless, LM-based text completion has shown to be very powerful and can be leveraged for a wide range of downstream applications.\nKey Challenges in Using Language Models. While the newer generation of language models can be prompted with examples or instructions in a conceptually simple manner, making the best use of these models and keeping up as new models are released requires a deep understanding of their internals, as well as the use of vendor-specific libraries and implementations. For example, as LMs operate on tokens, it can be hard to constrain the decoding procedure to a set of legal words or phrases. Further, many prompting techniques can require back-and-forth interaction between the LM and the user (e.g. chatbots like ChatGPT [16]) or very task-specific interfaces (e.g. to perform arithmetic calculations with external control logic). To implement these prompts, a lot of manual work and interaction with a model's decoding procedure is required, which restricts the generality of the resulting implementations. Lastly, as an LM only produces one (sub-word) token at a time, completing a sequence may require many calls. Also, decoding becomes increasingly expensive as the prefix, the prompt, and the so-far generated response grow. Because of these factors, and as language models are typically very large neural networks, practical inference demands high computational costs and significant latency. In the case of pay-to-use APIs, such as OpenAI's GPT models, this results in high usage costs per query answered","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fexport.arxiv.org%2Fpdf%2F2212.06094v3.pdf","id":"https://export.arxiv.org/pdf/2212.06094v3.pdf","publishedDate":"2023-05-31T00:00:00.000Z","summary":"The paper introduces Language Model Programming (LMP), a novel approach that extends traditional prompting of large language models (LLMs) by combining text prompts with scripting and constraints. This method enables more flexible, high-level control over LLM outputs, facilitating complex tasks and interactive workflows that are difficult with existing APIs. To implement LMP, the authors developed LMQL (Language Model Query Language), which optimizes inference by reducing costly model calls and allowing constraints to guide output generation. The approach captures many state-of-the-art prompting techniques, improves accuracy, and significantly decreases computational costs (by 26-85%). Overall, the work aims to make programming with LLMs more intuitive, efficient, and adaptable for various downstream applications.","title":"Prompting Is Programming: A Query Language for Large Language Models","url":"https://export.arxiv.org/pdf/2212.06094v3.pdf"},{"content":"Type-directed\nPrompt Construction\nfor LLM-powered\nProgramming Assistants\nAndrew\nBlinn\nKevin\nLi\nCyrus\nOmar\nFPLab October 2023 Experience Report\n@ UMich\n1\nview: (Action -> Event, Model) -> Node\nHazel Demo:\nTodo MVU\n2\n: (Action -> Event, Model) -> Node equals expected type\nDiv<\nStylect\nDisplay(\"flex\"),\nFlexDirection(\"column\") ,\nGap(\"lem\"),\nPadding(\"1em\"),\nBorderRadius(\"1lem\") ,\nBackgroundColor (\"#4CAF50\") ,\nColor (\"white\") ,\nFontFamily(\"Comic Sans MS, cursive, sans-serif\")\nDiv([], (Text(\"Hazel TODOs\")1),\nbutter,\nadd_button,\ntodos_deck(todos)\nLo. . Render (\"todo_app\", Model.init, view, update)\nRender (\"todo_app\", Model.init, view, update)\nHazel TODOs\nHazel TODOs\nAdd Todo\nAdd Todo\ntodos:\n@ start slides Pending\nYou're caught up\nIntro\n- Large language models have changed\nthe landscape for code completion\n- But how do they get fed? Poor or missing\ninformation can lead to hallucination\n- We have a wealth of semantic information available\nthat can be used to inform and constrain generation\n4","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fandrewblinn.com%2Fpapers%2F2023-MWPLS-Type-directed-Prompt-Construction-for-LLM-powered-Programming-Assistants.pdf","id":"https://andrewblinn.com/papers/2023-MWPLS-Type-directed-Prompt-Construction-for-LLM-powered-Programming-Assistants.pdf","publishedDate":"1970-01-01T00:00:00.000Z","summary":"The webpage presents a research report titled \"Type-directed Prompt Construction for LLM-powered Programming Assistants\" by Andrew Blinn, Kevin Li, and Cyrus Omar, published in October 2023. It discusses methods for constructing prompts for large language models (LLMs) to enhance programming assistance. The authors emphasize leveraging semantic type information to guide prompt design, reducing hallucinations, and improving the accuracy of code generation. The report includes practical demonstrations, such as a Hazel demo for a Todo application, illustrating how type constraints can ensure correct rendering and behavior. Overall, the paper advocates for type-aware prompt engineering to make LLM-based programming tools more reliable and effective.","title":"Type-directed Prompt Construction for LLM-powered Programming Assistants","url":"https://andrewblinn.com/papers/2023-MWPLS-Type-directed-Prompt-Construction-for-LLM-powered-Programming-Assistants.pdf"},{"content":"# Computer Science > Artificial Intelligence\n\n**arXiv:2503.06410** (cs)\n\n\\[Submitted on 9 Mar 2025\\]\n\n# Title:Performant LLM Agentic Framework for Conversational AI\n\nAuthors: [Alex Casella](https://arxiv.org/search/cs?searchtype=author&query=Casella,+A), [Wayne Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+W)\n\nView a PDF of the paper titled Performant LLM Agentic Framework for Conversational AI, by Alex Casella and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.06410) [HTML (experimental)](https://arxiv.org/html/2503.06410v1)\n\n> Abstract:The rise of Agentic applications and automation in the Voice AI industry has led to an increased reliance on Large Language Models (LLMs) to navigate graph-based logic workflows composed of nodes and edges. However, existing methods face challenges such as alignment errors in complex workflows and hallucinations caused by excessive context size. To address these limitations, we introduce the Performant Agentic Framework (PAF), a novel system that assists LLMs in selecting appropriate nodes and executing actions in order when traversing complex graphs. PAF combines LLM-based reasoning with a mathematically grounded vector scoring mechanism, achieving both higher accuracy and reduced latency. Our approach dynamically balances strict adherence to predefined paths with flexible node jumps to handle various user inputs efficiently. Experiments demonstrate that PAF significantly outperforms baseline methods, paving the way for scalable, real-time Conversational AI systems in complex business environments.\n\n| | |\n| --- | --- |\n| Comments: | 6 pages, 3 figures |\n| Subjects: | Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2503.06410](https://arxiv.org/abs/2503.06410) \\[cs.AI\\] |\n| (or [arXiv:2503.06410v1](https://arxiv.org/abs/2503.06410v1) \\[cs.AI\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.06410](https://doi.org/10.48550/arXiv.2503.06410) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Haohan Wang \\[ [view email](https://arxiv.org/show-email/7703ccd0/2503.06410)\\] **\\[v1\\]**\nSun, 9 Mar 2025 02:58:34 UTC (140 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Performant LLM Agentic Framework for Conversational AI, by Alex Casella and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.06410)\n- [HTML (experimental)](https://arxiv.org/html/2503.06410v1)\n- [TeX Source](https://arxiv.org/src/2503.06410)\n- [Other Formats](https://arxiv.org/format/2503.06410)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.AI\n\n[<Â prev](https://arxiv.org/prevnext?id=2503.06410&function=prev&context=cs.AI)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2503.06410&function=next&context=cs.AI)\n\n[new](https://arxiv.org/list/cs.AI/new) \\| [recent](https://arxiv.org/list/cs.AI/recent) \\| [2025-03](https://arxiv.org/list/cs.AI/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.06410?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.06410)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.06410)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.06410)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.06410) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.06410","id":"https://arxiv.org/abs/2503.06410","publishedDate":"2025-03-09T00:00:00.000Z","summary":"The paper introduces the Performant Agentic Framework (PAF), a novel system designed to enhance Large Language Models (LLMs) used in conversational AI, especially for navigating complex graph-based workflows. Traditional methods often struggle with alignment errors and hallucinations due to large context sizes. PAF addresses these issues by combining LLM reasoning with a mathematically grounded vector scoring mechanism, enabling more accurate and faster decision-making. It dynamically balances strict adherence to predefined paths with flexible node jumps, improving efficiency in handling diverse user inputs. Experiments demonstrate that PAF significantly outperforms existing approaches, facilitating scalable, real-time conversational AI in complex business environments.","title":"Performant LLM Agentic Framework for Conversational AI","url":"https://arxiv.org/abs/2503.06410"}],"status":"completed","toolCallId":"call_VIo7KyPDSQAzWKOdAG10URtz","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Hereâ€™s a focused deep dive on the by operator from the MTP paper.\n\nWhat it is\n- A language-level construct that replaces a functionâ€™s body with a declarative clause delegating its implementation to an LLM at runtime.\n- Syntax pattern (in Jac, the reference implementation): define a normal function with name, params, return type, docstring/comments, then specify by <model/adapter/options> instead of writing a body.\n\nIntended developer experience\n- You declare intent via ordinary code semantics (names, types, docstrings) rather than writing prompts.\n- The compiler and runtime use your functionâ€™s â€œmeaningâ€ to auto-generate an effective prompt and to parse/validate the result back into the declared return type.\n\nHow it works end-to-end\n1) Declaration time (compile-time)\n- You write a function signature and annotate types. Optionally include docstrings/comments and meaningful parameter names.\n- You attach a by clause indicating the model (e.g., by gpt-4o) and possibly extra options (temperature, tools, system hints).\n- The compiler lifts the functionâ€™s semantics into MT-IR (Meaning-Typed Intermediate Representation), capturing:\n  - Function name and purpose\n  - Parameter names, types, and defaults\n  - Return type and shape (primitives, enums, records/structs, lists, unions)\n  - Docstrings and inline comments\n  - Related symbols in scope that may be relevant context\n\n2) Call time (runtime)\n- When the function is called, MT-Runtime binds actual argument values to the MT-IR and assembles a context-aware prompt automatically.\n- It invokes the specified model and enforces output shaping:\n  - Prefer structured decoding (e.g., JSON/object mode) aligned to the return type\n  - Validate and coerce results to the declared type; if invalid, perform guided repairs/retries\n  - Surface typed errors if recovery fails\n\n3) Post-processing and control\n- The runtime can:\n  - Cache results keyed by inputs (memoization)\n  - Log prompts/traces for observability\n  - Apply guardrails (regex/schema validation, safe tool calls)\n  - Fall back to alternative models or deterministic code if configured\n\nWhy it works\n- Modern LLMs can infer task intent from code semantics. The by operator standardizes how that intent is extracted and presented.\n- Types provide a contract that tames free-form outputs, making LLM calls predictable, testable, and composable.\n\nKey semantics and guarantees\n- Fidelity to signature: Inputs are passed as explicit, labeled arguments; outputs must conform to the return type or an error is raised.\n- Meaning-first prompting: The primary prompt content is derived from code meaning (names/types/docs), not ad hoc strings.\n- Determinism envelope: While the LLM is nondeterministic, the operator narrows variability via schema-constrained decoding, retries, and temperature settings.\n- Backwards compatibility: You can replace a by function with a hand-written body later without changing call sites.\n\nTypical usage patterns\n- Pure inference helpers: classify, extract, summarize with typed outputs.\n- Schema-constrained generation: return well-typed objects, lists, unions.\n- Tool-augmented behaviors: allow the model to call registered tools/APIs (if enabled in options).\n- Incremental migration: swap brittle prompt-engineered functions for by functions, keeping the same signatures.\n\nConfiguration knobs (conceptual)\n- Model and endpoint: model name/provider; can be overridden centrally.\n- Decoding: temperature, max tokens, top-p, etc.\n- Output mode: JSON/schema-first, function-call mode, or free text with post-parse.\n- Guardrails: validation strategies, retry limits, safety policies.\n- Context scope: what symbols/comments to include, depth limits.\n- Cost/perf: caching, batching, parallelism.\n\nError handling and robustness\n- Schema mismatch: runtime prompts the model to repair to the required type, with a bounded number of retries.\n- Ambiguity: the runtime can request clarification from the model (self-ask) or escalate an error.\n- Underspecified names: the paper reports resilience even when up to ~50% of naming quality is degraded; meaning is still recovered from remaining semantics and types.\n\nHow it differs from â€œprompt functionsâ€ in frameworks\n- You do not write the prompt template; the operator delegates prompt assembly to the language/runtime using MT-IR.\n- Output typing is not an add-on; it is the source of truth that drives structured decoding and validation.\n- It is a first-class construct of the language, not a library convention.\n\nMental model\n- Think of by as â€œthis functionâ€™s body is implemented by a model, but the contract (signature, types, doc) is mine; the language will translate between my code world and the model world.â€\n\nA small, concrete sketch (illustrative, not exact)\n- Without by:\n  - You handcraft a prompt, bind variables, call an SDK, parse JSON, validate types, handle retries.\n- With by:\n  - You declare:\n    - function extract_items(text: str) -> list[Item] by gpt-4o\n    - where Item is a typed record with fields you defined\n  - Call extract_items(doc) as normal; runtime handles prompt, parsing, validation, retries.\n\nTesting and debugging\n- Unit tests call the function like any other. You can:\n  - Inject a mock model or recorded traces\n  - Inspect generated prompts/traces in logs\n  - Verify typed outputs without brittle string asserts\n\nPerformance and cost\n- Fewer tokens: prompts are concise and structured; runtime can elide redundant context.\n- Fewer round-trips: schema-first decoding reduces back-and-forth; validation-guided retries are targeted.\n- Centralized controls let teams swap models or adjust temperatures without touching call sites.\n\nWhen not to use it\n- Deterministic or compute-heavy code you can write normally.\n- Tight latency SLOs where LLM variability is unacceptable.\n- Tasks lacking stable type contracts or where free-form prose is the actual required output (unless you still want minimal constraints like str with style hints).\n\nBottom line\n- The by operator turns â€œprompted function programmingâ€ into â€œtyped, meaning-driven function delegation.â€ You keep familiar language constructs; the compiler/runtime shoulder prompt construction, context assembly, and robust typed IO with LLMs.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0aef07f0c5467c4a0068c86c77b0fc81a2bff158bc244206fb"}},"resumableStreamId":"STREAM:c2f2cfe8-89d0-4e0f-8172-b606027ce95a","role":"assistant","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","timeToFirstToken":0.904,"tokens":1310,"tokensPerSecond":30.860562086268228,"updated_at":1757965416453,"userId":"google:104458561188320708185","id":"j979xmyv4ftxwdkwy4wefh1ed97qm2aa"},{"_creationTime":1757965470391.4841,"_id":"j97bnaqqt96nxh96y0f22px3hn7qm9gc","attachmentIds":[],"content":"explain to me about MT-IR in detail","created_at":1757965470392,"messageId":"9a0881d0-2f32-4752-ac39-e00dc3573cd5","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","updated_at":1757965470392,"userId":"google:104458561188320708185","id":"j97bnaqqt96nxh96y0f22px3hn7qm9gc","providerMetadata":{}},{"_creationTime":1757965470391.4844,"_id":"j9764v4rzvcp5j684h0tcf2rd17qnqzx","attachmentIds":[],"byok":false,"content":"Hereâ€™s a detailed, practitioner-oriented explanation of MT-IR (Meaning-Typed Intermediate Representation) as presented in the MTP paper.\n\nWhat MT-IR is\n- MT-IR is a compiler/runtime intermediate representation that captures the â€œmeaningâ€ of your codebaseâ€”names, types, signatures, docs, and symbol relationshipsâ€”so the system can automatically construct effective LLM prompts and validate/shape outputs without manual prompt engineering.\n- Think of it as a semantic graph (plus schemas) extracted from your code and environment, purpose-built for AI delegation.\n\nWhy an IR at all?\n- Traditional IRs focus on control flow and machine-level operations. Theyâ€™re great for code generation and optimization, but they discard much of the semantic richness developers encode in names, types, and docs.\n- LLMs do better when given high-level intent, typing contracts, and labeled examples. MT-IR preserves and structures those semantics explicitly so that:\n  - Prompt assembly can be automated and consistent.\n  - Output can be schema-constrained to declared types.\n  - Context can be selected from the codebase itself, not from ad hoc prompt text.\n\nCore contents of MT-IR\n- Symbol metadata\n  - Functions: name, parameters, parameter names/types/defaults, return type, visibility, purity hints, docstring, comments.\n  - Types: primitives, enums, records/structs/classes (field names, types, optionality), unions/variants, generics and constraints.\n  - Constants and literals that may inform intent (e.g., label sets, regexes).\n  - Error types and exceptions (useful for guardrails and repair strategies).\n- Relationships and scope\n  - Call graph edges (who calls whom), import/use relationships, module/package boundaries.\n  - Ownership/namespace info for disambiguation.\n  - Nearby symbols that might be relevant context (same module, test cases, example calls).\n- Behavioral contracts\n  - Pre/post-conditions if annotated, invariants, assertions.\n  - Examples and unit tests (optionally harvested as few-shot hints).\n- Runtime bindings (late-bound layer)\n  - At call time, actual argument values and environmental signals (e.g., feature flags, locales) are bound into the IR slice used for that call.\n\nHow MT-IR is built (compile-time phase)\n1) Frontend parsing\n- Parse source files (in Jac for the reference implementation) to capture AST nodes for functions, types, docs, and comments.\n2) Semantic lifting\n- Transform AST into MT-IR nodes with normalized, model-facing metadata:\n  - Canonical field names, friendly descriptions synthesized from docs/comments.\n  - Type schemas rendered in a model-friendly shape (e.g., JSON Schema-like forms).\n3) Linking\n- Resolve cross-references (types used by functions, imports, enum membership).\n- Build a lightweight graph for context selection (neighbors, parents, dependents).\n4) Serialization and indexing\n- Store MT-IR in an efficient index so the runtime can pull a minimal â€œIR sliceâ€ for a specific call site.\n\nHow MT-IR is used (runtime phase)\n- IR slice selection\n  - Given a by function call, the runtime extracts just the relevant subset: the function node, its parameter/return schemas, key related symbols (types/enums), and any examples/tests flagged as helpful.\n- Prompt assembly\n  - The runtime converts the IR slice into structured model input:\n    - Task framing: derived from function name and docstring.\n    - Argument spec: labeled params with types and current values.\n    - Output contract: the declared return type rendered as a schema with field descriptions and constraints.\n    - Optional few-shot material: examples from tests or doc blocks.\n- Structured decoding and validation\n  - The output is requested in schema-aligned form (e.g., JSON/object mode).\n  - The runtime validates against the MT-IR type schema; on mismatch, it issues targeted repair prompts or coercions.\n- Observability and governance\n  - Prompts, schemas, and model responses are logged as IR-aligned traces, enabling reproducible debugging and audits.\n\nDesign principles and guarantees\n- Meaning-first: Prefer semantically rich signals (names/types/docs) over long, brittle prompt prose.\n- Minimal sufficient context: Include only what the IR graph deems relevant to reduce tokens and ambiguity.\n- Type-driven IO: Return shapes are first-class; the schema comes from the IR, not from hand-written prompt text.\n- Replaceability: Because the contract is in the IR, you can swap models, adjust decoding policies, or even replace the by function with a hand-written implementation without changing call sites.\n- Robustness to naming noise: The paper reports that MT-IR-driven prompts remain effective even when identifier quality degrades (up to ~50% corruption), because types, structure, and remaining names still carry enough meaning.\n\nWhat MT-IR looks like (conceptual sketch)\n- Function node\n  - id: f.extract_items\n  - name: extract_items\n  - doc: â€œExtract line items from purchase text.â€\n  - params:\n    - text: str (doc: â€œraw purchase receiptâ€)\n  - returns: list[Item]\n  - related_types: [Item]\n  - examples: [input/output pairs]\n- Type node (Item)\n  - fields:\n    - name: str\n    - qty: int >= 1\n    - price: float >= 0\n  - descriptions on each field\n- Relationships\n  - extract_items -> Item (uses)\n  - extract_items <- parse_receipt (caller)\n\nHow MT-IR differs from classic IRs and from libraries\n- Classic compiler IRs (e.g., LLVM IR) abstract away names and docs; MT-IR preserves them because LLMs need them.\n- Prompt libraries store templates; MT-IR stores structured semantics that can synthesize prompts dynamically and consistently across the codebase.\n\nInteractions with other MTP parts\n- by operator: The by declaration hooks a function into the MT-runtime; MT-IR is the substrate that provides meaning and schemas for that function.\n- MT-runtime: Uses MT-IR to build prompts, validate outputs, manage retries, and log traces.\n- Tool use: If tool APIs are allowed, their signatures and capabilities are also represented in MT-IR so the model can call them in a typed, controlled way.\n\nBenefits youâ€™ll notice\n- Less boilerplate: No manual prompt text, no hand-written JSON schemas; both are derived from code.\n- Better correctness: Type-driven validation reduces silent failures and parsing hacks.\n- Maintainability: Rename a field or tighten a type; the IR updates and propagates to all LLM call sites.\n- Team scalability: Shared semantics + centralized policies yield consistent behavior across engineers and services.\n\nLimitations and open questions\n- Ambiguous or sparse docs: If names/types are too generic, intent may still be under-specified; runtime may need clarifying self-asks.\n- Large codebases: IR slicing and relevance ranking must be efficient to avoid token and latency inflation.\n- Non-functional requirements: Latency/cost/SLO knobs must be tuned; not all tasks benefit from LLM delegation.\n\nIf you want, I can show a small before/after example of how MT-IR would render a function and how that becomes a concrete, schema-first prompt at runtime.","created_at":1757965470393,"messageId":"06e4f820-1fc6-4b09-b028-8f76cb2e2e10","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Hereâ€™s a detailed, practitioner-oriented explanation of MT-IR (Meaning-Typed Intermediate Representation) as presented in the MTP paper.\n\nWhat MT-IR is\n- MT-IR is a compiler/runtime intermediate representation that captures the â€œmeaningâ€ of your codebaseâ€”names, types, signatures, docs, and symbol relationshipsâ€”so the system can automatically construct effective LLM prompts and validate/shape outputs without manual prompt engineering.\n- Think of it as a semantic graph (plus schemas) extracted from your code and environment, purpose-built for AI delegation.\n\nWhy an IR at all?\n- Traditional IRs focus on control flow and machine-level operations. Theyâ€™re great for code generation and optimization, but they discard much of the semantic richness developers encode in names, types, and docs.\n- LLMs do better when given high-level intent, typing contracts, and labeled examples. MT-IR preserves and structures those semantics explicitly so that:\n  - Prompt assembly can be automated and consistent.\n  - Output can be schema-constrained to declared types.\n  - Context can be selected from the codebase itself, not from ad hoc prompt text.\n\nCore contents of MT-IR\n- Symbol metadata\n  - Functions: name, parameters, parameter names/types/defaults, return type, visibility, purity hints, docstring, comments.\n  - Types: primitives, enums, records/structs/classes (field names, types, optionality), unions/variants, generics and constraints.\n  - Constants and literals that may inform intent (e.g., label sets, regexes).\n  - Error types and exceptions (useful for guardrails and repair strategies).\n- Relationships and scope\n  - Call graph edges (who calls whom), import/use relationships, module/package boundaries.\n  - Ownership/namespace info for disambiguation.\n  - Nearby symbols that might be relevant context (same module, test cases, example calls).\n- Behavioral contracts\n  - Pre/post-conditions if annotated, invariants, assertions.\n  - Examples and unit tests (optionally harvested as few-shot hints).\n- Runtime bindings (late-bound layer)\n  - At call time, actual argument values and environmental signals (e.g., feature flags, locales) are bound into the IR slice used for that call.\n\nHow MT-IR is built (compile-time phase)\n1) Frontend parsing\n- Parse source files (in Jac for the reference implementation) to capture AST nodes for functions, types, docs, and comments.\n2) Semantic lifting\n- Transform AST into MT-IR nodes with normalized, model-facing metadata:\n  - Canonical field names, friendly descriptions synthesized from docs/comments.\n  - Type schemas rendered in a model-friendly shape (e.g., JSON Schema-like forms).\n3) Linking\n- Resolve cross-references (types used by functions, imports, enum membership).\n- Build a lightweight graph for context selection (neighbors, parents, dependents).\n4) Serialization and indexing\n- Store MT-IR in an efficient index so the runtime can pull a minimal â€œIR sliceâ€ for a specific call site.\n\nHow MT-IR is used (runtime phase)\n- IR slice selection\n  - Given a by function call, the runtime extracts just the relevant subset: the function node, its parameter/return schemas, key related symbols (types/enums), and any examples/tests flagged as helpful.\n- Prompt assembly\n  - The runtime converts the IR slice into structured model input:\n    - Task framing: derived from function name and docstring.\n    - Argument spec: labeled params with types and current values.\n    - Output contract: the declared return type rendered as a schema with field descriptions and constraints.\n    - Optional few-shot material: examples from tests or doc blocks.\n- Structured decoding and validation\n  - The output is requested in schema-aligned form (e.g., JSON/object mode).\n  - The runtime validates against the MT-IR type schema; on mismatch, it issues targeted repair prompts or coercions.\n- Observability and governance\n  - Prompts, schemas, and model responses are logged as IR-aligned traces, enabling reproducible debugging and audits.\n\nDesign principles and guarantees\n- Meaning-first: Prefer semantically rich signals (names/types/docs) over long, brittle prompt prose.\n- Minimal sufficient context: Include only what the IR graph deems relevant to reduce tokens and ambiguity.\n- Type-driven IO: Return shapes are first-class; the schema comes from the IR, not from hand-written prompt text.\n- Replaceability: Because the contract is in the IR, you can swap models, adjust decoding policies, or even replace the by function with a hand-written implementation without changing call sites.\n- Robustness to naming noise: The paper reports that MT-IR-driven prompts remain effective even when identifier quality degrades (up to ~50% corruption), because types, structure, and remaining names still carry enough meaning.\n\nWhat MT-IR looks like (conceptual sketch)\n- Function node\n  - id: f.extract_items\n  - name: extract_items\n  - doc: â€œExtract line items from purchase text.â€\n  - params:\n    - text: str (doc: â€œraw purchase receiptâ€)\n  - returns: list[Item]\n  - related_types: [Item]\n  - examples: [input/output pairs]\n- Type node (Item)\n  - fields:\n    - name: str\n    - qty: int >= 1\n    - price: float >= 0\n  - descriptions on each field\n- Relationships\n  - extract_items -> Item (uses)\n  - extract_items <- parse_receipt (caller)\n\nHow MT-IR differs from classic IRs and from libraries\n- Classic compiler IRs (e.g., LLVM IR) abstract away names and docs; MT-IR preserves them because LLMs need them.\n- Prompt libraries store templates; MT-IR stores structured semantics that can synthesize prompts dynamically and consistently across the codebase.\n\nInteractions with other MTP parts\n- by operator: The by declaration hooks a function into the MT-runtime; MT-IR is the substrate that provides meaning and schemas for that function.\n- MT-runtime: Uses MT-IR to build prompts, validate outputs, manage retries, and log traces.\n- Tool use: If tool APIs are allowed, their signatures and capabilities are also represented in MT-IR so the model can call them in a typed, controlled way.\n\nBenefits youâ€™ll notice\n- Less boilerplate: No manual prompt text, no hand-written JSON schemas; both are derived from code.\n- Better correctness: Type-driven validation reduces silent failures and parsing hacks.\n- Maintainability: Rename a field or tighten a type; the IR updates and propagates to all LLM call sites.\n- Team scalability: Shared semantics + centralized policies yield consistent behavior across engineers and services.\n\nLimitations and open questions\n- Ambiguous or sparse docs: If names/types are too generic, intent may still be under-specified; runtime may need clarifying self-asks.\n- Large codebases: IR slicing and relevance ranking must be efficient to avoid token and latency inflation.\n- Non-functional requirements: Latency/cost/SLO knobs must be tuned; not all tasks benefit from LLM delegation.\n\nIf you want, I can show a small before/after example of how MT-IR would render a function and how that becomes a concrete, schema-first prompt at runtime.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_04ea75b4e74145ec0068c86c9f428c819cab7f147e98654660"}},"resumableStreamId":"STREAM:2bccf882-023b-454d-a096-a9ca7250a33b","role":"assistant","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","timeToFirstToken":1.094,"tokens":1506,"tokensPerSecond":47.6974726040413,"updated_at":1757965470393,"userId":"google:104458561188320708185","id":"j9764v4rzvcp5j684h0tcf2rd17qnqzx"},{"_creationTime":1757965510594.0393,"_id":"j975zw8h9xzqjny9nb8tzc69ed7qn1sp","attachmentIds":[],"content":"explain to me about MT-Runtime in detail","created_at":1757965510594,"messageId":"94b21ec3-5d3b-45ad-8eaa-f756cd7ba414","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","updated_at":1757965510594,"userId":"google:104458561188320708185","id":"j975zw8h9xzqjny9nb8tzc69ed7qn1sp","providerMetadata":{}},{"_creationTime":1757965510594.0396,"_id":"j971681vnhq1j24vmjxyys931n7qnm0h","attachmentIds":[],"byok":false,"content":"Hereâ€™s a detailed, developer-focused explanation of MT-Runtime in MTP and how it operates end to end.\n\nWhat MT-Runtime is\n- Itâ€™s the execution engine that activates at by call sites. It binds real argument values to the programâ€™s semantic model (MT-IR), assembles the model request, invokes the selected LLM/tooling, enforces the declared type contract on outputs, and handles errors, retries, caching, and governance.\n- Conceptually, itâ€™s the â€œtranslator and traffic copâ€ between your typed, symbolic code and the nondeterministic LLM world.\n\nKey responsibilities\n1) IR-slice and context construction\n- Selects the minimal relevant MT-IR slice for a call: function node (name, doc), parameter and return schemas, related types/enums, and optional examples/tests.\n- Binds runtime values (actual args, defaults, possibly environment like locale) to that slice.\n\n2) Prompt/request assembly\n- Builds a structured model request guided by the functionâ€™s meaning and contract:\n  - Task framing: distilled from function name and docstring.\n  - Inputs: labeled arguments with types and current values.\n  - Output contract: a schema derived from the declared return type (e.g., JSON Schema-like).\n  - Optional: few-shot snippets (e.g., tests), policy/system hints, tool specs.\n- Chooses output mode that favors structure (JSON/object/function-call) to minimize parsing brittleness.\n\n3) Invocation and decoding\n- Calls the configured model/endpoint with chosen decoding parameters (temperature, max tokens, etc.).\n- Validates the response against the return-type schema:\n  - Type checks (primitives, enums, unions), field presence, constraints (ranges, regex), and nested shapes.\n  - Coercions where safe (e.g., \"3\" -> 3) and unit conversions if configured.\n\n4) Error handling and self-repair\n- If validation fails, performs bounded, targeted retries:\n  - Repair prompts that reference the exact schema violations.\n  - Strategy escalation: switch to stricter object mode; reduce temperature; add clarifying constraints.\n- If still failing, surfaces a typed exception aligned to the functionâ€™s contract.\n\n5) Tool and API mediation (optional)\n- If enabled, exposes registered tools with typed signatures (also in MT-IR) so the model can request tool calls.\n- Executes tool calls deterministically under policy, returns results to the model, and keeps a trace.\n\n6) Observability and governance\n- Logs IR slice, request, response, validation results, and retry trail as a structured trace.\n- Supports redaction policies, safety filters, and audit hooks.\n- Provides memoization/caching keyed on normalized inputs to cut latency/cost.\n\n7) Policy and routing\n- Centralized knobs: model routing/fallbacks, budgets, rate limits, batching, parallelism.\n- Can swap models or change decoding policies without touching call sites.\n- Supports feature flags and A/B to compare models or policies.\n\nHow MT-Runtime uses MT-IR\n- MT-IR supplies the semantics: names, docs, parameter labels, and the canonical return-type schema.\n- At call time, MT-Runtime pulls a minimal IR slice and binds live values.\n- The IR schema is the source of truth for output shaping and validation; no hand-written prompt templates are needed.\n\nLifecycle of a by function call\n1) Intercept call â†’ fetch MT-IR slice for the function.\n2) Bind args â†’ render â€œmeaning-firstâ€ request (framing + typed inputs + output schema).\n3) Invoke model â†’ request structured output mode.\n4) Validate â†’ coerce/repair â†’ possibly retry with targeted guidance.\n5) Return typed value â†’ cache, log trace, enforce policies.\n\nContracts and guarantees\n- Signature fidelity: Inputs are explicit and labeled; outputs must conform to the declared return type or raise.\n- Determinism envelope: Nondeterminism is bounded via schema constraints, retries, and decoding controls.\n- Replaceability: You can later replace the by body with a manual implementation; call sites remain unchanged.\n- Resilience to naming noise: Even if identifiers degrade, types and remaining semantics keep the system functional.\n\nConfiguration knobs youâ€™ll actually use\n- Model routing: default model, per-function overrides, fallbacks.\n- Decoding policy: temperature, max tokens, top-p; strict object/JSON modes.\n- Validation and retries: retry count, repair strategy, coercion rules.\n- Tooling: which tools are exposed, rate limits, side-effect policies.\n- Context limits: IR slice size, neighbor relevance thresholds.\n- Cost/latency: caching, batching, concurrency, circuit breakers.\n\nFailure modes and mitigations\n- Ambiguous intent â†’ clarify via repair prompts or require richer types/docs.\n- Hallucinated fields â†’ schema-first decoding rejects extras unless allowed; repair narrows output.\n- Partial outputs â†’ field-level repair prompts; union-alt disambiguation.\n- Model drift/outages â†’ routing and fallback policies, cached results.\n\nWhat you get as a developer\n- Write normal function signatures and types; add a by clause; call it like any function.\n- No prompt templates, no manual JSON parsing, no bespoke retries.\n- Uniform tracing and debuggability: inspect the IR-derived request and how the runtime repaired outputs.\n\nExample at a glance (illustrative)\n- You declare:\n  - function extract_items(text: str) -> list[Item] by gpt-4o\n  - Item has typed fields with descriptions.\n- MT-Runtime constructs a minimal, typed request, asks for object output, validates to list[Item], repairs if needed, and returns a proper Python/Jac object list. You get logs for the whole interaction.\n\nBottom line\n- MT-Runtime operationalizes meaning-typed programming: it turns your codeâ€™s semantics into reliable, typed LLM calls with automated prompting, structured decoding, validation, repair, and governanceâ€”so AI integration behaves like standard, testable, maintainable code.","created_at":1757965510595,"messageId":"73910ec3-8ce9-4af5-8a92-330e3b8f6e38","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":["arxiv.org","jaseci.org","github.com"],"queries":["\"MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\" MT-Runtime details paper 2405.08965","arXiv 2405.08965 MTP Meaning-Typed by operator MT-Runtime"]},"result":[{"content":"\\\\UseRawInputEncoding\n\n# Meaning-Typed Programming: Language-level Abstractions and Runtime for GenAI Applications\n\nJason Mars\nUniversity of MichiganUnited States[profmars@umich.edu](mailto:profmars@umich.edu),Â Yiping Kang\nUniversity of MichiganUnited States[ypkang@umich.edu](mailto:ypkang@umich.edu),Â Jayanaka L. Dantanarayana\nUniversity of MichiganUnited States[jayanaka@umich.edu](mailto:jayanaka@umich.edu),Â Kugesan Sivasothynathan\nUniversity of Moratuwa\nJaseci LabsSri Lanka[kugesan.sivasothynathan@jaseci.org](mailto:kugesan.sivasothynathan@jaseci.org),Â Christopher Clarke\nUniversity of MichiganUnited States[csclarke@umich.edu](mailto:csclarke@umich.edu),Â Baichuan Li\nUniversity of MichiganUnited States[patrli@umich.edu](mailto:patrli@umich.edu),Â Krisztian Flautner\nUniversity of MichiganUnited States[manowar@umich.edu](mailto:manowar@umich.edu)Â andÂ Lingjia Tang\nUniversity of MichiganUnited States[lingjia@umich.edu](mailto:lingjia@umich.edu)\n\n###### Abstract.\n\nSoftware is rapidly evolving from being programmed with traditional logical code, to _neuro-integrated_ applications that leverage generative AI and large language models (LLMs) for application functionality. This shift increases the complexity of building applications, as developers now must reasoning about, program, and prompt LLMs. Despite efforts to create tools to assist with prompt engineering, these solutions often introduce additional layers of complexity to the development of neuro-integrated applications.\n\nThis paper proposes _meaning-typed programming_ (MTP), a novel approach to simplify the creation of neuro-integrated applications by introducing new language-level abstractions that hide the complexities of LLM integration. _Our key insight is that typical conventional code already possesses a high level of semantic richness that can be automatically reasoned about, as it is designed to be readable and maintainable by humans._ Leveraging this insight, we conceptualize LLMs as meaning-typed code constructs and introduce a by abstraction at the language level, MT-IR, a new meaning-based intermediate representation at the compiler level, and MT-Runtime, an automated run-time engine for LLM integration and operations. We implement MTP in a production-grade Python super-set language called Jac and perform an extensive evaluation. Our results demonstrate that MTP not only simplifies the development process but also meets or exceeds the efficacy of state-of-the-art manual and tool-assisted prompt engineering techniques in terms of accuracy and usability.\n\n## 1\\. Introduction\n\nSoftware development is rapidly evolving towards _neuro-integrated_ applications that leverage generative AI and large language models (LLMs) for critical functionality. This shift represents a convergence of conventional programming with the powerful capabilities of AI, promising to revolutionize how we build software.\nHowever, integrating LLMs into traditional programming remains a complex challenge due to the fundamental differences between operating assumptions. In conventional programming, code describes operations performed on explicitly defined objects (FigureÂ [1](https://arxiv.org/html/2405.08965v3#S1.F1) left). In contrast, LLMs process natural language text as input and produce text-based outputs. To integrate LLMs into programs, developers must manually construct textual input, a process known as _prompt engineering_.\n\nFigure 1. Comparison of LLM as Meaning Typed Code construct abstraction (Right) with the present-day LLM abstraction (Middle) and Symbolic Programming Abstraction (Left) of a programming function.\n\n\\\\Description\n\nPrompt engineering, the primary means of LLM programming, can be tedious, complex, and time-consuming. Developers must craft precise instructions, manage textual context, and handle the inherent variability in LLM output. This process lacks the structure, consistency, and debugging capabilities developers are accustomed to in traditional programming environments.\n\nSeveral efforts have been made to address these challenges by developing infrastructure and tools to facilitate prompt engineering. Frameworks such as LangChain, Language Model Query Language (LMQL)Â ( [Beurer2023LMQL,](https://arxiv.org/html/2405.08965v3#bib.bib4)), DSPyÂ ( [khattab2023dspy,](https://arxiv.org/html/2405.08965v3#bib.bib16)), and SGLangÂ ( [zheng2023SGLang,](https://arxiv.org/html/2405.08965v3#bib.bib46)) have been introduced to assist developers in generating and managing prompts. However, while these frameworks provide more sophisticated tooling to help bridge the gap between conventional programming and the incorporation of AI models, in practice, they introduce additional layers of complexity for developers:\n\n1. (1)\n\n\n_Prompt design complexity:_ Developers remain responsible for the manual construction of prompts, including determining the appropriate prompt language and selecting relevant information to include.\n\n2. (2)\n\n\n_Steep learning curve:_ These systems require developers to familiarize themselves with new query languages, frameworks, or specialized syntax, complicating the integration process.\n\n3. (3)\n\n\n_Input/output conversion complexity:_ Parsing LLM output and converting them back to objects remains a challenge, especially given the variability in output across different LLMs.\n\n\nThe key insights of this work are: _(1) In practice, programs are written to be readable by various developers to comprehend the intentions and semantics conveyed by the code. This semantic richness can be leveraged to automatically translate intent embodied in the code to prompts for the LLM. (2) As state-of-the-art LLMs continue to advance, AI is increasingly feasible to infer code intentions without requiring explicit descriptions or prompt construction by developers. This trend is not only supported by our empirical findings, but we surprisingly find less can be more in prompt engineering._\n\nBased on this insight, this paper proposes _meaning-typed programming_, a novel approach to simplify the creation of neuro-integrated applications by introducing new language-level abstractions that hide the complexities of LLM integration. There are three key components to enable meaning-typed programs. These include (1) The â€™byâ€™ operator that enables the seamless integration of LLM functionality into code. It acts as a bridge between traditional and LLM operations, allowing developers to replace function bodies with a simple by model\\_name clause:\n\n\\\\Description\n\nThis declaration, along with the function signature and available semantic information, is sufficient for the LLM to produce the correct return value at runtime, significantly simplifying LLM integration in existing codebases. (2) MT-IR, a meaning-based intermediate representation at the compiler level that collects and organizes semantically rich information, preserving the meaning behind variable names, function signatures, and comments. MT-IR is also used dynamically to bind run-time values to semantic representations, enhancing the modelâ€™s understanding of program context and enabling effective, context-aware LLM integration.\n(3) MT-Runtime, an automated runtime engine within the languageâ€™s virtual machine, triggered at by call sites. It binds runtime values to MT-IR, manages LLM interactions, generates context-aware prompts, and handles responses and errors. MT-Runtime allows developers to focus on application logic while automatically managing LLM integration, ensuring seamless and contextualized operations.\n\nWe implement _meaning-typed programming (MTP)_ in a production-grade Python superset language called Jac and perform an extensive evaluation. Specifically, we make the following contributions:\n\n- â€¢\n\n\nWe introduce the concept of meaning-typed programming, a novel paradigm that treats LLMs as meaning-typed code constructs, enabling seamless integration of LLMs into conventional programs without explicit prompt engineering.\n\n- â€¢\n\n\nWe present the design and implementation of the MTP framework: the â€˜byâ€™ operator for intuitive LLM integration; MT-IR, a semantically rich intermediate representation for implied meaning; and MT-Runtime, an automated runtime engine for LLM operations\n\n- â€¢\n\n\nWe implement MTP in Jac, a production-grade Python superset language, conduct extensive evaluation and show that MTP significantly reduces development complexity, lines of code, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches.\n\n- â€¢\n\n\nWe provide an empirical analysis of LLM evolution trends, demonstrating how MTP takes advantage of advances in LLM capabilities to simplify neuro-integrated programming over time.\n\n- â€¢\n\n\nWe present multiple case studies demonstrating the application of MTP in complex problem domains, analyzing its impact on code structure, development workflow, and complexity reduction compared to traditional LLM integration methods.\n\n\nOur results show that MTP simplifies the development process and meets or exceeds the efficacy of state-of-the-art manual and tool-assisted prompt engineering techniques. This approach significantly reduces development complexity and lines of code while improving run-time performance and costs associated with LLM integration.\n\n## 2\\. Problem, Background, and Insights\n\nConventional Prompt Engineering\n\n[â¬‡](data:;base64,CmdwdCA9IE9wZW5BSSgpCgpkZWYgcmVzcG9uZF93aXRoX2NvbmZpZGVuY2VfbGV2ZWwocXVlcnk6IHN0ciwKICAgIGNoYXRfaGlzdG9yeTogZGljdFtzdHIsc3RyXSwKICAgIHVzZXJfZGF0YTpkaWN0KSAtPiB0dXBsZVtzdHIsZmxvYXRdOgoKICAgIHN5c3RlbV9tZXNzYWdlID0gKAogICAgICAgICJZb3UgYXJlIGEgY2hhdGJvdC4gUmVzcG9uZCB0byB0aGUgY2hhdCB1c2luZyB0aGUgYXZhaWxhYmxlIGluZm9ybWF0aW9uLiBBZGRpdGlvbmFsbHkgZ2VuZXJhdGUgYSBjb25maWRlbmNlIGxldmVsIG9mIHRoZSByZXNwb25zZSBhcyBhIHBlcmNlbnRhZ2UiICkKCiAgICBwcm9tcHQgPSAgc3lzdGVtX21lc3NhZ2UKICAgIHByb21wdCArPSBmJ1xuUXVlcnkgZm9ybSB0aGUgdXNlcjpcbntxdWVyeX0nCiAgICBwcm9tcHQgKz0gJ1xuQ2hhdCBIaXN0b3J5OlxuJwogICAgZm9yIHVzZXIsY29udGVudCBpbiBjaGF0X2hpc3Rvcnk6CiAgICA","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2405.08965v3","id":"https://arxiv.org/html/2405.08965v3","publishedDate":"2024-10-19T00:00:00.000Z","summary":"The paper introduces Meaning-Typed Programming (MTP), a novel approach to simplify developing neuro-integrated applications that leverage large language models (LLMs) and generative AI. Traditional methods require complex prompt engineering, which is tedious and error-prone. MTP addresses this by introducing language-level abstractions that treat LLM interactions as meaning-typed code constructs, effectively hiding the complexity of prompt management. The approach includes a new intermediate representation (MT-IR) and a runtime engine (MT-Runtime) to facilitate seamless LLM integration. Implemented as an extension of Python called Jac, MTP enhances developer productivity and achieves comparable or better accuracy than existing prompt engineering techniques, promoting more accessible and maintainable AI-powered software development.","title":"Meaning-Typed Programming: Language-level Abstractions and Runtime for GenAI Applications","url":"https://arxiv.org/html/2405.08965v3"},{"content":"\\\\UseRawInputEncoding\n\n# MTLLM: LLMs are Meaning-Typed Code Constructs\n\nJason Mars\nUniversity of MichiganUnited States[profmars@umich.edu](mailto:profmars@umich.edu),Â Yiping Kang\nUniversity of MichiganUnited States[ypkang@umich.edu](mailto:ypkang@umich.edu),Â Jayanaka L. Dantanarayana\nUniversity of MichiganUnited States[jayanaka@umich.edu](mailto:jayanaka@umich.edu),Â Chandra Irugalbandara\nJaseci LabsSri Lanka[chandra.irugalbandara@jaseci.org](mailto:chandra.irugalbandara@jaseci.org),Â Kugesan Sivasothynathan\nUniversity of Moratuwa\nJaseci LabsSri Lanka[kugesan.sivasothynathan@jaseci.org](mailto:kugesan.sivasothynathan@jaseci.org),Â Christopher Clarke\nUniversity of MichiganUnited States[csclarke@umich.edu](mailto:csclarke@umich.edu),Â Baichuan Li\nUniversity of MichiganUnited States[patrli@umich.edu](mailto:patrli@umich.edu)Â andÂ Lingjia Tang\nUniversity of MichiganUnited States[lingjia@umich.edu](mailto:lingjia@umich.edu)\n\n###### Abstract.\n\nProgramming with Generative AI (GenAI) models, which frequently involves using large language models (LLMs) to accomplish specific functionalities, has experienced significant growth in adoption.\nHowever, it remains a complex process, as developers often need to manually configure text inputs for LLMs, a practice known as prompt engineering, and subsequently translate the natural language outputs produced by LLMs back into symbolic code representations (values, types, etc.) that the code can understand. Although some infrastructures are proposed to facilitate prompt engineering, these tools are often complex and challenging for developers to adopt.\n\nInstead, this paper presents a simplified approach to integrating LLMs into programming through the introduction of an abstraction layer that hides the complexity of gluing traditional programming and LLMs together. Our approach utilizes the semantic richness in existing programs to automatically translate between the traditional programming languages and the natural language understood by LLMs, eliminating developer efforts such as prompt engineering, decreasing the overall complexity.\n\nSpecifically in this paper, we design three novel code constructs coupled with an automated runtime management system that bridges the gap between traditional symbolic code and LLMs.\nWe present a fully functional and production-grade implementation for our approach and compare it to SOTA LLM software development tools. We present real-world case studies demonstrating the efficacy of our proposed abstraction that seamlessly utilizes LLMs to solve problems in place of potentially complex traditional programming logic.\n\nRaise of Abstraction, Neurosymbolic Programming, Large Language Models\n\n## 1\\. Introduction\n\nFigure 1. Comparison of LLM as Meaning Typed Code construct abstraction (Right) with the present-day LLM abstraction (Middle) and Symbolic Programming Abstraction (Left) of a programming function.\n\n\\\\Description\n\nProgramming with Generative AI (GenAI) models, which often involves integrating AI models such as large language models (LLMs) to accomplish specific functionalities and tasks in traditional programming, has experienced tremendous adoption in recent years.\n\nHowever, integrating LLMs into traditional programming remains a complex challenge due to the fundamental differences in how conventional symbolic programming and those models operate. In conventional symbolic programming, code describes operations or functions performed on explicitly defined objects and variables with typed values (FigureÂ [1](https://arxiv.org/html/2405.08965v2#S1.F1) left). In contrast, LLMs do not directly operate on variables or objects. Instead, LLMs process natural language text as input and produce text-based outputs. As a result, to integrate LLMs into programming, developers are required to manually construct textual inputs, or prompts. This process, known as _prompt engineering_, which serve as the primary means of programming with LLMs, can be tedious, intricate and time-consuming. This is particularly true because developers need to translate traditional function inputs into the prompt to embed knowledge from existing programs. The manual nature of this task often increases complexity and decreases code maintainability. Additionally, developers must convert the natural language outputs produced by LLMs back into symbolic representations, such as values and types, that are usable within the code.\n\nCurrently, to the best of our knowlege, there is no automated solution to seamlessly bridge the gap between symbolic programming and LLMs. There have been several efforts in designing infrastructures to facilitate better prompt engineering, such as LangChain, Language Model Query Language (LMQL)Â ( [Beurer2023LMQL,](https://arxiv.org/html/2405.08965v2#bib.bib1)), DSPyÂ ( [khattab2023dspy,](https://arxiv.org/html/2405.08965v2#bib.bib3)), and SGLangÂ ( [zheng2023SGLang,](https://arxiv.org/html/2405.08965v2#bib.bib7)).\nHowever, although these systems are effective in prompt engineering, they have limitations when used in the context of integrating LLMs with symbolic programming:\n\n- â€¢\n\n\nPrompt design complexity Developers remain responsible for the manual construction of prompts. For instance, while tools such as LMQLÂ ( [Beurer2023LMQL,](https://arxiv.org/html/2405.08965v2#bib.bib1)) and LangChain can assist in generating prompts, developers must still determine the appropriate type of prompt, select the relevant information to include, and explicitly specify these decisions programmatically.\n\n- â€¢\n\n\nSteep learning curve Many of these systems present a significant learning curve, making them challenging for developers to adopt for this purpose. For example, tools like LMQL and LangChain require developers to familiarize themselves with new query languages and frameworks, which can introduce additional complexity. Similarly, SGLang and DSPy involve learning specialized syntax and workflows that may differ from conventional programming practices, complicating the integration process.\n\n- â€¢\n\n\nOutput conversion complexity For all of these systems, we still need to parse LLM outputs and convert them back to symbolic code constructs (data structures, variables, objects etc). Since each LLM may have drastically different text output, developing generalized code that work with all LLMs and future models can be non-trivial and require significant manual effort.\n\n\nOverall, despite the support offered by these infrastructures for prompt engineering, the burden of prompt design, configuration and interpretation of model output remains largely on the developer and integrating LLMs into symbolic programming remains a highly complex process.\n\nInstead of employing sophisticated yet complicated prompt engineering infrastructures to configure LLM inputs in programming, this paper investigates an alternative approach aimed at simplifying the integration of LLMs into programming workflows. We argue the need to introduce an abstraction layer that hides the complexity of gluing traditional programming and LLMs together from developers. In fact, we aim to eliminate developerâ€™s need for prompt engineering to support automatic and seamless LLM integration.\n\nOur unique insight is that the semantic richness in existing programs can be utilized to automatically translate between the traditional programming languages and the natural language understood by LLMs. In other words, a well-written program should be human readable, allowing humans to comprehend the intentions and semantics conveyed by the code. Consequently, it should also be readable (intepretable) by AI without requiring manual translation by developers. In fact, as the state-of-the-art LLMs continue to advance, it is becoming increasingly feasible for AI to infer the intentions and semantics of code, without the need for explicit descriptions or prompt construction by developers. This trend is further supported by our findings in the empirical study presented in the Evaluation section.\n\nBased on this insight, we design three novel code constructs coupled with an automated runtime management system that bridges the gap between traditional symbolic code and LLMs. Our system Meaning-typed LLM (MT-LLM) can automatically leverage LLMs for functionalities in traditional programming without the additional overhead and complexity of integration. For example, with MT-LLM, developers can use LLMs as a function by providing traditional function inputs, just as using a traditional function call, without the need to engineer the prompt or interpreting results.\n\nSpecifically, we make the following contributions in this paper:\n\n- â€¢\n\n\nWe introduce a novel abstraction of treating LLMs asÂ meaning-type wrapped code constructs for seamless integration into conventional symbolic programming. We propose a new runtime system, Automatic Meaning-type Transformation, that abstracts away many of the current complexity of programming with LLMs. MT-LLM supports three common symbolic code operations: instantiating an object of a custom type, a standalone function call and member method of a class.\n\n- â€¢\n\n\nWe present a fully functional and production-grade implementation for our approach and compare it to SOTA LLM software development tools. We conduct both user study and quantitative measurments.\n\n- â€¢\n\n\nWe studied the trend of LLMs development to demonstrate the need for complicated prompt engineering and the growing capacities for SOTA LLMs to integrate into traditional programming with without human effort.\n\n- â€¢\n\n\nWe present real-world case studies demonstrating the efficacy of our proposed abstraction that seamlessly utilizes LLMs to solve problems in place of potentially complex traditional programming logic.\n\n\n## 2\\. Problem, Background and Insights\n\nThere has been a surge in interest in adopting GenAI models in traditional programs. Instead of implementing complex functions to accomplish a task, developers can now utilize large language models (LLMs) LLMs to genera","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2405.08965v2","id":"https://arxiv.org/html/2405.08965v2","publishedDate":"2024-09-24T00:00:00.000Z","summary":"The paper \"MTLLM: LLMs are Meaning-Typed Code Constructs\" introduces a novel abstraction layer that simplifies integrating large language models (LLMs) into traditional programming. Unlike conventional code, which explicitly manipulates typed objects and variables, LLMs process natural language, making their integration complex and often reliant on prompt engineering. The authors propose a solution that leverages the semantic richness of existing programs to automatically translate between symbolic code and natural language, eliminating manual prompt design. They develop three new code constructs and an automated runtime system that bridge symbolic programming and LLMs, providing a production-grade implementation. This approach reduces complexity, enhances usability, and demonstrates effectiveness through real-world case studies, positioning LLMs as meaning-typed code constructs that seamlessly augment traditional programming paradigms.","title":"MTLLM: LLMs are Meaning-Typed Code Constructs","url":"https://arxiv.org/html/2405.08965v2"},{"content":"# Computer Science > Computation and Language\n\n**arXiv:2503.14985** (cs)\n\n\\[Submitted on 19 Mar 2025 ( [v1](https://arxiv.org/abs/2503.14985v1)), last revised 26 Mar 2025 (this version, v2)\\]\n\n# Title:ML-Triton, A Multi-Level Compilation and Language Extension to Triton GPU Programming\n\nAuthors: [Dewei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+D), [Wei Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu,+W), [Liyang Ling](https://arxiv.org/search/cs?searchtype=author&query=Ling,+L), [Ettore Tiotto](https://arxiv.org/search/cs?searchtype=author&query=Tiotto,+E), [Quintin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+Q), [Whitney Tsang](https://arxiv.org/search/cs?searchtype=author&query=Tsang,+W), [Julian Opperman](https://arxiv.org/search/cs?searchtype=author&query=Opperman,+J), [Jacky Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng,+J)\n\nView a PDF of the paper titled ML-Triton, A Multi-Level Compilation and Language Extension to Triton GPU Programming, by Dewei Wang and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.14985)\n\n> Abstract:In the era of LLMs, dense operations such as GEMM and MHA are critical components. These operations are well-suited for parallel execution using a tilebased approach. While traditional GPU programming often relies on low level interfaces like CUDA or SYCL, Triton has emerged as a DSL that offers a more user-friendly and portable alternative by programming at a higher level. The current Triton starts at the workgroup (aka threadblock) level, and directly lowers to per-thread level. And then attempt to coalesce and amend through a series of passes, promoting information from low-level representation. We believe this is pre-mature lowering based on the below observations. 1. GPU has a hierarchical structure both physically and logically. Modern GPUs often feature SIMD units capable of directly operating on tiles on a warp or warpgroup basis, such as blocked load and blocked MMA. 2. Multi-level gradual lowering can make compiler decoupled and clean by separating considerations inter and intra a logical layer. 3. Kernel developers often need fine control to get good performance on the latest hardware. FlashAttention2 advocates explicit data partition between warps to make a performance boost. In this context, we propose ML-Triton which features multi-level compilation flow and programming interface. Our approach begins at the workgroup level and progressively lowers to the warp and intrinsic level, implementing a multilevel lowering align with the hierarchical nature of GPU. Additionally, we extend triton language to support user-set compiler hint and warp level programming, enabling researchers to get good out-of-the box performance without awaiting compiler updates. Experimental results demonstrate that our approach achieves performance above 95% of expert-written kernels on Intel GPU, as measured by the geometric mean.\n\n| | |\n| --- | --- |\n| Subjects: | Computation and Language (cs.CL) |\n| Cite as: | [arXiv:2503.14985](https://arxiv.org/abs/2503.14985) \\[cs.CL\\] |\n| (or [arXiv:2503.14985v2](https://arxiv.org/abs/2503.14985v2) \\[cs.CL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2503.14985](https://doi.org/10.48550/arXiv.2503.14985) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Dewei Wang \\[ [view email](https://arxiv.org/show-email/c5ca3da8/2503.14985)\\] **[\\[v1\\]](https://arxiv.org/abs/2503.14985v1)**\nWed, 19 Mar 2025 08:31:39 UTC (2,148 KB)\n**\\[v2\\]**\nWed, 26 Mar 2025 04:06:37 UTC (2,148 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled ML-Triton, A Multi-Level Compilation and Language Extension to Triton GPU Programming, by Dewei Wang and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.14985)\n- [TeX Source](https://arxiv.org/src/2503.14985)\n- [Other Formats](https://arxiv.org/format/2503.14985)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[<Â prev](https://arxiv.org/prevnext?id=2503.14985&function=prev&context=cs.CL)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2503.14985&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2025-03](https://arxiv.org/list/cs.CL/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.14985?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.14985)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.14985)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.14985)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.14985) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.14985","id":"https://arxiv.org/abs/2503.14985","publishedDate":"2025-03-19T00:00:00.000Z","summary":"The paper introduces ML-Triton, an extension to the Triton GPU programming language designed to enhance performance and flexibility for dense operations like GEMM and MHA, critical in large language models. Unlike traditional approaches that lower code directly from high-level to low-level representations, ML-Triton employs a multi-level compilation strategy that progressively lowers from the workgroup (threadblock) level down to warp and intrinsic levels, aligning with the hierarchical structure of modern GPUs. This approach enables better exploitation of GPU hardware features, such as warp-level operations and SIMD units, and provides developers with explicit control through user-set compiler hints and warp-level programming. Experimental results show that ML-Triton achieves over 95% of the performance of expert-crafted kernels on Intel GPUs, demonstrating its effectiveness in balancing ease of programming with high performance.","title":"ML-Triton, A Multi-Level Compilation and Language Extension to Triton GPU Programming","url":"https://arxiv.org/abs/2503.14985"},{"content":"# Computer Science > Computation and Language\n\n**arXiv:2503.12123** (cs)\n\n\\[Submitted on 15 Mar 2025\\]\n\n# Title:MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling\n\nAuthors: [Zhaopeng Feng](https://arxiv.org/search/cs?searchtype=author&query=Feng,+Z), [Jiahan Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren,+J), [Jiayuan Su](https://arxiv.org/search/cs?searchtype=author&query=Su,+J), [Jiamei Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng,+J), [Zhihang Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+Z), [Hongwei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+H), [Zuozhu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+Z)\n\nView a PDF of the paper titled MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling, by Zhaopeng Feng and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.12123) [HTML (experimental)](https://arxiv.org/html/2503.12123v1)\n\n> Abstract:Process reward models (PRMs) have shown success in complex reasoning tasks for large language models (LLMs). However, their application to machine translation (MT) remains underexplored due to the lack of systematic methodologies and evaluation benchmarks. To address this gap, we introduce \\\\textbf{MT-RewardTree}, a comprehensive framework for constructing, evaluating, and deploying process reward models in MT. Unlike traditional vanilla preference pair construction, we propose a novel method for automatically generating token-level preference pairs using approximate Monte Carlo Tree Search (MCTS), which mitigates the prohibitive cost of human annotation for fine-grained steps. Then, we establish the first MT-specific reward model benchmark and provide a systematic comparison of different reward modeling architectures, revealing that token-level supervision effectively captures fine-grained preferences. Experimental results demonstrate that our MT-PRM-Qwen-2.5-3B achieves state-of-the-art performance in both token-level and sequence-level evaluation given the same input prefix. Furthermore, we showcase practical applications where PRMs enable test-time alignment for LLMs without additional alignment training and significantly improve performance in hypothesis ensembling. Our work provides valuable insights into the role of reward models in MT research. Our code and data are released in \\\\href{ [this https URL](https://sabijun.github.io/MT_RewardTreePage/)}{ [this https URL](https://sabijun.github.io/MT)\\\\\\_RewardTreePage}.\n\n| | |\n| --- | --- |\n| Comments: | Under review. Project page: [this https URL](https://sabijun.github.io/MT_RewardTreePage) |\n| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2503.12123](https://arxiv.org/abs/2503.12123) \\[cs.CL\\] |\n| | (or [arXiv:2503.12123v1](https://arxiv.org/abs/2503.12123v1) \\[cs.CL\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2503.12123](https://doi.org/10.48550/arXiv.2503.12123) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Zhaopeng Feng \\[ [view email](https://arxiv.org/show-email/6e3d8ef0/2503.12123)\\]\n\n**\\[v1\\]**\nSat, 15 Mar 2025 13:04:51 UTC (902 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling, by Zhaopeng Feng and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.12123)\n- [HTML (experimental)](https://arxiv.org/html/2503.12123v1)\n- [TeX Source](https://arxiv.org/src/2503.12123)\n- [Other Formats](https://arxiv.org/format/2503.12123)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[<Â prev](https://arxiv.org/prevnext?id=2503.12123&function=prev&context=cs.CL)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2503.12123&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2025-03](https://arxiv.org/list/cs.CL/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.12123?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2503.12123?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.12123)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.12123)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.12123)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2503.12123&description=MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2503.12123&title=MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.12123) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.12123","id":"https://arxiv.org/abs/2503.12123","publishedDate":"2025-03-15T00:00:00.000Z","summary":"The paper introduces **MT-RewardTree**, a comprehensive framework for improving machine translation (MT) with reward modeling techniques adapted from large language models (LLMs). It addresses the challenge of applying reward models to MT by proposing a novel method that automatically generates fine-grained token-level preference data using approximate Monte Carlo Tree Search (MCTS), reducing reliance on costly human annotations. The framework includes establishing a new MT-specific reward model benchmark and systematically compares various reward modeling architectures, highlighting that token-level supervision effectively captures detailed translation preferences. Experimental results demonstrate that their proposed MT-PRM-Qwen-2.5-3B model achieves state-of-the-art performance at both token and sequence levels. The framework also enables practical applications such as test-time alignment of LLMs without additional training and improves hypothesis ensembling in MT tasks. The authors provide their code and data publicly, aiming to advance the integration of reward modeling in machine translation research.","title":"MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling","url":"https://arxiv.org/abs/2503.12123"},{"content":"# Computer Science > Programming Languages\n\n**arXiv:2503.07328** (cs)\n\n\\[Submitted on 10 Mar 2025\\]\n\n# Title:Complete the Cycle: Reachability Types with Expressive Cyclic References\n\nAuthors: [Haotian Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng,+H), [Siyuan He](https://arxiv.org/search/cs?searchtype=author&query=He,+S), [Songlin Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia,+S), [Yuyan Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao,+Y), [Tiark Rompf](https://arxiv.org/search/cs?searchtype=author&query=Rompf,+T)\n\nView a PDF of the paper titled Complete the Cycle: Reachability Types with Expressive Cyclic References, by Haotian Deng and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.07328)\n\n> Abstract:Reachability Types (RT) are a qualified type system for tracking aliasing and separation in functional and higher-order programming. By formalizing resource reachability with a sound static type system, RT enable higher-order programming patterns with runtime safety and non-interference guarantees. However, previous RT systems have been based on calculi that restrict cyclic dependencies and are shown to be terminating in the absence of built-in recursive constructs. While termination is sometimes a desirable property, simplifying reasoning and ensuring predictable behavior, it implies an inability to encode expressive programs involving non-termination and advanced recursive patterns, such as mutual recursion and various fixed-point combinators.\n>\n> In this paper, we address this limitation by extending RT with an expressive cyclic reference type that permits the formation of cyclic dependencies through the store, thereby allowing the system to encode recursive programming patterns without relying on extra built-in constructs. In addition, we redesign qualifier typing in the reference introduction rule, allowing separate references to point to a shared and tracked referent. We formalize the system as the $\\\\lambda^{\\\\circ}\\_{<:}$-calculus, with a mechanized soundness proof via the standard progress and preservation lemmas. As a demonstration, we implement a well-typed fixpoint operator, proving that recursive patterns can be encoded using the novel cyclic reference type.\n\n| | |\n| --- | --- |\n| Subjects: | Programming Languages (cs.PL); Logic in Computer Science (cs.LO); Software Engineering (cs.SE) |\n| Cite as: | [arXiv:2503.07328](https://arxiv.org/abs/2503.07328) \\[cs.PL\\] |\n| | (or [arXiv:2503.07328v1](https://arxiv.org/abs/2503.07328v1) \\[cs.PL\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2503.07328](https://doi.org/10.48550/arXiv.2503.07328) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Haotian Deng \\[ [view email](https://arxiv.org/show-email/5dc4d868/2503.07328)\\]\n\n**\\[v1\\]**\nMon, 10 Mar 2025 13:42:02 UTC (101 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Complete the Cycle: Reachability Types with Expressive Cyclic References, by Haotian Deng and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.07328)\n- [TeX Source](https://arxiv.org/src/2503.07328)\n- [Other Formats](https://arxiv.org/format/2503.07328)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.PL\n\n[<Â prev](https://arxiv.org/prevnext?id=2503.07328&function=prev&context=cs.PL)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2503.07328&function=next&context=cs.PL)\n\n[new](https://arxiv.org/list/cs.PL/new) \\| [recent](https://arxiv.org/list/cs.PL/recent) \\| [2025-03](https://arxiv.org/list/cs.PL/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.07328?context=cs)\n\n[cs.LO](https://arxiv.org/abs/2503.07328?context=cs.LO)\n\n[cs.SE](https://arxiv.org/abs/2503.07328?context=cs.SE)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.07328)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.07328)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.07328)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2503.07328&description=Complete the Cycle: Reachability Types with Expressive Cyclic References) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2503.07328&title=Complete the Cycle: Reachability Types with Expressive Cyclic References)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.07328) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2503.07328","id":"https://arxiv.org/abs/2503.07328","publishedDate":"2025-03-10T00:00:00.000Z","summary":"The paper \"Complete the Cycle: Reachability Types with Expressive Cyclic References\" introduces an extension to Reachability Types (RT), a static type system designed to track aliasing and resource reachability in higher-order programming. Traditional RT systems restrict cyclic dependencies and guarantee termination, limiting their expressiveness for recursive and non-terminating patterns. This work addresses that limitation by incorporating an expressive cyclic reference type, enabling cyclic dependencies through the store without relying on built-in recursion constructs. The authors redesign qualifier typing to allow references to share and track referents separately. Formalized as the Î»Â°_{<:}-calculus, the system's soundness is proven via progress and preservation lemmas. They demonstrate the system's capabilities by implementing a well-typed fixpoint operator, showing that complex recursive patterns can be encoded safely within this framework. This advancement broadens the expressiveness of reachability analysis while maintaining safety guarantees, facilitating more expressive recursive programming in typed languages.","title":"Complete the Cycle: Reachability Types with Expressive Cyclic References","url":"https://arxiv.org/abs/2503.07328"},{"content":"# Computer Science > Programming Languages\n\n**arXiv:2405.08965** (cs)\n\n\\[Submitted on 14 May 2024 ( [v1](https://arxiv.org/abs/2405.08965v1)), last revised 3 Sep 2025 (this version, v5)\\]\n\n# Title:MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\n\nAuthors: [Jayanaka L. Dantanarayana](https://arxiv.org/search/cs?searchtype=author&query=Dantanarayana,+J+L), [Yiping Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang,+Y), [Kugesan Sivasothynathan](https://arxiv.org/search/cs?searchtype=author&query=Sivasothynathan,+K), [Christopher Clarke](https://arxiv.org/search/cs?searchtype=author&query=Clarke,+C), [Baichuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+B), [Savini Kashmira](https://arxiv.org/search/cs?searchtype=author&query=Kashmira,+S), [Krisztian Flautner](https://arxiv.org/search/cs?searchtype=author&query=Flautner,+K), [Lingjia Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+L), [Jason Mars](https://arxiv.org/search/cs?searchtype=author&query=Mars,+J)\n\nView a PDF of the paper titled MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming, by Jayanaka L. Dantanarayana and 8 other authors\n\n[View PDF](https://arxiv.org/pdf/2405.08965) [HTML (experimental)](https://arxiv.org/html/2405.08965v5)\n\n> Abstract:Software development is shifting from traditional programming to AI-integrated applications that leverage generative AI and large language models (LLMs) during runtime. However, integrating LLMs remains complex, requiring developers to manually craft prompts and process outputs. Existing tools attempt to assist with prompt engineering, but often introduce additional complexity. This paper presents Meaning-Typed Programming (MTP), a novel paradigm that abstracts LLM integration through intuitive language-level constructs. By leveraging the inherent semantic richness of code, MTP automates prompt generation and response handling without additional developer effort. We introduce the (1) by operator for seamless LLM invocation, (2) MT-IR, a meaning-based intermediate representation for semantic extraction, and (3) MT-Runtime, an automated system for managing LLM interactions. We implement MTP in Jac, a programming language that supersets Python, and find that MTP significantly reduces coding complexity while maintaining accuracy and efficiency. MTP significantly reduces development complexity, lines of code modifications needed, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches. Our user study shows that developers using MTP completed tasks 3.2x faster with 45% fewer lines of code compared to existing frameworks. Moreover, MTP demonstrates resilience even when up to 50% of naming conventions are degraded, demonstrating robustness to suboptimal code. MTP is developed as part of the Jaseci open-source project, and is available under the module byLLM.\n\n| | |\n| --- | --- |\n| Comments: | OOPSLA 2025 |\n| Subjects: | Programming Languages (cs.PL); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2405.08965](https://arxiv.org/abs/2405.08965) \\[cs.PL\\] |\n| (or [arXiv:2405.08965v5](https://arxiv.org/abs/2405.08965v5) \\[cs.PL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2405.08965](https://doi.org/10.48550/arXiv.2405.08965) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jayanaka L. Dantanarayana \\[ [view email](https://arxiv.org/show-email/13cb6fa0/2405.08965)\\] **[\\[v1\\]](https://arxiv.org/abs/2405.08965v1)**\nTue, 14 May 2024 21:12:01 UTC (623 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2405.08965v2)**\nMon, 14 Oct 2024 21:20:40 UTC (4,649 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2405.08965v3)**\nThu, 16 Jan 2025 18:56:27 UTC (2,918 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2405.08965v4)**\nWed, 4 Jun 2025 00:51:13 UTC (3,247 KB)\n**\\[v5\\]**\nWed, 3 Sep 2025 21:19:19 UTC (3,661 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming, by Jayanaka L. Dantanarayana and 8 other authors\n\n- [View PDF](https://arxiv.org/pdf/2405.08965)\n- [HTML (experimental)](https://arxiv.org/html/2405.08965v5)\n- [TeX Source](https://arxiv.org/src/2405.08965)\n- [Other Formats](https://arxiv.org/format/2405.08965)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.PL\n\n[<Â prev](https://arxiv.org/prevnext?id=2405.08965&function=prev&context=cs.PL)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2405.08965&function=next&context=cs.PL)\n\n[new](https://arxiv.org/list/cs.PL/new) \\| [recent](https://arxiv.org/list/cs.PL/recent) \\| [2024-05](https://arxiv.org/list/cs.PL/2024-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2405.08965?context=cs) [cs.AI](https://arxiv.org/abs/2405.08965?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2405.08965)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2405.08965)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2405.08965)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2405.08965) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2405.08965","id":"https://arxiv.org/abs/2405.08965","publishedDate":null,"summary":"The paper introduces Meaning-Typed Programming (MTP), a novel language abstraction designed to simplify the integration of large language models (LLMs) and generative AI into software development. As AI-driven applications become more prevalent, developers face challenges in managing LLM prompts and responses. MTP addresses this by providing intuitive language-level constructs that automate prompt generation and response handling, reducing coding complexity and improving efficiency.\n\nKey features of MTP include:\n- The **\"by\" operator** for seamless LLM invocation.\n- **MT-IR**, a meaning-based intermediate representation that captures semantic information.\n- **MT-Runtime**, an automated system managing LLM interactions at runtime.\n\nImplemented in Jac, an extension of Python, MTP significantly reduces lines of code, development effort, and costs while maintaining or exceeding current accuracy and performance standards. User studies demonstrate that developers using MTP complete tasks over three times faster with fewer lines of code and greater robustness, even when naming conventions are degraded. MTP is open-source as part of the Jaseci project and aims to streamline AI-integrated programming by abstracting complex prompt engineering tasks.","title":"MTP: A Meaning-Typed Language Abstraction for AI ...","url":"https://arxiv.org/abs/2405.08965"},{"content":"[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\n[We gratefully acknowledge support from\\\n\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=http%3A%2F%2Farxiv.org%2Flist%2Fcs%2F2024-05%3Fskip%3D2980%26show%3D2000","id":"http://arxiv.org/list/cs/2024-05?skip=2980&show=2000","publishedDate":null,"summary":"The webpage lists recent computer science research papers from May 2024 on arXiv, specifically from the Cornell University submission portal. It acknowledges support from the Simons Foundation and associated institutions. The page provides access to a collection of scholarly articles in various CS subfields, offering researchers and readers updates on the latest academic contributions in computer science.","title":"Computer Science May 2024","url":"http://arxiv.org/list/cs/2024-05?skip=2980&show=2000"},{"content":"License: arXiv.org perpetual non-exclusive license\n\narXiv:2403.13430v1 \\[cs.CV\\] 20 Mar 2024\n\n# MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining\n\nDi Wang,\nJing Zhang,\nMinqiang Xu,\nLin Liu,\nDongsheng Wang,\n\nErzhong Gao,\nChengxi Han,\nHaonan Guo,\nBo Du,\nDacheng Tao,\nand Liangpei Zhang\nD. Wang and B. Du are with the School of Computer Science, Wuhan University, Wuhan 430072, China, also with the Institute of Artificial Intelligence, Wuhan University, Wuhan 430072, China, also with the National Engineering Research Center for Multimedia Software, Wuhan University, Wuhan 430072, China, and also with the Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan 430072, China (e-mail: wd74108520@gmail.com; dubo@whu.edu.cn). (Corresponding author: Minqiang Xu, Jing Zhang, Bo Du and Liangpei Zhang.)J. Zhang is with the School of Computer Science, Faculty of Engineering, The University of Sydney, Australia (e-mail: jing.zhang1@sydney.edu.au).M. Xu, L. Liu, D. Wang and E. Gao are with the iFlytek Co., Ltd and also with the National Engineering Research Center of Speech and Language Information Processing, Hefei 230088, China (e-mail: mqxu7@iflytek.com; linliu@iflytek.com; dswang7@iflytek.com; ezgao@iflytek.com).C. Han, H. Guo and L. Zhang are with the State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan 430079, China (e-mail: chengxihan@whu.edu.cn; haonan.guo@whu.edu.cn; zlp62@whu.edu.cn).D. Tao is with the School of Computer Science and Engineering, Nanyang Technological University, Singapore (e-mail: dacheng.tao@gmail.com).\n\n###### Abstract\n\nFoundation models have reshaped the landscape of Remote Sensing (RS) by enhancing various image interpretation tasks. Pretraining is an active research topic, encompassing supervised and self-supervised learning methods to initialize model weights effectively. However, transferring the pretrained models to downstream tasks may encounter task discrepancy due to their formulation of pretraining as image classification or object discrimination tasks. In this study, we explore the Multi-Task Pretraining (MTP) paradigm for RS foundation models to address this issue. Using a shared encoder and task-specific decoder architecture, we conduct multi-task supervised pretraining on the SAMRS dataset, encompassing semantic segmentation, instance segmentation, and rotated object detection. MTP supports both convolutional neural networks and vision transformer foundation models with over 300 million parameters. The pretrained models are finetuned on various RS downstream tasks, such as scene classification, horizontal and rotated object detection, semantic segmentation, and change detection. Extensive experiments across 14 datasets demonstrate the superiority of our models over existing ones of similar size and their competitive performance compared to larger state-of-the-art models, thus validating the effectiveness of MTP. The codes and pretrained models will be released at https://github.com/ViTAE-Transformer/MTP.\n\n###### Index Terms:\n\nRemote sensing, Foundation model, Multi-task pretraining, Scene classification, Semantic segmentation, Object detection, Change detection.\n\n## I Introduction\n\nRemote sensing (RS) image is one of the most important data resources for recording ground surfaces and land objects. Precisely understanding RS images is beneficial to many applications, including urban planning \\[ [1](https://arxiv.org/html/2403.13430v1#bib.bib1)\\], environmental survey \\[ [2](https://arxiv.org/html/2403.13430v1#bib.bib2)\\], disaster assessment \\[ [3](https://arxiv.org/html/2403.13430v1#bib.bib3)\\], etc.\n\nUtilizing its inherent capability to automatically learn and extract deep features from objects, deep learning methods have found widespread application in the RS domain, particularly for tasks such as scene classification, land use and land cover classification, and ship detection. Typically, ImageNet pretrained weights are employed in training deep networks for RS tasks due to their extensive representational ability. However, these weights are derived from pretraining models on natural images, leading to domain gaps between natural images and RS images. For instance, RS images are captured from a birdâ€™s-eye view, lack the vibrant colors of natural images, and possess lower spatial resolution. These disparities may impede the modelâ€™s finetuning performance \\[ [4](https://arxiv.org/html/2403.13430v1#bib.bib4), [5](https://arxiv.org/html/2403.13430v1#bib.bib5)\\]. Moreover, relying solely on limited task-specific data for training restricts the model size and generalization capability of current RS deep models due to the notorious overfitting issue.\n\nTo tackle these challenges, the development of RS vision foundation models is imperative, which should excel in extracting representative RS features. However, the RS domain has long grappled with a scarcity of adequately large annotated datasets, impeding related investigations. Until recently, the most expansive RS scene labeling datasets were fMoW \\[ [6](https://arxiv.org/html/2403.13430v1#bib.bib6)\\] and BigEarthNet \\[ [7](https://arxiv.org/html/2403.13430v1#bib.bib7)\\], boasting 132,716 and 590,326 unique scene instances \\[ [8](https://arxiv.org/html/2403.13430v1#bib.bib8)\\], respectively â€” yet still falling short of benchmarks set by natural image datasets like ImageNet-1K \\[ [9](https://arxiv.org/html/2403.13430v1#bib.bib9)\\]. Long et al. \\[ [8](https://arxiv.org/html/2403.13430v1#bib.bib8)\\] addressed this gap by introducing MillionAID, a large-scale RS scene labeling dataset with a closed sample capacity of 100,0848 compared to ImageNet-1K, igniting interest in supervised RS pretraining \\[ [10](https://arxiv.org/html/2403.13430v1#bib.bib10), [5](https://arxiv.org/html/2403.13430v1#bib.bib5)\\]. These studies show the feasibility of pretraining RS foundation models on large-scale RS datasets. Nonetheless, supervised pretraining of RS foundation models may not be the most preferable choice due to the expertise and substantial time and labor costs associated with labeling RS images.\n\n![Refer to caption](https://arxiv.org/html/2403.13430v1/x1.png)\n\nFigure 1: Comparison of various pretraining methods. (a) \\[ [11](https://arxiv.org/html/2403.13430v1#bib.bib11), [12](https://arxiv.org/html/2403.13430v1#bib.bib12)\\] sequentially pretrains a foundational model on both natural and RS images. (b) \\[ [13](https://arxiv.org/html/2403.13430v1#bib.bib13)\\] employs a two-stage pretraining strategy to initialize task-specific decoders (e.g., segmentation) using existing foundational models pretrained on either natural or RS images, preserving the decoder during subsequent finetuning. We extend (b) by incorporating multi-task decoders to enhance the representation capacity of the foundational model, facilitating easy transferability across diverse tasks during finetuning, as depicted in (c).\n\nConstructing large-scale RS annotation datasets is challenging due to the high complexity and cost of labeling. Despite this challenge, the advancement of earth observation technologies grants easy access to a vast amount of unlabeled RS images. Efficiently leveraging these unlabeled RS images is crucial for developing robust RS foundation models. In the realm of deep learning, unsupervised pretraining has emerged as a promising approach for learning effective knowledge from massive unlabeled data \\[ [14](https://arxiv.org/html/2403.13430v1#bib.bib14), [15](https://arxiv.org/html/2403.13430v1#bib.bib15), [16](https://arxiv.org/html/2403.13430v1#bib.bib16), [17](https://arxiv.org/html/2403.13430v1#bib.bib17)\\]. Typically, unsupervised pretraining employs self-supervised learning (SSL) to learn effective feature representation. SSL encompasses two primary techniques: contrastive-based \\[ [18](https://arxiv.org/html/2403.13430v1#bib.bib18), [19](https://arxiv.org/html/2403.13430v1#bib.bib19), [20](https://arxiv.org/html/2403.13430v1#bib.bib20)\\] and generative-based learning \\[ [21](https://arxiv.org/html/2403.13430v1#bib.bib21), [22](https://arxiv.org/html/2403.13430v1#bib.bib22), [23](https://arxiv.org/html/2403.13430v1#bib.bib23)\\]. Contrastive learning aims to bring similar samples closer while maximizing distances between dissimilar samples through the object discrimination pretext task. When applied to the RS domain, data characteristics like geographic coordinates \\[ [24](https://arxiv.org/html/2403.13430v1#bib.bib24), [25](https://arxiv.org/html/2403.13430v1#bib.bib25), [26](https://arxiv.org/html/2403.13430v1#bib.bib26)\\] and temporal information \\[ [27](https://arxiv.org/html/2403.13430v1#bib.bib27), [28](https://arxiv.org/html/2403.13430v1#bib.bib28), [29](https://arxiv.org/html/2403.13430v1#bib.bib29)\\] are usually leveraged in formulating the pretext task. However, designing these pretext tasks and gathering requisite data can be inefficient, especially for training large-scale models. Generative-based learning, exemplified by masked image modeling (MIM), circumvents this challenge by enhancing network representation through reconstructing masked regions. Many RS studies leverage MIM initialization for its efficiency \\[ [30](https://arxiv.org/html/2403.13430v1#bib.bib30), [31](https://arxiv.org/html/2403.13430v1#bib.bib31), [32](https://arxiv.org/html/2403.13430v1#bib.bib32), [33](https://arxiv.org/html/2403.13430v1#bib.bib33), [34](https://arxiv.org/html/2403.13430v1#bib.bib34), [35](https://arxiv.org/html/2403.13430v1#bib.bib35), [36](https://arxiv.org/html/2403.13430v1#bib.bib36), [37](https://arxiv.org/html/2403.13430v1#bib.bib37)\\]. Recent approaches have attempted to combine contrastive-based and generative-based learning techniques to pretrain more powerful models \\[ [38](https://arxiv.org/html/2403.13430v1#bib.bib38), [39](https://arxiv.org/html/2403.13430v1#bib.bib39), [40](https://arxiv.org/html/2403.13430v1#bib.bib40)\\","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2403.13430v1","id":"https://arxiv.org/html/2403.13430v1","publishedDate":"2024-03-20T00:00:00.000Z","summary":"The paper introduces MTP (Multi-Task Pretraining), a novel approach to developing foundation models for remote sensing (RS). Unlike traditional pretraining methods focused on single tasks like image classification, MTP employs a shared encoder with task-specific decoders to jointly pretrain on multiple RS tasksâ€”including semantic segmentation, instance segmentation, and rotated object detectionâ€”using the SAMRS dataset. This multi-task pretraining aims to bridge the gap between pretraining objectives and downstream applications, enhancing model transferability. The approach supports both CNNs and vision transformers with over 300 million parameters. Extensive experiments on 14 datasets demonstrate that MTP models outperform existing models of similar size and are competitive with larger state-of-the-art models across various RS tasks such as scene classification, object detection, semantic segmentation, and change detection. The authors plan to release their code and pretrained models to advance RS foundation modeling.","title":"MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining","url":"https://arxiv.org/html/2403.13430v1"}],"status":"completed","toolCallId":"call_MC6GbxtTw6CtUVSmgT5umT6o","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Hereâ€™s a detailed, developer-focused explanation of MT-Runtime in MTP and how it operates end to end.\n\nWhat MT-Runtime is\n- Itâ€™s the execution engine that activates at by call sites. It binds real argument values to the programâ€™s semantic model (MT-IR), assembles the model request, invokes the selected LLM/tooling, enforces the declared type contract on outputs, and handles errors, retries, caching, and governance.\n- Conceptually, itâ€™s the â€œtranslator and traffic copâ€ between your typed, symbolic code and the nondeterministic LLM world.\n\nKey responsibilities\n1) IR-slice and context construction\n- Selects the minimal relevant MT-IR slice for a call: function node (name, doc), parameter and return schemas, related types/enums, and optional examples/tests.\n- Binds runtime values (actual args, defaults, possibly environment like locale) to that slice.\n\n2) Prompt/request assembly\n- Builds a structured model request guided by the functionâ€™s meaning and contract:\n  - Task framing: distilled from function name and docstring.\n  - Inputs: labeled arguments with types and current values.\n  - Output contract: a schema derived from the declared return type (e.g., JSON Schema-like).\n  - Optional: few-shot snippets (e.g., tests), policy/system hints, tool specs.\n- Chooses output mode that favors structure (JSON/object/function-call) to minimize parsing brittleness.\n\n3) Invocation and decoding\n- Calls the configured model/endpoint with chosen decoding parameters (temperature, max tokens, etc.).\n- Validates the response against the return-type schema:\n  - Type checks (primitives, enums, unions), field presence, constraints (ranges, regex), and nested shapes.\n  - Coercions where safe (e.g., \"3\" -> 3) and unit conversions if configured.\n\n4) Error handling and self-repair\n- If validation fails, performs bounded, targeted retries:\n  - Repair prompts that reference the exact schema violations.\n  - Strategy escalation: switch to stricter object mode; reduce temperature; add clarifying constraints.\n- If still failing, surfaces a typed exception aligned to the functionâ€™s contract.\n\n5) Tool and API mediation (optional)\n- If enabled, exposes registered tools with typed signatures (also in MT-IR) so the model can request tool calls.\n- Executes tool calls deterministically under policy, returns results to the model, and keeps a trace.\n\n6) Observability and governance\n- Logs IR slice, request, response, validation results, and retry trail as a structured trace.\n- Supports redaction policies, safety filters, and audit hooks.\n- Provides memoization/caching keyed on normalized inputs to cut latency/cost.\n\n7) Policy and routing\n- Centralized knobs: model routing/fallbacks, budgets, rate limits, batching, parallelism.\n- Can swap models or change decoding policies without touching call sites.\n- Supports feature flags and A/B to compare models or policies.\n\nHow MT-Runtime uses MT-IR\n- MT-IR supplies the semantics: names, docs, parameter labels, and the canonical return-type schema.\n- At call time, MT-Runtime pulls a minimal IR slice and binds live values.\n- The IR schema is the source of truth for output shaping and validation; no hand-written prompt templates are needed.\n\nLifecycle of a by function call\n1) Intercept call â†’ fetch MT-IR slice for the function.\n2) Bind args â†’ render â€œmeaning-firstâ€ request (framing + typed inputs + output schema).\n3) Invoke model â†’ request structured output mode.\n4) Validate â†’ coerce/repair â†’ possibly retry with targeted guidance.\n5) Return typed value â†’ cache, log trace, enforce policies.\n\nContracts and guarantees\n- Signature fidelity: Inputs are explicit and labeled; outputs must conform to the declared return type or raise.\n- Determinism envelope: Nondeterminism is bounded via schema constraints, retries, and decoding controls.\n- Replaceability: You can later replace the by body with a manual implementation; call sites remain unchanged.\n- Resilience to naming noise: Even if identifiers degrade, types and remaining semantics keep the system functional.\n\nConfiguration knobs youâ€™ll actually use\n- Model routing: default model, per-function overrides, fallbacks.\n- Decoding policy: temperature, max tokens, top-p; strict object/JSON modes.\n- Validation and retries: retry count, repair strategy, coercion rules.\n- Tooling: which tools are exposed, rate limits, side-effect policies.\n- Context limits: IR slice size, neighbor relevance thresholds.\n- Cost/latency: caching, batching, concurrency, circuit breakers.\n\nFailure modes and mitigations\n- Ambiguous intent â†’ clarify via repair prompts or require richer types/docs.\n- Hallucinated fields â†’ schema-first decoding rejects extras unless allowed; repair narrows output.\n- Partial outputs â†’ field-level repair prompts; union-alt disambiguation.\n- Model drift/outages â†’ routing and fallback policies, cached results.\n\nWhat you get as a developer\n- Write normal function signatures and types; add a by clause; call it like any function.\n- No prompt templates, no manual JSON parsing, no bespoke retries.\n- Uniform tracing and debuggability: inspect the IR-derived request and how the runtime repaired outputs.\n\nExample at a glance (illustrative)\n- You declare:\n  - function extract_items(text: str) -> list[Item] by gpt-4o\n  - Item has typed fields with descriptions.\n- MT-Runtime constructs a minimal, typed request, asks for object output, validates to list[Item], repairs if needed, and returns a proper Python/Jac object list. You get logs for the whole interaction.\n\nBottom line\n- MT-Runtime operationalizes meaning-typed programming: it turns your codeâ€™s semantics into reliable, typed LLM calls with automated prompting, structured decoding, validation, repair, and governanceâ€”so AI integration behaves like standard, testable, maintainable code.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0e9c6a795adc8ef20068c86cd04bd88192b05062a4e5a04836"}},"resumableStreamId":"STREAM:cb87a32a-114a-4e86-8c34-e5bdea0c2e82","role":"assistant","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","timeToFirstToken":1.016,"tokens":1227,"tokensPerSecond":41.031300160513645,"updated_at":1757965510595,"userId":"google:104458561188320708185","id":"j971681vnhq1j24vmjxyys931n7qnm0h"},{"_creationTime":1757965558680.1365,"_id":"j979mv5ka7cyz0418jvg0kd4z97qmrkz","attachmentIds":[],"content":"what does MTP excel against it peers?","created_at":1757965558682,"messageId":"751d7f3f-bc45-484a-b70c-508ae1116f1f","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","updated_at":1757965558682,"userId":"google:104458561188320708185","id":"j979mv5ka7cyz0418jvg0kd4z97qmrkz","providerMetadata":{}},{"_creationTime":1757965558680.1367,"_id":"j978m3j2wdae5kstnz07rm9nxh7qm0nj","attachmentIds":[],"byok":false,"content":"Hereâ€™s where MTP (Meaning-Typed Programming) stands out compared to common AI-integration approaches and frameworks (e.g., LangChain, LMQL, DSPy, SGLang, function-calling SDKs, schema/guardrail libraries):\n\n1) Prompting is a language/runtime responsibility, not yours\n- MTP advantage: You donâ€™t write or maintain prompt templates. The by operator plus MT-IR automatically assembles prompts from code semantics (names, types, docs, examples).\n- Why it matters: Fewer brittle templates, less â€œprompt drift,â€ easier refactors. Rename a field or tighten a type and the prompt reshapes automatically.\n\n2) Strong, first-class typing from the source of truth\n- MTP advantage: Return types and parameter types are the contract that drives structured decoding and validation. The runtime enforces schemas, coerces safely, and performs targeted repairs.\n- Why it matters: Predictable outputs without hand-rolled JSON parsing or regex hacks. Better testability and composability than free-form LLM outputs.\n\n3) Semantic IR for context, not ad hoc stuffing\n- MTP advantage: MT-IR captures function intent, types, docs, relationships, and examples. The runtime selects a minimal relevant â€œIR sliceâ€ for each call.\n- Why it matters: More concise, relevant model inputs; lower token costs; less ambiguity. It scales better as codebases grow compared to manual context curation.\n\n4) Developer ergonomics: feels like normal programming\n- MTP advantage: Declare a function and add by <model>. No new DSLs or complex chains to learn.\n- Why it matters: Faster onboarding, less boilerplate, easier reviews. You can later replace the by body with native code without changing call sites.\n\n5) Built-in robustness: validation, repair, and retries\n- MTP advantage: Automatic schema validation, guided repair prompts on mismatch, bounded retries, coercion rules, and typed error surfacing.\n- Why it matters: Fewer silent failures and edge-case bugs. Production-friendly behavior without writing bespoke retry/guard code.\n\n6) Observability and governance out of the box\n- MTP advantage: IR-aligned traces (prompts, schemas, responses, validation results, repairs), caching/memoization, policy controls, model routing/fallbacks.\n- Why it matters: Easier debugging and auditing; controllable cost/latency; safer rollout and A/B testing than ad hoc wrappers.\n\n7) Performance and cost efficiency\n- MTP advantage: Minimal sufficient context, structured output modes, fewer round-trips due to schema-first decoding, caching.\n- Why it matters: Lower tokens and latency for many tasks versus verbose prompt templates or multi-hop chains.\n\n8) Resilience to naming noise and code drift\n- MTP advantage: Even with partially degraded identifiers, the combination of remaining names, types, and structure keeps behavior stable (as reported in the paperâ€™s evaluation).\n- Why it matters: Real-world codebases arenâ€™t perfectly clean; MTP degrades gracefully.\n\n9) Tool/API use with typed contracts\n- MTP advantage: If tools are enabled, their signatures live in MT-IR; the model calls them under policy with typed IO.\n- Why it matters: Safer, more predictable tool use than unconstrained tool-calling; simpler to maintain as APIs evolve.\n\nHow this differs from popular peers\n- LangChain/LMQL/SGLang: Great for building prompt chains and control, but you still craft prompts/templates and parse outputs. MTP removes that step by leaning on code semantics and types.\n- DSPy: Automates prompt optimization but still revolves around prompt/spec artifacts. MTP instead treats program structure as the spec and makes the compiler/runtime generate and enforce it.\n- Function-calling SDKs and JSON-mode: Provide the mechanism, not the abstraction. You still have to design prompts and schemas and wire validation. MTP lifts this into the language with MT-IR and MT-Runtime.\n- Guardrails/JsonSchema libs: Validate outputs, but donâ€™t solve prompt assembly, context selection, or developer ergonomics. MTP unifies all of these.\n\nWhen MTP is especially strong\n- Tasks with clear type contracts: extraction, classification, transformation, structured generation.\n- Teams that want maintainable AI features embedded in existing codebases.\n- Systems that need auditing, reproducibility, and easy model swaps.\n\nWhere peers may be preferable\n- Highly custom, multi-step agentic workflows with bespoke control logic and tool orchestration you want to script yourself.\n- Research setups needing full transparency over every prompt token and hand-tuned few-shot examples.\n- Ultra-low-latency paths where even structured LLM calls are too slow (you may replace by implementations with native code).\n\nBottom line\nMTP excels by turning AI integration into typed, meaning-driven programming. It automates prompt construction, enforces IO contracts, and provides production-grade runtime controlsâ€”yielding simpler code, stronger correctness, and better maintainability than template-centric or library-only approaches.","created_at":1757965558683,"messageId":"6abaa74b-5f9c-427c-a735-463ba146a8d9","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Hereâ€™s where MTP (Meaning-Typed Programming) stands out compared to common AI-integration approaches and frameworks (e.g., LangChain, LMQL, DSPy, SGLang, function-calling SDKs, schema/guardrail libraries):\n\n1) Prompting is a language/runtime responsibility, not yours\n- MTP advantage: You donâ€™t write or maintain prompt templates. The by operator plus MT-IR automatically assembles prompts from code semantics (names, types, docs, examples).\n- Why it matters: Fewer brittle templates, less â€œprompt drift,â€ easier refactors. Rename a field or tighten a type and the prompt reshapes automatically.\n\n2) Strong, first-class typing from the source of truth\n- MTP advantage: Return types and parameter types are the contract that drives structured decoding and validation. The runtime enforces schemas, coerces safely, and performs targeted repairs.\n- Why it matters: Predictable outputs without hand-rolled JSON parsing or regex hacks. Better testability and composability than free-form LLM outputs.\n\n3) Semantic IR for context, not ad hoc stuffing\n- MTP advantage: MT-IR captures function intent, types, docs, relationships, and examples. The runtime selects a minimal relevant â€œIR sliceâ€ for each call.\n- Why it matters: More concise, relevant model inputs; lower token costs; less ambiguity. It scales better as codebases grow compared to manual context curation.\n\n4) Developer ergonomics: feels like normal programming\n- MTP advantage: Declare a function and add by <model>. No new DSLs or complex chains to learn.\n- Why it matters: Faster onboarding, less boilerplate, easier reviews. You can later replace the by body with native code without changing call sites.\n\n5) Built-in robustness: validation, repair, and retries\n- MTP advantage: Automatic schema validation, guided repair prompts on mismatch, bounded retries, coercion rules, and typed error surfacing.\n- Why it matters: Fewer silent failures and edge-case bugs. Production-friendly behavior without writing bespoke retry/guard code.\n\n6) Observability and governance out of the box\n- MTP advantage: IR-aligned traces (prompts, schemas, responses, validation results, repairs), caching/memoization, policy controls, model routing/fallbacks.\n- Why it matters: Easier debugging and auditing; controllable cost/latency; safer rollout and A/B testing than ad hoc wrappers.\n\n7) Performance and cost efficiency\n- MTP advantage: Minimal sufficient context, structured output modes, fewer round-trips due to schema-first decoding, caching.\n- Why it matters: Lower tokens and latency for many tasks versus verbose prompt templates or multi-hop chains.\n\n8) Resilience to naming noise and code drift\n- MTP advantage: Even with partially degraded identifiers, the combination of remaining names, types, and structure keeps behavior stable (as reported in the paperâ€™s evaluation).\n- Why it matters: Real-world codebases arenâ€™t perfectly clean; MTP degrades gracefully.\n\n9) Tool/API use with typed contracts\n- MTP advantage: If tools are enabled, their signatures live in MT-IR; the model calls them under policy with typed IO.\n- Why it matters: Safer, more predictable tool use than unconstrained tool-calling; simpler to maintain as APIs evolve.\n\nHow this differs from popular peers\n- LangChain/LMQL/SGLang: Great for building prompt chains and control, but you still craft prompts/templates and parse outputs. MTP removes that step by leaning on code semantics and types.\n- DSPy: Automates prompt optimization but still revolves around prompt/spec artifacts. MTP instead treats program structure as the spec and makes the compiler/runtime generate and enforce it.\n- Function-calling SDKs and JSON-mode: Provide the mechanism, not the abstraction. You still have to design prompts and schemas and wire validation. MTP lifts this into the language with MT-IR and MT-Runtime.\n- Guardrails/JsonSchema libs: Validate outputs, but donâ€™t solve prompt assembly, context selection, or developer ergonomics. MTP unifies all of these.\n\nWhen MTP is especially strong\n- Tasks with clear type contracts: extraction, classification, transformation, structured generation.\n- Teams that want maintainable AI features embedded in existing codebases.\n- Systems that need auditing, reproducibility, and easy model swaps.\n\nWhere peers may be preferable\n- Highly custom, multi-step agentic workflows with bespoke control logic and tool orchestration you want to script yourself.\n- Research setups needing full transparency over every prompt token and hand-tuned few-shot examples.\n- Ultra-low-latency paths where even structured LLM calls are too slow (you may replace by implementations with native code).\n\nBottom line\nMTP excels by turning AI integration into typed, meaning-driven programming. It automates prompt construction, enforces IO contracts, and provides production-grade runtime controlsâ€”yielding simpler code, stronger correctness, and better maintainability than template-centric or library-only approaches.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0edeea433e9f08ba0068c86cf778b08194a27654fe26f6249f"}},"resumableStreamId":"STREAM:66d87d32-91b7-4b1b-8403-e6662c4d68e1","role":"assistant","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","timeToFirstToken":0.998,"tokens":1028,"tokensPerSecond":46.406644998194295,"updated_at":1757965558683,"userId":"google:104458561188320708185","id":"j978m3j2wdae5kstnz07rm9nxh7qm0nj"},{"_creationTime":1757965615809.466,"_id":"j976tg15wtqqptgg9jmnp0qrxh7qmeye","attachmentIds":[],"content":"what limitations did MTP specify?","created_at":1757965615812,"messageId":"689919fd-6483-4812-be02-ed39b2b07279","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","updated_at":1757965615812,"userId":"google:104458561188320708185","id":"j976tg15wtqqptgg9jmnp0qrxh7qmeye","providerMetadata":{}},{"_creationTime":1757965615809.4663,"_id":"j97f3th0ayscamqe4y0c71pe7x7qnbxs","attachmentIds":[],"byok":false,"content":"Here are the main limitations and caveats the MTP paper itself highlights or implies, grouped by theme. Iâ€™ve kept the wording faithful to the paperâ€™s claims and evaluation notes.\n\nDependence on code semantics\n- Quality of names/docs matters: MTP relies on semantic signals (function/variable names, docstrings, type names, field descriptions). While their experiments show robustness even with degraded naming (they report tolerating up to ~50% degradation), extremely sparse or generic naming and missing docs reduce intent clarity and can hurt results.\n- Types as contracts: If return types are vague (e.g., plain str when a structured object would be better), the runtime has less leverage to constrain outputs, increasing variability.\n\nModel capabilities and nondeterminism\n- Model-bound ceilings: MTPâ€™s effectiveness scales with the underlying LLM. On weaker or highly latency- or cost-constrained models, accuracy can drop, and more retries/repairs may be needed.\n- Variability isnâ€™t eliminated: Schema-first decoding and repair narrow the spread, but cannot fully remove nondeterminism for ill-posed or borderline tasks.\n\nTask fit and expressivity\n- Best for typed, structure-friendly tasks: extraction, classification, transformation, schema-constrained generation. For tasks that truly require open-ended prose or multi-turn, agentic reasoning with bespoke control, MTP provides less advantage or may need complementary orchestration.\n- Ambiguous intent: When the functionâ€™s contract underspecifies behavior (fuzzy requirements, missing constraints), MT-Runtime may need clarifications or will fail after bounded retries.\n\nContext selection and scale\n- IR slicing limits: MT-IR must select a â€œminimal sufficientâ€ slice to keep tokens and latency in check. In very large codebases, picking the right neighboring context efficiently is nontrivial; overly small slices can omit useful signals, while large slices increase cost.\n- Cross-module semantics: If meaning is scattered across distant modules/tests not indexed or linked, the automatic context may miss helpful examples unless configured.\n\nValidation, repair, and failure handling\n- Not all mismatches are repairable: The runtime can coerce simple type mismatches and run targeted repair prompts, but deeply incorrect or contradictory outputs will surface typed errors after retries.\n- Constraint expressivity: The quality of validation depends on the precision of the declared types and constraints; complex invariants beyond the type system may need custom validators or guards.\n\nRuntime, ops, and governance\n- Overheads and tuning: Although prompts tend to be concise, thereâ€™s still runtime overhead for validation, repairs, caching, logging, and possible tool mediation. Teams must tune retry limits, coercion rules, and caching to meet SLOs/cost targets.\n- Safety and policy scope: MTP provides guardrails and policies, but domain-specific compliance/safety requirements may require additional, external enforcement.\n\nPortability and adoption\n- Language integration: The reference implementation is in Jac (a Python superset) via the byLLM module. Using MTP as-is may require adopting Jac or waiting for similar constructs in other ecosystems; embedding the same abstractions into mainstream languages would need additional compiler/runtime work.\n- Learning curve is shifted, not erased: You donâ€™t learn a prompting framework, but teams do need to learn how to write â€œmeaning-richâ€ typed interfaces, configure the runtime, and read IR-aligned traces.\n\nEvaluation scope and external validity\n- Reported robustness and speedups are task- and setup-dependent: The paperâ€™s results (e.g., 3.2x faster dev time, 45% fewer LOC, resilience to 50% naming degradation) come from specific tasks, users, and models. Different domains, code quality, and model choices may change outcomes.\n- Less-is-more trend has boundaries: Their observation that shorter, semantics-driven prompts work better as models improve is empirical, not a guarantee across all models or future architectures.\n\nWhen to think twice\n- Ultra-low-latency paths with strict determinism\n- Tasks lacking stable schemas/contracts\n- Highly specialized multi-step agent workflows where you want explicit, custom control logic\n- Codebases with very weak or inconsistent naming/types and little documentation\n\nIf you share your target domain and stack, I can suggest concrete mitigations (e.g., type refinements, validator hooks, IR-slice tuning, or when to fall back to hand-written bodies) to work around these limits.","created_at":1757965615813,"messageId":"3fd75b60-499c-4c30-8958-bde2719f46d9","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":null,"domains":null,"queries":["\"MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\" limitations section arXiv 2405.08965","arXiv:2405.08965 MTP limitations threats to validity","Meaning-Typed Programming MT-IR MT-Runtime limitations evaluation OOPSLA 2025"]},"result":[{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2405.08965v5","id":"https://arxiv.org/html/2405.08965v5","publishedDate":null,"summary":"Failed to fetch content","title":"MTP: A Meaning-Typed Language Abstraction for AI- ...","url":"https://arxiv.org/html/2405.08965v5"},{"content":"\\\\UseRawInputEncoding\n\n# Meaning-Typed Programming: Language-level Abstractions and Runtime for GenAI Applications\n\nJason Mars\nUniversity of MichiganUnited States[profmars@umich.edu](mailto:profmars@umich.edu),Â Yiping Kang\nUniversity of MichiganUnited States[ypkang@umich.edu](mailto:ypkang@umich.edu),Â Jayanaka L. Dantanarayana\nUniversity of MichiganUnited States[jayanaka@umich.edu](mailto:jayanaka@umich.edu),Â Kugesan Sivasothynathan\nUniversity of Moratuwa\nJaseci LabsSri Lanka[kugesan.sivasothynathan@jaseci.org](mailto:kugesan.sivasothynathan@jaseci.org),Â Christopher Clarke\nUniversity of MichiganUnited States[csclarke@umich.edu](mailto:csclarke@umich.edu),Â Baichuan Li\nUniversity of MichiganUnited States[patrli@umich.edu](mailto:patrli@umich.edu),Â Krisztian Flautner\nUniversity of MichiganUnited States[manowar@umich.edu](mailto:manowar@umich.edu)Â andÂ Lingjia Tang\nUniversity of MichiganUnited States[lingjia@umich.edu](mailto:lingjia@umich.edu)\n\n###### Abstract.\n\nSoftware is rapidly evolving from being programmed with traditional logical code, to _neuro-integrated_ applications that leverage generative AI and large language models (LLMs) for application functionality. This shift increases the complexity of building applications, as developers now must reasoning about, program, and prompt LLMs. Despite efforts to create tools to assist with prompt engineering, these solutions often introduce additional layers of complexity to the development of neuro-integrated applications.\n\nThis paper proposes _meaning-typed programming_ (MTP), a novel approach to simplify the creation of neuro-integrated applications by introducing new language-level abstractions that hide the complexities of LLM integration. _Our key insight is that typical conventional code already possesses a high level of semantic richness that can be automatically reasoned about, as it is designed to be readable and maintainable by humans._ Leveraging this insight, we conceptualize LLMs as meaning-typed code constructs and introduce a by abstraction at the language level, MT-IR, a new meaning-based intermediate representation at the compiler level, and MT-Runtime, an automated run-time engine for LLM integration and operations. We implement MTP in a production-grade Python super-set language called Jac and perform an extensive evaluation. Our results demonstrate that MTP not only simplifies the development process but also meets or exceeds the efficacy of state-of-the-art manual and tool-assisted prompt engineering techniques in terms of accuracy and usability.\n\n## 1\\. Introduction\n\nSoftware development is rapidly evolving towards _neuro-integrated_ applications that leverage generative AI and large language models (LLMs) for critical functionality. This shift represents a convergence of conventional programming with the powerful capabilities of AI, promising to revolutionize how we build software.\nHowever, integrating LLMs into traditional programming remains a complex challenge due to the fundamental differences between operating assumptions. In conventional programming, code describes operations performed on explicitly defined objects (FigureÂ [1](https://arxiv.org/html/2405.08965v3#S1.F1) left). In contrast, LLMs process natural language text as input and produce text-based outputs. To integrate LLMs into programs, developers must manually construct textual input, a process known as _prompt engineering_.\n\nFigure 1. Comparison of LLM as Meaning Typed Code construct abstraction (Right) with the present-day LLM abstraction (Middle) and Symbolic Programming Abstraction (Left) of a programming function.\n\n\\\\Description\n\nPrompt engineering, the primary means of LLM programming, can be tedious, complex, and time-consuming. Developers must craft precise instructions, manage textual context, and handle the inherent variability in LLM output. This process lacks the structure, consistency, and debugging capabilities developers are accustomed to in traditional programming environments.\n\nSeveral efforts have been made to address these challenges by developing infrastructure and tools to facilitate prompt engineering. Frameworks such as LangChain, Language Model Query Language (LMQL)Â ( [Beurer2023LMQL,](https://arxiv.org/html/2405.08965v3#bib.bib4)), DSPyÂ ( [khattab2023dspy,](https://arxiv.org/html/2405.08965v3#bib.bib16)), and SGLangÂ ( [zheng2023SGLang,](https://arxiv.org/html/2405.08965v3#bib.bib46)) have been introduced to assist developers in generating and managing prompts. However, while these frameworks provide more sophisticated tooling to help bridge the gap between conventional programming and the incorporation of AI models, in practice, they introduce additional layers of complexity for developers:\n\n1. (1)\n\n\n_Prompt design complexity:_ Developers remain responsible for the manual construction of prompts, including determining the appropriate prompt language and selecting relevant information to include.\n\n2. (2)\n\n\n_Steep learning curve:_ These systems require developers to familiarize themselves with new query languages, frameworks, or specialized syntax, complicating the integration process.\n\n3. (3)\n\n\n_Input/output conversion complexity:_ Parsing LLM output and converting them back to objects remains a challenge, especially given the variability in output across different LLMs.\n\n\nThe key insights of this work are: _(1) In practice, programs are written to be readable by various developers to comprehend the intentions and semantics conveyed by the code. This semantic richness can be leveraged to automatically translate intent embodied in the code to prompts for the LLM. (2) As state-of-the-art LLMs continue to advance, AI is increasingly feasible to infer code intentions without requiring explicit descriptions or prompt construction by developers. This trend is not only supported by our empirical findings, but we surprisingly find less can be more in prompt engineering._\n\nBased on this insight, this paper proposes _meaning-typed programming_, a novel approach to simplify the creation of neuro-integrated applications by introducing new language-level abstractions that hide the complexities of LLM integration. There are three key components to enable meaning-typed programs. These include (1) The â€™byâ€™ operator that enables the seamless integration of LLM functionality into code. It acts as a bridge between traditional and LLM operations, allowing developers to replace function bodies with a simple by model\\_name clause:\n\n\\\\Description\n\nThis declaration, along with the function signature and available semantic information, is sufficient for the LLM to produce the correct return value at runtime, significantly simplifying LLM integration in existing codebases. (2) MT-IR, a meaning-based intermediate representation at the compiler level that collects and organizes semantically rich information, preserving the meaning behind variable names, function signatures, and comments. MT-IR is also used dynamically to bind run-time values to semantic representations, enhancing the modelâ€™s understanding of program context and enabling effective, context-aware LLM integration.\n(3) MT-Runtime, an automated runtime engine within the languageâ€™s virtual machine, triggered at by call sites. It binds runtime values to MT-IR, manages LLM interactions, generates context-aware prompts, and handles responses and errors. MT-Runtime allows developers to focus on application logic while automatically managing LLM integration, ensuring seamless and contextualized operations.\n\nWe implement _meaning-typed programming (MTP)_ in a production-grade Python superset language called Jac and perform an extensive evaluation. Specifically, we make the following contributions:\n\n- â€¢\n\n\nWe introduce the concept of meaning-typed programming, a novel paradigm that treats LLMs as meaning-typed code constructs, enabling seamless integration of LLMs into conventional programs without explicit prompt engineering.\n\n- â€¢\n\n\nWe present the design and implementation of the MTP framework: the â€˜byâ€™ operator for intuitive LLM integration; MT-IR, a semantically rich intermediate representation for implied meaning; and MT-Runtime, an automated runtime engine for LLM operations\n\n- â€¢\n\n\nWe implement MTP in Jac, a production-grade Python superset language, conduct extensive evaluation and show that MTP significantly reduces development complexity, lines of code, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches.\n\n- â€¢\n\n\nWe provide an empirical analysis of LLM evolution trends, demonstrating how MTP takes advantage of advances in LLM capabilities to simplify neuro-integrated programming over time.\n\n- â€¢\n\n\nWe present multiple case studies demonstrating the application of MTP in complex problem domains, analyzing its impact on code structure, development workflow, and complexity reduction compared to traditional LLM integration methods.\n\n\nOur results show that MTP simplifies the development process and meets or exceeds the efficacy of state-of-the-art manual and tool-assisted prompt engineering techniques. This approach significantly reduces development complexity and lines of code while improving run-time performance and costs associated with LLM integration.\n\n## 2\\. Problem, Background, and Insights\n\nConventional Prompt Engineering\n\n[â¬‡](data:;base64,CmdwdCA9IE9wZW5BSSgpCgpkZWYgcmVzcG9uZF93aXRoX2NvbmZpZGVuY2VfbGV2ZWwocXVlcnk6IHN0ciwKICAgIGNoYXRfaGlzdG9yeTogZGljdFtzdHIsc3RyXSwKICAgIHVzZXJfZGF0YTpkaWN0KSAtPiB0dXBsZVtzdHIsZmxvYXRdOgoKICAgIHN5c3RlbV9tZXNzYWdlID0gKAogICAgICAgICJZb3UgYXJlIGEgY2hhdGJvdC4gUmVzcG9uZCB0byB0aGUgY2hhdCB1c2luZyB0aGUgYXZhaWxhYmxlIGluZm9ybWF0aW9uLiBBZGRpdGlvbmFsbHkgZ2VuZXJhdGUgYSBjb25maWRlbmNlIGxldmVsIG9mIHRoZSByZXNwb25zZSBhcyBhIHBlcmNlbnRhZ2UiICkKCiAgICBwcm9tcHQgPSAgc3lzdGVtX21lc3NhZ2UKICAgIHByb21wdCArPSBmJ1xuUXVlcnkgZm9ybSB0aGUgdXNlcjpcbntxdWVyeX0nCiAgICBwcm9tcHQgKz0gJ1xuQ2hhdCBIaXN0b3J5OlxuJwogICAgZm9yIHVzZXIsY29udGVudCBpbiBjaGF0X2hpc3Rvcnk6CiAgICA","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2405.08965v3","id":"https://arxiv.org/html/2405.08965v3","publishedDate":"2024-10-19T00:00:00.000Z","summary":"The paper introduces \"Meaning-Typed Programming\" (MTP), a novel approach designed to simplify the development of neuro-integrated applications that leverage large language models (LLMs) and generative AI. Traditional methods rely heavily on prompt engineering, which is complex, time-consuming, and lacks the structure of conventional programming. MTP addresses this by introducing language-level abstractions that treat LLM interactions as meaning-typed code constructs, effectively hiding the complexities of prompt design and LLM integration. The authors develop a new intermediate representation (MT-IR) and runtime (MT-Runtime) to automate and streamline these processes. Implemented in a Python-based language called Jac, MTP demonstrates improved usability and accuracy, making it easier for developers to build AI-enhanced applications without deep expertise in prompt engineering.","title":"Meaning-Typed Programming: Language-level Abstractions and Runtime for GenAI Applications","url":"https://arxiv.org/html/2405.08965v3"},{"content":"\\\\UseRawInputEncoding\n\n# MTLLM: LLMs are Meaning-Typed Code Constructs\n\nJason Mars\nUniversity of MichiganUnited States[profmars@umich.edu](mailto:profmars@umich.edu),Â Yiping Kang\nUniversity of MichiganUnited States[ypkang@umich.edu](mailto:ypkang@umich.edu),Â Jayanaka L. Dantanarayana\nUniversity of MichiganUnited States[jayanaka@umich.edu](mailto:jayanaka@umich.edu),Â Chandra Irugalbandara\nJaseci LabsSri Lanka[chandra.irugalbandara@jaseci.org](mailto:chandra.irugalbandara@jaseci.org),Â Kugesan Sivasothynathan\nUniversity of Moratuwa\nJaseci LabsSri Lanka[kugesan.sivasothynathan@jaseci.org](mailto:kugesan.sivasothynathan@jaseci.org),Â Christopher Clarke\nUniversity of MichiganUnited States[csclarke@umich.edu](mailto:csclarke@umich.edu),Â Baichuan Li\nUniversity of MichiganUnited States[patrli@umich.edu](mailto:patrli@umich.edu)Â andÂ Lingjia Tang\nUniversity of MichiganUnited States[lingjia@umich.edu](mailto:lingjia@umich.edu)\n\n###### Abstract.\n\nProgramming with Generative AI (GenAI) models, which frequently involves using large language models (LLMs) to accomplish specific functionalities, has experienced significant growth in adoption.\nHowever, it remains a complex process, as developers often need to manually configure text inputs for LLMs, a practice known as prompt engineering, and subsequently translate the natural language outputs produced by LLMs back into symbolic code representations (values, types, etc.) that the code can understand. Although some infrastructures are proposed to facilitate prompt engineering, these tools are often complex and challenging for developers to adopt.\n\nInstead, this paper presents a simplified approach to integrating LLMs into programming through the introduction of an abstraction layer that hides the complexity of gluing traditional programming and LLMs together. Our approach utilizes the semantic richness in existing programs to automatically translate between the traditional programming languages and the natural language understood by LLMs, eliminating developer efforts such as prompt engineering, decreasing the overall complexity.\n\nSpecifically in this paper, we design three novel code constructs coupled with an automated runtime management system that bridges the gap between traditional symbolic code and LLMs.\nWe present a fully functional and production-grade implementation for our approach and compare it to SOTA LLM software development tools. We present real-world case studies demonstrating the efficacy of our proposed abstraction that seamlessly utilizes LLMs to solve problems in place of potentially complex traditional programming logic.\n\nRaise of Abstraction, Neurosymbolic Programming, Large Language Models\n\n## 1\\. Introduction\n\nFigure 1. Comparison of LLM as Meaning Typed Code construct abstraction (Right) with the present-day LLM abstraction (Middle) and Symbolic Programming Abstraction (Left) of a programming function.\n\n\\\\Description\n\nProgramming with Generative AI (GenAI) models, which often involves integrating AI models such as large language models (LLMs) to accomplish specific functionalities and tasks in traditional programming, has experienced tremendous adoption in recent years.\n\nHowever, integrating LLMs into traditional programming remains a complex challenge due to the fundamental differences in how conventional symbolic programming and those models operate. In conventional symbolic programming, code describes operations or functions performed on explicitly defined objects and variables with typed values (FigureÂ [1](https://arxiv.org/html/2405.08965v2#S1.F1) left). In contrast, LLMs do not directly operate on variables or objects. Instead, LLMs process natural language text as input and produce text-based outputs. As a result, to integrate LLMs into programming, developers are required to manually construct textual inputs, or prompts. This process, known as _prompt engineering_, which serve as the primary means of programming with LLMs, can be tedious, intricate and time-consuming. This is particularly true because developers need to translate traditional function inputs into the prompt to embed knowledge from existing programs. The manual nature of this task often increases complexity and decreases code maintainability. Additionally, developers must convert the natural language outputs produced by LLMs back into symbolic representations, such as values and types, that are usable within the code.\n\nCurrently, to the best of our knowlege, there is no automated solution to seamlessly bridge the gap between symbolic programming and LLMs. There have been several efforts in designing infrastructures to facilitate better prompt engineering, such as LangChain, Language Model Query Language (LMQL)Â ( [Beurer2023LMQL,](https://arxiv.org/html/2405.08965v2#bib.bib1)), DSPyÂ ( [khattab2023dspy,](https://arxiv.org/html/2405.08965v2#bib.bib3)), and SGLangÂ ( [zheng2023SGLang,](https://arxiv.org/html/2405.08965v2#bib.bib7)).\nHowever, although these systems are effective in prompt engineering, they have limitations when used in the context of integrating LLMs with symbolic programming:\n\n- â€¢\n\n\nPrompt design complexity Developers remain responsible for the manual construction of prompts. For instance, while tools such as LMQLÂ ( [Beurer2023LMQL,](https://arxiv.org/html/2405.08965v2#bib.bib1)) and LangChain can assist in generating prompts, developers must still determine the appropriate type of prompt, select the relevant information to include, and explicitly specify these decisions programmatically.\n\n- â€¢\n\n\nSteep learning curve Many of these systems present a significant learning curve, making them challenging for developers to adopt for this purpose. For example, tools like LMQL and LangChain require developers to familiarize themselves with new query languages and frameworks, which can introduce additional complexity. Similarly, SGLang and DSPy involve learning specialized syntax and workflows that may differ from conventional programming practices, complicating the integration process.\n\n- â€¢\n\n\nOutput conversion complexity For all of these systems, we still need to parse LLM outputs and convert them back to symbolic code constructs (data structures, variables, objects etc). Since each LLM may have drastically different text output, developing generalized code that work with all LLMs and future models can be non-trivial and require significant manual effort.\n\n\nOverall, despite the support offered by these infrastructures for prompt engineering, the burden of prompt design, configuration and interpretation of model output remains largely on the developer and integrating LLMs into symbolic programming remains a highly complex process.\n\nInstead of employing sophisticated yet complicated prompt engineering infrastructures to configure LLM inputs in programming, this paper investigates an alternative approach aimed at simplifying the integration of LLMs into programming workflows. We argue the need to introduce an abstraction layer that hides the complexity of gluing traditional programming and LLMs together from developers. In fact, we aim to eliminate developerâ€™s need for prompt engineering to support automatic and seamless LLM integration.\n\nOur unique insight is that the semantic richness in existing programs can be utilized to automatically translate between the traditional programming languages and the natural language understood by LLMs. In other words, a well-written program should be human readable, allowing humans to comprehend the intentions and semantics conveyed by the code. Consequently, it should also be readable (intepretable) by AI without requiring manual translation by developers. In fact, as the state-of-the-art LLMs continue to advance, it is becoming increasingly feasible for AI to infer the intentions and semantics of code, without the need for explicit descriptions or prompt construction by developers. This trend is further supported by our findings in the empirical study presented in the Evaluation section.\n\nBased on this insight, we design three novel code constructs coupled with an automated runtime management system that bridges the gap between traditional symbolic code and LLMs. Our system Meaning-typed LLM (MT-LLM) can automatically leverage LLMs for functionalities in traditional programming without the additional overhead and complexity of integration. For example, with MT-LLM, developers can use LLMs as a function by providing traditional function inputs, just as using a traditional function call, without the need to engineer the prompt or interpreting results.\n\nSpecifically, we make the following contributions in this paper:\n\n- â€¢\n\n\nWe introduce a novel abstraction of treating LLMs asÂ meaning-type wrapped code constructs for seamless integration into conventional symbolic programming. We propose a new runtime system, Automatic Meaning-type Transformation, that abstracts away many of the current complexity of programming with LLMs. MT-LLM supports three common symbolic code operations: instantiating an object of a custom type, a standalone function call and member method of a class.\n\n- â€¢\n\n\nWe present a fully functional and production-grade implementation for our approach and compare it to SOTA LLM software development tools. We conduct both user study and quantitative measurments.\n\n- â€¢\n\n\nWe studied the trend of LLMs development to demonstrate the need for complicated prompt engineering and the growing capacities for SOTA LLMs to integrate into traditional programming with without human effort.\n\n- â€¢\n\n\nWe present real-world case studies demonstrating the efficacy of our proposed abstraction that seamlessly utilizes LLMs to solve problems in place of potentially complex traditional programming logic.\n\n\n## 2\\. Problem, Background and Insights\n\nThere has been a surge in interest in adopting GenAI models in traditional programs. Instead of implementing complex functions to accomplish a task, developers can now utilize large language models (LLMs) LLMs to genera","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2405.08965v2","id":"https://arxiv.org/html/2405.08965v2","publishedDate":"2024-09-24T00:00:00.000Z","summary":"The paper \"MTLLM: LLMs are Meaning-Typed Code Constructs\" introduces a novel abstraction for integrating large language models (LLMs) into programming, aiming to simplify their adoption and reduce reliance on complex prompt engineering. Traditional programming involves explicit, typed code constructs, whereas LLMs operate on natural language inputs and outputs, making integration challenging. The authors propose an abstraction layer that leverages the semantic richness of existing code to automatically translate between symbolic code and natural language, eliminating the need for manual prompt design. They develop three new code constructs coupled with an automated runtime system that bridges symbolic programming and LLMs, resulting in a production-grade implementation. Case studies demonstrate how this approach enables seamless, efficient use of LLMs for problem-solving, representing a significant step toward neurosymbolic programming and more accessible AI-assisted development.","title":"MTLLM: LLMs are Meaning-Typed Code Constructs","url":"https://arxiv.org/html/2405.08965v2"},{"content":"# Meta Prompting for AI Systems\n\nYifan Zhang1Yang Yuan1,2,3Andrew Chi-Chih Yao1,2,3â€ 1IIIS, Tsinghua University\n2Shanghai Artificial Intelligence Laboratory\n3Shanghai Qizhi Institute\nzhangyif21, yangjq21@mails.tsinghua.edu.cn,\n{yuanyang, andrewcyao}@tsinghua.edu.cnCorresponding authors.\n\n###### Abstract\n\nIn this work, we present a comprehensive study of Meta Prompting (MP), an innovative technique reshaping the utilization of language models (LMs) and AI systems in problem-solving and data interaction. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting, sets it apart from few-shot prompting, and underlines its effectiveness in various AI applications. A key focus is applying Meta Prompting for complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency, and enabling more equitable problem-solving comparisons, especially against few-shot prompting methods. Additionally, the paper introduces Meta Prompting for prompting tasks, allowing LLMs to self-generate new prompts in a recursive, metaprogramming-like manner.\nEmpirical experiments, including using a Qwen-72B base language model equipped with meta prompt without instruction-tuning to solve MATH problems with accuracy at 46.3%, which surpass the supervised fine-tuned counterpart trained with extensive mathematical QA instruction pairs and even the initial version of GPT-4, solving GSM8K problems with 83.5% accuracy with zero-shot meta-prompted Qwen-72B base language model, and solving the Game of 24 tasks with a 100% success rate using GPT-4, demonstrate the meta promptingâ€™s efficacy in achieving high accuracy and efficiency, showcasing Meta Promptingâ€™s transformative impact on AI problem-solvingâ€ â€ The code is available at [https://github.com/meta-prompting/meta-prompting](https://github.com/meta-prompting/meta-prompting)..\n\n## 1 Introduction\n\nThe emergence of foundation models, especially Large Language Models (LLMs), has revolutionized the field of artificial intelligence. These models, exemplified by their extensive training data and capacity for generalization, have dramatically expanded the horizons of computational linguistics, text understanding, and text generationÂ \\[ [10](https://arxiv.org/html/2311.11482v5#bib.bib10), [35](https://arxiv.org/html/2311.11482v5#bib.bib35), [36](https://arxiv.org/html/2311.11482v5#bib.bib36), [5](https://arxiv.org/html/2311.11482v5#bib.bib5), [37](https://arxiv.org/html/2311.11482v5#bib.bib37), [34](https://arxiv.org/html/2311.11482v5#bib.bib34)\\]. However, a critical challenge faced by LLMs is their limited efficacy in executing complex reasoning tasks, particularly in areas requiring deep, abstract thought such as advanced mathematicsÂ \\[ [25](https://arxiv.org/html/2311.11482v5#bib.bib25)\\]. This limitation points towards a need for enhanced methodologies that can augment LLMsâ€™ reasoning faculties.\n\nThe root of this challenge lies in the architecture of modern LLMs, which is predominantly oriented toward auto-regressive token predictionÂ \\[ [35](https://arxiv.org/html/2311.11482v5#bib.bib35), [36](https://arxiv.org/html/2311.11482v5#bib.bib36), [5](https://arxiv.org/html/2311.11482v5#bib.bib5)\\]. While efficient for a broad spectrum of tasks, this approach is not meticulously designed to support the depth and sophistication of human-like analytical thinking. This discrepancy is highlighted by the dual-process theory of cognitive psychology, articulated by Kahneman \\[ [21](https://arxiv.org/html/2311.11482v5#bib.bib21)\\], which differentiates the fast, intuitive responses of System 1 thinking from the slower, more deliberate reasoning of System 2 thinking. LLMs, in their typical operations, mirror System 1 processes and thus encounter difficulties with tasks that require the more deliberate, structured approach characteristic of System 2 thinking.\n\nAttempts to bridge this gap have led to the development of innovative methodologies such as Chain-of-Thought (CoT)Â \\[ [44](https://arxiv.org/html/2311.11482v5#bib.bib44)\\] and Tree-of-Thought (ToT)Â \\[ [49](https://arxiv.org/html/2311.11482v5#bib.bib49), [28](https://arxiv.org/html/2311.11482v5#bib.bib28)\\], which guide LLMs in articulating intermediate steps in reasoning tasks. These methods, although valuable, have not fully realized the depth and flexibility of human cognitive processes in an abstract sense.\n\nIn response to these challenges, we introduce Meta Prompting (MP) and establish a theoretical framework for it, a novel approach that represents a substantial advance in the field of LLM reasoning. Meta Prompting extends beyond existing methods by abstracting and generalizing key principles for enhanced cognitive processing. Unlike its predecessors, Meta Prompting shifts the focus from content-driven reasoning to a more structure-oriented perspective. This method draws inspiration from category theory and type theory, establishing a functorial relationship between tasks and their corresponding prompts. This categorical approach allows for a more systematic and adaptable framework, capable of addressing a wide range of cognitive tasks with depth and nuance akin to human reasoning.\n\nFurthermore, a pivotal aspect of meta prompting is its application to Meta Prompting for prompting tasks in an in-context and recursive way utilizing the functorial and compositional properties of Meta Prompting, which we call Recursive Meta Prompting (RMP). This concept, akin to metaprogramming in programming language theory, involves using LLMs to design new prompts autonomously. The functorial nature of Meta Prompting allows for this advanced capability, where LLMs can not only solve problems but also generate the structures to solve them. This self-referential and recursive ability marks a significant leap in LLMsâ€™ autonomy and adaptability.\n\nThe practical efficacy of the Meta Prompting framework is empirically validated through a series of experiments, ranging from solving the Game of 24 puzzlesÂ \\[ [49](https://arxiv.org/html/2311.11482v5#bib.bib49)\\] to addressing complex MATH problemsÂ \\[ [17](https://arxiv.org/html/2311.11482v5#bib.bib17)\\], underscoring the Meta Promptingâ€™s versatility and empowering LLMs with advanced reasoning capabilities.\n\nIn summary, our contributions can be listed as follows:\n\n- â€¢\n\n\nWe propose the structured and syntax-oriented Meta Prompting (MP), and introduce a theoretical framework for meta prompting based on category theory. We further investigate meta prompting for prompting tasks and Recursive Meta Prompting (RMP) in a metaprogramming-like manner.\n\n- â€¢\n\n\nOur experiments on solving MATH problems with a Qwen-72B base language modelÂ \\[ [3](https://arxiv.org/html/2311.11482v5#bib.bib3)\\] equipped with meta prompt without instruction-tuning to solve MATH problems with accuracy at 46.3% which surpasses the supervised fine-tuned counterpart trained with extensive mathematical QA instruction pairs and even the initial version of GPT-4, solving GSM8K problems with 83.5% accuracy with zero-shot meta-prompted Qwen-72B base language model, and solving the Game of 24 tasks with 100% success rate using GPT-4, show the efficacy of meta prompting in problem-solving and in-context alignment.\n\n\n## 2 Meta Prompting\n\nMeta Prompting is an advanced prompting technique that focuses on the structural and syntactical aspects of problems, prioritizing the general format and pattern over specific content details. It aims to construct a more abstract, structured approach to interacting with large language models (LLMs), emphasizing the structure and syntax of information. This technique is particularly effective in contexts where the underlying pattern or framework of a problem is crucial for understanding or solving it.\n\n###### Definition 2.1(Meta Prompt).\n\nA Meta Prompt is an example-agnostic structured prompt designed to capture the reasoning structure of a specific category of tasks. It provides a scaffold that outlines the general approach to a problem, enabling LLMs to fill in specific details as needed. This approach allows for more efficient and targeted use of LLM capabilities by focusing on the \"how\" of problem-solving rather than the \"what\".\n\nA structure meta prompt for solving quadratic equations in the form aâ¢x2+bâ¢x+c=0ð‘Žsuperscriptð‘¥2ð‘ð‘¥ð‘0ax^{2}+bx+c=0italic\\_a italic\\_x start\\_POSTSUPERSCRIPT 2 end\\_POSTSUPERSCRIPT + italic\\_b italic\\_x + italic\\_c = 0:{\"Problem\": \"Solve the quadratic equation aâ¢x2+bâ¢x+c=0ð‘Žsuperscriptð‘¥2ð‘ð‘¥ð‘0ax^{2}+bx+c=0italic\\_a italic\\_x start\\_POSTSUPERSCRIPT 2 end\\_POSTSUPERSCRIPT + italic\\_b italic\\_x + italic\\_c = 0 for xð‘¥xitalic\\_x.\",\"Solution\": {\"Step 1\": \"Identify the coefficients að‘Žaitalic\\_a, bð‘bitalic\\_b, and cð‘citalic\\_c from the equation.\",\"Step 2\": \"Compute the discriminant using Î”=b2âˆ’4â¢aâ¢cÎ”superscriptð‘24ð‘Žð‘\\\\Delta=b^{2}-4acroman\\_Î” = italic\\_b start\\_POSTSUPERSCRIPT 2 end\\_POSTSUPERSCRIPT - 4 italic\\_a italic\\_c.\",\"Step 3\": \"Check if Î”>0Î”0\\\\Delta>0roman\\_Î” > 0, Î”=0Î”0\\\\Delta=0roman\\_Î” = 0, or Î”<0Î”0\\\\Delta<0roman\\_Î” < 0 to determine the nature of the roots.\",\"Step 4\": \"If Î”>0Î”0\\\\Delta>0roman\\_Î” > 0, calculate the two distinct real roots using x1,2=âˆ’bÂ±Î”2â¢asubscriptð‘¥12plus-or-minusð‘Î”2ð‘Žx\\_{1,2}=\\\\frac{-b\\\\pm\\\\sqrt{\\\\Delta}}{2a}italic\\_x start\\_POSTSUBSCRIPT 1 , 2 end\\_POSTSUBSCRIPT = divide start\\_ARG - italic\\_b Â± square-root start\\_ARG roman\\_Î” end\\_ARG end\\_ARG start\\_ARG 2 italic\\_a end\\_ARG.\",\"Step 5\": \"If Î”=0Î”0\\\\Delta=0roman\\_Î” = 0, calculate the single real root using x=âˆ’b2â¢að‘¥ð‘2ð‘Žx=\\\\frac{-b}{2a}italic\\_x = divide start\\_ARG - italic\\_b end\\_ARG start\\_ARG 2 italic\\_a end\\_ARG.\",\"Step 6\": \"If Î”<0Î”0\\\\Delta<0roman\\_Î” < 0, calculate the complex roots using x1,2=âˆ’bÂ±iâ¢\\|Î”\\|2â¢asubscriptð‘¥12plus-or-minusð‘ð‘–Î”2ð‘Žx\\_{1,2}=\\\\frac{-b\\\\pm i\\\\sqrt{\\|\\\\Delta\\|}}{2a}italic\\_x start\\_POSTSUBSCRIPT 1","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2311.11482v5","id":"https://arxiv.org/html/2311.11482v5","publishedDate":"2024-01-01T00:00:00.000Z","summary":"The webpage presents a comprehensive study of Meta Prompting (MP), an innovative approach that enhances the capabilities of Large Language Models (LLMs) in complex problem-solving. Grounded in type theory and category theory, MP emphasizes the structure and syntax of information rather than just content, enabling more efficient and effective reasoning. Unlike traditional few-shot prompting, Meta Prompting allows LLMs to decompose intricate tasks into simpler sub-problems, improving accuracy and token efficiency. Empirical results demonstrate MPâ€™s superior performance in solving mathematical problems (e.g., achieving 46.3% accuracy on MATH problems with a Qwen-72B model without instruction tuning, and 83.5% on GSM8K), as well as perfect success rates on certain tasks like the Game of 24 with GPT-4. The approach also introduces recursive, metaprogramming-like prompt generation, showcasing its potential to significantly advance AI reasoning and problem-solving capabilities. The authors provide code for implementation at their GitHub repository.","title":"Meta Prompting for AI Systems","url":"https://arxiv.org/html/2311.11482v5"},{"content":"Automatic Programming: Large Language Models and Beyond\nMICHAEL R. LYU, Chinese University of Hong Kong, Hong Kong, China\nBAISHAKHI RAY, Columbia University, USA\nABHIK ROYCHOUDHURY, National University of Singapore, [Corresponding Author], Singapore\nSHIN HWEI TAN, Concordia University, Canada\nPATANAMON THONGTANUNAM, University of Melbourne, Australia\nAutomatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large\nLanguage Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns\naround quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality,\nsecurity and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of\nautomatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable\nautomatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future,\nwhere programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of\nautomatically generated programs from LLMs, can help produce higher assurance code from LLMs, along with evidence of assurance.\n1 CHALLENGES IN AUTOMATIC PROGRAMMING\nThe task of programming both in terms of intent capture (capturing the desire of the user) as well as in generation of\ncorrect code â€” has occupied much of the Computing profession for the last 50â€“60 years. There has been significant\nprogress in modeling and system design to support accurate intent capture leading to the growth of formal specifications.\nHowever, despite all the progress â€“ software engineers are reluctant to write formal specifications, and for large software\nsystems a formal description of intent is not available â€” leading to tremendous hardship in debugging and fixing\nerrors. The field of automated program repair has shown promise in terms of code generation at a micro-scale. The key\nquestion there is how to trust the automatically generated code.\nRecent developments on automated code generation from Large Language Models (LLMs) bring the trust issues in\nauto-coding even more into the forefront. This raises not only the overall question of correctness of automatically\ngenerated code, but at what point we can start trusting automatically generated code enough to integrate it into\nour code-base. In past decades, niche industries have generated code from models, however there is no precedent of\nautomatically generated code from natural language specifications being used widely. We discuss the trust issues for\nsuch automatically generated code thoroughly in this article. While the immediate motivation of the article is to study\nthe trust issues in code from Large Language Models (LLMs), we study the topic of automated programming more\nbroadly in this article.\nWe notice that increasingly many organizations are moving towards automatically generated code, even apart from\nthe popularity of large language models. A recent keynote at Oracle CloudWorld 2023 [27] mentions how Oracle is\nconsidering moving away from writing software in Java, and instead automatically generating code for new software\nprojects in a language called Apex. Apex is a well-known low code application platform to assemble an application out\nof application pages. This movement towards low code enables other benefits such as easily achieving security audit of\nAuthorsâ€™ addresses: Michael R. Lyu, lyu@cse.cuhk.edu.hk Chinese University of Hong Kong, Hong Kong, Hong Kong, China; Baishakhi Ray, rayb@cs.\ncolumbia.edu Columbia University, New York, USA; Abhik Roychoudhury, abhik@comp.nus.edu.sg National University of Singapore, [Corresponding\nAuthor], Singapore, Singapore; Shin Hwei Tan, shinhwei.tan@concordia.ca Concordia University, Montreal, Canada; Patanamon Thongtanunam,\npatanamon.t@unimelb.edu.au University of Melbourne, Melbourne, Australia.\n1\narXiv:2405.02213v2 [cs.SE] 15 May 2024\n2 Authors\na software project. Overall, we note that automatic programming goes beyond the use of large language models and\nimplicitly includes recent trends in the growth of low-code no-code application development.\nAs a result of the recent interest in automatic programming, the set of problems associated with automatically\ngenerated code have received wide attention. Apart from correctness, there are concerns about security, privacy,\nexplainability of the code - particularly when generated from a large language model. Pragmatically speaking, there\ncould remain concerns about â€œpassing the blameâ€ when a software project which includes automatically generated code\nfails. To understand the underlying issue, we can draw an analogy between interaction between application software\nand systems software which leads to the well-known application compatibility (often called appcompat by developers)\nproblem (e.g. see [1]). Typically due to version change of systems software such as operating system (OS), a specific\napplication running on top of the OS (such as PDF reader) may fail. However, this need not be owing to the operating\nsystem itself. It may be due to a mistaken understanding of the expectations between the application software and the\nOS. In a similar fashion, when automatically generated code and manually written code co-exist in a software project,\nerrors may creep in due to mistaken understanding of the expectations between different software components.\nIn this article, we thus examine how trust boundaries can shift when we integrate automatically generated code\ninto a software project. One of the technical questions that could be of interest for the research community are the\nacceptability criterion for integrating LLM generated code into a software project. The capability of LLMs augmented\nby program analysis tools in automating key programming tasks such as bug-fixes and feature additions, as articulated\nin the recently proposed SWEbench [71] is also worthy of study. The last mile improvement of automatically generated\ncode from LLMs, through the systematic use of program repair [28] remains a possibility to explore [93]. Such a\npenultimate auto-repair strategy can help provide specific evidence of correctness (such as passing curated tests) that\nbuilds confidence towards accepting LLM-generated code into a code repository.\nWe also study the impact of LLMs in automating non-code artifacts and processes such as test generation, code\nreview and code summarisation. More importantly from the human-LLM interaction perspective, we seek to provide an\nemerging new outlook in future day programming. Traditionally, when formal specifications are unavailable, software\nengineers have resorted to program comprehension or specification inference to understand the functionality of a\ncomplex software system. This practice is particularly relevant when the software system is not built as a monolithic\nartifact, but rather assembled via the co-operation of different teams or via open-source contributions.\nWe note that the traditional program comprehension problem is conducted by the human, and may involve the\nusage of analysis / debugging tools to understand the working of a complex software system. In the age of LLM driven\nprogramming, we could postulate a new comprehension problem - where based on the natural language requirements\nLLMs augmented by program analysis tools can automate bulk of the comprehension tasks. There could be structured\nprovisions for consulting the human to disambiguate requirements, at different stages of the comprehension process.\nStudying the mechanisms for such human-LLM collaboration and providing adequate primitives for consulting the\nhuman programmer by the LLM / analyzers could point us to the programming environments of the near future. We\nalso underline the possibility of automated program repair of automatically generated code as a flexible mechanism for\ntrusted automatic programming. These could feature in future day programming environments of 2030-35 and beyond.\n2 HISTORICAL MILESTONES AND LITERATURE REVIEW\nIn this section, we will delve into the development of automatic programming, pinpointing historical milestones and\nconducting a thorough literature review. Specifically, we will first introduce several key tasks that propelled the field\nAutomatic Programming: Large Language Models and Beyond 3\nforward including code generation, program repair, and software testing. Additionally, we will highlight recent advances\nof LLMs in automatic programming and logging.\n2.1 Code Generation\nCode generation, also known as program synthesis, refers to the automated generation of software code based on\nuser intent. This technique boosts developer productivity by reducing manual coding and accelerating the software\ndevelopment lifecycle. Early research in code generation mainly centers on deductive and inductive program synthesis\nwhich crafts code based on specification and/or input-output pairs. With the advent of deep learning techniques, natural\nlanguage-based code generation that describes usersâ€™ intent in plain language gains prominence. Besides, there are also\nother works that focus on generating code based on images and structured data. In this paper, we will mainly introduce\ndeductive, inductive, and natural language-based code generation methodologies.\nDeductive and Inductive Program Synthesis. Deductive program synthesis crafts programs from high-level descriptions,\nwhich involves mechanical theorem proving and formal methods [45, 49]. The requirement of detailed specifications\nhelps reduce logical errors. It has diverse applications in fields such as robotics [31] and software engineering [58]. For\nexample, STRIPS [31] is an automatic planner that addresses robot problems, and PROW [144] generates LISP code from\nspecifications in predicate calculus by inco","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2405.02213","id":"https://arxiv.org/pdf/2405.02213","publishedDate":"2024-05-16T00:00:00.000Z","summary":"The paper \"Automatic Programming: Large Language Models and Beyond\" explores the rise of automated code generation driven by Large Language Models (LLMs), exemplified by tools like GitHub Copilot. It discusses critical challenges related to code quality, security, and trustworthiness, emphasizing the importance of ensuring correctness and reliability in automatically generated code. The authors examine how advances in software engineering, such as program repair and analysis, can enhance the trust and safety of auto-generated programs. They highlight organizational shifts towards automated and low-code development platforms, like Oracle's move to generate code in Apex, which can streamline security audits and reduce manual coding efforts. The article underscores the need for mechanisms like automated repair to produce higher assurance code and anticipates a future programming environment where developers may adopt new roles to fully leverage the potential of automatic programming technologies.","title":"Automatic Programming: Large Language Models and Beyond","url":"https://arxiv.org/pdf/2405.02213"},{"content":"Commonly, AI or machine learning (ML) models are evaluated on benchmark datasets. This practice supports innovative methodological research, but benchmark performance can be poorly correlated with performance in real-world applications -- a construct validity issue. To improve the validity and practical usefulness of evaluations, we propose using an estimands framework adapted from international clinical trials guidelines. This framework provides a systematic structure for inference and reporting in evaluations, emphasizing the importance of a well-defined estimation target. We illustrate our proposal on examples of commonly used evaluation methodologies - involving cross-validation, clustering evaluation, and LLM benchmarking - that can lead to incorrect rankings of competing models (rank reversals) with high probability, even when performance differences are large. We demonstrate how the estimands framework can help uncover underlying issues, their causes, and potential solutions. Ultimately, we believe this framework can improve the validity of evaluations through better-aligned inference, and help decision-makers and model users interpret reported results more effectively.\n\nTo read the file of this research,\n\nyou can request a copy directly from the authors.\n\nResearchGate has not been able to resolve any citations for this publication.\n\n- [Yupeng Chang](https://www.researchgate.net/scientific-contributions/Yupeng-Chang-2284287605)\n- [Xu Wang](https://www.researchgate.net/scientific-contributions/Xu-Wang-2255559833)\n- [Jindong Wang](https://www.researchgate.net/profile/Jindong-Wang-6)\n- [Xing Xie](https://www.researchgate.net/scientific-contributions/Xing-Xie-10753387)\n\nLarge language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate , where to evaluate , and how to evaluate . Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the â€˜whereâ€™ and â€˜howâ€™ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.\n\nAdversarial benchmark construction, where harder instances challenge new generations of AI systems, is becoming the norm. While this approach may lead to better machine learning models â€”on average and for the new benchmarkâ€”, it is unclear how these models behave on the original distribution. Two opposing effects are intertwined here. On the one hand, the adversarial benchmark has a higher proportion of difficult instances, with lower expected performance. On the other hand, models trained on the adversarial benchmark may improve on these difficult instances (but may also neglect some easy ones). To disentangle these two effects we can control for difficulty, showing that we can recover the performance on the original distribution, provided the harder instances were obtained from this distribution in the first place. We show this difficulty-aware rectification works in practice, through a series of experiments with several benchmark construction schemas and the use of a populational difficulty metric. As a take-away message, instead of distributional averages we recommend using difficulty-conditioned characteristic curves when evaluating models built with adversarial benchmarks.\n\nAggregate metrics and lack of access to results limit understanding.\n\nIn AI evaluation, performance is often calculated by averaging across various instances. But to fully understand the capabilities of an AI system, we need to understand the factors that cause its pattern of success and failure. In this paper, we present a new methodology to identify and build informative instance features that can provide explanatory and predictive power to analyse the behaviour of AI systems more robustly. The methodology builds on these relevant features that should relate monotonically with success, and represents patterns of performance in a new type of plots known as â€˜agent characteristic gridsâ€™. We illustrate this methodology with the Animal-AI competition as a representative example of how we can revisit existing competitions and benchmarks in AIâ€”even when evaluation data is sparse. Agents with the same average performance can show very different patterns of performance at the instance level. With this methodology, these patterns can be visualised, explained and predicted, progressing towards a capability-oriented evaluation rather than relying on a less informative average performance score.\n\nEmpirical research aims to establish generalizable claims from data. Such claims may involve concepts that must be measured indirectly by using indicators. Construct validity is concerned with whether one can justifiably make claims at the conceptual level that are supported by results at the operational level. We report a quantitative analysis of the awareness of construct validity in the software engineering literature between 2000 and 2019 and a qualitative review of 83 articles about human-centric experiments published in five high-quality journals between 2015 and 2019. Over the two decades, the appearance in the literature of the term construct validity increased sevenfold. Some of the reviewed articles we reviewed employed various ways to ensure that the indicators span the concept in an unbiased manner. We also found articles that reuse formerly validated constructs. However, the articles disagree about how to define construct validity. Several interpret construct validity excessively by including threats to internal, external, or statistical conclusion validity. A few articles also include fundamental challenges of a study, such as cheating and misunderstanding of experiment material. The diversity of topics included as threats to construct validity calls for a more minimalist approach. Based on the review, we propose seven guidelines to improve how construct validity is handled and reported in software engineering.\n\nDeep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models. In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable effort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientific practices in this area.\n\n- [Dana Jayne Linnell](https://www.researchgate.net/profile/Dana-Linnell)\n\nWith a lack of consensus of what evaluation is within the field of evaluation, there is a difficulty in communicating to nonevaluators what evaluation is and how evaluation differs from research. To understand how evaluation is defined, both evaluators and researchers were asked how they defined evaluation and, if at all, differentiated evaluation from research. Overall, evaluators believed research and evaluation intersect, whereas researchers believed evaluation is a subcomponent of research. Furthermore, evaluators perceived greater differences between evaluation and research than researchers did, particularly in characteristics relevant at the beginning (e.g., purpose, questions, audience) and end (e.g., rendering value judgments, disseminating results) of studies. This study suggests that greater consensus on a definition of evaluation is needed to be able to distinguish the field and discipline of evaluation from related fields and to be able to communicate this information to nonevaluators.\n\nOne of the most critical tasks for improving data quality and increasing the reliability of data analytics is Entity Resolution (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F381484611_Improving_the_Validity_and_Practical_Usefulness_of_AIML_Evaluations_Using_an_Estimands_Framework","id":"https://www.researchgate.net/publication/381484611_Improving_the_Validity_and_Practical_Usefulness_of_AIML_Evaluations_Using_an_Estimands_Framework","publishedDate":"2024-06-14T00:00:00.000Z","summary":"This webpage discusses methods to enhance the validity and practical relevance of AI/ML model evaluations. It introduces an estimands framework adapted from clinical trials to systematically define and report evaluation targets, addressing issues like construct validity and rank reversals observed in common practices such as cross-validation, clustering, and LLM benchmarking. The framework helps identify underlying problems, ensuring more aligned inference and better interpretability of results. Additionally, the content emphasizes the importance of comprehensive LLM evaluation across various tasksâ€”ranging from NLP and reasoning to ethics and societal impactâ€”and highlights challenges like adversarial benchmarking, which tests models on harder instances but raises questions about their performance on original data distributions. Overall, the webpage advocates for rigorous, well-structured evaluation methodologies to improve the reliability and usefulness of AI/ML assessments.","title":"Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework","url":"https://www.researchgate.net/publication/381484611_Improving_the_Validity_and_Practical_Usefulness_of_AIML_Evaluations_Using_an_Estimands_Framework"},{"content":"[Skip to main content](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#main-content)\n\n**Official websites use .gov**\nA\n**.gov** website belongs to an official\ngovernment organization in the United States.\n\n**Secure .gov websites use HTTPS**\nA **lock** (\n\nLocked padlock icon\n) or **https://** means you've safely\nconnected to the .gov website. Share sensitive\ninformation only on official, secure websites.\n\nSearch PMC Full-Text ArchiveSearch in PMC\n\n- [Journal List](https://pmc.ncbi.nlm.nih.gov/journals/)\n- [User Guide](https://pmc.ncbi.nlm.nih.gov/about/userguide/)\n\nNewTry this search in PMC Beta Search\n\n- ## PERMALINK\n\n\n\nCopy\n\n\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with,\nthe contents by NLM or the National Institutes of Health.\nLearn more:\n[PMC Disclaimer](https://pmc.ncbi.nlm.nih.gov/about/disclaimer/)\n\\|\n[PMC Copyright Notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n\nEpidemiology\n\n. 2020 Apr 2;31(3):376â€“384. doi: [10.1097/EDE.0000000000001161](https://doi.org/10.1097/EDE.0000000000001161)\n\n# A Graphical Catalog of Threats to Validity\n\nLinking Social Science with Epidemiology\n\n[Ellicott C Matthay](https://pubmed.ncbi.nlm.nih.gov/?term=%22Matthay%20EC%22%5BAuthor%5D)\n\n### Ellicott C Matthay\n\n1From the Center for Health and Community, University of California, San Francisco, CA.\n\nFind articles by [Ellicott C Matthay](https://pubmed.ncbi.nlm.nih.gov/?term=%22Matthay%20EC%22%5BAuthor%5D)\n\n1, [M Maria Glymour](https://pubmed.ncbi.nlm.nih.gov/?term=%22Glymour%20MM%22%5BAuthor%5D)\n\n### M Maria Glymour\n\n1From the Center for Health and Community, University of California, San Francisco, CA.\n\nFind articles by [M Maria Glymour](https://pubmed.ncbi.nlm.nih.gov/?term=%22Glymour%20MM%22%5BAuthor%5D)\n\n1,âœ‰\n\n- Author information\n- Article notes\n- Copyright and License information\n\n1From the Center for Health and Community, University of California, San Francisco, CA.\n\nâœ‰\n\nCorrespondence: M. Maria Glymour, 550 16th Street, 2nd floor, University of California, San Francisco, San Francisco, CA 94143. E-mail: Maria.glymour@ucsf.edu.\n\nâœ‰\n\nCorresponding author.\n\nReceived 2019 Apr 17; Accepted 2020 Jan 15; Issue date 2020 May.\n\nCopyright Â© 2020 The Author(s). Published by Wolters Kluwer Health, Inc.\n\nThis is an open-access article distributed under the terms of the [Creative Commons Attribution-Non Commercial-No Derivatives License 4.0 (CCBY-NC-ND)](http://creativecommons.org/licenses/by-nc-nd/4.0/), where it is permissible to download and share the work provided it is properly cited. The work cannot be changed in any way or used commercially without permission from the journal.\n\n[PMC Copyright notice](https://pmc.ncbi.nlm.nih.gov/about/copyright/)\n\nPMCID: PMC7144753Â Â PMID: [31977593](https://pubmed.ncbi.nlm.nih.gov/31977593/)\n\nSupplemental Digital Content is available in the text.\n\n**Keywords:** Causal inference, Directed acyclic graph, Epidemiologic methods, Quasi-experimental designs, The Campbell tradition, Threats to validity\n\n## Abstract\n\nDirected acyclic graphs (DAGs), a prominent tool for expressing assumptions in epidemiologic research, are most useful when the hypothetical data generating structure is correctly encoded. Understanding a studyâ€™s data generating structure and translating that data structure into a DAG can be challenging, but these skills are often glossed over in training. Campbell and Stanleyâ€™s framework for causal inference has been extraordinarily influential in social science training programs but has received less attention in epidemiology. Their work, along with subsequent revisions and enhancements based on practical experience conducting empirical studies, presents a catalog of 37 threats to validity describing reasons empirical studies may fail to deliver causal effects. We interpret most of these threats to study validity as suggestions for common causal structures. Threats are organized into issues of statistical conclusion validity, internal validity, construct validity, or external validity. To assist epidemiologists in drawing the correct DAG for their application, we map the correspondence between threats to validity and epidemiologic concepts that can be represented with DAGs. Representing these threats as DAGs makes them amenable to formal analysis with d-separation rules and breaks down cross-disciplinary language barriers in communicating methodologic issues.\n\nDirected acyclic graphs (DAGs) have rapidly gained popularity among epidemiologists. DAGs are a particularly useful tool for causal inference because, when one has correctly specified the hypothetical DAG to encode prior knowledge of the data generating structure, a set of mathematics-based rules can be applied to determine whether and how the causal effect of interest can be identifiedâ€”for example, which confounders need to be controlled. However, the DAG framework is entirely silent on what the causal model should look like, and epidemiologists often struggle to tie their methodologic training in DAGs to specific, real-world settings.\n\nNearly 50 years ago, Campbell and Stanley[1](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R1) presented a catalog of threats to validity, describing reasons empirical studies may fail to deliver causal effects. Their work, along with subsequent revisions[2](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R2), [3](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R3) (hereafter, â€œthe Campbell traditionâ€), has been extraordinarily influential in social science training programs,[4](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R4), [5](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R5) but has received less attention in epidemiology. The framework delineates four types of validityâ€”internal, statistical conclusion, construct, and external (Box 1). The Campbell tradition guides researchers to assess alternative explanations for an association besides the causal relation of interest (â€œthreats to validityâ€) when evaluating evidence from a specific study design and analysis, and to incorporate design or analysis featuresâ€”for example, randomization or maskingâ€”to diminish the influence of such threats. The Campbell traditionâ€™s most recent set of 37 threats to validity are based on decades of empirical research experience. They are verbal descriptions of biases that can arise in epidemiologic research. Most can be considered possible causal structures and represented in a DAG. Epidemiologic research and, in particular, the challenging task of correctly drawing oneâ€™s DAG, could be enhanced by considering these threats. However, to our knowledge, no comprehensive crosswalk between the Campbell traditionâ€™s named biases and DAGs has been published. The DAG for threats deemed plausible in the given study context can be incorporated into the investigatorâ€™s DAG to provide insight into the problem and to determine effective analytic solutions.\n\n## BOX 1. SHADISH, COOK, AND CAMPBELLâ€™S VALIDITY TYPOLOGY.\n\n1. Statistical conclusion validity: appropriate use of statistical methods to assess the relationships among study variables;\n\n2. Internal validity: the extent to which the estimated association in the study sample corresponds to a causal effect from exposure to outcome;\n\n3. Construct validity: the extent to which measured variables capture the concepts the investigator intends to assess with those measures; and\n\n4. External validity: the extent to which study results can be generalized to other units, treatments, observations made on units, and settings of study conduct.\n\n\nWe define the Campbell traditionâ€™s named threats to validity. For each threat, we provide the epidemiologic analog, a corresponding DAG, and one or more examples, to illustrate how they might inform the epidemiologistâ€™s DAGs. We aim to enhance epidemiologic research by facilitating the use of cross-disciplinary concepts to inform DAG specification and to facilitate cross-disciplinary communication by using DAGs as common language to understand biases in causal research.\n\n## REPRESENTING THREATS TO VALIDITY AS DIRECTED ACYCLIC GRAPHS\n\nDAGs are causal models that visually represent background knowledge and assumptions about the relationships between variables.[6](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R6) They encode the hypothesized data generating mechanisms and can include features of the study design such as instruments and study implementation such as measurement. DAGs are interpreted with mathematics-based rules that provide a flexible but rigorous method for determining sets of variables that, when measured and adjusted appropriately, are sufficient to control confounding and identify causal effects. Box 2 presents an introduction to key concepts of DAGs and notation used in this article. For a more detailed introduction, we refer the reader to Glymour and Greenland.[7](https://pmc.ncbi.nlm.nih.gov/pmc.ncbi.nlm.nih.gov#R7)\n\n### BOX 2. INTRODUCTION TO KEY CONCEPTS OF DIRECTED ACYCLIC GRAPHS.\n\nDAGs are comprised of variables (â€œnodesâ€) and directed arrows (â€œedgesâ€) that indicate potential direct causal effects of one node on another. A â€œpathâ€ is a sequence of nodes following edges, not necessarily in the indicated direction, from one node in the graph to another. DAGs are â€œacyclic,â€ meaning no directed path leads back to the same node. Direct and indirect effects of a node are referred to as its â€œdescendants.â€ A â€œbackdoor pathâ€ is a path connecting the outcome and exposure but with an arrow pointing into the exposure. â€œCollidersâ€ on a path are nodes with at least two directed arrows pointing into them from other nodes on the path. Paths are â€œblockedâ€ by conditioning on a proposed covariate set if either (1) one or more nodes on the path are in the covariate set or (2) the path contains at least one collider and neither the collider nor any of its descendants are in the covariate set. The â€œbackdoor criterionâ€ can be used to ","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpmc.ncbi.nlm.nih.gov%2Farticles%2FPMC7144753%2F","id":"https://pmc.ncbi.nlm.nih.gov/articles/PMC7144753/","publishedDate":"2020-04-02T00:00:00.000Z","summary":"This article introduces a comprehensive graphical catalog of threats to validity in causal research, linking social science frameworks with epidemiological methods. It emphasizes the importance of correctly modeling data-generating processes using Directed Acyclic Graphs (DAGs) to accurately infer causal effects. The authors, rooted in both epidemiology and social science, adapt Campbell and Stanleyâ€™s influential framework to identify 37 specific threats that can compromise study validity. These threats are organized around common causal structures and highlight issues like confounding, selection bias, measurement error, and other biases that can distort causal inference. The paper aims to improve researchersâ€™ understanding of potential pitfalls in causal studies and enhance the design and interpretation of epidemiologic research by integrating graphical tools and social science insights.","title":"A Graphical Catalog of Threats to Validity - PubMed Central","url":"https://pmc.ncbi.nlm.nih.gov/articles/PMC7144753/"},{"content":"Naoufal Acharki\nRamiro Lugo\nAntoine Bertoncello\nJosselin Garnier\nComparison of meta-learners for estimating multi-valued treatment heterogeneous effects\nConditional Average Treatment Effects (CATE) estimation is one of the main challenges in causal inference with observational data. In addition to Machine Learning based-models, nonparametric estimators called meta-learners have been developed to estimate the CATE with the main advantage of not restraining the estimation to a specific supervised learning method. This task becomes, however, more complicated when the treatment is not binary as some limitations of the naive extensions emerge. This paper looks into meta-learners for estimating the heterogeneous effects of multivalued treatments. We consider different metalearners, and we carry out a theoretical analysis of their error upper bounds as functions of important parameters such as the number of treatment levels, showing that the naive extensions do not always provide satisfactory results. We introduce and discuss meta-learners that perform well as the number of treatments increases. We empirically confirm the strengths and weaknesses of those methods with synthetic and semi-synthetic datasets.\nIntroduction\nWith the rapid development of Machine Learning (ML) and its efficiency in predicting outcomes, the question of counterfactual prediction \"what would happen if?\" arises. Engineers and specialists want to know how the outcome would be affected after an intervention on a parameter. It will help them personalize the parameter at efficient levels and optimize the outcome. Recently, much effort has been devoted to supervised ML to find the optimal intervention strategy. Yet, the results are not always convincing. These models cannot distinguish between correlations and causal relationships in the data (Pearl, 2019).\nBased on the Potential Outcomes theory (Neyman, 1923;Rubin, 1974), epidemiologists and statisticians have developed a set of tools that reduce the inference of causal effects to statistical inference under certain assumptions about, for example, the data-generating process. They have been successfully applied in many fields such as medicine (Alaa & van der Schaar, 2017), economics (Knaus et al., 2020b), public policy (Imai & Strauss, 2011) and marketing (Diemert et al., 2018) to infer causal effects. However, most of these studies are limited to a binary treatment setting whereas many causal questions in real-world cases are not binary. It would be helpful to give an in-depth analysis of the impact of the treatment across its possible levels instead of just considering a binary scenario where the treatment is either assigned or not. In addition, the heterogeneity of effects may provide valuable information regarding this treatment's effectiveness and help users personalize their intervention policies and strategies.\nThe problem of Causal Inference beyond binary treatment settings is gaining attention from the Causal Inference community. There are two major challenges: Firstly, the lack of the ground truth due to the fundamental problem of causal inference (Holland, 1986) makes Heterogeneous Treatment Effects estimation more challenging (Alaa & van der Schaar, 2018) as standard metrics can not be used to assess performances. Secondly, binarizing the multi-valued treatment setting leads to a violation of the Stable Unit Treatment Value Assumption (SUTVA) as it violates the principle of \"no hidden variations of treatment\". It may yield a bias known as position bias in recommendation systems (Chen et al., 2020;Wu et al., 2022). It happens when some units tend to have/select high treatment values and, therefore, there is a hidden variation of the treatment that is not taken into consideration when one attempt to binarize the treatment. From a statistical point of view, this bias was established by Heiler & Knaus (2021) who show that the binarization of multi-valued treatments does not disassociate the heterogeneity of the treatment from the heterogeneity of the effects of each value.\nThe extension of the binary setting does not seem trivial as several versions are possible and turn out to have different behaviours. Moreover, -to the best of our knowledgethere is no result so far on the impact of the number of arXiv:2205.14714v3 [stat.ML] 3 Jun 2023 possible treatments on the performances of heterogeneous treatment effect (nonparametric or ML-based) estimators.\nIn this paper, we study the problem of nonparametric estimation of Heterogeneous Treatment Effects, also known as Conditional Average Treatment Effects (CATEs), for multivalued treatments. We consider nonparametric estimators, also referred to as meta-learners (KÃ¼nzel et al., 2019) or model-agnostic algorithms (Curth & van der Scha","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fexport.arxiv.org%2Fpdf%2F2205.14714v3.pdf","id":"https://export.arxiv.org/pdf/2205.14714v3.pdf","publishedDate":"2023-06-06T00:00:00.000Z","summary":"This paper evaluates methods for estimating heterogeneous causal effects when treatments have multiple levels, moving beyond traditional binary treatment models. It reviews various meta-learnersâ€”model-agnostic algorithms designed to infer Conditional Average Treatment Effects (CATE)â€”and analyzes their theoretical error bounds as the number of treatment levels increases. The authors highlight that naive extensions of binary treatment methods often underperform in multi-valued contexts, especially as the number of treatment options grows. They introduce and discuss advanced meta-learners that maintain effectiveness in high-treatment scenarios. Empirical tests on synthetic and semi-synthetic datasets demonstrate the strengths and limitations of these approaches. The study emphasizes the importance of accurately modeling multi-valued treatments to better understand treatment heterogeneity, avoid biases from treatment binarization, and improve causal inference in real-world applications such as medicine, economics, and policy.","title":"Comparison of meta-learners for estimating multi-valued treatment heterogeneous effects","url":"https://export.arxiv.org/pdf/2205.14714v3.pdf"},{"content":"License: arXiv.org perpetual non-exclusive license\n\narXiv:2403.13430v1 \\[cs.CV\\] 20 Mar 2024\n\n# MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining\n\nDi Wang,\nJing Zhang,\nMinqiang Xu,\nLin Liu,\nDongsheng Wang,\n\nErzhong Gao,\nChengxi Han,\nHaonan Guo,\nBo Du,\nDacheng Tao,\nand Liangpei Zhang\nD. Wang and B. Du are with the School of Computer Science, Wuhan University, Wuhan 430072, China, also with the Institute of Artificial Intelligence, Wuhan University, Wuhan 430072, China, also with the National Engineering Research Center for Multimedia Software, Wuhan University, Wuhan 430072, China, and also with the Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan 430072, China (e-mail: wd74108520@gmail.com; dubo@whu.edu.cn). (Corresponding author: Minqiang Xu, Jing Zhang, Bo Du and Liangpei Zhang.)J. Zhang is with the School of Computer Science, Faculty of Engineering, The University of Sydney, Australia (e-mail: jing.zhang1@sydney.edu.au).M. Xu, L. Liu, D. Wang and E. Gao are with the iFlytek Co., Ltd and also with the National Engineering Research Center of Speech and Language Information Processing, Hefei 230088, China (e-mail: mqxu7@iflytek.com; linliu@iflytek.com; dswang7@iflytek.com; ezgao@iflytek.com).C. Han, H. Guo and L. Zhang are with the State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan 430079, China (e-mail: chengxihan@whu.edu.cn; haonan.guo@whu.edu.cn; zlp62@whu.edu.cn).D. Tao is with the School of Computer Science and Engineering, Nanyang Technological University, Singapore (e-mail: dacheng.tao@gmail.com).\n\n###### Abstract\n\nFoundation models have reshaped the landscape of Remote Sensing (RS) by enhancing various image interpretation tasks. Pretraining is an active research topic, encompassing supervised and self-supervised learning methods to initialize model weights effectively. However, transferring the pretrained models to downstream tasks may encounter task discrepancy due to their formulation of pretraining as image classification or object discrimination tasks. In this study, we explore the Multi-Task Pretraining (MTP) paradigm for RS foundation models to address this issue. Using a shared encoder and task-specific decoder architecture, we conduct multi-task supervised pretraining on the SAMRS dataset, encompassing semantic segmentation, instance segmentation, and rotated object detection. MTP supports both convolutional neural networks and vision transformer foundation models with over 300 million parameters. The pretrained models are finetuned on various RS downstream tasks, such as scene classification, horizontal and rotated object detection, semantic segmentation, and change detection. Extensive experiments across 14 datasets demonstrate the superiority of our models over existing ones of similar size and their competitive performance compared to larger state-of-the-art models, thus validating the effectiveness of MTP. The codes and pretrained models will be released at https://github.com/ViTAE-Transformer/MTP.\n\n###### Index Terms:\n\nRemote sensing, Foundation model, Multi-task pretraining, Scene classification, Semantic segmentation, Object detection, Change detection.\n\n## I Introduction\n\nRemote sensing (RS) image is one of the most important data resources for recording ground surfaces and land objects. Precisely understanding RS images is beneficial to many applications, including urban planning \\[ [1](https://arxiv.org/html/2403.13430v1#bib.bib1)\\], environmental survey \\[ [2](https://arxiv.org/html/2403.13430v1#bib.bib2)\\], disaster assessment \\[ [3](https://arxiv.org/html/2403.13430v1#bib.bib3)\\], etc.\n\nUtilizing its inherent capability to automatically learn and extract deep features from objects, deep learning methods have found widespread application in the RS domain, particularly for tasks such as scene classification, land use and land cover classification, and ship detection. Typically, ImageNet pretrained weights are employed in training deep networks for RS tasks due to their extensive representational ability. However, these weights are derived from pretraining models on natural images, leading to domain gaps between natural images and RS images. For instance, RS images are captured from a birdâ€™s-eye view, lack the vibrant colors of natural images, and possess lower spatial resolution. These disparities may impede the modelâ€™s finetuning performance \\[ [4](https://arxiv.org/html/2403.13430v1#bib.bib4), [5](https://arxiv.org/html/2403.13430v1#bib.bib5)\\]. Moreover, relying solely on limited task-specific data for training restricts the model size and generalization capability of current RS deep models due to the notorious overfitting issue.\n\nTo tackle these challenges, the development of RS vision foundation models is imperative, which should excel in extracting representative RS features. However, the RS domain has long grappled with a scarcity of adequately large annotated datasets, impeding related investigations. Until recently, the most expansive RS scene labeling datasets were fMoW \\[ [6](https://arxiv.org/html/2403.13430v1#bib.bib6)\\] and BigEarthNet \\[ [7](https://arxiv.org/html/2403.13430v1#bib.bib7)\\], boasting 132,716 and 590,326 unique scene instances \\[ [8](https://arxiv.org/html/2403.13430v1#bib.bib8)\\], respectively â€” yet still falling short of benchmarks set by natural image datasets like ImageNet-1K \\[ [9](https://arxiv.org/html/2403.13430v1#bib.bib9)\\]. Long et al. \\[ [8](https://arxiv.org/html/2403.13430v1#bib.bib8)\\] addressed this gap by introducing MillionAID, a large-scale RS scene labeling dataset with a closed sample capacity of 100,0848 compared to ImageNet-1K, igniting interest in supervised RS pretraining \\[ [10](https://arxiv.org/html/2403.13430v1#bib.bib10), [5](https://arxiv.org/html/2403.13430v1#bib.bib5)\\]. These studies show the feasibility of pretraining RS foundation models on large-scale RS datasets. Nonetheless, supervised pretraining of RS foundation models may not be the most preferable choice due to the expertise and substantial time and labor costs associated with labeling RS images.\n\n![Refer to caption](https://arxiv.org/html/2403.13430v1/x1.png)\n\nFigure 1: Comparison of various pretraining methods. (a) \\[ [11](https://arxiv.org/html/2403.13430v1#bib.bib11), [12](https://arxiv.org/html/2403.13430v1#bib.bib12)\\] sequentially pretrains a foundational model on both natural and RS images. (b) \\[ [13](https://arxiv.org/html/2403.13430v1#bib.bib13)\\] employs a two-stage pretraining strategy to initialize task-specific decoders (e.g., segmentation) using existing foundational models pretrained on either natural or RS images, preserving the decoder during subsequent finetuning. We extend (b) by incorporating multi-task decoders to enhance the representation capacity of the foundational model, facilitating easy transferability across diverse tasks during finetuning, as depicted in (c).\n\nConstructing large-scale RS annotation datasets is challenging due to the high complexity and cost of labeling. Despite this challenge, the advancement of earth observation technologies grants easy access to a vast amount of unlabeled RS images. Efficiently leveraging these unlabeled RS images is crucial for developing robust RS foundation models. In the realm of deep learning, unsupervised pretraining has emerged as a promising approach for learning effective knowledge from massive unlabeled data \\[ [14](https://arxiv.org/html/2403.13430v1#bib.bib14), [15](https://arxiv.org/html/2403.13430v1#bib.bib15), [16](https://arxiv.org/html/2403.13430v1#bib.bib16), [17](https://arxiv.org/html/2403.13430v1#bib.bib17)\\]. Typically, unsupervised pretraining employs self-supervised learning (SSL) to learn effective feature representation. SSL encompasses two primary techniques: contrastive-based \\[ [18](https://arxiv.org/html/2403.13430v1#bib.bib18), [19](https://arxiv.org/html/2403.13430v1#bib.bib19), [20](https://arxiv.org/html/2403.13430v1#bib.bib20)\\] and generative-based learning \\[ [21](https://arxiv.org/html/2403.13430v1#bib.bib21), [22](https://arxiv.org/html/2403.13430v1#bib.bib22), [23](https://arxiv.org/html/2403.13430v1#bib.bib23)\\]. Contrastive learning aims to bring similar samples closer while maximizing distances between dissimilar samples through the object discrimination pretext task. When applied to the RS domain, data characteristics like geographic coordinates \\[ [24](https://arxiv.org/html/2403.13430v1#bib.bib24), [25](https://arxiv.org/html/2403.13430v1#bib.bib25), [26](https://arxiv.org/html/2403.13430v1#bib.bib26)\\] and temporal information \\[ [27](https://arxiv.org/html/2403.13430v1#bib.bib27), [28](https://arxiv.org/html/2403.13430v1#bib.bib28), [29](https://arxiv.org/html/2403.13430v1#bib.bib29)\\] are usually leveraged in formulating the pretext task. However, designing these pretext tasks and gathering requisite data can be inefficient, especially for training large-scale models. Generative-based learning, exemplified by masked image modeling (MIM), circumvents this challenge by enhancing network representation through reconstructing masked regions. Many RS studies leverage MIM initialization for its efficiency \\[ [30](https://arxiv.org/html/2403.13430v1#bib.bib30), [31](https://arxiv.org/html/2403.13430v1#bib.bib31), [32](https://arxiv.org/html/2403.13430v1#bib.bib32), [33](https://arxiv.org/html/2403.13430v1#bib.bib33), [34](https://arxiv.org/html/2403.13430v1#bib.bib34), [35](https://arxiv.org/html/2403.13430v1#bib.bib35), [36](https://arxiv.org/html/2403.13430v1#bib.bib36), [37](https://arxiv.org/html/2403.13430v1#bib.bib37)\\]. Recent approaches have attempted to combine contrastive-based and generative-based learning techniques to pretrain more powerful models \\[ [38](https://arxiv.org/html/2403.13430v1#bib.bib38), [39](https://arxiv.org/html/2403.13430v1#bib.bib39), [40](https://arxiv.org/html/2403.13430v1#bib.bib40)\\","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2403.13430v1","id":"https://arxiv.org/html/2403.13430v1","publishedDate":"2024-03-20T00:00:00.000Z","summary":"The paper introduces MTP (Multi-Task Pretraining), a novel approach to advancing remote sensing (RS) foundation models. Unlike traditional pretraining methods focused on single tasks like image classification, MTP employs a shared encoder with task-specific decoders to jointly pretrain on multiple RS tasksâ€”semantic segmentation, instance segmentation, and rotated object detectionâ€”using the SAMRS dataset. This multi-task supervised pretraining enhances the model's ability to transfer effectively to diverse downstream RS applications, including scene classification, object detection (both horizontal and rotated), semantic segmentation, and change detection. The approach supports both CNNs and vision transformers with over 300 million parameters. Extensive experiments across 14 datasets demonstrate that MTP models outperform similar-sized existing models and achieve competitive results compared to larger, state-of-the-art models. The authors plan to release the code and pretrained models, aiming to improve the generalization and efficiency of RS foundation models through multi-task pretraining.","title":"MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining","url":"https://arxiv.org/html/2403.13430v1"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2F2025.splashcon.org%2Fdetails%2FOOPSLA%2F109%2FMeaning-Typed-Programming-Language-Abstraction-and-Runtime-for-Model-Integrated-Appl","id":"https://2025.splashcon.org/details/OOPSLA/109/Meaning-Typed-Programming-Language-Abstraction-and-Runtime-for-Model-Integrated-Appl","publishedDate":null,"summary":"Failed to fetch content","title":"Meaning-Typed Programming: Language Abstraction and Runtime ...","url":"https://2025.splashcon.org/details/OOPSLA/109/Meaning-Typed-Programming-Language-Abstraction-and-Runtime-for-Model-Integrated-Appl"},{"content":"# Computer Science > Programming Languages\n\n**arXiv:2405.08965** (cs)\n\n\\[Submitted on 14 May 2024 ( [v1](https://arxiv.org/abs/2405.08965v1)), last revised 3 Sep 2025 (this version, v5)\\]\n\n# Title:MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\n\nAuthors: [Jayanaka L. Dantanarayana](https://arxiv.org/search/cs?searchtype=author&query=Dantanarayana,+J+L), [Yiping Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang,+Y), [Kugesan Sivasothynathan](https://arxiv.org/search/cs?searchtype=author&query=Sivasothynathan,+K), [Christopher Clarke](https://arxiv.org/search/cs?searchtype=author&query=Clarke,+C), [Baichuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+B), [Savini Kashmira](https://arxiv.org/search/cs?searchtype=author&query=Kashmira,+S), [Krisztian Flautner](https://arxiv.org/search/cs?searchtype=author&query=Flautner,+K), [Lingjia Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+L), [Jason Mars](https://arxiv.org/search/cs?searchtype=author&query=Mars,+J)\n\nView a PDF of the paper titled MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming, by Jayanaka L. Dantanarayana and 8 other authors\n\n[View PDF](https://arxiv.org/pdf/2405.08965) [HTML (experimental)](https://arxiv.org/html/2405.08965v5)\n\n> Abstract:Software development is shifting from traditional programming to AI-integrated applications that leverage generative AI and large language models (LLMs) during runtime. However, integrating LLMs remains complex, requiring developers to manually craft prompts and process outputs. Existing tools attempt to assist with prompt engineering, but often introduce additional complexity. This paper presents Meaning-Typed Programming (MTP), a novel paradigm that abstracts LLM integration through intuitive language-level constructs. By leveraging the inherent semantic richness of code, MTP automates prompt generation and response handling without additional developer effort. We introduce the (1) by operator for seamless LLM invocation, (2) MT-IR, a meaning-based intermediate representation for semantic extraction, and (3) MT-Runtime, an automated system for managing LLM interactions. We implement MTP in Jac, a programming language that supersets Python, and find that MTP significantly reduces coding complexity while maintaining accuracy and efficiency. MTP significantly reduces development complexity, lines of code modifications needed, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches. Our user study shows that developers using MTP completed tasks 3.2x faster with 45% fewer lines of code compared to existing frameworks. Moreover, MTP demonstrates resilience even when up to 50% of naming conventions are degraded, demonstrating robustness to suboptimal code. MTP is developed as part of the Jaseci open-source project, and is available under the module byLLM.\n\n| | |\n| --- | --- |\n| Comments: | OOPSLA 2025 |\n| Subjects: | Programming Languages (cs.PL); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2405.08965](https://arxiv.org/abs/2405.08965) \\[cs.PL\\] |\n| (or [arXiv:2405.08965v5](https://arxiv.org/abs/2405.08965v5) \\[cs.PL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2405.08965](https://doi.org/10.48550/arXiv.2405.08965) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jayanaka L. Dantanarayana \\[ [view email](https://arxiv.org/show-email/13cb6fa0/2405.08965)\\] **[\\[v1\\]](https://arxiv.org/abs/2405.08965v1)**\nTue, 14 May 2024 21:12:01 UTC (623 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2405.08965v2)**\nMon, 14 Oct 2024 21:20:40 UTC (4,649 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2405.08965v3)**\nThu, 16 Jan 2025 18:56:27 UTC (2,918 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2405.08965v4)**\nWed, 4 Jun 2025 00:51:13 UTC (3,247 KB)\n**\\[v5\\]**\nWed, 3 Sep 2025 21:19:19 UTC (3,661 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming, by Jayanaka L. Dantanarayana and 8 other authors\n\n- [View PDF](https://arxiv.org/pdf/2405.08965)\n- [HTML (experimental)](https://arxiv.org/html/2405.08965v5)\n- [TeX Source](https://arxiv.org/src/2405.08965)\n- [Other Formats](https://arxiv.org/format/2405.08965)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.PL\n\n[<Â prev](https://arxiv.org/prevnext?id=2405.08965&function=prev&context=cs.PL)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2405.08965&function=next&context=cs.PL)\n\n[new](https://arxiv.org/list/cs.PL/new) \\| [recent](https://arxiv.org/list/cs.PL/recent) \\| [2024-05](https://arxiv.org/list/cs.PL/2024-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2405.08965?context=cs) [cs.AI](https://arxiv.org/abs/2405.08965?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2405.08965)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2405.08965)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2405.08965)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2405.08965) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2405.08965","id":"https://arxiv.org/abs/2405.08965","publishedDate":"2024-05-14T00:00:00.000Z","summary":"The paper introduces Meaning-Typed Programming (MTP), a novel paradigm designed to simplify the integration of large language models (LLMs) into software development. As AI-driven applications become prevalent, developers face challenges in manually crafting prompts and handling LLM responses. MTP addresses this by providing language-level abstractions that leverage the semantic richness of code to automate prompt generation and response processing. Key features include the 'by' operator for seamless LLM invocation, MT-IR for semantic extraction via a meaning-based intermediate representation, and MT-Runtime for managing LLM interactions automatically. Implemented in Jac, a superset of Python, MTP significantly reduces coding complexity, lines of code, and costs while maintaining or improving accuracy and runtime performance. User studies demonstrate that MTP enables developers to complete tasks 3.2 times faster with 45% fewer lines of code, and it remains robust even when naming conventions are partially degraded. Developed as part of the open-source Jaseci project, MTP offers a promising approach to making AI integration more accessible, efficient, and resilient in modern software development.","title":"MTP: A Meaning-Typed Language Abstraction for AI-Integrated ...","url":"https://arxiv.org/abs/2405.08965"},{"content":"Now on home page\n\n## ADS\n\n## MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\n\n- [Dantanarayana, Jayanaka L.](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Dantanarayana%2C+Jayanaka+L.%22);\n- [Kang, Yiping](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Kang%2C+Yiping%22);\n- [Sivasothynathan, Kugesan](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Sivasothynathan%2C+Kugesan%22);\n- [Clarke, Christopher](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Clarke%2C+Christopher%22);\n- [Li, Baichuan](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Li%2C+Baichuan%22);\n- [Kashmira, Savini](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Kashmira%2C+Savini%22);\n- [Flautner, Krisztian](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Flautner%2C+Krisztian%22);\n- [Tang, Lingjia](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Tang%2C+Lingjia%22);\n- [Mars, Jason](https://ui.adsabs.harvard.edu/search/?q=author%3A%22Mars%2C+Jason%22)\n\n#### Abstract\n\nSoftware development is shifting from traditional programming to AI-integrated applications that leverage generative AI and large language models (LLMs) during runtime. However, integrating LLMs remains complex, requiring developers to manually craft prompts and process outputs. Existing tools attempt to assist with prompt engineering, but often introduce additional complexity. This paper presents Meaning-Typed Programming (MTP), a novel paradigm that abstracts LLM integration through intuitive language-level constructs. By leveraging the inherent semantic richness of code, MTP automates prompt generation and response handling without additional developer effort. We introduce the (1) by operator for seamless LLM invocation, (2) MT-IR, a meaning-based intermediate representation for semantic extraction, and (3) MT-Runtime, an automated system for managing LLM interactions. We implement MTP in Jac, a programming language that supersets Python, and find that MTP significantly reduces coding complexity while maintaining accuracy and efficiency. MTP significantly reduces development complexity, lines of code modifications needed, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches. Our user study shows that developers using MTP completed tasks 3.2x faster with 45% fewer lines of code compared to existing frameworks. Moreover, MTP demonstrates resilience even when up to 50% of naming conventions are degraded, demonstrating robustness to suboptimal code. MTP is developed as part of the Jaseci open-source project, and is available under the module byLLM.\n\nPublication:\n\narXiv e-prints\n\nPub Date:May 2024DOI:\n\n[10.48550/arXiv.2405.08965](https://ui.adsabs.harvard.edu/link_gateway/2024arXiv240508965D/doi:10.48550/arXiv.2405.08965)\n\narXiv:[arXiv:2405.08965](https://ui.adsabs.harvard.edu/link_gateway/2024arXiv240508965D/arXiv:2405.08965)Bibcode:[2024arXiv240508965D](https://ui.adsabs.harvard.edu/abs/2024arXiv240508965D/abstract)Keywords:\n\n- Programming Languages;\n- Artificial Intelligence\n\nE-Print:OOPSLA 2025\n\nfull text sources\n\nPreprint\n\n\\|\n\nðŸŒ“","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fui.adsabs.harvard.edu%2Fabs%2F2024arXiv240508965M%2Fabstract","id":"https://ui.adsabs.harvard.edu/abs/2024arXiv240508965M/abstract","publishedDate":null,"summary":"The paper introduces Meaning-Typed Programming (MTP), a novel language-level abstraction designed to simplify AI-integrated software development, especially with large language models (LLMs). MTP automates prompt generation and response handling through intuitive constructs, reducing developer effort and complexity. Implemented as an extension of Python within the Jac language, MTP features key components: the 'by' operator for LLM invocation, MT-IR for semantic representation, and MT-Runtime for managing LLM interactions. Evaluation shows MTP significantly cuts coding lines, reduces costs, and improves runtime performance while maintaining high accuracy. User studies indicate developers complete tasks over three times faster with 45% fewer lines of code, demonstrating robustness even with degraded naming conventions. Overall, MTP advances AI application development by offering a semantically rich, efficient, and developer-friendly framework, and is open-source via the Jaseci project.","title":"Meaning-Typed Programming: Language-level Abstractions and ...","url":"https://ui.adsabs.harvard.edu/abs/2024arXiv240508965M/abstract"}],"status":"completed","toolCallId":"call_23ZDLDTCdUSdzlO5TvDp4m6Q","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Here are the main limitations and caveats the MTP paper itself highlights or implies, grouped by theme. Iâ€™ve kept the wording faithful to the paperâ€™s claims and evaluation notes.\n\nDependence on code semantics\n- Quality of names/docs matters: MTP relies on semantic signals (function/variable names, docstrings, type names, field descriptions). While their experiments show robustness even with degraded naming (they report tolerating up to ~50% degradation), extremely sparse or generic naming and missing docs reduce intent clarity and can hurt results.\n- Types as contracts: If return types are vague (e.g., plain str when a structured object would be better), the runtime has less leverage to constrain outputs, increasing variability.\n\nModel capabilities and nondeterminism\n- Model-bound ceilings: MTPâ€™s effectiveness scales with the underlying LLM. On weaker or highly latency- or cost-constrained models, accuracy can drop, and more retries/repairs may be needed.\n- Variability isnâ€™t eliminated: Schema-first decoding and repair narrow the spread, but cannot fully remove nondeterminism for ill-posed or borderline tasks.\n\nTask fit and expressivity\n- Best for typed, structure-friendly tasks: extraction, classification, transformation, schema-constrained generation. For tasks that truly require open-ended prose or multi-turn, agentic reasoning with bespoke control, MTP provides less advantage or may need complementary orchestration.\n- Ambiguous intent: When the functionâ€™s contract underspecifies behavior (fuzzy requirements, missing constraints), MT-Runtime may need clarifications or will fail after bounded retries.\n\nContext selection and scale\n- IR slicing limits: MT-IR must select a â€œminimal sufficientâ€ slice to keep tokens and latency in check. In very large codebases, picking the right neighboring context efficiently is nontrivial; overly small slices can omit useful signals, while large slices increase cost.\n- Cross-module semantics: If meaning is scattered across distant modules/tests not indexed or linked, the automatic context may miss helpful examples unless configured.\n\nValidation, repair, and failure handling\n- Not all mismatches are repairable: The runtime can coerce simple type mismatches and run targeted repair prompts, but deeply incorrect or contradictory outputs will surface typed errors after retries.\n- Constraint expressivity: The quality of validation depends on the precision of the declared types and constraints; complex invariants beyond the type system may need custom validators or guards.\n\nRuntime, ops, and governance\n- Overheads and tuning: Although prompts tend to be concise, thereâ€™s still runtime overhead for validation, repairs, caching, logging, and possible tool mediation. Teams must tune retry limits, coercion rules, and caching to meet SLOs/cost targets.\n- Safety and policy scope: MTP provides guardrails and policies, but domain-specific compliance/safety requirements may require additional, external enforcement.\n\nPortability and adoption\n- Language integration: The reference implementation is in Jac (a Python superset) via the byLLM module. Using MTP as-is may require adopting Jac or waiting for similar constructs in other ecosystems; embedding the same abstractions into mainstream languages would need additional compiler/runtime work.\n- Learning curve is shifted, not erased: You donâ€™t learn a prompting framework, but teams do need to learn how to write â€œmeaning-richâ€ typed interfaces, configure the runtime, and read IR-aligned traces.\n\nEvaluation scope and external validity\n- Reported robustness and speedups are task- and setup-dependent: The paperâ€™s results (e.g., 3.2x faster dev time, 45% fewer LOC, resilience to 50% naming degradation) come from specific tasks, users, and models. Different domains, code quality, and model choices may change outcomes.\n- Less-is-more trend has boundaries: Their observation that shorter, semantics-driven prompts work better as models improve is empirical, not a guarantee across all models or future architectures.\n\nWhen to think twice\n- Ultra-low-latency paths with strict determinism\n- Tasks lacking stable schemas/contracts\n- Highly specialized multi-step agent workflows where you want explicit, custom control logic\n- Codebases with very weak or inconsistent naming/types and little documentation\n\nIf you share your target domain and stack, I can suggest concrete mitigations (e.g., type refinements, validator hooks, IR-slice tuning, or when to fall back to hand-written bodies) to work around these limits.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_089688494fa2aaa60068c86d404d9881a29527eac2ff79bada"}},"resumableStreamId":"STREAM:80e4aad6-77dd-46cb-af66-159fb39b3b9e","role":"assistant","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","timeToFirstToken":1.687,"tokens":891,"tokensPerSecond":23.776485029620535,"updated_at":1757965615813,"userId":"google:104458561188320708185","id":"j97f3th0ayscamqe4y0c71pe7x7qnbxs"},{"_creationTime":1757965679919.4348,"_id":"j975n7kdcdrgcjm63v2w0fgvk97qmkpm","attachmentIds":[],"content":"what do reviewers think about this paper","created_at":1757965679919,"messageId":"56c5562e-494b-41de-834d-3da8cfebd76e","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","updated_at":1757965679919,"userId":"google:104458561188320708185","id":"j975n7kdcdrgcjm63v2w0fgvk97qmkpm","providerMetadata":{}},{"_creationTime":1757965679919.435,"_id":"j973w1xz48hkn5d1n8va5x6wyn7qnaa5","attachmentIds":[],"byok":false,"content":"Short answer: there isnâ€™t a public, formal set of peer reviews available for this paper. Itâ€™s an arXiv preprint (now tagged â€œOOPSLA 2025â€ on arXiv), but OOPSLA/ PACMPL reviews are not publicly posted. I couldnâ€™t find official PC reviews or an OpenReview thread.\n\nWhat I can report with confidence\n- Status: The arXiv page notes â€œComments: OOPSLA 2025,â€ which implies acceptance to the OOPSLA 2025 issue of PACMPL. Those reviews are private.\n- Public discussion: I donâ€™t see a public review thread, blog post, or HN/Reddit discussion dedicated to this paper that summarizes reviewer opinions.\n\nCommon points reviewers likely focused on (inference based on typical OOPSLA criteria and whatâ€™s in the paper)\n- Novelty and clarity of abstraction: Treating LLM integration as a language feature (the by operator) plus a meaning-typed IR is a clean, language-centric alternative to framework-based prompting. This is likely seen as a strong conceptual contribution if clearly formalized.\n- Technical substance: Reviewers usually ask whether MT-IR is sufficiently specified (whatâ€™s captured, how itâ€™s constructed, how it selects context) and whether MT-Runtimeâ€™s validation/repair pipeline is principled and reproducible.\n- Empirical evaluation: The paper reports developer-speed gains (3.2x), fewer LOC, robustness to degraded naming, and comparable or better accuracy/cost. Reviewers often probe:\n  - Task selection and external validity (are tasks representative?),\n  - Baselines (which frameworks, versions, configs?),\n  - Ablations (impact of types, docs, naming, retry policies),\n  - Sensitivity to model choice and settings.\n- Limitations and scope: Theyâ€™d expect a frank discussion that MTP shines on typed, schema-friendly tasks, and less so on open-ended, multi-step agent workflows or ultra-low-latency paths.\n- Reproducibility and availability: Implemented in Jac (byLLM in Jaseci). Reviewers typically want artifacts, instructions, and dataset/task details to reproduce results.\n\nIf you want, I can:\n- Monitor for a camera-ready PDF or artifact appendix that may include a rebuttal summary.\n- Reach out for public talks/slides where authors often summarize reviewer feedback post-acceptance.\n- Summarize likely strengths/concerns for a paper club discussion.","created_at":1757965679920,"messageId":"ac337426-bb2c-4e0a-9b2c-cba5f632a838","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"news","domains":null,"queries":["\"MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\" reviews OOPSLA 2025","arXiv 2405.08965 reviews discussion","Meaning-Typed Programming MTP reviews Hacker News","byLLM Jac MTP paper discussion"]},"result":[{"content":"[SPLASH 2025](https://2025.splashcon.org)\n\nSun 12 - Sat 18 October 2025 [Singapore](https://2025.splashcon.org/venue/ecbs-2024-venue1)\n\nco-located with [ICFP/SPLASH 2025](https://conf.researchr.org/home/icfp-splash-2025)\n\n### SPLASH 2025\n\n##### Singapore\n\n### Gardens by the Bay\n\n### Marina Bay and Merlion\n\n### Lau Pa Sat\n\n##### Hawker Centre\n\n### ArtScience Museum\n\n### Singapore Botanic Gardens\n\n### The Rain Vortex at Jewel Changi Airport\n\n### Little India\n\n### Flower Dome\n\n### Resorts World Sentosa\n\n### National University of Singapore\n\n[ICFP/SPLASH 2025](https://conf.researchr.org/home/icfp-splash-2025) ( [series](https://2025.splashcon.org/series/icfp-splash)) / [SPLASH 2025](https://2025.splashcon.org) ( [series](https://2025.splashcon.org/series/splash)) /\n\n# OOPSLASPLASH 2025\n\n- [Program](https://2025.splashcon.org/2025.splashcon.org#program)\n- [Accepted Papers](https://2025.splashcon.org/2025.splashcon.org#event-overview)\n- [Call for Papers](https://2025.splashcon.org/2025.splashcon.org#Call-for-Papers)\n\n**Dates**\n\n**Tracks**\n\n**Plenary**\n\n#### This program is tentative and subject to change.\n\nÃ— You're viewing the program in a time zone which is different from your device's time zone [change time zone](https://2025.splashcon.org/2025.splashcon.org#userProgramSettings)\n\n#### Thu 16 Oct Displayed time zone: **Perth** [change](https://2025.splashcon.org/2025.splashcon.org\\#userProgramSettings)\n\n| | |\n| --- | --- |\n| 09:00 - 10:00 | Thursday SPLASH Keynote[SPLASH Keynotes](https://2025.splashcon.org/track/splash-2025-keynotes) at [Orchid Plenary Ballroom](https://2025.splashcon.org/room/splash-2025-venue-orchid-plenary-ballroom) |\n| 09:00 **60m** Keynote | **[Automating maintenance of the Linux kernel: a perspective over 20 years](https://2025.splashcon.org/2025.splashcon.org)** SPLASH Keynotes K: [Julia Lawall](https://2025.splashcon.org/profile/julialawall) Inria |\n\n| | |\n| --- | --- |\n| 10:00 - 10:30 | Coffee break[ICFP/SPLASH Catering](https://2025.splashcon.org/track/icfp-splash-2025-catering) at [Garden Walk](https://2025.splashcon.org/room/splash-2025-venue-garden-walk) |\n| 10:00 **30m** Coffee break | **[Break](https://2025.splashcon.org/2025.splashcon.org)** ICFP/SPLASH Catering |\n\n| | |\n| --- | --- |\n| 10:30 - 12:15 | Runtime[OOPSLA](https://2025.splashcon.org/track/OOPSLA) at [Orchid East](https://2025.splashcon.org/room/splash-2025-venue-orchid-east) |\n| 10:30 **15m** Talk | **[Unveiling Heisenbugs with Diversified Execution](https://2025.splashcon.org/2025.splashcon.org)** OOPSLA [Arjun Ramesh](https://2025.splashcon.org/profile/arjunramesh) Carnegie Mellon University, [Tianshu Huang](https://2025.splashcon.org/profile/tianshuhuang) Carnegie Mellon University, [Jaspreet Riar](https://2025.splashcon.org/profile/jaspreetriar) Carnegie Mellon University, [Ben L. Titzer](https://2025.splashcon.org/profile/benltitzer) Carnegie Mellon University, [Anthony Rowe](https://2025.splashcon.org/profile/anthonyrowe) Carnegie Mellon University |\n| 10:45 **15m** Talk | **[TailTracer: Continuous Tail Tracing for Production Use](https://2025.splashcon.org/2025.splashcon.org)** OOPSLA [Tianyi Liu](https://2025.splashcon.org/profile/tianyiliu1) Nanjing University, [Yi Li](https://2025.splashcon.org/profile/yili) Nanyang Technological University, [Yiyu Zhang](https://2025.splashcon.org/profile/yiyuzhang) Nanjing University, [Zhuangda Wang](https://2025.splashcon.org/profile/zhuangdawang) Xiamen University, [Rongxin Wu](https://2025.splashcon.org/profile/rongxinwu) Xiamen University, [Xuandong Li](https://2025.splashcon.org/profile/xuandongli) Nanjing University, [Zhiqiang Zuo](https://2025.splashcon.org/profile/zhiqiangzuo) Nanjing University |\n| 11:00 **15m** Talk | **[Heap-Snapshot Matching and Ordering using CAHPs: A Context-Augmented Heap-Path Representation for Exact and Partial Path Matching using Prefix Trees](https://2025.splashcon.org/2025.splashcon.org)** OOPSLA [Matteo Basso](https://2025.splashcon.org/profile/matteobasso) UniversitÃ  della Svizzera italiana (USI), [Aleksandar Prokopec](https://2025.splashcon.org/profile/aleksandarprokopec) Oracle Labs, [Andrea RosÃ ](https://2025.splashcon.org/profile/andrearosa) USI Lugano, [Walter Binder](https://2025.splashcon.org/profile/walterbinder) USI Lugano |\n| 11:15 **15m** Talk | **[Float Self-Tagging](https://2025.splashcon.org/2025.splashcon.org)** OOPSLA [Olivier MelanÃ§on](https://2025.splashcon.org/profile/oliviermelancon) UniversitÃ© de MontrÃ©al, [Manuel Serrano](https://2025.splashcon.org/profile/manuelserrano) Inria; UniversitÃ© CÃ´te dâ€™Azur, [Marc Feeley](https://2025.splashcon.org/profile/marcfeeley) UniversitÃ© de MontrÃ©al [Pre-print](https://arxiv.org/abs/2411.16544) |\n| 11:30 **15m** Talk | **[HEMVM: a Heterogeneous Blockchain Framework for Interoperable Virtual Machines](https://2025.splashcon.org/2025.splashcon.org)** OOPSLA [Vladyslav Nekriach](https://2025.splashcon.org/profile/vladyslavnekriach) University Of Toronto, [Sidi Mohamed Beillahi](https://2025.splashcon.org/profile/sidimohamedbeillahi1) University of Toronto, [Chenxing Li](https://2025.splashcon.org/profile/chenxingli) Shanghai Tree-Graph Blockchain Research Institute, [Peilun Li](https://2025.splashcon.org/profile/peilunli) Shanghai Tree-Graph Blockchain Research Institute, [Ming Wu](https://2025.splashcon.org/profile/mingwu1) Shanghai Tree-Graph Blockchain Research Institute, [Andreas Veneris](https://2025.splashcon.org/profile/andreasveneris) University of Toronto, [Fan Long](https://2025.splashcon.org/profile/fanlong) University of Toronto |\n| 11:45 **15m** Talk | **[Advancing Performance via a Systematic Application of Research and Industrial Best Practice](https://2025.splashcon.org/2025.splashcon.org)** OOPSLA [Wenyu Zhao](https://2025.splashcon.org/profile/wenyuzhao) Australian National University, [Stephen M. Blackburn](https://2025.splashcon.org/profile/stephenmblackburn1) Google; Australian National University, [Kathryn S McKinley](https://2025.splashcon.org/profile/kathrynsmckinley) Google, [Man Cao](https://2025.splashcon.org/profile/mancao1) Google, [Sara S. Hamouda](https://2025.splashcon.org/profile/sarashamouda1) Canva |\n\n| | |\n| --- | --- |\n| 10:30 - 12:15 | Code[OOPSLA](https://2025.splashcon.org/track/OOPSLA) at [Orchid Plenary Ballroom](https://2025.splashcon.org/room/splash-2025-venue-orchid-plenary-ballroom) |\n| 10:30 **15m** Talk | **[ABC: Towards a Universal Code Styler through Model Merging](https://2025.splashcon.org/2025.splashcon.org)** OOPSLA [Yitong Chen](https://2025.splashcon.org/profile/yitongchen) School of Computer Science and Engineering, College of Software Engineering, School of Artificial Intelligence, Southeast University, [Zhiqiang Gao](https://2025.splashcon.org/profile/zhiqianggao) School of Computer Science and Engineering, College of Software Engineering, School of Artifical Intelligence, Southeast University, [Chuanqi Shi](https://2025.splashcon.org/profile/chuanqishi) School of Computer Science and Engineering, College of Software Engineering, School of Artifical Intelligence, Southeast University, [Baixuan Li](https://2025.splashcon.org/profile/baixuanli) School of Computer Science and Engineering, College of Software Engineering, School of Artifical Intelligence, Southeast University, [Miao Gao](https://2025.splashcon.org/profile/miaogao) School of Computer Science and Engineering, College of Software Engineering, School of Artifical Intelligence, Southeast University |\n| 10:45 **15m** Talk | **[Binary Cryptographic Function Identification via Similarity Analysis with Path-insensitive Emulation](https://2025.splashcon.org/2025.splashcon.org)** OOPSLA [Yikun Hu](https://2025.splashcon.org/profile/yikunhu1) Shanghai Jiao Tong University, [Yituo He](https://2025.splashcon.org/profile/yituohe) Shanghai Jiao Tong University, [Wenyu He](https://2025.splashcon.org/profile/wenyuhe) Shanghai Jiao Tong University, [Haoran Li](https://2025.splashcon.org/profile/haoranli4) Shanghai Jiao Tong University, [Yubo Zhao](https://2025.splashcon.org/profile/yubozhao) Shanghai Jiao Tong University, [Shuai Wang](https://2025.splashcon.org/profile/shuaiwang1) Hong Kong University of Science and Technology, [Dawu Gu](https://2025.splashcon.org/profile/dawugu) Shanghai Jiao Tong University |\n| 11:00 **15m** Talk | **[Boosting Program Reduction with the Missing Piece of Syntax-Guided Transformations](https://2025.splashcon.org/2025.splashcon.org)** OOPSLA [Zhenyang Xu](https://2025.splashcon.org/profile/zhenyangxu) University of Waterloo, [Yongqiang Tian](https://2025.splashcon.org/profile/yongqiangtian) Monash University, [Mengxiao Zhang](https://2025.splashcon.org/profile/mengxiaozhang), [Chengnian Sun](https://2025.splashcon.org/profile/chengniansun) University of Waterloo |\n| 11:15 **15m** Talk | **[Code Style Sheets: CSS for Code](https://2025.splashcon.org/2025.splashcon.org)** OOPSLA [Sam Cohen](https://2025.splashcon.org/profile/samcohen) University of Chicago, [Ravi Chugh](https://2025.splashcon.org/profile/ravichugh) University of Chicago |\n| 11:30 **15m** Talk | **[Enhancing APR with PRISM: A Semantic-Based Approach to Overfitting Patch Detection](https://2025.splashcon.org/2025.splashcon.org)** OOPSLA [Dowon Song](https://2025.splashcon.org/profile/dowonsong) Korea University, [Hakjoo Oh](https://2025.splashcon.org/profile/hakjoooh) Korea University |\n| 11:45 **15m** Talk | **[PAFL: Enhancing Fault Localizers by Leveraging Project-Specific Fault Patterns](https://2025.splashcon.org/2025.splashcon.org)** OOPSLA [Donguk Kim](https://2025.splashcon.org/profile/kimdonguk), [Doha Hwang](https://2025.splashcon.org/profile/dohahwang) Samsung, [Minseok Jeon](https://2025.splashcon.org/profile/minseokjeon) Korea University, [Hakjoo Oh](https://2025.splashcon.org/profile/hakjoooh) Korea University |\n| 12:00 **15m** Talk | **[Stencil-Lifting: Hierarchical Recursive Lifting System for Extracting Summary of Stencil Kernel in Legacy Codes](https://2025.splashcon.or","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2F2025.splashcon.org%2Ftrack%2FOOPSLA","id":"https://2025.splashcon.org/track/OOPSLA","publishedDate":"2025-10-18T00:00:00.000Z","summary":"The SPLASH 2025 conference will be held in Singapore from October 12 to 18, 2025, co-located with ICFP/SPLASH 2025. The event features a comprehensive program including keynotes, paper presentations, and tracks focused on programming languages, software development, and runtime systems. Notable highlights include a keynote by Julia Lawall on automating Linux kernel maintenance, along with sessions on debugging, tail tracing, heap analysis, and other advanced topics. The conference offers networking opportunities at various Singapore landmarks such as Gardens by the Bay, Marina Bay, and the ArtScience Museum. Registration details, accepted papers, and the full program are available on the official website.","title":"SPLASH 2025 - OOPSLA - SPLASH 2025","url":"https://2025.splashcon.org/track/OOPSLA"},{"content":"[Conference Partner](https://www.myhuiban.com/) Â» [Conferences](https://www.myhuiban.com/conferences) Â» OOPSLA\n\nConference Information\n\n##### OOPSLA 2025: Object-Oriented Programming Systems, Languages and Applications\n\n[https://2025.splashcon.org/track/OOPSLA](https://2025.splashcon.org/track/OOPSLA)\n\n| | |\n| --- | --- |\n| Submission Date: | 2025-03-25 |\n| Notification Date: | |\n| Conference Date: | 2025-10-12 |\n| Location: | Singapore |\n| Years: | 40 |\n\nCCF: a Â  CORE: a\\* Â  QUALIS: a1 Â  Viewed: 41433 Â  Tracked: 49 Â  Attend: 2\n\nCall For Papers\n\n```\nIntroduction\n\nThe OOPSLA issue of the Proceedings of the ACM on Programming Languages (PACMPL) welcomes papers focusing on all practical and theoretical investigations of programming systems, languages, and environments. Papers may target any stage of software development, including requirements, modeling, prototyping, design, implementation, generation, analysis, verification, testing, evaluation, maintenance, and reuse of software systems. Contributions may include the development of new tools, techniques, principles, and evaluations.\n\nOOPSLA 2025 will have two rounds of reviewing, with the Round 1 submission deadline October 15, 2024 and Round 2 submission deadline March 25, 2025 (AoE). Papers accepted in either round will be published in the 2025 volume of PACMPL(OOPSLA) and invited to present at the SPLASH conference in 2025.\n```\n\nLast updated by [Dou Sun](https://www.myhuiban.com/researcher/1) in 2025-02-15\n\nAcceptance Ratio\n\n| Year | Submitted | Accepted | Accepted(%) |\n| --- | --- | --- | --- |\n| 2011 | 166 | 61 | 36.7% |\n| 2009 | 144 | 25 | 17.4% |\n| 2008 | 117 | 33 | 28.2% |\n| 2007 | 156 | 33 | 21.2% |\n| 2006 | 157 | 26 | 16.6% |\n| 2005 | 174 | 32 | 18.4% |\n| 2004 | 173 | 27 | 15.6% |\n| 2003 | 147 | 26 | 17.7% |\n| 2002 | 125 | 25 | 20% |\n| 2001 | 145 | 27 | 18.6% |\n| 2000 | 142 | 26 | 18.3% |\n| 1999 | 152 | 30 | 19.7% |\n\n232410041003100210011000999998997608996995\n\nBest Papers\n\n| Year | Best Papers |\n| --- | --- |\n| 2016 | [Hybrid STM/HTM for nested transactions on OpenJDK](http://scholar.google.com/scholar?q=Hybrid+STM%2FHTM+for+nested+transactions+on+OpenJDK) |\n| 2016 | [Modeling and Analysis of Remote Memory Access Programming](http://scholar.google.com/scholar?q=Modeling+and+Analysis+of+Remote+Memory+Access+Programming) |\n| 2016 | [Semantics-based Program Verifiers for all Languages](http://scholar.google.com/scholar?q=Semantics-based+Program+Verifiers+for+all+Languages) |\n| 2016 | [Asserting Reliable Convergence for Configuration Management Scripts](http://scholar.google.com/scholar?q=Asserting+Reliable+Convergence+for+Configuration+Management+Scripts) |\n\n3680368136823683\n\nRelated Conferences\n\n| CCF | CORE | QUALIS | Short | Full Name | Submission | Notification | Conference |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| | | | NMCI | [International Conference on New Material and Chemical Industry](https://www.myhuiban.com/conference/1892) | 2022-01-25 | | 2022-11-16 |\n| | | b4 | LANC | [Latin America Networking Conference](https://www.myhuiban.com/conference/914) | 2024-05-17 | 2024-06-28 | 2024-08-15 |\n| b | b | a2 | VMCAI | [International Conference on Verification, Model Checking, and Abstract Interpretation](https://www.myhuiban.com/conference/217) | 2024-10-01 | 2024-11-08 | 2025-01-20 |\n| | b | b1 | RTCSA | [International Conference on Embedded and Real-Time Computing Systems and Applications](https://www.myhuiban.com/conference/49) | 2025-03-28 | 2025-05-07 | 2025-08-20 |\n| b | a | a2 | RTAS | [IEEE Real-Time and Embedded Technology and Applications Symposium](https://www.myhuiban.com/conference/349) | 2024-10-31 | 2025-01-23 | 2025-05-06 |\n| | | | ICBAT | [International Conference on Biotechnology and Agroforestry Technology](https://www.myhuiban.com/conference/3712) | 2020-04-15 | | 2020-04-17 |\n| | | b3 | ICSNC | [International Conference on Systems and Networks Communications](https://www.myhuiban.com/conference/661) | 2023-08-10 | 2023-09-18 | 2023-11-13 |\n| | | | PerDis | [International Symposium on Pervasive Displays](https://www.myhuiban.com/conference/1638) | 2015-02-25 | 2015-03-27 | 2015-06-10 |\n| | | | INFOCOMP | [International Conference on Advanced Communications and Computation](https://www.myhuiban.com/conference/394) | 2022-03-22 | 2022-04-19 | 2022-06-26 |\n| | | | ICSSC | [International Conference on Smart Sustainable City and Big Data](https://www.myhuiban.com/conference/1342) | 2017-05-07 | 2017-05-10 | 2017-06-05 |\n\n189291421749349371266116383941342\n\n| Short | Full Name | Submission | Conference |\n| --- | --- | --- | --- |\n| NMCI | [International Conference on New Material and Chemical Industry](https://www.myhuiban.com/conference/1892) | 2022-01-25 | 2022-11-16 |\n| LANC | [Latin America Networking Conference](https://www.myhuiban.com/conference/914) | 2024-05-17 | 2024-08-15 |\n| VMCAI | [International Conference on Verification, Model Checking, and Abstract Interpretation](https://www.myhuiban.com/conference/217) | 2024-10-01 | 2025-01-20 |\n| RTCSA | [International Conference on Embedded and Real-Time Computing Systems and Applications](https://www.myhuiban.com/conference/49) | 2025-03-28 | 2025-08-20 |\n| RTAS | [IEEE Real-Time and Embedded Technology and Applications Symposium](https://www.myhuiban.com/conference/349) | 2024-10-31 | 2025-05-06 |\n| ICBAT | [International Conference on Biotechnology and Agroforestry Technology](https://www.myhuiban.com/conference/3712) | 2020-04-15 | 2020-04-17 |\n| ICSNC | [International Conference on Systems and Networks Communications](https://www.myhuiban.com/conference/661) | 2023-08-10 | 2023-11-13 |\n| PerDis | [International Symposium on Pervasive Displays](https://www.myhuiban.com/conference/1638) | 2015-02-25 | 2015-06-10 |\n| INFOCOMP | [International Conference on Advanced Communications and Computation](https://www.myhuiban.com/conference/394) | 2022-03-22 | 2022-06-26 |\n| ICSSC | [International Conference on Smart Sustainable City and Big Data](https://www.myhuiban.com/conference/1342) | 2017-05-07 | 2017-06-05 |\n\n189291421749349371266116383941342\n\nRelated Journals\n\n| CCF | Full Name | Impact Factor | Publisher | ISSN |\n| --- | --- | --- | --- | --- |\n| c | [ACM Transactions on Computing Education](https://www.myhuiban.com/journal/591) | 3.200 | ACM | 1946-6226 |\n| | [Operations Research and Applications: An International Journal](https://www.myhuiban.com/journal/1091) | | AIRCC | 2393-8463 |\n| c | [Peer-to-Peer Networking and Applications](https://www.myhuiban.com/journal/48) | 3.300 | Springer | 1936-6442 |\n| | [Journal of Complex Analysis](https://www.myhuiban.com/journal/649) | | Hindawi | 2314-4963 |\n| | [IEEE Transactions on Emerging Topics in Computational Intelligence](https://www.myhuiban.com/journal/1052) | 5.300 | IEEE | 2471-285X |\n| | [IEEE Wireless Communications](https://www.myhuiban.com/journal/325) | 10.90 | IEEE | 1536-1284 |\n| | [Smart Science](https://www.myhuiban.com/journal/1009) | | Taylor & Francis | 2308-0477 |\n| | [International Journal for Numerical Methods in Fluids](https://www.myhuiban.com/journal/851) | 1.700 | Wiley-Blackwell | 0271-2091 |\n| | [Journal of Sensors](https://www.myhuiban.com/journal/657) | 1.400 | Hindawi | 1687-725X |\n| c | [IEEE Transactions on Network and Service Management](https://www.myhuiban.com/journal/43) | 4.700 | IEEE | 1932-4537 |\n\n5911091486491052325100985165743\n\n| Full Name | Impact Factor | Publisher |\n| --- | --- | --- |\n| [ACM Transactions on Computing Education](https://www.myhuiban.com/journal/591) | 3.200 | ACM |\n| [Operations Research and Applications: An International Journal](https://www.myhuiban.com/journal/1091) | | AIRCC |\n| [Peer-to-Peer Networking and Applications](https://www.myhuiban.com/journal/48) | 3.300 | Springer |\n| [Journal of Complex Analysis](https://www.myhuiban.com/journal/649) | | Hindawi |\n| [IEEE Transactions on Emerging Topics in Computational Intelligence](https://www.myhuiban.com/journal/1052) | 5.300 | IEEE |\n| [IEEE Wireless Communications](https://www.myhuiban.com/journal/325) | 10.90 | IEEE |\n| [Smart Science](https://www.myhuiban.com/journal/1009) | | Taylor & Francis |\n| [International Journal for Numerical Methods in Fluids](https://www.myhuiban.com/journal/851) | 1.700 | Wiley-Blackwell |\n| [Journal of Sensors](https://www.myhuiban.com/journal/657) | 1.400 | Hindawi |\n| [IEEE Transactions on Network and Service Management](https://www.myhuiban.com/journal/43) | 4.700 | IEEE |\n\n5911091486491052325100985165743\n\nRecommendation\n\n- [Track It 49](http://www.myhuiban.com/conference/track/id/162)\n- [Attend It 2](http://www.myhuiban.com/conference/attend/id/162)\n- [Edit CFP](https://www.myhuiban.com/conference/edit/162)\n\n| Tracker |\n| --- |\n| [Robin Geens (191)](https://www.myhuiban.com/researcher/66778) |\n| [Saner Saner (21)](https://www.myhuiban.com/researcher/71850) |\n| [Shengxin Dai (1068)](https://www.myhuiban.com/researcher/1800) |\n| [Dihao Fan (99)](https://www.myhuiban.com/researcher/60853) |\n| [Wenyuan Xu (184)](https://www.myhuiban.com/researcher/45957) |\n| [Xian Zhang (3220)](https://www.myhuiban.com/researcher/974) |\n| [ZiTian Wang (538)](https://www.myhuiban.com/researcher/36074) |\n| [Zewei Mo (210)](https://www.myhuiban.com/researcher/47772) |\n| [Siyuan Li (523)](https://www.myhuiban.com/researcher/53309) |\n| [Tingting Yu (142)](https://www.myhuiban.com/researcher/53062) |\n| [Dongdong An (215)](https://www.myhuiban.com/researcher/18156) |\n| [Renzhi Xiao (453)](https://www.myhuiban.com/researcher/38890) |\n| [Li Chao (1012)](https://www.myhuiban.com/researcher/6902) |\n| [Xiaohan Zhang (576)](https://www.myhuiban.com/researcher/21274) |\n| [Daniel Owen (447)](https://www.myhuiban.com/researcher/29833) |\n| [Hao Chen (176)](https://www.myhuiban.com/researcher/174) |\n| [Donglei Liu (1952)](https://www.myhuiban.com/researcher/155) |\n| [Le Qi (402)](https://www.myhuiban.com/researcher/36342) |\n| [Goudan Lee (228)](https://www.myhuiban.com/researcher/16465) |\n| [Dexin Liu (838)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.myhuiban.com%2Fconference%2F162%3Flang%3Den_us","id":"https://www.myhuiban.com/conference/162?lang=en_us","publishedDate":"2025-10-12T00:00:00.000Z","summary":"The webpage provides information about the upcoming OOPSLA 2025 conference, a premier event focused on research and developments in object-oriented programming systems, languages, and applications. Scheduled for October 12, 2025, in Singapore, the conference invites submissions of research papers covering both practical and theoretical aspects of programming languages, tools, and environments across all software development stages. Submissions are accepted in two rounds, with deadlines on October 15, 2024, and March 25, 2025. Accepted papers will be published in the PACMPL volume and may be presented at SPLASH 2025. The site also highlights the conference's history, acceptance rates, notable past papers, and related conferences in the field.","title":"Object-Oriented Programming Systems, Languages and Applications","url":"https://www.myhuiban.com/conference/162?lang=en_us"},{"content":"The Art, Science, and Engineering of Programming\n\n[â„¹ï¸ŽÂ Â Upcoming Submission Deadline: October 1, 2025](https://programming-journal.org/timeline/)\n\n# Meta-compilation of Baseline JIT Compilers with Druid\n\nNahuel Palumbo[1](https://programming-journal.org/programming-journal.org#fn:1) , Guillermo Polito[2](https://programming-journal.org/programming-journal.org#fn:2) , StÃ©phane Ducasse[3](https://programming-journal.org/programming-journal.org#fn:3) , and Pablo Tesone[4](https://programming-journal.org/programming-journal.org#fn:4)\n\nThe Art, Science, and Engineering of Programming, 2025, Vol. 10, Issue 1, Article 9\n\nSubmission date: 2024-09-30\nPublication date: 2025-02-15\nDOI: [https://doi.org/10.22152/programming-journal.org/2025/10/9](https://doi.org/10.22152/programming-journal.org/2025/10/9)\nFull text: [PDF](https://arxiv.org/pdf/2502.20543v1.pdf)\n\n### Abstract\n\nVirtual Machines (VMs) combine interpreters and just-in-time (JIT) compiled code to achieve good performance.\nHowever, implementing different execution engines increases the cost of developing and maintaining such solutions.\nJIT compilers based on meta-compilation cope with these issues by automatically generating optimizing JIT compilers.\nThis leaves open the question of how meta-compilation applies to baseline JIT compilers, which improve warmup times by trading off optimizations.\n\nIn this paper, we present Druid, an ahead-of-time automatic approach to generate baseline JIT compiler frontends from interpreters.\nLanguage developers guide meta-compilation by annotating interpreter code and using Druidâ€™s intrinsics.\nDruid targets the meta-compilation to an existing JIT compiler infrastructure to achieve good warm-up performance.\n\nWe applied Druid in the context of the Pharo programming language and evaluated it by comparing an autogenerated JIT compiler frontend against the one in production for more than 10 years.\nOur generated JIT compiler frontend is 2x faster on average than the interpreter and achieves on average 0.7x the performance of the handwritten JIT compiler.\nOur experiment only required changes in 60 call sites in the interpreter, showing that our solution makes language VMs **easier to maintain and evolve in the long run**.\n\n1. University of Lille - Inria - CNRS - Centrale Lille - UMR 9189 CRIStAL, France\n[https://orcid.org/0009-0001-5004-5632](https://orcid.org/0009-0001-5004-5632)\n\n2. University of Lille - Inria - CNRS - Centrale Lille - UMR 9189 CRIStAL, France\n[https://orcid.org/0000-0003-0813-8584](https://orcid.org/0000-0003-0813-8584)\n\n3. University of Lille - Inria - CNRS - Centrale Lille - UMR 9189 CRIStAL, France\n[https://orcid.org/0000-0001-6070-6599](https://orcid.org/0000-0001-6070-6599)\n\n4. University of Lille - Inria - CNRS - Centrale Lille - UMR 9189 CRIStAL, France\n[https://orcid.org/0000-0002-5615-6691?](https://orcid.org/0000-0002-5615-6691?)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fprogramming-journal.org%2F2025%2F10%2F9%2F","id":"https://programming-journal.org/2025/10/9/","publishedDate":"2025-10-09T00:00:00.000Z","summary":"The paper introduces Druid, an automated, ahead-of-time meta-compilation approach for generating baseline JIT compiler frontends from interpreters, aimed at improving startup times and maintaining ease of development. By allowing language developers to annotate interpreter code and utilize Druidâ€™s intrinsics, Druid adapts existing JIT infrastructures to produce efficient, warm-up optimized JIT compilers. Applied to the Pharo language, Druid's autogenerated JIT frontend outperforms the interpreter by doubling its speed and approaches the performance of a long-standing handwritten JIT, while requiring minimal modifications (only 60 call sites). This approach simplifies the maintenance and evolution of language VMs, making them more scalable and adaptable over time.","title":"Meta-compilation of Baseline JIT Compilers with Druid","url":"https://programming-journal.org/2025/10/9/"},{"content":"The Art, Science, and Engineering of Programming\n\n[â„¹ï¸ŽÂ Â Upcoming Submission Deadline: October 1, 2025](https://programming-journal.org/timeline/)\n\n# On the State of Coherence in the Land of Type Classes\n\nDimi Racordon[1](https://programming-journal.org/programming-journal.org#fn:1) , Eugene Flesselle[2](https://programming-journal.org/programming-journal.org#fn:2) , and Cao Nguyen Pham[3](https://programming-journal.org/programming-journal.org#fn:3)\n\nThe Art, Science, and Engineering of Programming, 2025, Vol. 10, Issue 1, Article 15\n\nSubmission date: 2024-10-01\nPublication date: 2025-02-15\nDOI: [https://doi.org/10.22152/programming-journal.org/2025/10/15](https://doi.org/10.22152/programming-journal.org/2025/10/15)\nFull text: [PDF](https://arxiv.org/pdf/2502.20546v1.pdf)\n\n### Abstract\n\nType classes are a popular tool for implementing generic algorithms and data structures without loss of efficiency, bridging the gap between parametric and ad-hoc polymorphism. Since their initial development in Haskell, they now feature prominently in numerous other industry-ready programming languages, notably including Swift, Rust, and Scala. The success of type classes hinges in large part on the compilersâ€™ ability to infer arguments to implicit parameters by means of a type-directed resolution. This technique, sometimes dubbed **implicit programming**, lets users elide information that the language implementation can deduce from the context, such as the implementation of a particular type class.\n\nOne drawback of implicit programming is that a type-directed resolution may yield ambiguous results, thereby threatening coherence, the property that valid programs have exactly one meaning. This issue has divided the community on the right approach to address it. One side advocates for flexibility where implicit resolution is context-sensitive and often relies on dependent typing features to uphold soundness. The other holds that context should not stand in the way of equational reasoning and typically imposes that type class instances be unique across the entire program to fend off ambiguities.\n\nAlthough there exists a large body of work on type classes and implicit programming, most of the scholarly literature focuses on a few select languages and offers little insight into other mainstream projects. Meanwhile, the latter have evolved similar features and/or restrictions under different names, making it difficult for language users and designers to get a sense of the full design space. To alleviate this issue, we set to examine Swift, Rust, and Scala, three popular languages featuring type classes heavily, and relate their approach to coherence to Haskellâ€™s. It turns out that, beyond superficial syntactic differences, Swift, Rust, and Haskell are actually strikingly similar in that the three languages offer comparable strategies to work around the limitations of the uniqueness of type class instances.\n\n1. EPFL, Switzerland\n[https://orcid.org/0000-0003-0299-3993](https://orcid.org/0000-0003-0299-3993)\n\n2. EPFL, Switzerland\n[https://orcid.org/0009-0003-7545-594X](https://orcid.org/0009-0003-7545-594X)\n\n3. EPFL, Switzerland\n[https://orcid.org/0009-0005-2543-3309](https://orcid.org/0009-0005-2543-3309)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fprogramming-journal.org%2F2025%2F10%2F15%2F","id":"https://programming-journal.org/2025/10/15/","publishedDate":"2025-10-15T00:00:00.000Z","summary":"The article \"On the State of Coherence in the Land of Type Classes\" examines how various programming languagesâ€”specifically Haskell, Swift, Rust, and Scalaâ€”implement and manage type classes, which facilitate generic programming without sacrificing efficiency. It discusses the core challenge of maintaining coherence, ensuring that programs have a single, unambiguous meaning, especially when implicit resolution of type class instances can lead to ambiguities. The paper compares different approaches to this problem: some languages favor flexibility with context-sensitive, dependent typing, while others enforce unique instances across the program to preserve reasoning clarity. Despite superficial differences, Swift, Rust, and Haskell share similar strategies to address these coherence issues, highlighting a common design space in type class implementation. The work provides insights into how mainstream languages handle the limitations of implicit programming and type class coherence.","title":"On the State of Coherence in the Land of Type Classes","url":"https://programming-journal.org/2025/10/15/"},{"content":"The Art, Science, and Engineering of Programming\n\n[â„¹ï¸ŽÂ Â Upcoming Submission Deadline: October 1, 2025](https://programming-journal.org/timeline/)\n\n# Probing the Design Space: Parallel Versions for Exploratory Programming\n\nTom Beckmann[1](https://programming-journal.org/programming-journal.org#fn:1) , Joana Bergsiek[2](https://programming-journal.org/programming-journal.org#fn:2), Eva Krebs[3](https://programming-journal.org/programming-journal.org#fn:3) , Toni Mattis[4](https://programming-journal.org/programming-journal.org#fn:4) , Stefan Ramson[5](https://programming-journal.org/programming-journal.org#fn:5) , Martin C. Rinard[6](https://programming-journal.org/programming-journal.org#fn:6) , and Robert Hirschfeld[7](https://programming-journal.org/programming-journal.org#fn:7)\n\nThe Art, Science, and Engineering of Programming, 2025, Vol. 10, Issue 1, Article 5\n\nSubmission date: 2024-09-26\nPublication date: 2025-02-15\nDOI: [https://doi.org/10.22152/programming-journal.org/2025/10/5](https://doi.org/10.22152/programming-journal.org/2025/10/5)\nFull text: [PDF](https://arxiv.org/pdf/2502.20535v1.pdf)\n\n### Abstract\n\nExploratory programming involves open-ended tasks. To evaluate their progress on these, programmers require frequent feedback and means to tell if the feedback they observe is bringing them in the right direction. Collecting, comparing, and sharing feedback is typically done through ad-hoc means: relying on memory to compare outputs, code comments, or manual screenshots. To approach this issue, we designed Exploriants: an extension to example-based live programming. Exploriants allows programmers to place variation points. It collects outputs captured in probes and presents them in a comparison view that programmers can customize to suit their program domain. We find that the addition of variation points and the comparisons view encourages a structured approach to exploring variations of a program. We demonstrate Exploriantsâ€™ capabilities and applicability in three case studies on image processing, data processing, and game development. Given Exploriants, exploratory programmers are given a straightforward means to evaluate their progress and do not have to rely on ad-hoc methods that may introduce errors.\n\n1. Hasso Plattner Institute, Germany / University of Potsdam, Germany\n[https://orcid.org/0000-0003-0015-1717](https://orcid.org/0000-0003-0015-1717)\n\n2. [joana.bergsiek@student.hpi.uni-potsdam.de](mailto:joana.bergsiek@student.hpi.uni-potsdam.de), Hasso Plattner Institute, Germany / University of Potsdam, Germany\n\n3. Hasso Plattner Institute, Germany / University of Potsdam, Germany\n[https://orcid.org/0000-0002-9089-7784](https://orcid.org/0000-0002-9089-7784)\n\n4. Hasso Plattner Institute, Germany / University of Potsdam, Germany\n[https://orcid.org/0000-0001-7024-9838](https://orcid.org/0000-0001-7024-9838)\n\n5. Hasso Plattner Institute, Germany / University of Potsdam, Germany\n[https://orcid.org/0000-0002-0913-1264?](https://orcid.org/0000-0002-0913-1264?)\n\n6. Massachusetts Institute of Technology, USA\n[https://orcid.org/0000-0001-8095-8523](https://orcid.org/0000-0001-8095-8523)\n\n7. Hasso Plattner Institute, Germany / University of Potsdam, Germany\n[https://orcid.org/0000-0002-4249-6003](https://orcid.org/0000-0002-4249-6003)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fprogramming-journal.org%2F2025%2F10%2F5%2F","id":"https://programming-journal.org/2025/10/5/","publishedDate":"2025-10-05T00:00:00.000Z","summary":"This paper introduces Exploriants, a tool designed to enhance exploratory programming by enabling structured comparison of program variations. It extends example-based live programming with features like variation points and customizable comparison views, allowing programmers to systematically evaluate different outputs without relying on ad-hoc methods such as manual screenshots or memory. The authors demonstrate Exploriants' effectiveness through case studies in image processing, data processing, and game development, showing that it facilitates better feedback and progress assessment during open-ended programming tasks. Overall, Exploriants aims to improve the exploration process by providing a straightforward, organized approach to evaluating multiple program versions.","title":"Probing the Design Space: Parallel Versions for Exploratory Programming","url":"https://programming-journal.org/2025/10/5/"},{"content":"# Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review\n\nSungduk Yu â€ƒâ€ƒMan Luo â€ƒâ€ƒAvinash Madusu â€ƒâ€ƒVasudev Lal â€ƒâ€ƒPhillip Howard\nIntel Labs\n{sungduk.yu, man.luo, avinash.madasu, vasudev.lal, phillip.r.howard}@intel.com\n\n###### Abstract\n\nPeer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in the linguistic capabilities of large language models (LLMs), a new potential risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. In this study, we investigate the ability of existing AI text detection algorithms to distinguish between peer reviews written by humans and different state-of-the-art LLMs. Our analysis shows that existing approaches fail to identify many GPT-4o written reviews without also producing a high number of false positive classifications. To address this deficiency, we propose a new detection approach which surpasses existing methods in the identification of GPT-4o written peer reviews at low levels of false positive classifications. Our work reveals the difficulty of accurately identifying AI-generated text at the individual review level, highlighting the urgent need for new tools and methods to detect this type of unethical application of generative AI.\n\n## 1 Introduction\n\nRecent advancements in large language models (LLMs) have enabled their application to a broad range of domains, where LLMs have demonstrated the ability to produce plausible and authoritative responses to queries even in highly technical subject areas. These advancements have coincided with a surge in interest in AI research, resulting in large increases in paper submissions to leading AI conferences (Table [S1](https://arxiv.org/html/2410.03019v2#A0.T1)). Consequently, workloads for peer reviewers of such conferences have also increased significantly, which could make LLMs an appealing tool for lessening the burden of fulfilling their peer review obligations.\n\nDespite their impressive capabilities, the use of LLMs in the peer review process raises several ethical and methodological concerns which could compromise the integrity of the publication process. Reviewers are selected based on their expertise in a technical domain related to a submitted manuscript, which is necessary to critically evaluate the proposed research. Offloading this responsibility to an LLM circumvents the role that reviewer selection plays in ensuring proper vetting of a manuscript. Furthermore, LLMs are prone to hallucination and may not possess the ability to rigorously evaluate research publications. Therefore, the use of LLMs in an undisclosed manner in peer review poses a significant ethical concern that could undermine confidence in this important process.\n\nMotivating the need for detection tools to address this problem is the apparent increase in AI-generated text among peer reviews submitted to recent AI conferences. Figure [1](https://arxiv.org/html/2410.03019v2#S1.F1) shows the proportion of reviews submitted to ICLR between 2019 and 2024 which are flagged as containing AI text using methods deployed in this study. There is a consistent upward trend in recent years which provides indirect evidence of the increasing use of LLMs in peer review writing, echoing a recent study which estimated that 6.5% to 16.9% of peer reviews submitted to recent AI conferences might have involved substantial use of LLMs for task beyond simple grammar checks \\[ [1](https://arxiv.org/html/2410.03019v2#bib.bib1)\\].\n\nFigure 1: Proportion of reviews submitted to ICLR detected as AI-generated.\n\nIn this work, we investigate the suitability of various AI text detection methods for identifying LLM generations in the peer review process. While limited prior work has analyzed the presence of AI-generated text in peer reviews at the corpus level \\[ [1](https://arxiv.org/html/2410.03019v2#bib.bib1)\\] (see AppendixÂ [A](https://arxiv.org/html/2410.03019v2#A1) for further discussion of related work), our study is the first to investigate the detectability LLM generations at the individual review level, which is necessary to address this problem in practice. Specifically, we produce AI-generated peer reviews for papers submitted to AI conferences prior to the introduction of ChatGPT using different LLMs and prompting methods. We then evaluate multiple open-source and proprietary AI text detection models on their ability to distinguish real peer reviews collected for these conferences from our AI-generated peer reviews for the same papers.\n\nOur results show that all existing AI-text detection methods are limited in their ability to robustly detect AI-generated reviews while maintaining a low number of false positives. We propose an alternative approach to detecting AI-generated peer reviews by comparing the semantic similarity of a given review to a set of reference AI-generated reviews for the same paper, which surpasses the performance of all existing approaches in detecting GPT-4o written peer reviews. Our work demonstrate the challenging nature of detecting AI-written text in peer reviews and motivates the need for further research on methods to address this unethical use of LLMs in the peer review process.\n\n## 2 Methodology\n\n### 2.1 Data Collection\n\nWe used the OpenReview client API \\[ [2](https://arxiv.org/html/2410.03019v2#bib.bib2)\\] to collect submitted manuscripts and their reviews for the ICLR conference from 2019 to 2024. The total numbers of submission and reviews are summarized in Table [S1](https://arxiv.org/html/2410.03019v2#A0.T1).\nWe use two LLMs to generate AI peer reviews for these manuscripts, OpenAIâ€™s GPT-4o \\[ [3](https://arxiv.org/html/2410.03019v2#bib.bib3)\\] and the open-source instruction-tuned Llama-3.1 (70b) \\[ [4](https://arxiv.org/html/2410.03019v2#bib.bib4)\\], to generate AI peer reviews. Manuscripts are converted from PDF to Markdown format, excluding the Bibliography and Acknowledgement sections to focus on the core content relevant for review. Prompts used for generating AI reviews are adapted from Lu et al. \\[ [5](https://arxiv.org/html/2410.03019v2#bib.bib5)\\] with two major changes. First, recognizing that LLM-generated text is sensitive to prompt variations, we introduce four distinct reviewer archetypesâ€”\"balanced,\" \"nitpicky,\" \"innovative,\" and \"conservative\"â€”to simulate the diversity of real-world peer review scenarios. Second, we provide the LLMs with the ICLR 2022 reviewer guidelines \\[ [6](https://arxiv.org/html/2410.03019v2#bib.bib6)\\] with a minor modification. The ICLR guideline emphasize general instructions about how to approach peer review, rather than focusing on rubric and scoring scales as in the NeurIPS reviewer form. Our complete prompts are included in Appendix [F](https://arxiv.org/html/2410.03019v2#A6). We collect a total of 16,000 AI-generated reviews for 500 random submissions for each ICLR conference of year from 2021-2024.\nEach AI-generated review contains five sections, and they were combined for detection tasks (see Appendix [C](https://arxiv.org/html/2410.03019v2#A3) for details).\n\n### 2.2 AI-Generated Text Detection Methods\n\nOpen-source AI text detection models. We utilize two supervised fine-tuned pretrained language models. The first is fromÂ \\[ [7](https://arxiv.org/html/2410.03019v2#bib.bib7)\\], where the author collect a Human ChatGPT Comparison Corpus (HC3) consisting of question-answer pairs created by human experts and generated by ChatGPT. The authors trained two Roberta-based modelsÂ \\[ [8](https://arxiv.org/html/2410.03019v2#bib.bib8)\\]: one detects whether answers are human or AI-generated using paired data, while the other evaluates single answers. We use the latter for our assessment. The second model is a LongformerÂ \\[ [9](https://arxiv.org/html/2410.03019v2#bib.bib9)\\]. As tested in Â \\[ [10](https://arxiv.org/html/2410.03019v2#bib.bib10)\\], this model has demonstrated improved detection AUROC and generalization performance on the large-scale MAGE testbed, which includes eight distinct writing tasks, surpassing other methods such as GLTRÂ \\[ [11](https://arxiv.org/html/2410.03019v2#bib.bib11)\\], FastTextÂ \\[ [12](https://arxiv.org/html/2410.03019v2#bib.bib12)\\], and DetectGPTÂ \\[ [13](https://arxiv.org/html/2410.03019v2#bib.bib13)\\]. For both models, we calculate the probability of each sentence in the review, averaging these to determine the final probability of the entire review being classified as AI-generated.\n\nOriginality AI API.\nThe Originality AI API is a commercial AI text detection service, and some studies have reported its high performance \\[ [14](https://arxiv.org/html/2410.03019v2#bib.bib14), [15](https://arxiv.org/html/2410.03019v2#bib.bib15)\\]. It returns an AI score ranging between 0 and 1 which indicates the modelâ€™s confidence that an input text was AI generated. While values greater than 0.5 indicate that the model has more confidence that an input text is AI-written than human-written, a higher threshold on the AI score may provide a more desirable balance between true positive and false positive classifications. Therefore, we investigate the trade-off between true positive and false positive classifications using a range of different thresholds on the returned score.\n\nLLM-based detection.\nWe employ the LLM-as-a-judge approach \\[ [16](https://arxiv.org/html/2410.03019v2#bib.bib16)\\], utilizing both GPT-4o and Llama-3.1-70B models. The LLMs are instructed to provide two outputs: a binary decision (\"human\" or \"AI\") and a rationale for their decision, encouraging chain-of-thought reasoning.\n\nAnchor Embeddings Detection.\nWe propose a new method aiming to detect AI-generated reviews by comparing their semantic sim","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2410.03019v2","id":"https://arxiv.org/html/2410.03019v2","publishedDate":"2025-08-20T00:00:00.000Z","summary":"The webpage discusses the growing concern over the use of large language models (LLMs) like GPT-4 in peer review processes. As LLMs become more capable of producing human-like, technical reviews, there is a risk that reviewers may rely on AI-generated assessments without proper oversight, potentially undermining the integrity of scientific publishing. The study examines current AI text detection tools and finds they are often ineffective at reliably distinguishing between human and AI-written reviews, especially for advanced models like GPT-4. To address this, the authors propose a new detection approach that improves identification accuracy with fewer false positives. The increasing prevalence of AI-generated peer reviews, highlighted by rising detection rates at major conferences, underscores the urgent need for more robust tools to ensure ethical and trustworthy peer review practices.","title":"Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review","url":"https://arxiv.org/html/2410.03019v2"},{"content":"\\\\newmdenv\n\n\\[frametitle=,\nmiddlelinecolor=ikmgreen,\nmiddlelinewidth=0pt,\nbackgroundcolor=ikmgray!20,\nroundcorner=2pt,\nbottomline=false,\nleftline=true,\ntopline=false,\nrightline=false,\nskipabove=10pt,\nskipbelow=10pt,\nleftmargin=10pt,\nrightmargin=10pt,\ninnerleftmargin=5pt,\ninnerrightmargin=10pt,\ninnertopmargin=10pt,\ninnerbottommargin=10pt\\]Algorithmus\n\\\\DeclareCaptionTypekasten\\[Box\\]\n\n# A review on data-driven constitutive laws for solids\n\nJan Niklas Fuhg\n\nDepartment of Aerospace Engineering and Engineering Mechanics\n\nThe University of Texas at Austin,\nTexas, USA\n\n&Govinda Anantha Padmanabha &\nNikolaos Bouklas\n\nSibley School of Mechanical and Aerospace Engineering\n\nCornell University,\nNew York, USA\n\n&Bahador Bahmani & WaiChing Sun\n\nDepartment of Civil Engineering and Engineering Mechanics\n\nColumbia University,\nNew York, USA\n&Nikolaos N. Vlassis\n\nDepartment of Mechanical and Aerospace Engineering\n\nRutgers University,\nNew Jersey, USA\n&Moritz Flaschel, Pietro Carrara & Laura De Lorenzis\n\nDepartment of Mechanical and Process Engineering\n\nETH ZÃ¼rich,\nZÃ¼rich, Switzerland\n\n###### Abstract\n\nThis review article highlights state-of-the-art data-driven techniques to discover, encode, surrogate, or emulate constitutive laws that describe the path-independent and path-dependent response of solids. Our objective is to provide an organized taxonomy to a large spectrum of methodologies developed in the past decades and to discuss the benefits and drawbacks of the various techniques for interpreting and forecasting mechanics behavior across different scales. Distinguishing between machine-learning-based and model-free methods, we further categorize approaches based on their interpretability and on their learning process/type of required data, while discussing the key problems of generalization and trustworthiness. We attempt to provide a road map of how these can be reconciled in a data-availability-aware context. We also touch upon relevant aspects such as data sampling techniques, design of experiment, verification, and validation.\n\n_Keywords_â€‚Machine Learning Â â‹…â‹…\\\\cdotâ‹…\nArtificial Intelligence Â â‹…â‹…\\\\cdotâ‹…\nData-Driven Constitutive Models\n\n## 1 Introduction\n\nProblems in solid mechanics are formulated in terms of three sets of equations: the first encodes basic conservation principles (e.g., balance of linear momentum) and governs the equilibrium of deformable bodies following the definition of a stress tensor; the second describes the kinematics of motion in terms of displacements, strains and strain rates. The third set, denoted as constitutive law (CL) (or material model), describes the response of a material to external stimuli by establishing a relation between kinematic and static quantities (e.g., between strains and stresses) eventually mediated though other measurable and/or internal (i.e., non-measurable) field variables, whose evolution laws are part and parcel of the description. The material behavior and, therefore, the CL can be classified either as path-independent (also, history-independent) if the current state of a point does not depend on its previous states, or path-dependent (also, history-dependent) when the current state depends on the history of states experienced by the point. While conservation principles and kinematics are considered axiomatic and epistemic, material modeling is empirical in nature; it is in continuous evolution and it constitutes one of the most important research fields is mechanics.\n\nSince the introduction of Youngâ€™s modulus in the 19th century \\[ [1](https://arxiv.org/html/2405.03658v1#bib.bib1)\\], engineers have predominantly defined CLs using the so-called phenomenological approach, where experimental observations and physical requirements are distilled into a priori selected analytical ansatz relationships whose parameters are meant to be characteristic of the material \\[ [2](https://arxiv.org/html/2405.03658v1#bib.bib2), [3](https://arxiv.org/html/2405.03658v1#bib.bib3), [4](https://arxiv.org/html/2405.03658v1#bib.bib4), [5](https://arxiv.org/html/2405.03658v1#bib.bib5)\\]. Thus, constitutive modeling is traditionally based on data and, due to the limitations of traditional experimental setups, hinged on limited observations on a restricted set of load cases (e.g., inducing uniaxial, biaxial and hydrostatic stress states). On the other hand, CLs were expected to generalize to significantly more complex conditions, a task for which the development of continuum thermodynamics theories and computational techniques has been crucial.\n\nThe development of full-field experimental methods such as digital image correlation (DIC) \\[ [6](https://arxiv.org/html/2405.03658v1#bib.bib6), [7](https://arxiv.org/html/2405.03658v1#bib.bib7)\\], X-ray computed tomography \\[ [8](https://arxiv.org/html/2405.03658v1#bib.bib8), [9](https://arxiv.org/html/2405.03658v1#bib.bib9), [10](https://arxiv.org/html/2405.03658v1#bib.bib10)\\] and digital volume correlation (DVC) \\[ [11](https://arxiv.org/html/2405.03658v1#bib.bib11), [12](https://arxiv.org/html/2405.03658v1#bib.bib12)\\] and advances in the related computational approaches \\[ [13](https://arxiv.org/html/2405.03658v1#bib.bib13), [14](https://arxiv.org/html/2405.03658v1#bib.bib14)\\] have shifted the constitutive modeling paradigm from a limited- to a large-data regime. In particular, the computational multiscale techniques available to date deliver in-silico observations of arbitrary complex implicit macroscopic behaviors from simple CLs defined at the micro-scale. However, along with the opportunity to obtain better CL comes also the challenge of analyzing the increased amount of data available. In this context, the recent breakthrough in big data analysis and information mining gives a unique opportunity to improve constitutive modeling for applications in mechanics and material science.\nAlthough the preliminary results obtained in this area bring great promise of efficient and highly accurate predictions, the fast development of this discipline calls for a timely identification of the most promising directions to pursue in order to boost the progress of mechanics and that can introduce a diversity of solution techniques.\n\n### 1.1 Classification and nomenclature\n\nAvailable techniques that deal with large data\nare often referred to as data-driven (DD) approaches, due to their strict dependence upon a set of experimental or numerical observations of the material behavior\n(FigureÂ [1](https://arxiv.org/html/2405.03658v1#S1.F1)). In general, each method deploys a set of algorithms, assumptions and procedures, whose purpose is to analyze the available data and deduct useful information to describe the behavior of a certain material or class of materials.\nSince the recent breakthroughs in deep learning, machine learning (ML) techniques have received renewed interest in applications\nin mechanics and materials. Particularly the rise of sparse identification and discovery \\[ [15](https://arxiv.org/html/2405.03658v1#bib.bib15), [16](https://arxiv.org/html/2405.03658v1#bib.bib16)\\], Physics Informed neural networks (PINNs) and operator learning approaches \\[ [17](https://arxiv.org/html/2405.03658v1#bib.bib17), [18](https://arxiv.org/html/2405.03658v1#bib.bib18)\\] for the solution of forward and inverse problems involving partial differential equations (PDEs) \\[ [19](https://arxiv.org/html/2405.03658v1#bib.bib19), [20](https://arxiv.org/html/2405.03658v1#bib.bib20)\\] have had a significant impact on the computational engineering field, shifting the attention to combining data with physics. The great promise of fast and highly accurate predictions for mechanistic simulations as well as cost reduction in an industrial setting\nhas led to numerous attempts to integrate a wide spectrum of these techniques in the simulation workflow and also to facilitate material innovation. Combining physical constraints and data in the context of ML approaches has shown potential towards moderating the need for large training datasets \\[ [21](https://arxiv.org/html/2405.03658v1#bib.bib21)\\].\n\nFor the purpose of this review on DD constitutive modeling in solid mechanics, with the intent of providing a structured presentation, we propose for the available DD approaches the classification outlined in Figure [1](https://arxiv.org/html/2405.03658v1#S1.F1). We distinguish among two broad categories with corresponding subcategories, as follows:\n\n- â€¢\n\n\nmethods based on machine learning (ML) techniques. Within this category, we distinguish among\n\n\n\n- â€“\n\n\nuninterpretable (black-box or grey-box) approaches. This subcategory includes methods that obtain CLs in which the relation existing between inputs and outputs cannot be physically explained. These methods are also sometimes referred to as encoding or learning of CLs. Prominent ML techniques used in uninterpretable approaches are those based on many different types of neural networks (NNs) (see Section [2.1.1](https://arxiv.org/html/2405.03658v1#S2.SS1.SSS1)). The difference between black-box and grey-box approaches is that the latter encodes some known information about the physical system in the learning framework; this augments the clarity of the learned model but still does not enable interpretability. Recently, sparsification approaches for NN models aim to balance expressivity and interpretability.\n\n- â€“\n\n\ninterpretable approaches. Within this category fall the techniques that aim at defining an interpretable analytical expression for the CL, namely a relation between input and output quantities (e.g., strains and stresses) through mathematical operators and parameters whose role can be physically explained. In the literature, some of these methods are referred to as methods for automated discovery of CLs from data, whereby the term discovery is used to refer to simultaneous model selection and parameter identification. The special case of parameter identification, in which the functional form of the model is known a priori and only th","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2405.03658","id":"https://arxiv.org/html/2405.03658","publishedDate":"2025-09-22T00:00:00.000Z","summary":"The webpage presents a comprehensive review of data-driven approaches to modeling constitutive laws in solid mechanics. It categorizes methods into machine learning-based and model-free techniques, analyzing their interpretability, data requirements, and ability to generalize. The review emphasizes the importance of understanding both path-independent and path-dependent material responses, discussing how data-driven models can enhance the discovery, encoding, and prediction of material behavior. It also covers key aspects such as data sampling, experimental design, and validation, aiming to provide a roadmap for integrating data-driven techniques into solid mechanics modeling with attention to trustworthiness and applicability across scales.","title":"A review on data-driven constitutive laws for solids","url":"https://arxiv.org/html/2405.03658"},{"content":"\n \n [3176] \n \n arXiv:2405.07041\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3177] \n \n arXiv:2405.07043\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3178] \n \n arXiv:2405.07044\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3179] \n \n arXiv:2405.07045\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3180] \n \n arXiv:2405.07046\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3181] \n \n arXiv:2405.07047\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3182] \n \n arXiv:2405.07052\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3183] \n \n arXiv:2405.07054\n \n \n [ pdf, other]\n \n \n \n \n \n [3184] \n \n arXiv:2405.07056\n \n \n [ pdf, other]\n \n \n \n \n \n [3185] \n \n arXiv:2405.07059\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3186] \n \n arXiv:2405.07060\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3187] \n \n arXiv:2405.07061\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3188] \n \n arXiv:2405.07065\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3189] \n \n arXiv:2405.07067\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3190] \n \n arXiv:2405.07070\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3191] \n \n arXiv:2405.07072\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3192] \n \n arXiv:2405.07076\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3193] \n \n arXiv:2405.07079\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3194] \n \n arXiv:2405.07081\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3195] \n \n arXiv:2405.07083\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3196] \n \n arXiv:2405.07086\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3197] \n \n arXiv:2405.07087\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3198] \n \n arXiv:2405.07088\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3199] \n \n arXiv:2405.07089\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3200] \n \n arXiv:2405.07090\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3201] \n \n arXiv:2405.07094\n \n \n [ pdf, other]\n \n \n \n \n \n [3202] \n \n arXiv:2405.07096\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3203] \n \n arXiv:2405.07097\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3204] \n \n arXiv:2405.07098\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3205] \n \n arXiv:2405.07099\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3206] \n \n arXiv:2405.07101\n \n \n [ pdf, other]\n \n \n \n \n \n [3207] \n \n arXiv:2405.07104\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3208] \n \n arXiv:2405.07106\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3209] \n \n arXiv:2405.07107\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3210] \n \n arXiv:2405.07108\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3211] \n \n arXiv:2405.07111\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3212] \n \n arXiv:2405.07116\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3213] \n \n arXiv:2405.07117\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3214] \n \n arXiv:2405.07119\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3215] \n \n arXiv:2405.07121\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3216] \n \n arXiv:2405.07122\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3217] \n \n arXiv:2405.07124\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3218] \n \n arXiv:2405.07128\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3219] \n \n arXiv:2405.07131\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3220] \n \n arXiv:2405.07135\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3221] \n \n arXiv:2405.07139\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3222] \n \n arXiv:2405.07140\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3223] \n \n arXiv:2405.07142\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3224] \n \n arXiv:2405.07145\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3225] \n \n arXiv:2405.07146\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3226] \n \n arXiv:2405.07147\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3227] \n \n arXiv:2405.07151\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3228] \n \n arXiv:2405.07155\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3229] \n \n arXiv:2405.07157\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3230] \n \n arXiv:2405.07162\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3231] \n \n arXiv:2405.07164\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3232] \n \n arXiv:2405.07166\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3233] \n \n arXiv:2405.07167\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3234] \n \n arXiv:2405.07169\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3235] \n \n arXiv:2405.07171\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3236] \n \n arXiv:2405.07172\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3237] \n \n arXiv:2405.07174\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3238] \n \n arXiv:2405.07175\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3239] \n \n arXiv:2405.07176\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3240] \n \n arXiv:2405.07178\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3241] \n \n arXiv:2405.07180\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3242] \n \n arXiv:2405.07188\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3243] \n \n arXiv:2405.07189\n \n \n [ pdf, other]\n \n \n \n \n \n [3244] \n \n arXiv:2405.07193\n \n \n [ pdf, other]\n \n \n \n \n \n [3245] \n \n arXiv:2405.07194\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3246] \n \n arXiv:2405.07195\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3247] \n \n arXiv:2405.07196\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3248] \n \n arXiv:2405.07200\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3249] \n \n arXiv:2405.07201\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3250] \n \n arXiv:2405.07202\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3251] \n \n arXiv:2405.07204\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3252] \n \n arXiv:2405.07206\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3253] \n \n arXiv:2405.07210\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3254] \n \n arXiv:2405.07212\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3255] \n \n arXiv:2405.07213\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3256] \n \n arXiv:2405.07216\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3257] \n \n arXiv:2405.07220\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3258] \n \n arXiv:2405.07223\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3259] \n \n arXiv:2405.07224\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3260] \n \n arXiv:2405.07229\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3261] \n \n arXiv:2405.07232\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3262] \n \n arXiv:2405.07233\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3263] \n \n arXiv:2405.07236\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3264] \n \n arXiv:2405.07237\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3265] \n \n arXiv:2405.07241\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3266] \n \n arXiv:2405.07244\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3267] \n \n arXiv:2405.07248\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3268] \n \n arXiv:2405.07250\n \n \n [ pdf, other]\n \n \n \n \n \n [3269] \n \n arXiv:2405.07252\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3270] \n \n arXiv:2405.07257\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3271] \n \n arXiv:2405.07259\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3272] \n \n arXiv:2405.07260\n \n \n [ pdf, other]\n \n \n \n \n \n [3273] \n \n arXiv:2405.07263\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3274] \n \n arXiv:2405.07264\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3275] \n \n arXiv:2405.07265\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3276] \n \n arXiv:2405.07266\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3277] \n \n arXiv:2405.07267\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3278] \n \n arXiv:2405.07272\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3279] \n \n arXiv:2405.07274\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3280] \n \n arXiv:2405.07275\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3281] \n \n arXiv:2405.07277\n \n \n [ pdf, other]\n \n \n \n \n \n [3282] \n \n arXiv:2405.07278\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3283] \n \n arXiv:2405.07280\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3284] \n \n arXiv:2405.07282\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3285] \n \n arXiv:2405.07283\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3286] \n \n arXiv:2405.07284\n \n \n [ pdf, other]\n \n \n \n \n \n [3287] \n \n arXiv:2405.07288\n \n \n [ pdf, other]\n \n \n \n \n \n [3288] \n \n arXiv:2405.07291\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3289] \n \n arXiv:2405.07293\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3290] \n \n arXiv:2405.07305\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3291] \n \n arXiv:2405.07306\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3292] \n \n arXiv:2405.07309\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3293] \n \n arXiv:2405.07310\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3294] \n \n arXiv:2405.07311\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3295] \n \n arXiv:2405.07312\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3296] \n \n arXiv:2405.07314\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3297] \n \n arXiv:2405.07316\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3298] \n \n arXiv:2405.07317\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3299] \n \n arXiv:2405.07319\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3300] \n \n arXiv:2405.07320\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3301] \n \n arXiv:2405.07324\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3302] \n \n arXiv:2405.07326\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3303] \n \n arXiv:2405.07327\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3304] \n \n arXiv:2405.07331\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3305] \n \n arXiv:2405.07332\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3306] \n \n arXiv:2405.07335\n \n \n [ pdf, other]\n \n \n \n \n \n [3307] \n \n arXiv:2405.07336\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3308] \n \n arXiv:2405.07340\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3309] \n \n arXiv:2405.07343\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3310] \n \n arXiv:2405.07344\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3311] \n \n arXiv:2405.07346\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3312] \n \n arXiv:2405.07348\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3313] \n \n arXiv:2405.07349\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3314] \n \n arXiv:2405.07351\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3315] \n \n arXiv:2405.07353\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3316] \n \n arXiv:2405.07354\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3317] \n \n arXiv:2405.07358\n \n \n [ pdf, other]\n \n \n \n \n \n [3318] \n \n arXiv:2405.07359\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3319] \n \n arXiv:2405.07363\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3320] \n \n arXiv:2405.07364\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3321] \n \n arXiv:2405.07368\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3322] \n \n arXiv:2405.07369\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3323] \n \n arXiv:2405.07371\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3324] \n \n arXiv:2405.07373\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3325] \n \n arXiv:2405.07374\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3326] \n \n arXiv:2405.07376\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3327] \n \n arXiv:2405.07378\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3328] \n \n arXiv:2405.07381\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3329] \n \n arXiv:2405.07387\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3330] \n \n arXiv:2405.07391\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3331] \n \n arXiv:2405.07392\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3332] \n \n arXiv:2405.07393\n \n \n [ pdf, html, other]\n \n \n \n \n \n [3333] \n \n arXiv:2405.0739","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.arxiv.org%2Flist%2Fcs%2F2024-05%3Fskip%3D3175%26show%3D2000","id":"https://www.arxiv.org/list/cs/2024-05?skip=3175&show=2000","publishedDate":null,"summary":"This webpage lists recent preprints from arXiv's computer science category, specifically from May 2024. It features a series of numbered entries (from [3176] to [3189]), each linking to individual research papers with options to view PDFs, HTML versions, or other formats. The papers cover various topics within computer science, but specific details or abstracts are not provided on this page. Users interested in the latest research can access the full texts via the provided links.","title":"Computer Science May 2024 - arXiv","url":"https://www.arxiv.org/list/cs/2024-05?skip=3175&show=2000"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2405.16012v1","id":"https://arxiv.org/html/2405.16012v1","publishedDate":"2025-12-31T00:00:00.000Z","summary":"Failed to fetch content","title":"Pessimistic Backward Policy for GFlowNets","url":"https://arxiv.org/html/2405.16012v1"},{"content":"# A magnetised Galactic halo from inner Galaxy outflows\n\nHeshou Zhang1,âˆ—, Gabriele Ponti1,2,âˆ—, Ettore Carretti3,âˆ—, Ruo-Yu Liu4,5,âˆ—, Mark R. Morris6,âˆ—,\nMarijke Haverkorn7, Nicola Locatelli1,2, Xueying Zheng2, Felix Aharonian8,9,10, Haiming Zhang4,5,\nYi Zhang2, Giovanni Stel1,11, Andrew Strong2, Micheal Yeung2, Andrea Merloni2\n\nLarge-scale magnetic fields are observed off the midplanes of disk galaxies, indicating that they harbour magnetised halos\\[ [1](https://arxiv.org/html/2408.06312v1#bib.bib1), [2](https://arxiv.org/html/2408.06312v1#bib.bib2)\\]. These halos are crucial to studies of galaxy evolution, galactic-scale outflows, and feedback from star formation activity\\[ [3](https://arxiv.org/html/2408.06312v1#bib.bib3), [4](https://arxiv.org/html/2408.06312v1#bib.bib4), [5](https://arxiv.org/html/2408.06312v1#bib.bib5)\\]. Identifying the magnetised halo of the Milky Way is challenging because of the potential contamination from foreground emission arising in local spiral arms\\[ [6](https://arxiv.org/html/2408.06312v1#bib.bib6), [7](https://arxiv.org/html/2408.06312v1#bib.bib7)\\]. Additionally, it is unclear how our magnetic halo is influenced by recently revealed large-scale structures such as the X-ray emitting eROSITA Bubbles\\[ [8](https://arxiv.org/html/2408.06312v1#bib.bib8)\\], which, according to previous simulations, might be transient structures powered by the Galactic Center\\[ [9](https://arxiv.org/html/2408.06312v1#bib.bib9), [10](https://arxiv.org/html/2408.06312v1#bib.bib10)\\] or the Galaxyâ€™s star-forming ring\\[ [11](https://arxiv.org/html/2408.06312v1#bib.bib11)\\]. Here we report the identification of several kpc-scale magnetised structures based on their polarized radio emission and their gamma-ray counterparts, which can be interpreted as the radiation of relativistic electrons. These non-thermal structures extend far above and below the Galactic plane and are spatially coincident with the thermal X-ray emission from the eROSITA Bubbles. The morphological consistency of these structures suggests a common origin, which can be sustained by Galactic outflows driven by the active star-forming regions located at 3âˆ’5353-53 - 5Â kpc from the Galactic Centre. These results reveal how X-ray-emitting and magnetised halos of spiral galaxies can be related to intense star formation activities and suggest that the X-shaped coherent magnetic structures observed in their halos can stem from galaxy outflows.\n\n## Main\n\nX-ray-emitting galactic halos have been discovered in star-forming galaxies\\[ [12](https://arxiv.org/html/2408.06312v1#bib.bib12), [13](https://arxiv.org/html/2408.06312v1#bib.bib13)\\]. Several of them are accompanied by large-scale coherent magnetic structures revealed by radio data\\[ [14](https://arxiv.org/html/2408.06312v1#bib.bib14), [2](https://arxiv.org/html/2408.06312v1#bib.bib2)\\]. However, the relationship between the X-ray-emitting and magnetised galactic halos is unclear, and similarly for their nature and origins. The recent discovery of the X-ray emitting large structures of the Milky Way, the eROSITA Bubbles\\[ [8](https://arxiv.org/html/2408.06312v1#bib.bib8)\\], provides important physical insights to our understanding of galactic halos.\nFigure [1](https://arxiv.org/html/2408.06312v1#Sx1.F1) compares the eROSITA all-sky emission at 0.6-1.0Â keV with the magnetic field determined from the polarized synchrotron emission at 22.8Â GHz from WMAP\\[ [15](https://arxiv.org/html/2408.06312v1#bib.bib15)\\], for which Faraday rotation effects are marginal. Several magnetic structures revealed through their polarized emission and coherent field line direction, here denoted as magnetic ridges, appear in the inner Galaxy, emerging from the Galactic plane\\[ [16](https://arxiv.org/html/2408.06312v1#bib.bib16)\\] and stretching for more than 20âˆ˜. The polarized intensity is enhanced at the edges of the eROSITA Bubbles. The magnetic field directions are parallel to the Bubblesâ€™ edges in the east. The magnetic ridges show a general tilt westwards, starting from the disc and rising to high latitudes. This implies a potential connection between magnetic ridges and the eROSITA Bubbles.\n\nA key issue for these extended structures is whether they are local objects within the Local hot Bubble (LB)\\[ [17](https://arxiv.org/html/2408.06312v1#bib.bib17)\\], or distant Galactic structures. Thus far, they have mostly been modeled as shells of old supernova remnants in the LB\\[ [18](https://arxiv.org/html/2408.06312v1#bib.bib18), [16](https://arxiv.org/html/2408.06312v1#bib.bib16)\\]. Extended Data FigureÂ [3](https://arxiv.org/html/2408.06312v1#Sx2.F3), panels a-d, reveal an anti-correlation between the X-ray maps of eROSITA\\[ [8](https://arxiv.org/html/2408.06312v1#bib.bib8)\\] (0.6â€“1.0Â keV) at mid/low Galactic latitudes and the dust column density based on the dust distribution within 500Â pc from the Sun by \\[Ref.\\[ [19](https://arxiv.org/html/2408.06312v1#bib.bib19)\\]\\]. Therefore, the local dust within 500Â pc is responsible for the X-ray absorption, implying that the X-ray emitting eROSITA Bubbles are not local and the bulk of the emitting structures must originate from a distance beyond the 500-pc of our Local Arm.\n\nWe also estimate our distance from the polarized synchrotron emission of the magnetised ridges thanks to the Faraday-rotation depolarization of the foreground turbulent magnetised medium\\[ [20](https://arxiv.org/html/2408.06312v1#bib.bib20), [21](https://arxiv.org/html/2408.06312v1#bib.bib21), [22](https://arxiv.org/html/2408.06312v1#bib.bib22)\\] (see Methods and Extended Data FiguresÂ [3](https://arxiv.org/html/2408.06312v1#Sx2.F3)â€“ [5](https://arxiv.org/html/2408.06312v1#Sx2.F5)).\nThe depolarization is largest at lower frequencies and decreases at higher latitudes because of a decline with latitude of the magnetic field strength and electron density in the foreground medium. Using the depolarization expected from the magneto-ionic medium out to different depths, we find that the observed depolarization is consistent with that produced by the medium out to distances of severalÂ kpc (Extended Data FigureÂ [4](https://arxiv.org/html/2408.06312v1#Sx2.F4)). The polarized magnetic ridges thus are several kpc-scale Galactic structures. This indicates that the bulk of the emission associated with the North Polar Spur (NPS) is beyond several kpc from us, however it does not exclude a smaller contribution from some local features.\n\nThe fact that synchrotron radiation is enhanced at the edges of the eROSITA Bubbles indicates the presence of relativistic electrons. Those electrons can also give rise to gamma-ray emission via inverse Compton (IC) scattering of photons from interstellar radiation and the Cosmic Microwave Background (CMB). We investigate the potential gamma-ray counterparts of the eROSITA Bubbles using the Fermi-LAT data of the diffuse all-sky gamma-ray intensity from \\[Ref.\\[ [23](https://arxiv.org/html/2408.06312v1#bib.bib23)\\]\\]. The relative excess of the gamma-ray intensity above the background is presented in FigureÂ [2](https://arxiv.org/html/2408.06312v1#Sx1.F2)(a) and Extended Data FigureÂ [6](https://arxiv.org/html/2408.06312v1#Sx2.F6). The horizontal cuts at north and south high Galactic latitudes above and below the Fermi Bubbles are shown in Extended Data FigureÂ [8](https://arxiv.org/html/2408.06312v1#Sx2.F8). We observe that extended structures with Î³âˆ’limit-fromð›¾\\\\gamma-italic\\_Î³ -ray enhancements show agreement with a large part of the edges of the eROSITA Bubbles. The consistency in the north can be observed in all three energy ranges for the eROSITA Bubble at mid/high latitude (b>30âˆ˜ð‘superscript30b>30^{\\\\circ}italic\\_b > 30 start\\_POSTSUPERSCRIPT âˆ˜ end\\_POSTSUPERSCRIPT). The consistency in the south is observed for EÎ³â‰³100greater-than-or-equivalent-tosubscriptð¸ð›¾100E\\_{\\\\gamma}\\\\gtrsim 100italic\\_E start\\_POSTSUBSCRIPT italic\\_Î³ end\\_POSTSUBSCRIPT â‰³ 100Â GeV and for bâ‰³âˆ’60âˆ˜greater-than-or-equivalent-toð‘superscript60b\\\\gtrsim-60^{\\\\circ}italic\\_b â‰³ - 60 start\\_POSTSUPERSCRIPT âˆ˜ end\\_POSTSUPERSCRIPT (Extended Data FigureÂ [8](https://arxiv.org/html/2408.06312v1#Sx2.F8)), while no clear structure is observed at the cap of the southern eROSITA Bubble.\nThe similarity in the morphology between radio and Î³ð›¾\\\\gammaitalic\\_Î³-ray bands implies a common origin of the emissions in these two bands and we will study their spectral energy distribution.\n\nWe define the region outside the Fermi Bubbles but inside the edges of the eROSITA Bubbles as the outer region111The eROSITA Bubbles contain the Fermi Bubbles in projection, but it is extremely difficult to imagine a scenario in which the two Galaxy-scale features are centered at different distances. It is worth noting that the origin of the Fermi Bubbles and its potential low-latitude radio counterparts (the so-called â€œradio hazeâ€)\\[ [24](https://arxiv.org/html/2408.06312v1#bib.bib24), [25](https://arxiv.org/html/2408.06312v1#bib.bib25)\\] can either be the outflows from the star formation activity of the CMZ\\[ [26](https://arxiv.org/html/2408.06312v1#bib.bib26), [27](https://arxiv.org/html/2408.06312v1#bib.bib27)\\] or the past activity of the central Supermassive Black hole Sgr Aâˆ—\\[ [9](https://arxiv.org/html/2408.06312v1#bib.bib9)\\]. Investigating the origin of the Fermi Bubbles is beyond the scope of this work.. In FigureÂ [2](https://arxiv.org/html/2408.06312v1#Sx1.F2), two patches within the outer region are selected for further investigation: patch R1 in the north (20âˆ˜ sâ¢yâ¢n,mâ¢aâ¢x0.04{}\\_{syn,max}>0.04start\\_FLOATSUBSCRIPT italic\\_s italic\\_y italic\\_n , italic\\_m italic\\_a italic\\_x end\\_FLOATSUBSCRIPT > 0.04Â K are preserved in FigureÂ [1](https://arxiv.org/html/2408.06312v1#Sx1.F1), and the corresponding magnetic field directions are overlaid. Several coherent magnetic structures extending more than 15âˆ˜superscript1515^{\\\\circ}15 start\\_POSTSUPERSCRIPT âˆ˜ end\\_POSTSUPERSCRIPT are found and we define them as magnetic ridges. Extended Data Figure [3](","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2408.06312v1","id":"https://arxiv.org/html/2408.06312v1","publishedDate":"2025-12-31T00:00:00.000Z","summary":"This research explores the magnetic and thermal structures of the Milky Way's halo, focusing on the relationship between large-scale magnetic fields and X-ray-emitting features. The study identifies several kiloparsec-scale magnetized structures, observed through polarized radio emission and gamma-ray counterparts, which extend above and below the Galactic plane. These structures spatially coincide with the eROSITA Bubblesâ€”large X-ray emitting features likely powered by activity near the Galactic Center or star-forming regions 3â€“5 kpc away. The findings suggest that these magnetized halos result from galactic outflows driven by intense star formation, linking the observed magnetic ridges and the X-ray structures. Overall, the study provides evidence that galaxy outflows can shape the magnetic and thermal halo features, offering insights into the interplay between star formation, magnetic fields, and large-scale galactic structures.","title":"A magnetised Galactic halo from inner Galaxy outflows","url":"https://arxiv.org/html/2408.06312v1"},{"content":"# Computer Science > Programming Languages\n\n**arXiv:2405.08965** (cs)\n\n\\[Submitted on 14 May 2024 ( [v1](https://arxiv.org/abs/2405.08965v1)), last revised 3 Sep 2025 (this version, v5)\\]\n\n# Title:MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming\n\nAuthors: [Jayanaka L. Dantanarayana](https://arxiv.org/search/cs?searchtype=author&query=Dantanarayana,+J+L), [Yiping Kang](https://arxiv.org/search/cs?searchtype=author&query=Kang,+Y), [Kugesan Sivasothynathan](https://arxiv.org/search/cs?searchtype=author&query=Sivasothynathan,+K), [Christopher Clarke](https://arxiv.org/search/cs?searchtype=author&query=Clarke,+C), [Baichuan Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+B), [Savini Kashmira](https://arxiv.org/search/cs?searchtype=author&query=Kashmira,+S), [Krisztian Flautner](https://arxiv.org/search/cs?searchtype=author&query=Flautner,+K), [Lingjia Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+L), [Jason Mars](https://arxiv.org/search/cs?searchtype=author&query=Mars,+J)\n\nView a PDF of the paper titled MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming, by Jayanaka L. Dantanarayana and 8 other authors\n\n[View PDF](https://arxiv.org/pdf/2405.08965) [HTML (experimental)](https://arxiv.org/html/2405.08965v5)\n\n> Abstract:Software development is shifting from traditional programming to AI-integrated applications that leverage generative AI and large language models (LLMs) during runtime. However, integrating LLMs remains complex, requiring developers to manually craft prompts and process outputs. Existing tools attempt to assist with prompt engineering, but often introduce additional complexity. This paper presents Meaning-Typed Programming (MTP), a novel paradigm that abstracts LLM integration through intuitive language-level constructs. By leveraging the inherent semantic richness of code, MTP automates prompt generation and response handling without additional developer effort. We introduce the (1) by operator for seamless LLM invocation, (2) MT-IR, a meaning-based intermediate representation for semantic extraction, and (3) MT-Runtime, an automated system for managing LLM interactions. We implement MTP in Jac, a programming language that supersets Python, and find that MTP significantly reduces coding complexity while maintaining accuracy and efficiency. MTP significantly reduces development complexity, lines of code modifications needed, and costs while improving run-time performance and maintaining or exceeding the accuracy of existing approaches. Our user study shows that developers using MTP completed tasks 3.2x faster with 45% fewer lines of code compared to existing frameworks. Moreover, MTP demonstrates resilience even when up to 50% of naming conventions are degraded, demonstrating robustness to suboptimal code. MTP is developed as part of the Jaseci open-source project, and is available under the module byLLM.\n\n| | |\n| --- | --- |\n| Comments: | OOPSLA 2025 |\n| Subjects: | Programming Languages (cs.PL); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2405.08965](https://arxiv.org/abs/2405.08965) \\[cs.PL\\] |\n| (or [arXiv:2405.08965v5](https://arxiv.org/abs/2405.08965v5) \\[cs.PL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2405.08965](https://doi.org/10.48550/arXiv.2405.08965) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jayanaka L. Dantanarayana \\[ [view email](https://arxiv.org/show-email/13cb6fa0/2405.08965)\\] **[\\[v1\\]](https://arxiv.org/abs/2405.08965v1)**\nTue, 14 May 2024 21:12:01 UTC (623 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2405.08965v2)**\nMon, 14 Oct 2024 21:20:40 UTC (4,649 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2405.08965v3)**\nThu, 16 Jan 2025 18:56:27 UTC (2,918 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2405.08965v4)**\nWed, 4 Jun 2025 00:51:13 UTC (3,247 KB)\n**\\[v5\\]**\nWed, 3 Sep 2025 21:19:19 UTC (3,661 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MTP: A Meaning-Typed Language Abstraction for AI-Integrated Programming, by Jayanaka L. Dantanarayana and 8 other authors\n\n- [View PDF](https://arxiv.org/pdf/2405.08965)\n- [HTML (experimental)](https://arxiv.org/html/2405.08965v5)\n- [TeX Source](https://arxiv.org/src/2405.08965)\n- [Other Formats](https://arxiv.org/format/2405.08965)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.PL\n\n[<Â prev](https://arxiv.org/prevnext?id=2405.08965&function=prev&context=cs.PL)Â  \\| Â [nextÂ >](https://arxiv.org/prevnext?id=2405.08965&function=next&context=cs.PL)\n\n[new](https://arxiv.org/list/cs.PL/new) \\| [recent](https://arxiv.org/list/cs.PL/recent) \\| [2024-05](https://arxiv.org/list/cs.PL/2024-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2405.08965?context=cs) [cs.AI](https://arxiv.org/abs/2405.08965?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2405.08965)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2405.08965)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2405.08965)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\nÃ—\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2405.08965) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2405.08965","id":"https://arxiv.org/abs/2405.08965","publishedDate":"2024-05-14T00:00:00.000Z","summary":"The paper introduces Meaning-Typed Programming (MTP), a novel approach designed to simplify the integration of large language models (LLMs) and generative AI into software development. Traditional methods require manual prompt engineering and complex handling of AI responses, which can be error-prone and inefficient. MTP addresses this by providing language-level abstractions that leverage the semantic richness of code, automating prompt creation and response processing.\n\nKey components of MTP include:\n- The **\"by\" operator** for seamless invocation of LLMs.\n- **MT-IR**, a meaning-based intermediate representation that captures semantic information.\n- **MT-Runtime**, an automated runtime system managing AI interactions.\n\nImplemented as an extension of Python called Jac, MTP significantly reduces coding complexity, lines of code, and costs while improving runtime performance and maintaining or exceeding existing accuracy levels. Developer studies show tasks are completed over three times faster with fewer lines of code, and the system remains robust even when naming conventions are degraded.\n\nDeveloped as part of the open-source Jaseci project, MTP aims to make AI integration more intuitive, efficient, and resilient, facilitating the development of AI-enhanced applications with less effort and higher reliability.","title":"MTP: A Meaning-Typed Language Abstraction for AI-Integrated ...","url":"https://arxiv.org/abs/2405.08965"},{"content":"| | | |\n| --- | --- | --- |\n| | | I like this. Very much falls into the \"make bad state unrepresentable\". The issues I see with this approach is when developers stop at this first level of type implementation. Everything is a type and nothing works well together, tons of types seem to be subtle permutations of each other, things get hard to reason about etc. In systems like that I would actually rather be writing a weakly typed dynamic language like JS or a strongly typed dynamic language like Elixir. However, if the developers continue pushing logic into type controlled flows, eg:move conditional logic into union types with pattern matching, leverage delegation etc. the experience becomes pleasant again. Just as an example (probably not the actual best solution) the \"DewPoint\" function could just take either type and just work. |\n\n| | | |\n| --- | --- | --- |\n| | | Yep. For this reason, I wish more languages supported bound integers. Eg, rather than saying x: u32, I want to be able to use the type system to constrain x to the range of \\[0, 10). This would allow for some nice properties. It would also enable a bunch of small optimisations in our languages that we can't have today. Eg, I could make an integer that must fall within my array bounds. Then I don't need to do bounds checking when I index into my array. It would also allow a lot more peephole optimisations to be made with Option. Weirdly, rust already kinda supports this _within_ a function thanks to LLVM magic. But it doesn't support it for variables passed between functions. |\n\n| | | |\n| --- | --- | --- |\n| | | Ada has this ability to define ranges for subtypes. I wish language designers would look at Ada more often. |\n\n| | | |\n| --- | --- | --- |\n| | | Academic language designers do! But it takes a while for academic features to trickle down to practical languagesâ€”especially because expressive-enough refinement typing on even the integers leads to an undecidable theory. |\n\n| | | |\n| --- | --- | --- |\n| | | Aren't most type systems in widely used languages Turing complete and (consequently) undecidable? Typescript and python are two examples that come to mind But yeah maybe expressive enough refinement typing leads to hard to write and slow type inference engines |\n\n| | | |\n| --- | --- | --- |\n| | | Pascal had range types such as 0..9 (as of 1970). Subranges could also be defined for any scalar type. Further, array index types were such ranges. |\n\n| | | |\n| --- | --- | --- |\n| | | You can do this quite easily in Rust. But you have to overload operators to make your type make sense. That's also possible, you just need to define what type you get after dividing your type by a regular number and vice versa a regular number by your type. Or what should happen if when adding two of your types the sum is higher than the maximum value. This is quite verbose. Which can be done with generics or macros. |\n\n| | | |\n| --- | --- | --- |\n| | | You can do it _at runtime_ quite easily in rust. But the rust compiler doesnâ€™t understand what youâ€™re doing - so it canâ€™t make use of that information for peephole optimisations or to elide array bounds checks when using your custom type. And you donâ€™t get runtime errors instead of compile time errors if you try to assign the wrong value into your type. |\n\n| | | |\n| --- | --- | --- |\n| | | Off topic: do you use raku in day to day life? I tried learning it but perl5 remains my go-to when I just need to whip something up |\n\n| | | |\n| --- | --- | --- |\n| | | The generic magic for this is called â€œdependant typesâ€ I believe - generics that can take values as well as types as parameters. Idris supports these |\n\n| | | |\n| --- | --- | --- |\n| | | The full-blown version that guarantees no bounds-check errors at runtime requires dependent types (and consequently requires programmers to work with a proof assistant, which is why it's not very popular). You could have a more lightweight version that instead just crashes the program at runtime if an out-of-range assignment is attempted, and optionally requires such fallible assignments to be marked as such in the code. Rust can do this today with const generics, though it's rather clunky as there's very little syntactic sugar and no implicit widening. |\n\n| | | |\n| --- | --- | --- |\n| | | AIUI WUFFS doesn't need a full blown proof assistant because instead of attempting the difficult problem \"Can we prove this code is safe?\" it has the programmer provide elements of such a proof as they write their program so it can merely ask \"Is _this_ a proof that the program is safe?\" instead. |\n\n| | | |\n| --- | --- | --- |\n| | | ATS does this. Works quite well since multiplication by known factors and addition of type variables + inequalities is decidable (and in fact quadratic). |\n\n| | | |\n| --- | --- | --- |\n| | | This can be done in typescript. Itâ€™s not super well known because of typescripts association with frontend and JavaScript. But typescript is a language with one of the most powerful type systems ever. Among the popular languages like golang, rust or python typescript has the most powerful type system. How about a type with a number constrained between 0 and 10? You can already do this in typescript. ``` type onetonine = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 ``` You can even programmatically define functions at the type level. So you can create a function that outputs a type between 0 to N. ``` type Range = A['length'] extends N ? A[number] : Range; ``` The issue here is that itâ€™s a bit awkward you want these types to compose right? If I add two constrained numbers say one with max value of 3 and another with max value of two the result should be max value of 5. Typescript doesnâ€™t support this by default with default addition. But you can create a function that does this. ``` // Build a tuple of length L type BuildTuple = T['length'] extends L ? T : BuildTuple; // Add two numbers by concatenating their tuples type Add = [...BuildTuple, ...BuildTuple]['length']; // Create a union: 0 | 1 | 2 | ... | N-1 type Range = A['length'] extends N ? A[number] : Range; function addRanges< A extends number, B extends number >( a: Range, b: Range ): Range > { return (a + b) as Range >; } ``` The issue is to create these functions you have to use tuples to do addition at the type level and you need to use recursion as well. Typescript recursion stops at 100 so thereâ€™s limits. Additionally itâ€™s not intrinsic to the type system. Like you need peanno numbers built into the number system and built in by default into the entire language for this to work perfectly. That means the code in the function is not type checked but if you assume that code is correct then this function type checks when composed with other primitives of your program. |\n\n| | | |\n| --- | --- | --- |\n| | | Complexity is bad in software. I think this kind of thing does more harm than good. I get an error that I can't assign something that seems to me assignable, and to figure out why I need to study functions at type level using tuples and recursion. The cure is worse than the disease. |\n\n| | | |\n| --- | --- | --- |\n| | | It can work. It depends on context. Like let's say these types are from a well renowned library or one that's been used by the codebase for a long time. If you trust the type, then it's fine. The code is safer. In the world of of the code itself things are easier. Of course like what you're complaining about, this opens up the possibility of more bugs in the world of types, and debugging that can be a pain. Trade offs. In practice people usually don't go crazy with type level functions. They can do small stuff, but usually nothing super crazy. So type script by design sort of fits the complexity dynamic you're looking for. Yes you can do type level functions that are super complex, but the language is not designed around it and it doesn't promote that style either. But you CAN go a little deeper with types then say a language with less power in the type system like say Rust. |\n\n| | | |\n| --- | --- | --- |\n| | | Typescript's type system is turing complete, so you can do basically anything with it if this sort of thing is fun to you. Which is pretty much my problem with it: this sort of thing can be fun, _feels_ intellectually stimulating. But the added power doesn't make coding easier or make the code more sound. I've heard this sort of thing called the \"type puzzle trap\" and I agree with that. I'll take a modern hindley milner variant any day. Sophisticated enough to model nearly any _type information_ you'll have need of, without blurring the lines or admitting the temptation of encoding complex logic in it. |\n\n| | | |\n| --- | --- | --- |\n| | | >Which is pretty much my problem with it: this sort of thing can be fun, feels intellectually stimulating. But the added power doesn't make coding easier or make the code more sound. In practice nobody goes too crazy with it. You have a problem with a feature almost nobody uses. It's there and Range is like the upper bound of complexity I've seen in production but that is literally extremely rare as well. There is no \"temptation\" of coding complex logic in it at all as the language doesn't promote these features at all. It's just available if needed. It's not well known but typescript types can be easily used to be 1 to 1 with any hindley milner variant. It's the reputational baggage of JS and frontend that keeps this fact from being well known. In short: Typescript is more powerful then hindley milner, a subset of it has one to one parity with it, the parts that are more powerful then hindley milner aren't popular and used that widely nor does the flow of the language itself promote there usage. The feature is just there if you need it. If you want a language where you do this stuff in practice take a look at Idris. That language has these features built into the language AND it's an ML style language like haskell. |\n\n| | | |\n| --- | --- | --- |\n| | | I have definitely worked in TS co","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fnews.ycombinator.com%2Fitem%3Fid%3D44671484","id":"https://news.ycombinator.com/item?id=44671484","publishedDate":"2025-07-24T00:00:00.000Z","summary":"The webpage discusses the potential and challenges of leveraging advanced type systems to improve software safety and performance. It highlights ideas like making bad states unrepresentable through types, such as using range-constrained integers (e.g., x: 0..10), which can prevent errors like out-of-bounds access and enable optimizations. Languages like Ada support subtype ranges, and Rust can emulate such constraints with operator overloading, though often at verbosity. The conversation notes that many widely used languages have decidable, Turing-complete type systems, but more expressive systems like dependent types (supported by Idris) can provide stronger guarantees, albeit with complexity and decidability issues. Overall, the discussion emphasizes the benefits of integrating richer type informationâ€”such as bounds and dependent typesâ€”to enhance code safety and efficiency, while acknowledging practical limitations and the slow adoption of such features in mainstream languages.","title":"Use Your Type System | Hacker News","url":"https://news.ycombinator.com/item?id=44671484"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.reddit.com%2Fr%2Fhackernews%2Fcomments%2F1mh4be7%2Ftyped_languages_are_better_suited_for_vibecoding%2F","id":"https://www.reddit.com/r/hackernews/comments/1mh4be7/typed_languages_are_better_suited_for_vibecoding/","publishedDate":"2025-08-04T00:00:00.000Z","summary":"Failed to fetch content","title":"Typed languages are better suited for vibecoding : r/hackernews","url":"https://www.reddit.com/r/hackernews/comments/1mh4be7/typed_languages_are_better_suited_for_vibecoding/"},{"content":"HTML conversions [sometimes display errors](https://info.dev.arxiv.org/about/accessibility_html_error_messages.html) due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.\n\n- failed: forest\n\nAuthors: achieve the best HTML results from your LaTeX submissions by selecting from this list of [supported packages](https://corpora.mathweb.org/corpus/arxmliv/tex_to_html/info/loaded_file).\n\nLicense: CC BY-NC-ND 4.0\n\narXiv:2311.07989v3 \\[cs.CL\\] 05 Dec 2023\n\n# Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code\n\nZiyin Zhang\n\nShanghai Jiao Tong University\n\nAnt Group\nChaoyu Chen\n\nAnt Group\nBingchang Liu\n\nAnt Group\nCong Liao\n\nAnt Group\nZi Gong\n\nAnt Group\nHang Yu hyu.hugo@antgroup.com\n\nAnt Group\nJianguo Li lijg.zero@antgroup.com\n\nAnt Group\nRui Wang wangrui12@sjtu.edu.cn\n\nShanghai Jiao Tong University\n\n###### Abstract\n\nIn this work we systematically review the recent advancements in code processing with language models, covering 50+ models, 30+ evaluation tasks, 170+ datasets, and 700 related works. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also discuss code-specific features such as AST, CFG, and unit tests, along with their application in training code language models, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on GitHub at [https://github.com/codefuse-ai/Awesome-Code-LLM](https://github.com/codefuse-ai/Awesome-Code-LLM).\n\n## 1 Introduction\n\nLanguage modeling has advanced remarkably in recent years with the advent of pretrained TransformersÂ (Vaswani etÂ al., [2017](https://arxiv.org/html/2311.07989v3/#bib.bib565)) such as BERTÂ (Devlin etÂ al., [2019](https://arxiv.org/html/2311.07989v3/#bib.bib131)) and GPTÂ (Radford etÂ al., [2018](https://arxiv.org/html/2311.07989v3/#bib.bib463)). As large language models (LLMs) scaled to hundreds of billions of parameters and started to display early signs of artificial general intelligenceÂ (Brown etÂ al., [2020](https://arxiv.org/html/2311.07989v3/#bib.bib66); Chowdhery etÂ al., [2022](https://arxiv.org/html/2311.07989v3/#bib.bib105); OpenAI, [2023](https://arxiv.org/html/2311.07989v3/#bib.bib428)), their applications have also transcended text processing. Pioneered by CodexÂ (Chen etÂ al., [2021b](https://arxiv.org/html/2311.07989v3/#bib.bib91)), LLMs have achieved impressive results in code processing, giving rise to commercial products such as GitHub Copilot111 [https://github.com/features/copilot](https://github.com/features/copilot) and open-source multi-billion code models such as StarCoderÂ (Li etÂ al., [2023g](https://arxiv.org/html/2311.07989v3/#bib.bib322)) and Code LLaMAÂ (RoziÃ¨re etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib479)).\n\nThe application of pretrained Transformers in code processing, however, can be traced back to dates before decoder-only autoregressive models became dominantÂ (Feng etÂ al., [2020](https://arxiv.org/html/2311.07989v3/#bib.bib157); Liu etÂ al., [2020](https://arxiv.org/html/2311.07989v3/#bib.bib351)), and this domain is yet to witness a comprehensive review. In an attempt to bridge the gap between natural language processingÂ (NLP) community and software engineeringÂ (SE) community on the topic of language model applications, we undertake a panoramic survey of language models for code in this work, covering 50+ models, 30+ downstream tasks, 170+ datasets, and 700 related works. We break down different categories of code language models, ranging from colossal models trained on general domains to tiny models trained specifically for code understanding or generation. We emphasize on the relations and differences between such models, and highlight the integration of code-specific features, such as abstract syntax trees or data flows, into language models, as well as the latest techniques adapted from NLP.\n\nRelated to our work, we are aware of several surveys on similar topics, with three works concurrent to usÂ (Hou etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib227); Zheng etÂ al., [2023b](https://arxiv.org/html/2311.07989v3/#bib.bib682); She etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib503)). These works, however, focus either on NLP sideÂ (Zan etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib665); Xu & Zhu, [2022](https://arxiv.org/html/2311.07989v3/#bib.bib631)) or SE sideÂ (Niu etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib422); Hou etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib227); Zheng etÂ al., [2023b](https://arxiv.org/html/2311.07989v3/#bib.bib682); She etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib503)), and do not cover models, tasks, and challenges from the other side. For example, Zan etÂ al. ( [2023](https://arxiv.org/html/2311.07989v3/#bib.bib665)) focus on LLMs for text-to-code generation, while giving little discussion of other evaluation tasks in software engineering community. Hou etÂ al. ( [2023](https://arxiv.org/html/2311.07989v3/#bib.bib227)) and She etÂ al. ( [2023](https://arxiv.org/html/2311.07989v3/#bib.bib503)), in contrast, comprehensively review works from SE venues such as ASE and ICSE, but cite only a handful of works from deep learning and NLP venues such as ACL, EMNLP, NeurIPS, and ICLR.\n\n{forest}\n\nfor tree=\nforked edges,\ndraw,\nrounded corners,\ngrow=east,\nanchor=base west,\nanchor=center,\nreversed=true,\n,\nwhere level=0font=,\nwhere level=1text width=5.5em,font=,\nwhere level=2text width=4.5em,font=,\nwhere level=3text width=1.8em,font=,\nwhere level=4text width=5.5em,font=,\n\\[Code LMs\n\\[Raw LM\n\\[LaMDAÂ (Thoppilan etÂ al., [2022](https://arxiv.org/html/2311.07989v3/#bib.bib546)), PaLMÂ (Chowdhery etÂ al., [2022](https://arxiv.org/html/2311.07989v3/#bib.bib105)), GPT-NeoXÂ (Black etÂ al., [2022](https://arxiv.org/html/2311.07989v3/#bib.bib59)),\nBLOOMÂ (Scao etÂ al., [2022](https://arxiv.org/html/2311.07989v3/#bib.bib491)), LLaMAÂ (Touvron etÂ al., [2023a](https://arxiv.org/html/2311.07989v3/#bib.bib548)), GPT-4Â (OpenAI, [2023](https://arxiv.org/html/2311.07989v3/#bib.bib428)),\nLLaMA 2Â (Touvron etÂ al., [2023b](https://arxiv.org/html/2311.07989v3/#bib.bib549)), Phi-1.5Â (Li etÂ al., [2023h](https://arxiv.org/html/2311.07989v3/#bib.bib333)),\ntext width=27em,font=\\]\n\\]\n\\[Adapted LM\n\\[CodexÂ (Chen etÂ al., [2021b](https://arxiv.org/html/2311.07989v3/#bib.bib91)), PaLM CoderÂ (Chowdhery etÂ al., [2022](https://arxiv.org/html/2311.07989v3/#bib.bib105)), MinervaÂ (Lewkowycz etÂ al., [2022](https://arxiv.org/html/2311.07989v3/#bib.bib308)),\nPaLM 2\\*Â (Anil etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib25)), Code LLaMAÂ (RoziÃ¨re etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib479)),\ntext width=27em,font=\\]\n\\]\n\\[Specialized LM\n\\[Encoder\n\\[CuBERTÂ (Kanade etÂ al., [2020](https://arxiv.org/html/2311.07989v3/#bib.bib268)), CodeBERTÂ (Feng etÂ al., [2020](https://arxiv.org/html/2311.07989v3/#bib.bib157)), GraphCodeBERTÂ (Guo etÂ al., [2021a](https://arxiv.org/html/2311.07989v3/#bib.bib188)),\nSynCoBERTÂ (Wang etÂ al., [2021c](https://arxiv.org/html/2311.07989v3/#bib.bib586)), Code-MVPÂ (Wang etÂ al., [2022d](https://arxiv.org/html/2311.07989v3/#bib.bib588)), SCodeRÂ (Li etÂ al., [2022e](https://arxiv.org/html/2311.07989v3/#bib.bib326)),\ntext width=20.5em\\]\n\\]\n\\[Decoder\n\\[CLM\n\\[GPT-CÂ (Svyatkovskiy etÂ al., [2020](https://arxiv.org/html/2311.07989v3/#bib.bib536)), CodeGPTÂ (Lu etÂ al., [2021](https://arxiv.org/html/2311.07989v3/#bib.bib375)), PolyCoderÂ (Xu etÂ al., [2022](https://arxiv.org/html/2311.07989v3/#bib.bib625)),\nCodeGenÂ (Nijkamp etÂ al., [2023b](https://arxiv.org/html/2311.07989v3/#bib.bib418)), PyCodeGPTÂ (Zan etÂ al., [2022](https://arxiv.org/html/2311.07989v3/#bib.bib664)), PanGu-CoderÂ (Christopoulou etÂ al., [2022](https://arxiv.org/html/2311.07989v3/#bib.bib107)),\nCodeGeeXÂ (Zheng etÂ al., [2023a](https://arxiv.org/html/2311.07989v3/#bib.bib680)), JamÂ (Su etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib524)),\nPhi-1Â (Gunasekar etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib186)), CodeFuseÂ (Di etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib132)),\ntext width=16.8em\\]\n\\]\n\\[FIM\n\\[InCoderÂ (Fried etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib163)), SantaCoderÂ (Allal etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib9)), StarCoderÂ (Li etÂ al., [2023g](https://arxiv.org/html/2311.07989v3/#bib.bib322)),\ntext width=16.8em\\]\n\\]\n\\]\n\\[UniLM\n\\[CugLMÂ (Liu etÂ al., [2020](https://arxiv.org/html/2311.07989v3/#bib.bib351)), UniXcoderÂ (Guo etÂ al., [2022](https://arxiv.org/html/2311.07989v3/#bib.bib189)),\ntext width=20.5em\\]\n\\]\n\\[Encoder-Decoder\n\\[PyMT5Â (Clement etÂ al., [2020](https://arxiv.org/html/2311.07989v3/#bib.bib110)),\nT5-codeÂ (Mastropaolo etÂ al., [2021](https://arxiv.org/html/2311.07989v3/#bib.bib387)),\nDOBFÂ (Lachaux etÂ al., [2021](https://arxiv.org/html/2311.07989v3/#bib.bib286)),\nPLBARTÂ (Ahmad etÂ al., [2021](https://arxiv.org/html/2311.07989v3/#bib.bib5)),\nCodeT5Â (Wang etÂ al., [2021e](https://arxiv.org/html/2311.07989v3/#bib.bib593)),\nSPT-CodeÂ (Niu etÂ al., [2022](https://arxiv.org/html/2311.07989v3/#bib.bib421)),\nAlphaCodeÂ (Li etÂ al., [2022g](https://arxiv.org/html/2311.07989v3/#bib.bib334)),\nNatGenÂ (Chakraborty etÂ al., [2022a](https://arxiv.org/html/2311.07989v3/#bib.bib83)),\nERNIE-CodeÂ (Chai etÂ al., [2023](https://arxiv.org/html/2311.07989v3/#bib.bib80)),\nCodeT5+Â (Wang etÂ al., [2023d](https://arxiv.org/html/2311.07989v3/#bib.bib594)),\ntext w","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2311.07989v3","id":"https://arxiv.org/html/2311.07989v3","publishedDate":"2025-09-01T00:00:00.000Z","summary":"This survey comprehensively reviews recent advancements in code-focused language models, emphasizing the evolution from traditional statistical and RNN-based models to modern pretrained Transformers and large language models (LLMs) like GPT and Codex. It categorizes models into general-purpose LLMs and specialized, code-pretrained models, highlighting their relations and distinctions. The paper discusses key code-specific features such as Abstract Syntax Trees (ASTs), Control Flow Graphs (CFGs), and unit tests, and explores their roles in training and evaluating models. Covering over 50 models, 30 tasks, and 170 datasets, it also identifies current challenges and future directions in integrating NLP and software engineering perspectives for code understanding and generation. The survey is ongoing and maintained on GitHub for continuous updates.","title":"Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code","url":"https://arxiv.org/html/2311.07989v3"},{"content":"# [AI Newsletter](https://nlp.elvissaravia.com/)\n\nSubscribeSign in\n\n# ðŸ¥‡Top AI Papers of the Week\n\n### The Top AI Papers of the Week (September 1-7)\n\nSep 07, 2025\n\n32\n\n3\n\n[Share](javascript:void(0))\n\n## 1\\. Why Language Models Hallucinate\n\nThe paper argues that hallucinations are not mysterious glitches but the predictable result of how LLMs are trained and evaluated. Pretraining creates statistical pressure to make errors, and post-training benchmarks often reward confident guessing over honest uncertainty. The fix is to realign mainstream evaluations to stop penalizing abstentions.\n\n- **Pretraining inevitably produces some errors.** The authors reduce generation to a binary â€œIs-It-Validâ€ classification problem and show a lower bound: the generative error rate scales with the misclassification rate in that classifier. Even with error-free corpora, optimizing cross-entropy yields calibrated base models that still generate errors rather than always saying â€œI donâ€™t know.â€\n\n- **Arbitrary facts drive a floor on hallucinations.** For facts with no learnable pattern (for example, specific birthdays), the paper links hallucination rates to the â€œsingleton rateâ€ in training data. If many facts appear only once, a calibrated base model will hallucinate on at least that fraction of such prompts. This generalizes Good-Turing style missing-mass reasoning and recovers prior results while adding prompts and IDK.\n\n- **Model class limitations also matter.** When the model family cannot represent the needed distinctions, errors persist. The paper formalizes this via an agnostic-learning bound and gives simple cases like multiple choice, where even optimal thresholding leaves a fixed error tied to model capacity, with an example showing classic n-gram models must fail on certain context dependencies.\n\n- **Post-training often reinforces guessing.** Most popular benchmarks grade in a binary correct-incorrect fashion and give zero credit to abstentions, so a model that always guesses can outperform one that withholds uncertain answers. The authors survey widely used leaderboards and find that abstentions are largely penalized, explaining why overconfident hallucinations persist despite mitigation efforts.\n\n- **Proposed fix: explicit confidence targets.** Incorporate clear penalties for wrong answers and neutral credit for IDK directly into mainstream evaluations, instructing models to answer only above a stated confidence threshold. This promotes behavioral calibration, where models choose between answering and abstaining according to the target confidence, and should steer the field toward more trustworthy systems.\n\n\n**[Paper](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf) \\| [Tweet](https://x.com/omarsar0/status/1964443249642447263)**\n\n## 2\\. Disentangling the Factors of Convergence between Brains and Computer Vision Models\n\nLarge self-supervised ViTs trained on natural images develop brain-like internal representations. This paper teases apart what drives that convergence by varying model size, training amount, and image type in DINOv3, then comparing model activations to human fMRI (space) and MEG (time) with three metrics: overall linear predictability (encoding), cortical topography (spatial), and temporal alignment (temporal). Result: all three factors matter, and alignment unfolds in a consistent order from early sensory to higher associative cortex.\n\n- **Setup and metrics:** Eight DINOv3 variants spanning sizes and datasets; comparisons use encoding, spatial, and temporal scores against NSD fMRI and THINGS-MEG.\n\n- **Baseline alignment:** fMRI predictability concentrates along the visual pathway (voxel peaks around Râ‰ˆ0.45). MEG predictability rises ~70 ms after image onset and remains above chance up to 3 s. Spatial hierarchy holds (lower layers â†” early visual; higher layers â†” prefrontal; râ‰ˆ0.38). Temporal ordering is strong (earlier MEG windows â†” early layers; râ‰ˆ0.96).\n\n- **Training dynamics:** Alignment emerges quickly but not uniformly: temporal score reaches half its final value first (~0.7% of training), then encoding (~2%), then spatial (~4%). Early visual ROIs and early MEG windows converge sooner than prefrontal ROIs and late windows (distance-to-V1 vs half-time râ‰ˆ0.91; time-window vs half-time râ‰ˆ0.84).\n\n- **Scale and data effects:** Bigger models finish with higher encoding, spatial, and temporal scores; gains are largest in higher-level ROIs (e.g., BA44, IFS). Human-centric images beat satellite and cellular images across metrics and ROIs at matched data volume.\n\n- **Cortical correlates:** ROIs whose model alignment appears later are those with greater developmental expansion, thicker cortex, slower intrinsic timescales, and lower myelin (e.g., correlations up to \\|r\\|â‰ˆ0.88). This mirrors biological maturation trajectories.\n\n\n**[Paper](https://arxiv.org/abs/2508.18226) \\| [Tweet](https://x.com/JeanRemiKing/status/1962453435199983982)**\n\n## 3\\. Universal Deep Research\n\nProposes a general, model-agnostic deep-research agent that lets users â€œbring your own model and strategy.â€ Instead of a fixed pipeline, UDR compiles natural-language research strategies into executable code, runs them in a sandbox, and emits structured progress notifications before returning a final report.\n\n- **Motivation.** Current deep-research tools hard-code strategy and model choice, limiting source prioritization, domain-specific workflows, and model swap-ability. UDR targets all three gaps by separating the research strategy from the underlying model.\n\n- **Mechanism.** Users provide a strategy and a prompt. UDR converts the strategy to a single callable function under strict tool and control-flow constraints, then executes it in isolation. Orchestration is pure code; the LLM is called only for local tasks like summarization, ranking, or extraction. State lives in named variables, not a growing context.\n\n- **Phases and tools.** Phase 1 compiles the strategy step-by-step to reduce skipped steps and drift. Phase 2 executes with synchronous tool calls and yield-based notifications for real-time UI updates. The paper provides minimal, expansive, and intensive example strategies to show breadth.\n\n- **Efficiency and reliability.** Control logic runs on CPU while LLM calls remain scoped and infrequent, improving cost and latency. End-to-end strategy compilation proved more reliable than prompting LLMs to â€œself-orchestrateâ€ or stitching per-step code.\n\n- **Security, UI, and limits.** Strategies execute in a sandbox to contain prompt-injection or code exploits; the demo UI supports editing strategies, monitoring notifications, and viewing reports. Limitations include reliance on code-generation fidelity, no mid-execution interactivity, and assuming user-written strategies are sound. The authors recommend shipping a library of editable strategies and exploring tighter user control over free reasoning.\n\n\n**[Paper](https://arxiv.org/abs/2509.00244) \\| [Tweet](https://x.com/jiqizhixin/status/1963159853620457957)**\n\n## 4\\. Visual Story Telling\n\nA system and design framework that lets writers edit stories by acting directly on visuals of characters, locations, and timelines. Instead of only prompting, authors drag, connect, and reorder visual elements; the tool proposes synchronized text edits and can regenerate passages from the visual skeleton.\n\n- **Framework: eight elements + four operators.** Builds on narratology (fabula/syuzhet) with story elements (actors/characters, time/temporality, locations/space, events/focalization) and four compositional operators: position, associate, connect, unfold.\n\n- **Prototype with three coordinated views.** An entitiesâ€“actions graph, a locations canvas, and an event timeline enable direct manipulation: add/remove characters or actions, drag entities between locations, reorder events; coordinated highlighting and selection constrain edits to chosen scenes.\n\n- **Bi-directional editing and versioning.** Manual text edits can refresh visuals; visual edits generate tracked diffs in text; a history tree supports branching exploration; a â€œrefresh from visualsâ€ mode rewrites the story from the current visual state.\n\n- **Two studies: planning and editing.** With 12 participants, visuals improved planning, search, and reflection compared to text-only, though cognitive-load results were mixed and mental-model mismatches appeared. With 8 creative writers, participants successfully expressed spatial, temporal, and entity edits, found it helpful for exploration and inconsistency fixing, and gave a high Creativity Support Index, while asking for more control over style and alternative visual layouts.\n\n- **Implementation and limits.** React + Slate.js front end; GPT-4o prompts for extraction and edits; parallel sentence-level extraction for speed. Occasional LLM latency or unintended edits remain; future work includes richer constructs (relationships, emotions), style controls, support for long/nonlinear narratives, and a view-builder for custom diagrams.\n\n\n**[Paper](https://arxiv.org/abs/2410.07486) \\| [Tweet](https://x.com/damienhci/status/1963246088674017478)**\n\n## 5\\. rStar2-Agent\n\nrStar2-Agent is a 14B math-reasoning model trained with agentic RL that learns to think smarter by using a Python tool environment, not just longer CoT. It introduces GRPO-RoC, a rollout strategy that filters noisy successful traces, plus infrastructure for massive, low-latency tool execution. In one week and 510 RL steps on 64 MI300X GPUs, the model reaches frontier-level AIME while producing shorter solutions and showing transfer beyond math.\n\n- **Method in one line:** GRPO-RoC oversamples rollouts then keeps only the cleanest correct ones while preserving diverse failures, reducing tool-call errors and formatting issues during training.\n\n- **Infrastructure:** A dedicated, isolated code service reliably handles up to ~45K concurrent tool calls per training step with ~0.3 s end-to-end latency, and ","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fnlp.elvissaravia.com%2Fp%2Ftop-ai-papers-of-the-week-52c","id":"https://nlp.elvissaravia.com/p/top-ai-papers-of-the-week-52c","publishedDate":"2025-09-07T00:00:00.000Z","summary":"The webpage highlights the top AI papers of the week (September 1-7), focusing on two key studies. The first paper explains that hallucinations in large language models (LLMs) are predictable outcomes of training and evaluation methods, emphasizing that errors stem from statistical pressures, data sparsity, model limitations, and evaluation biases favoring confident guesses. It proposes improving trustworthiness by incorporating explicit confidence and abstention signals into evaluations. The second paper examines how self-supervised Vision Transformers (ViTs) develop brain-like representations, demonstrating that factors such as model size, training data, and image type influence alignment with human brain activity, with a clear hierarchy from early visual areas to higher cognition.","title":"Top AI Papers of the Week - AI Newsletter","url":"https://nlp.elvissaravia.com/p/top-ai-papers-of-the-week-52c"},{"content":"# [AI Newsletter](https://nlp.elvissaravia.com/)\n\nSubscribeSign in\n\n# ðŸ¥‡Top AI Papers of the Week\n\n### The Top AI Papers of the Week (August 25-31)\n\nAug 31, 2025\n\n45\n\n[1](https://nlp.elvissaravia.com/p/top-ai-papers-of-the-week-922/comments)\n3\n\n[Share](javascript:void(0))\n\n## 1\\. Anemoi Agent\n\nAnemoi replaces purely centralized, context-stuffed coordination with an A2A communication server (MCP) that lets agents talk directly, monitor progress, refine plans, and reach consensus. On GAIA, it holds up even with a small planner model and reduces redundant context passing for better cost and scalability.\n\n- **Design:** A semi-centralized planner proposes an initial plan, while worker agents (web, document processing, reasoning/coding) plus critique and answer-finding agents collaborate via MCP threads. All participants can list agents, create threads, send messages, wait for mentions, and update plans as execution unfolds.\n\n- **Communication pattern:** Five phases structure collaboration: agent discovery, thread initialization with a task plan and tentative allocation, execution with continuous critique, consensus voting before submission, and final answer synthesis. This reduces reliance on a single planner and minimizes token-heavy prompt concatenation.\n\n- **Results on GAIA:** With GPT-4.1-mini as planner and GPT-4o workers, Anemoi reaches 52.73% accuracy (pass@3), beating an OWL reproduction with the same LLM setup by +9.09 points and outperforming several proprietary and open-source systems that use stronger planners.\n\n- **Why it wins:** Most extra solves over OWL come from collaborative refinement enabled by A2A (52%), with smaller gains from reduced context redundancy (8%); remaining differences reflect stochastic worker behavior. OWLâ€™s few wins over Anemoi largely stem from worker stochasticity and web-agent latency.\n\n- **What still fails:** The largest error sources are LLM capability limits (45.6%) and tooling gaps (20.6%), followed by incorrect plans (11.8%) and communication latency (10.3%); minor shares come from benchmark annotation issues and hallucinations.\n\n\n**[Paper](https://arxiv.org/pdf/2508.17068)** \\| **[Tweet](https://x.com/omarsar0/status/1960799241888260513)**\n\n## 2\\. Deep Think with Confidence\n\nA lightweight test-time method that uses model-intrinsic confidence to prune weak reasoning paths, improving both accuracy and token efficiency for self-consistency ensembles. Works in offline and online modes without extra training or hyperparameter tuning.\n\n- **Key idea â€”** Compute local confidence signals during generation, such as sliding-window group confidence, lowest group confidence, and tail confidence, then filter or early-stop low-quality traces. Vote with confidence-weighted majority rather than treating traces equally.\n\n- **Offline gains â€”** On AIME 2025 with GPT-OSS-120B at K=512, DeepConf reaches 99.9% accuracy vs 97.0% for unweighted voting, by keeping only top-confidence traces and weighting their votes. Similar gains appear across AIME24, BRUMO25, and HMMT25.\n\n- **Online efficiency â€”** With a short warmup (Ninit=16) to set a stopping threshold and adaptive sampling until consensus, DeepConf-low cuts generated tokens by 43â€“85% while matching or improving accuracy; best case on AIME 2025 with GPT-OSS-120B saves 84.7% tokens with slightly higher accuracy than the baseline. A more conservative DeepConf-high saves 18â€“59% with near-identical accuracy.\n\n- **When to filter aggressively â€”** Retaining the top 10% most confident traces often yields the largest boosts, but can regress when the model is confidently wrong. Keeping top 90% is safer and still beats or matches plain voting.\n\n- **Easy to deploy â€”** Minimal changes to vLLM enable confidence-based early stopping via the OpenAI-compatible API by toggling a flag and providing a window size and threshold. No retraining required.\n\n\n**[Paper](https://arxiv.org/abs/2508.15260) \\| [Tweet](https://x.com/jiawzhao/status/1958982524333678877)**\n\n## 3\\. Fine-tuning LLM Agents without Fine-tuning LLMs\n\nA memoryâ€‘based learning framework that lets deepâ€‘research agents adapt online without updating model weights. The agent is cast as a memoryâ€‘augmented MDP with caseâ€‘based reasoning, implemented in a plannerâ€“executor loop over MCP tools. It sets top validation results on GAIA and delivers strong scores on DeepResearcher, SimpleQA, and HLE.\n\n- **Method in a line:** Decisions are guided by a learned caseâ€‘retrieval policy over an episodic Case Bank. Nonâ€‘parametric memory retrieves Topâ€‘K similar cases; parametric memory learns a Qâ€‘function (soft Qâ€‘learning or singleâ€‘step CE training in deepâ€‘research settings) to rank cases for reuse and revision.\n\n- **Architecture:** Planner (LLM CBR) + Executor (LLM MCP client) with three memories: Case, Subtask, Tool. Involves a loop for planning, tool execution, writing/reading of cases, and a replay buffer. Tools span search, crawl, multimodal document parsing, code execution, and math utilities.\n\n- **Results:**â€¢ GAIA: 87.88% Pass@3 on validation and 79.40% on test, competitive with or above openâ€‘source agent frameworks.â€¢ DeepResearcher: 66.6 F1 and 80.4 PM average across seven openâ€‘domain QA sets.â€¢ SimpleQA: 95.0% accuracy, beating recent webâ€‘agent baselines.â€¢ HLE: 24.4 PM, close to GPTâ€‘5 and ahead of several strong baselines.\n\n- **Ablations and scaling:**â€¢ Case count: performance peaks around K = 4 retrieved cases, emphasizing small, highâ€‘quality memory rather than many shots.â€¢ Continual learning: both nonâ€‘parametric and parametric CBR yield steady gains over iterations vs. noâ€‘CBR.â€¢ Component study: moving from offline to online tools helps, adding planning helps more, and adding CBR yields the largest consistent boost across benchmarks.â€¢ Cost profile: input tokens, not outputs, drive costs as difficulty rises.\n\n- **Practical takeaways for agent builders:**â€¢ Use a compact, curated case memory with adaptive retrieval rather than growing prompts. â€¢ Keep planning concise. A fast planner outperforms slowâ€‘think planners for multiâ€‘step tool use on GAIA by avoiding verbose or shortcut plans. â€¢ Separate planning and execution with explicit Subtask and Tool memories to coordinate longâ€‘horizon work and reduce hallucinations.\n\n\n**[Paper](https://arxiv.org/abs/2508.16153) \\| [Tweet](https://x.com/omarsar0/status/1960047046444085363)**\n\n## 4\\. Jet-Nemotron\n\nA hybrid-architecture LM family built by adapting after pretraining. Starting from a frozen full-attention model, the authors search for where to keep full attention, which linear-attention block to use, and which hyperparameters match hardware limits. The result, Jet-Nemotron-2B/4B, matches or surpasses popular full-attention baselines while massively increasing throughput on long contexts.\n\n- **PostNAS pipeline â€”** Begins with a pre-trained full-attention model and freezes MLPs, then proceeds in four steps: learn optimal placement or removal of full-attention layers, select a linear-attention block, design a new attention block, and run a hardware-aware hyperparameter search.\n\n- **Learning where full attention actually matters â€”** A once-for-all super-network plus beam search identifies only a few layers as critical, and the important layers differ by task.\n\n- **JetBlock: linear attention with dynamic convolution â€”** The new block adds a kernel generator that produces input-conditioned causal convolutions applied to V tokens and removes static convolutions on Q/K. They report higher math and retrieval accuracy vs. prior linear blocks at similar training and inference speed.\n\n- **Hardware-aware design insight â€”** Through grid search at fixed KV cache size, they show generation speed tracks KV cache more than parameter count. The work shares head/dimension settings that hold throughput roughly constant while improving accuracy. This leads to comparable tokens/s with more parameters and better scores.\n\n- **Results at a glance â€”** Jet-Nemotron-2B outperforms or matches small full-attention models on MMLU, MMLU-Pro, BBH, math, commonsense, retrieval, coding, and long-context tasks, while delivering up to 47x decoding throughput at 64K and as high as 53.6x decoding and 6.14x prefilling speedup at 256K on H100.\n\n\n**[Paper](https://www.arxiv.org/abs/2508.15884) \\| [Tweet](https://x.com/omarsar0/status/1960724749790929009)**\n\n## 5\\. Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains\n\nA test-time reasoning framework that replaces a single linear chain with multiple parallel, entity-grounded chains over medical knowledge graphs. MIRAGE decomposes a query into sub-questions, runs adaptive graph retrieval in Anchor and Bridge modes, then reconciles answers via cross-chain verification, yielding higher accuracy and clearer provenance than linear ToT or web-centric agentic RAG.\n\n- **Whatâ€™s new**: Parallel multi-chain inference over a structured KG, not just longer single chains. Two retrieval modes: Anchor (single-entity neighborhood) and Bridge (multi-hop paths between entity pairs). A synthesizer verifies cross-chain consistency and normalizes medical terms before emitting a concise final answer.\n\n- **Why it matters**: Linear chains accumulate early errors and treat evidence as flat text. Graph-based retrieval preserves relations and hierarchies, supporting precise multi-hop medical reasoning with traceable paths. The comparative schematic highlights these failure modes and MIRAGEâ€™s fix.\n\n- **Results**: State-of-the-art across three medical QA benchmarks. On ExplainCPE, MIRAGE reaches 84.8% accuracy and the best GPT-4o ranking; similar gains appear on GenMedGPT-5k and CMCQA. Robustness holds when swapping in DeepSeek-R1-32B as the backbone. Human evals on GenMedGPT-5k also prefer MIRAGE.\n\n- **Scaling insights**: More sub-questions help until over-decomposition adds noise, while allowing more retrieval steps shows diminishing but steady gains. Tuning the sub-question cap and retrieval budget is key.\n\n- **Interpretability**: Every claim ties back to expl","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fnlp.elvissaravia.com%2Fp%2Ftop-ai-papers-of-the-week-922","id":"https://nlp.elvissaravia.com/p/top-ai-papers-of-the-week-922","publishedDate":"2025-08-31T00:00:00.000Z","summary":"The webpage \"ðŸ¥‡Top AI Papers of the Week\" highlights recent advances in AI research from August 25-31, 2025. It features two notable papers:\n\n1. **Anemoi Agent**: Introduces a semi-centralized multi-agent system that enhances collaboration among AI agents through direct communication via an MCP server. This approach reduces redundancy, improves scalability, and achieves higher accuracy (52.73% pass@3 on GAIA) compared to previous systems by enabling iterative refinement and consensus among agents. Key challenges include LLM capability limits and communication latency.\n\n2. **Deep Think with Confidence**: Presents a lightweight, test-time confidence-based method to improve reasoning accuracy and efficiency. By assessing local confidence signals during generation, it filters out low-quality reasoning paths, significantly reducing token usage (up to 85%) while maintaining or enhancing accuracy across multiple benchmarks. It is easy to implement with minimal changes to existing APIs.\n\nOverall, these papers demonstrate innovative strategies for improving multi-agent collaboration and reasoning efficiency in AI systems.","title":"Top AI Papers of the Week - AI Newsletter","url":"https://nlp.elvissaravia.com/p/top-ai-papers-of-the-week-922"},{"content":"![Loading...](https://scholar9.com/assets/images/loader.gif)\n\nBack to Top\n\n![](https://scholar9.com/uploads/journalphotos/f3e4f4e804e1ed03f913b6395bbbac07.jpg)\n\n[Claim Your Journal](https://scholar9.com/publisher/register)\n\n[Get PRC](https://scholar9.com/publisher/register)\n\n[Visit Website](https://www.sciencedirect.com/journal/materials-today-proceedings)\n\n[Claim Your Journal](https://scholar9.com/publisher/register)\n\n# Materials Today: Proceedings (MTP)\n\nPublisher :\n\n**Elsevier**\n\n![Scopus Profile](https://scholar9.com/assets/images/Peer_reviewed_only.png)\n\n**Peer reviewed only**\n\n![Scopus Profile](https://scholar9.com/assets/images/open_access_journal.png)\n\n**Open Access**\n\n- Nanotechnology\n- Polymers\n- PHYSICS\n- SPACE SCIENCES\n- Metals\n- Alloys\n- Plastics\n+4\n\n\ne-ISSN :\n\n**2214-7853**\n\nIssue Frequency :\n\n**Monthly**\n\nImpact Factor :\n\n**3.2**\n\nEst. Year :\n\n**2014**\n\nMobile :\n\n**31204853767**\n\nCountry :\n\n**Netherlands The**\n\nLanguage :\n\n**English**\n\nAPC :\n\n**YES**\n\nImpact Factor Assignee :\n\n**Google Scholar**\n\nEmail :\n\n**mtproceedings@elsevier.com**\n\nDisclaimer\n\n## Journal Descriptions\n\nMaterials Today: Proceedings provides the materials science community with a fast and flexible route to the publication of research presented at national and international scientific conferences in the field of materials science.\nGuest Editors are responsible for quality control, the peer review process and the content of individual conference papers. Conference proceedings are only permitted for publication in Materials Today: Proceedings following approval of a completed conference proposal form. All papers must be original, and peer-review is mandatory.\n\n# Materials Today: Proceedings (MTP) is :\n\n## International, Peer-Reviewed, Open Access, Refereed, Nanotechnology, Polymers, PHYSICS, SPACE SCIENCES, Metals, Alloys, Plastics , Online Monthly Journal\n\n## UGC Approved, ISSN Approved: P-ISSN , E-ISSN - 2214-7853, Established in - 2014, Impact Factor - 3.2\n\n## Not Provide Crossref DOI\n\n## Indexed in Scopus\n\n## Not indexed in WoS, DOAJ, PubMed, UGC CARE\n\n## Indexing\n\n[![Scopus Profile](https://scholar9.com/uploads/indexingAgency/Scopus_1705551609.png)\\\n\\\n**Scopus**](https://https:%2F%2Fscholar9.com%2Fjournal%2Fmaterials-today-proceedings-656)\n\n[![Scopus Profile](https://scholar9.com/uploads/indexingAgency/Science%20Direct_1705552614.png)\\\n\\\n**Science Direct**](https://www.sciencedirect.com/journal/materials-today-proceedings)\n\n[![Scopus Profile](https://scholar9.com/uploads/indexingAgency/ISSN%20Portal_1707732990.png)\\\n\\\n**ISSN Portal**](https://portal.issn.org/resource/ISSN/2214-7853)\n\n[![Scopus Profile](https://scholar9.com/uploads/indexingAgency/INSPEC_1706855986.png)\\\n\\\n**INSPEC**](https://https:%2F%2Fscholar9.com%2Fjournal%2Fmaterials-today-proceedings-656)\n\n+1 More\n\n## Publications of MTP\n\n[![edit](https://scholar9.com/assets/images/light_edit.png)](javascript:;)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)[Anantkumar Gujar](https://scholar9.com/profile/anantkumar-gujar-S9-052023-2500004)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)January, 2022\n\n[**Investigating mechanical strength of a natural fibre polymer composite using SiO2 nano-filler**](https://scholar9.com/publication-detail/investigating-mechanical-strength-of-a-natural-fib--945)\n\nIntroducing nano-fillers in natural fibre polymer composite materials have attained a greater attention in recent days due to their distinct behavior in improving the mechanical characterist...\n\n[![edit](https://scholar9.com/assets/images/light_edit.png)](javascript:;)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)[NAVEEN KUMAR](https://scholar9.com/profile/naveen-kumar-S9-062023-2700066)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)May, 2021\n\n[**Mechanical, microstructural properties and wear characteristics of hybrid aluminium matrix nano composites (HAMNCs) â€“ review**](https://scholar9.com/publication-detail/mechanical-microstructural-properties-and-wear-ch--874)\n\nThis review article is on hybrid nano composite material and its mechanical, microstructural and tribological behavior of aluminum alloy. The aluminum alloys are commonly used as substitute ...\n\n[![edit](https://scholar9.com/assets/images/light_edit.png)](javascript:;)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)[NAVEEN KUMAR](https://scholar9.com/profile/naveen-kumar-S9-062023-2700066)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)March, 2021\n\n[**A review on tribological behaviour and mechanical properties of Al/ZrO2 metal matrix nano composites**](https://scholar9.com/publication-detail/a-review-on-tribological-behaviour-and-mechanical--875)\n\nThis article reviews the effects of nano-sized mono metallic zirconium oxide particles on the mechanical and the tribological behavior of aluminum alloys. The zirconium oxide nanoparticle pl...\n\n[![edit](https://scholar9.com/assets/images/light_edit.png)](javascript:;)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)[NAVEEN KUMAR](https://scholar9.com/profile/naveen-kumar-S9-062023-2700066)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)March, 2021\n\n[**Green synthesis of zinc oxide nanoparticles: Mechanical and microstructural characterization of aluminum nano composites**](https://scholar9.com/publication-detail/green-synthesis-of-zinc-oxide-nanoparticles-mecha--876)\n\nIn this research article mainly focused on the fabrication of aluminum nano composite to enhance the properties of mechanical & microstructural behavior. The nano-sized zinc oxide (ZnO) nano...\n\n[![edit](https://scholar9.com/assets/images/light_edit.png)](javascript:;)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)[Parshant Dahiya](https://scholar9.com/profile/parshant-dahiya-S9-032024-3001033)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)January, 2022\n\n[**Depletion in basic soil components and increased heavy metal load is associated with declined saffron growth in Kashmir valley region**](https://scholar9.com/publication-detail/depletion-in-basic-soil-components-and-increased-h--1116)\n\nKashmir is the only state in the Indian continent, where saffron is being cultivated. Saffron is known as the â€œgolden condimentâ€ over the world due to its distinct commercial properties,...\n\n[![edit](https://scholar9.com/assets/images/light_edit.png)](javascript:;)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)[Parshant Dahiya](https://scholar9.com/profile/parshant-dahiya-S9-032024-3001033)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)January, 2022\n\n[**Forensic limnology: Diversity of diatom population in relation to environmental factors in Gujarat region, India**](https://scholar9.com/publication-detail/forensic-limnology-diversity-of-diatom-population--1117)\n\nIn drowning cases, diatoms provide an insight into ante-mortem and post-mortem evidence as well as aid in determining the geographical location. Diatoms are unicellular aquatic organisms tha...\n\n[![edit](https://scholar9.com/assets/images/light_edit.png)](javascript:;)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)[SHREYAS KOTIAN](https://scholar9.com/profile/shreyas-kotian-S9-042024-2201198)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)February, 2022\n\n[**Colburn vs Bell Delaware approach for evaluation of thermohydraulic performance of a shell and tube heat exchanger: A theoretical study**](https://scholar9.com/publication-detail/colburn-vs-bell-delaware-approach-for-evaluation-o--1323)\n\nSTHX have assumed importance over the years due to its great versatility. Thermal and hydraulic characteristics dictate the performance of the heat exchanger. These factors are dependent on ...\n\n[![edit](https://scholar9.com/assets/images/light_edit.png)](javascript:;)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)[Sakshi Saxena](https://scholar9.com/profile/sakshi-saxena-S9-072024-0301547)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)April, 2022\n\n[**MWCNTs incorporated nanostructured Bismuth Vanadate for solar energy induced water splitting for hydrogen generation**](https://scholar9.com/publication-detail/mwcnts-incorporated-nanostructured-bismuth-vanadat--1551)\n\nMultiwall carbon nanotubes (MWCNTs) modified n- type Ni:Bismuth Vanadate photoanode has been prepared and investigated for solar energy induced water splitting for hydrogen generation. 0.5 w...\n\n[![edit](https://scholar9.com/assets/images/light_edit.png)](javascript:;)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)[Prashant K. Bhuva](https://scholar9.com/profile/prashant-k-bhuva-S9-072024-1001714)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)August, 2022\n\n[**A review on the application of artificial intelligence in the mix design optimization and development of self-compacting concrete**](https://scholar9.com/publication-detail/a-review-on-the-application-of-artificial-intellig--1679)\n\nObtaining the strength-oriented mix design, balanced proportions of the additional constituents of Self Compacting Concrete (SCC) has remained a time-consuming, approximated, and repetitive ...\n\n[![edit](https://scholar9.com/assets/images/light_edit.png)](javascript:;)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)[NILESH SONU VARKUTE](https://scholar9.com/profile/nilesh-sonu-varkute-S9-072024-1101741)\n\n- ![dott image](https://scholar9.com/assets/images/dott.png)September, 2021\n\n[**Experimental and computational study of phase change material based shell and tube heat exchanger for energy storage**](https://scholar9.com/publication-detail/experimental-and-computational-study-of-phase-chan--1699)\n\nThe storage of thermal energy is an important issue that needs to be addressed. The wastage of heat energy, such as in an automobile or thermal power plant exhausts, means the loss of useful...\n\nLoad More\n\n# Establish Your Own Journal Without the Expense!\n\nOJSCloud off","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fscholar9.com%2Fjournal%2Fmaterials-today-proceedings-656","id":"https://scholar9.com/journal/materials-today-proceedings-656","publishedDate":"2025-10-01T00:00:00.000Z","summary":"Materials Today: Proceedings (MTP) is an open access, peer-reviewed journal published monthly by Elsevier since 2014, focusing on materials science research presented at scientific conferences. It covers diverse fields such as nanotechnology, polymers, physics, space sciences, metals, alloys, and plastics. The journal is indexed in Scopus, has an impact factor of 3.2, and is UGC approved. It emphasizes original, peer-reviewed conference papers, ensuring quality control through guest editors. MTP is not indexed in WoS, PubMed, or DOAJ. Recent notable publications include studies on natural fiber polymer composites with nano-fillers and hybrid aluminium matrix nano composites, reflecting its focus on advanced materials and nanotechnology research.","title":"Materials Today: Proceedings(MTP) | Scholar9","url":"https://scholar9.com/journal/materials-today-proceedings-656"},{"content":"- 02/29/2024\n- Article\n\n- Analysing & automating\n\n# Module automation: How manufacturers of machines and package units can benefit from MTP\n\n##### A revolution is on the horizon in the automation of process plants: Module automation is poised to massively reduce the amount of work involved in setting up and converting systems in the process industry. Manufacturers of machines and package units required for the production of powders and bulk materials in particular could benefit from this technology.\n\nWritten by **Armin Scheuermann**\n\nModule automation allows the control logic of individual machines to be easily integrated into higher-level process control systems.\n\nTo date, production plants in the chemical and process industry have generally been automated centrally. This means that each plant has to be individually configured and adapted to its specific requirements: This is because although machines and process engineering units â€“ known as package units â€“ are generally already supplied with their own control system, the control system is usually also implemented in the central control system. This not only costs time and causes a great deal of engineering effort, but is also not very flexible. Every change or extension to the system has to be reprogrammed in the process control system.\n\nAs a result, users have been suffering for many years: As early as 2009, chemists and process engineers at the Tutzing Symposium outlined the vision of chemical plants consisting of individual, standardized modules developed for specific tasks such as mixing, heating, dosing or separating substances. However, it took until 2012 for Namur, the process automation interest group, to take up the concept. In November 2014, the automation supplier Wago finally presented the Module Type Package (MTP) approach, in which the control intelligence is transferred to the modules. This made it possible to use the module functions as a service via a standardized interface. The Module Type Package (MTP) makes it possible to easily integrate programmable logic controllers (PLCs) from different manufacturers into a control system, including the user interface (HMI, Human Machine Interface), communication and other automation aspects. OPC UA, which has been established in the industry for many years, is used as the communication protocol between PLCs and the control system.\n\n## Manufacturers of machines and package units benefit in several ways\n\nThe use of MTP brings considerable benefits not only for plant operators, but also for manufacturers of machines and package units (process equipment assemblies, PEA). In the past, PEA manufacturers had to develop specific automation interfaces for each system, which involved considerable effort. With MTP, the game changes fundamentally. The manufacturer-independent automation interface enables machine suppliers to develop the interface just once and use their established control technology. Integration into the operator's control system then takes place without any further modifications to the interface. This protects the know-how of PEA manufacturers and allows them to offer standardized package units that can be easily integrated into modular systems. MTP is therefore not only revolutionizing the way system operators design their systems, but also the efficiency and cost-effectiveness of machine manufacturers. The technology group GEA is one of the first to use the MTP information protocol in its \"Plug & Win\" separators.\n\nMTP has also already proven itself in several pilot projects in the chemical industry. In addition to pilots at Merck and Bayer, the specialty chemicals company Evonik is relying on the technology: together with Engie, Siemens and Yokogawa, it implemented the Module Type Package standard for the first time in 2018 in Singapore in a world-scale plant for the production of a synthetic amino acid and put it into operation in 2019. The project participants were impressed: module automation with MTP made it possible to create and expand the process plants quickly. At the same time, the engineering effort was significantly reduced. Companies in the specialty chemicals industry in particular are not only hoping for significantly faster time-to-market through the use of MTP. The protagonists of the technology expect that module automation will reduce engineering costs by 70 percent and increase flexibility by 80 percent.\n\n## Food and pharmaceutical producers are very interested\n\nProduction plants in the food, beverage and pharmaceutical industries, which rely much more heavily on skids, could benefit from module automation to an even greater extent than the chemical industry. Here, skids from different manufacturers are often used, each with its own logic, programming and operating philosophy. Standardization according to the MTP concept offers great potential for reducing engineering costs. The engineers in the pharmaceutical industry organized in the ISPE have also recognized this: MTP is to become the basis of the ISPE Pharma 4.0 Plug&Produce initiative. The responsible Architecture Team of the association proposes to supplement the MTP concept with additional information such as the audit trail, which is important in the pharmaceutical environment, as well as metadata of the implemented equipment (e.g. version information) and safety mechanisms. The resulting \"MTP+\" would thus enable further savings in the integration of system components.\n\nMTP also has potential beyond the process industry: modular production has long been used in shipbuilding - modular automation is a logical addition here. It is also worth using in production-related logistics, process analytics and hydrogen production. MTP enables easy integration of the electrolysers into upstream and downstream processes, which is particularly important for the rapid ramp-up of the hydrogen economy.\n\nSince November 2021, MTP technology has been further developed and promoted under the Profibus user organization. The MTP interface is now being standardized under VDI-VDE-Namur 2658. In addition, activities for the international standardization of MTP as an IEC 63280 standard have begun.\n\n## Conclusion\n\nModule automation with the Module Type Package (MTP) could revolutionize the way in which production systems are automated in the process industry. The separation of the module and control levels significantly increases flexibility, efficiency and speed in system configuration. MTP enables companies to meet the challenges of Industry 4.0 and make production more flexible and efficient.\n\n## Author\n\nArminScheuermann\n\nChemical engineer and freelance specialised journalist\n\nstar\\_borderMark","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.powtech-technopharm.com%2Fen%2Findustry-insights%2F2024%2Farticle%2Fmodule-automation","id":"https://www.powtech-technopharm.com/en/industry-insights/2024/article/module-automation","publishedDate":"2025-09-25T00:00:00.000Z","summary":"The webpage discusses the emerging role of Module Type Package (MTP) in revolutionizing automation in the process industry, particularly for manufacturers of machines and package units used in powder and bulk material production. Traditionally, plants relied on centralized automation, requiring extensive customization and reprogramming for each system change, leading to high effort and limited flexibility. MTP enables the integration of control logic from individual modulesâ€”such as mixers, heaters, or separatorsâ€”into higher-level control systems via standardized interfaces, primarily using OPC UA protocols. This approach allows machine manufacturers to develop universal automation interfaces once, simplifying integration, protecting their know-how, and reducing costs. Early adopters like GEA, Merck, Bayer, and Evonik have successfully implemented MTP in pilot projects, demonstrating its potential to enhance efficiency, flexibility, and scalability in process automation.","title":"Module automation: How manufacturers of machines and package units can benefit from MTP","url":"https://www.powtech-technopharm.com/en/industry-insights/2024/article/module-automation"},{"content":"![](https://images.squarespace-cdn.com/content/v1/63b2df2b8014e72c3f7b96fc/1672818071455-GBITB7JGHZ1FKKE771VR/unsplash-image-pYpOZMP43Cc.jpg?format=2500w)\n\n![](https://images.squarespace-cdn.com/content/v1/63b2df2b8014e72c3f7b96fc/0ac98059-f086-43fc-9104-6a2b46931a23/JADE0253.jpg?format=2500w)\n\n# _Welcome_ to the Studio\n\nBased in Perth, working with clients right across Australia, we are a boutique studio elevating the online presence & productivity of high-end design & construction businesses through experience and strategy.\n\n[Learn more](https://www.studiojacandco.com.au/services)\n\n## Iâ€™m _so_ excited you are here!\n\nWe invite you to take a look through our Services to see what suits your needs, or read more about our Studio and where it all began. We love to connect so please feel free to reach out to discuss anything in further detail!\n\n[![](https://images.squarespace-cdn.com/content/v1/63b2df2b8014e72c3f7b96fc/f74a8f79-41c2-4349-85e2-745c1c0d9365/Studio-Jac-%26-Co-81.jpg?format=2500w)](https://www.studiojacandco.com.au/services)\n\n### [_Services_](https://www.studiojacandco.com.au/services)\n\n[![](https://images.squarespace-cdn.com/content/v1/63b2df2b8014e72c3f7b96fc/f5c157c5-0341-43e7-ba1a-f15faa6b3ce4/Studio-Jac-%26-Co-59.jpg?format=2500w)](https://www.studiojacandco.com.au/about)\n\n### [_About_](https://www.studiojacandco.com.au/about)\n\n![](https://images.squarespace-cdn.com/content/v1/63b2df2b8014e72c3f7b96fc/250b25dd-5282-46ae-ae10-1932c5e03506/IMG_35082F173E02-1.jpg?format=2500w)\n\n### _Portfolio (coming soon)_\n\nProudly trusted by\n\n![](https://images.squarespace-cdn.com/content/v1/63b2df2b8014e72c3f7b96fc/2ae3ec7f-15b0-4cb7-a28a-09c7575839bd/JADE0268.jpg?format=2500w)\n\n# Whatâ€™s your _Strategy_?\n\nIf you feel like you are posting on social media just because you â€œhavenâ€™t posted in a whileâ€, you need a strategy. Start with your goals and work backwards. At Studio Jac & Co, we help you reach your goals through clever marketing and implementing specific steps to get you there.\n\n- What are your goals & objectives?\n\n- Are your goals SMART?\n\n(Specific, Measurable, Achievable, Relevant, and Time-bound?)\n\n- Who is your ideal client?\n\n- Who are your competitors?\n\n- What are your content pillars?\n\n- Are you engaging with your community?\n\n\nIn a 2-3 hour session, we work together to identify exactly what your goals are, who your ideal clients are, the type of content that is going to resonate with them the most to make them want to work with you! AND then we plan out exactly what to post, and when.\n\nAre you ready to create strategic content?\n\n[Book your session](https://www.studiojacandco.com.au/shop/p/content-strategy-session-workbook)\n\n## Shop our _digital_ products & services\n\n[sale](https://www.studiojacandco.com.au/shop/p/hashtaghero-instagram-hashtag-strategy)\n\n[#HashtagHero - Instagram Hashtag Strategy](https://www.studiojacandco.com.au/shop/p/hashtaghero-instagram-hashtag-strategy)\n\nSale Price:$19.00 Original Price:$29.00\n\n[sale](https://www.studiojacandco.com.au/shop/p/30-content-ideas-for-interior-designers)\n\n[30 Content Ideas for Interior Designers](https://www.studiojacandco.com.au/shop/p/30-content-ideas-for-interior-designers)\n\nSale Price:$19.00 Original Price:$29.00\n\n[Instagram Content Planner](https://www.studiojacandco.com.au/shop/p/instagram-content-planner)\n\n$149.00\n\n# Content\n\n# ã€°ï¸\n\n# Strategy\n\n# ã€°ï¸\n\n# Goals\n\n# ã€°ï¸\n\n# Branding\n\n# ã€°ï¸\n\n# Marketing\n\n# ã€°ï¸\n\n# Contentã€°ï¸Strategyã€°ï¸Goalsã€°ï¸Brandingã€°ï¸Marketingã€°ï¸Content ã€°ï¸ Strategy ã€°ï¸ Goals ã€°ï¸ Branding ã€°ï¸ Marketing ã€°ï¸\n\n- ## \"We have had an incredible experience with Jac!\"\n\n\n\n\n\nWe have had an incredible experience working with Jac on a number of different projects around Queensland. As we create video & photo content, having Jac organise all of the pre-production from talent booking, location scouting, scheduling and client requirements, she has made our lives an absolute breeze. Jac is extremely helpful & understanding of our needs as the production company and made the connection between our work and the client really smooth!\n\n\n\n\\- XMedia\n\n- ## \"...representing and responsible for multiple brands and teams...\"\n\n\n\n\n\nI have had the pleasure of working with Jac on a number of campaigns and unique scenarios where she was representing and responsible for multiple brands and teams, as a Photoshoot Creative Director and Marketing Liaison. Despite a huge range of moving parts, and different invested parties, Jac flawlessly executed these photoshoots and campaigns without a hiccup, and created some of the smoothest photoshoot productions I have worked on - with crystal clear expectations, and results I am now extremely proud of, Jac oversaw the entire spectrum of the production - from prep, execution, and then the post-production of all collateral. I very much look forward to working with her and her team again soon!\n\n\n\n\\- Brock Beazley Photography\n\n- ## \"Jac put the entire plan and strategy together so seamlessly...\"\n\n\n\n\n\nWe have had the pleasure of working with Jac when she approached us on behalf of our smart home integrators Electronic Living, to collaborate on multiple marketing campaigns that saw our projects featured in news outlets such as Courier Mail and Realestate.com.au, as well as throughout social media.\n\n\n\nJac put the entire plan and strategy together so seamlessly; organising every detail from the photographers and videographers, sending the press release to the journalists, scheduling of the interviews and editing of the TV presenter's script, and the final media promotions.\n\n\n\nJac is professional and organised, with an eye for detail and strategy. We are delighted to have had the opportunity to work with her and are excited to see Studio Jac & Co flourish.\n\n\n\n\\- Build Prestige Homes\n\n- ## \"...the results consistently exceed our expectations...\"\n\n\n\n\n\nAbsolutely love working with Studio Jac & Co! Jaclyn is incredibly easy to work with and a fantastic communicator. Her deep knowledge of marketing and meticulous attention to detail really set her apart. Our collaborations are always seamless, and the results consistently exceed our expectations. Highly recommend for any business looking to elevate their marketing initiatives!\n\n\n\n\\- Damian Cavanagh\n\nElectronic Living\n\n\n![](https://images.squarespace-cdn.com/content/v1/63b2df2b8014e72c3f7b96fc/7c7879d8-0deb-4aac-bf42-30ee5e676c17/SusanStaffordPhotography-Jacqueline-50.jpg?format=2500w)\n\n![](https://images.squarespace-cdn.com/content/v1/63b2df2b8014e72c3f7b96fc/1678066769692-1ZC913A05MVOM7EEFIVS/unsplash-image-pYpOZMP43Cc.jpg?format=2500w)\n\n# Hi, Iâ€™m _Jac_.\n\nFounder of Studio Jac & Co, Mother to 3 year old Ollie & baby Rosie, and FiancÃ© to Kalem.\n\nStudio Jac & Co was born in 2022, an extension of an existing freelancing side business, where I have worked with businesses in the construction industry to take their online presence to greater heights through strategy and creativity.\n\nThroughout my career in Marketing, I have collaborated with many of the most prestigious Architects, Builders, Contractors & Interior Designers in Australia. These collaborations & partnerships offered the opportunity to organise marketing campaigns & photo/videoshoots inside homes that most people would only dream of stepping foot in.\n\nThis is just the beginningâ€¦\n\n[Learn more](https://www.studiojacandco.com.au/about)\n\n![](https://images.squarespace-cdn.com/content/v1/63b2df2b8014e72c3f7b96fc/32cb7884-40fd-4ea8-aa40-1ad4c995d038/JADE0257.jpg?format=2500w)\n\n## Ready to elevate your brand?\n\nContact us today to discuss how we can help you achieve your goals.\n\n[Let's talk!](https://www.studiojacandco.com.au/contact)\n\nFirst Name\n\nLast Name\n\nEmail Address\n\nSubscribe\n\nThank you!","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.studiojacandco.com.au%2F%3Fsrsltid%3DAfmBOooIlAG_sr9fSIabGQUt8jqoZiFSEcJ3vGYIgXMTAZteYaSF9xy5","id":"https://www.studiojacandco.com.au/?srsltid=AfmBOooIlAG_sr9fSIabGQUt8jqoZiFSEcJ3vGYIgXMTAZteYaSF9xy5","publishedDate":"2025-09-15T00:00:00.000Z","summary":"Studio Jac & Co is a Perth-based boutique agency specializing in elevating the online presence and productivity of high-end design and construction businesses across Australia. They offer strategic branding, marketing, and content services, including social media planning, content creation, and digital products like hashtag strategies and content planners. Their approach involves tailored strategies to help clients achieve specific goals, identify target audiences, and develop engaging content. The studio emphasizes personalized service, experience-driven solutions, and provides resources to support clients' marketing efforts. They also showcase a portfolio and client testimonials highlighting their successful collaborations.","title":"Studio Jac & Co","url":"https://www.studiojacandco.com.au/?srsltid=AfmBOooIlAG_sr9fSIabGQUt8jqoZiFSEcJ3vGYIgXMTAZteYaSF9xy5"}],"status":"completed","toolCallId":"call_HsCP2v9WvJRzKQndnBURkBNw","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Short answer: there isnâ€™t a public, formal set of peer reviews available for this paper. Itâ€™s an arXiv preprint (now tagged â€œOOPSLA 2025â€ on arXiv), but OOPSLA/ PACMPL reviews are not publicly posted. I couldnâ€™t find official PC reviews or an OpenReview thread.\n\nWhat I can report with confidence\n- Status: The arXiv page notes â€œComments: OOPSLA 2025,â€ which implies acceptance to the OOPSLA 2025 issue of PACMPL. Those reviews are private.\n- Public discussion: I donâ€™t see a public review thread, blog post, or HN/Reddit discussion dedicated to this paper that summarizes reviewer opinions.\n\nCommon points reviewers likely focused on (inference based on typical OOPSLA criteria and whatâ€™s in the paper)\n- Novelty and clarity of abstraction: Treating LLM integration as a language feature (the by operator) plus a meaning-typed IR is a clean, language-centric alternative to framework-based prompting. This is likely seen as a strong conceptual contribution if clearly formalized.\n- Technical substance: Reviewers usually ask whether MT-IR is sufficiently specified (whatâ€™s captured, how itâ€™s constructed, how it selects context) and whether MT-Runtimeâ€™s validation/repair pipeline is principled and reproducible.\n- Empirical evaluation: The paper reports developer-speed gains (3.2x), fewer LOC, robustness to degraded naming, and comparable or better accuracy/cost. Reviewers often probe:\n  - Task selection and external validity (are tasks representative?),\n  - Baselines (which frameworks, versions, configs?),\n  - Ablations (impact of types, docs, naming, retry policies),\n  - Sensitivity to model choice and settings.\n- Limitations and scope: Theyâ€™d expect a frank discussion that MTP shines on typed, schema-friendly tasks, and less so on open-ended, multi-step agent workflows or ultra-low-latency paths.\n- Reproducibility and availability: Implemented in Jac (byLLM in Jaseci). Reviewers typically want artifacts, instructions, and dataset/task details to reproduce results.\n\nIf you want, I can:\n- Monitor for a camera-ready PDF or artifact appendix that may include a rebuttal summary.\n- Reach out for public talks/slides where authors often summarize reviewer feedback post-acceptance.\n- Summarize likely strengths/concerns for a paper club discussion.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0f194cdfd9090ad00068c86d7fbad4819fa9b0bac83a5955dc"}},"resumableStreamId":"STREAM:9f9dbe06-539d-4f7a-bb1e-67d4b25671e0","role":"assistant","status":"done","threadId":"5fa5b519-3482-41f3-aecb-87fcf1cf2fcc","timeToFirstToken":1.141,"tokens":508,"tokensPerSecond":19.459128169769404,"updated_at":1757965679920,"userId":"google:104458561188320708185","id":"j973w1xz48hkn5d1n8va5x6wyn7qnaa5"}],"version":"6.0.0"}