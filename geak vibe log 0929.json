{"threads":[{"_creationTime":1759178035375.9646,"_id":"jd7a7wvgb5crme2rv448bxhrz57rg5yn","createdAt":1759178035376,"generationStatus":"completed","lastMessageAt":1759178538854,"model":"gpt-5-chat","pinned":false,"threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","title":"Triton Kernel AI Agent Explanation","updatedAt":1759178538854,"userId":"google:104458561188320708185","userSetTitle":false,"visibility":"visible","id":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","last_message_at":1759178538854,"created_at":1759178035376,"updated_at":1759178538854,"status":"completed","user_edited_title":false}],"messages":[{"_creationTime":1759178035375.9648,"_id":"j971j0ttdppq7d3r4amrpn1x5d7rhy11","attachmentIds":[],"content":"Explain this paper to me in depth using the Feynman technique, as if you were its author.\n\nhttps://arxiv.org/abs/2507.23194\n\nGeak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","created_at":1759178035376,"messageId":"b9a19eff-9525-45bf-9964-87d64b09118c","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","spanId":"c0925eed9566510e","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","traceId":"8daf1257f2264c047c28eefca8c3f93a","updated_at":1759178035376,"userId":"google:104458561188320708185","id":"j971j0ttdppq7d3r4amrpn1x5d7rhy11","providerMetadata":{}},{"_creationTime":1759178035375.965,"_id":"j977d00g5h25mpr7a7fg92ksqh7rhq7j","attachmentIds":[],"byok":false,"content":"Sure—let me walk you through Geak (GEAK) as if I were the first author, using the Feynman technique: explain it simply, build intuition, then add detail, and finally show evidence and limitations.\n\nWhat problem are we solving?\n- Writing fast GPU kernels is hard. You typically need a GPU expert to pick tile sizes, manage memory hierarchy, avoid bank conflicts, coalesce loads, and adapt to each GPU architecture. It’s time-consuming and doesn’t scale with the explosion of AI workloads.\n- LLMs can write code, but “correct and fast Triton kernels” is a much higher bar than, say, Python scripts. General LLMs tend to produce kernels that don’t compile, don’t pass tests, or run slowly.\n- We want an automated system that can: from a brief description, generate Triton kernels that are both correct and performant on AMD Instinct GPUs (MI300X, MI250), then verify them, and iteratively improve them—without a human in the loop.\n\nOur idea in one sentence\n- Build an agent around a strong LLM that uses a structured loop—generate, test, reflect, and optimize—plus inference-time compute scaling (multiple tries, feedback, and parallelization) to steadily turn rough kernel drafts into correct, fast kernels.\n\nWhy Triton, and why AMD GPUs?\n- Triton is a Python-like DSL designed for kernels; it balances control and readability and is widely adopted.\n- AMD Instinct GPUs power many modern AI stacks; to democratize performance across diverse hardware, we need automated kernel generation beyond a single vendor.\n\nWhat is GEAK (the system)?\nThink of GEAK as four cooperating roles around an LLM:\n\n1) Generator\n- Takes a short task description (e.g., “fused softmax over rows with dropout mask”) and any helpful context (a 1-shot similar kernel retrieved by code similarity, and a “knowledge pack” with Triton and hardware tips).\n- Produces an initial Triton kernel.\n\n2) Evaluator\n- Stage 1: Functional tests. We compile and run with unit tests and fixtures. If it fails, we collect the error trace and observable outputs.\n- Stage 2: If it passes functionally, we run performance tests (latency, memory behavior) against a reference.\n\n3) Reflector\n- Reads the failing code plus the error trace and explains what went wrong in plain language (e.g., “mismatched block pointer strides”, “out-of-bounds due to tile size vs. shape”).\n- Proposes a concrete fix strategy. This guards against repeating the same mistake and helps the LLM escape the “debugging trap” where it keeps making the same fix.\n\n4) Optimizer\n- When code is correct, we shift to speed. The optimizer sees a ranked history of previous candidates and their timings and proposes a new optimization plan (not code yet): try different BLOCK sizes, vectorized loads, shared memory tiling, software pipelining, persistent threads, avoiding register spills, etc.\n- The Generator then implements that plan into a new kernel variant.\n\nKey techniques that make this work\n- Inference-time compute scaling: we allocate more effort at inference: multiple iterations, parallel runs (temperature=1 for diversity), and pass@k selection. Accuracy improves roughly log-linearly with more runs.\n- Reflexion-style loop: we write down “lessons learned” from crashes and errors into a short-term memory for the next attempt. This dramatically improves debuggability.\n- 1-shot retrieval: we retrieve the most similar kernel by code similarity, not instruction similarity. Seeing a concrete, structurally similar Triton kernel is more helpful than a stylistically similar prompt.\n- Knowledge injection: we augment prompts with targeted Triton and AMD ROCm/Mi300X tips (e.g., alignment constraints, wavefront sizes, LDS usage patterns, TMA/block pointer idioms, masking semantics).\n- Anti-debugging-trap: if an attempt exceeds a max number of reflection cycles without progress, we force a fresh strategy to avoid looping.\n\nWhat do we evaluate on?\nWe needed rigorous, GPU-targeted benchmarks that score both correctness and speed. We release two suites:\n\n- TritonBench-revised (184 kernels)\n  - Adapted from TritonBench-G; we fixed issues discovered on AMD GPUs (e.g., shared memory errors, HIP argument mismatches, missing function calls, inconsistent test harnesses). This yields a cleaner, stricter correctness signal.\n- ROCm Triton Benchmark (30 kernels)\n  - Real kernels from open-source AMD ROCm repos (e.g., Triton-based GEMM variants, layernorm, softmax, random ops, sorting/swizzle tests). These reflect practical, expert-authored patterns.\n\nHow do we measure?\n- Correctness: unit tests must pass end-to-end. We also use pass@k to fairly account for multiple candidates.\n- Performance: latency vs. reference on the same shapes/hardware; we report average speedups for correct kernels.\n\nMain results\n- Correctness:\n  - On TritonBench-revised: up to 54.89% of tasks produce a fully correct kernel (vs. ~15% when directly prompting frontier LLMs without an agent loop).\n  - On ROCm Triton Benchmark: up to 63.33% correctness.\n- Performance:\n  - On TritonBench-revised, GEAK-generated correct kernels achieve average speedup up to 2.59× over reference baselines.\n- Scaling:\n  - We observe a log-linear scaling law with more inference-time attempts: more parallel/sequential runs → higher pass rates.\n\nWhat changed vs. naïve “LLM writes code”?\n- The failure mode of direct prompting is “compiles rarely; passes tests even more rarely; performance is an afterthought.” GEAK’s loop changes the game:\n  - Errors are treated as training signals at inference time.\n  - We deliberately separate “fix bugs” (reflector) from “get faster” (optimizer), so we don’t prematurely tune performance on broken code.\n  - We guide the optimizer with sorted historical candidates, which focuses the LLM on productive directions instead of random tweaks.\n\nA concrete example of the loop\n- Task: write a block-pointer matmul variant using Triton for MI300X with FP16 inputs.\n  1) Generator drafts a tiling (e.g., BLOCK_M/N/K = 128/128/32) with prefetching and masks for edge tiles.\n  2) Evaluator: tests fail—incorrect accumulation shape on fringe tiles.\n  3) Reflector: identifies incorrect stride usage and masking in the last K-tile; proposes a corrected mask computation and pointer arithmetic.\n  4) Generator applies fix → tests pass.\n  5) Optimizer: proposes trying different BLOCK_K and vectorized loads (ldmatrix-like patterns), adjusts num_warps, adds software pipelining hints.\n  6) Evaluator times variants; the fastest passing kernel is selected.\n  7) Parallel runs add diversity; pass@k increases the chance we find both correct and fast solutions.\n\nWhat we open-sourced\n- GEAK-agent: the multi-agent framework with Generator/Reflector/Evaluator/Optimizer, configs, and scripts to run on your data.\n- GEAK-eval: the evaluation harnesses for the two benchmarks, including our fixes and standardized tests/perf runners.\n\nWhy the results matter\n- Portable performance: With an agentic system, teams can target new hardware (here, AMD Instinct GPUs) without paying the full expert-optimization cost each time.\n- Productivity: Instead of weeks per kernel, you describe intent and let the agent grind through the debugging and tuning loop automatically.\n- Research direction: It shows that inference-time scaling and agentic feedback can substitute for specialized training in many cases, which is promising for “AI that accelerates AI.”\n\nLimitations and what’s next\n- Not all kernels pass yet. Control-flow-heavy or numerically fragile kernels still fail at notable rates.\n- Performance tuning is good but not always expert-level. Truly bleeding-edge performance (e.g., micro-architectural tricks, exotic prefetching) may need custom guidance or additional domain tools.\n- Model dependence: stronger LLMs help. We experimented with multiple frontier models; results vary by backbone.\n- Generalization: we focused on Triton on AMD GPUs; extending to CUDA, HIP bare-metal, or embedded accelerators will need adapted knowledge packs and evaluators.\n\nPlanned extensions\n- Richer retrieval: structural program analysis to find not only similar code, but similar performance bottlenecks.\n- Learned optimizers: have the optimizer learn which strategies worked historically for a given pattern/hardware.\n- Multi-objective tuning: jointly optimize for speed, memory footprint, numerical stability, and energy.\n- Broader benchmarks: more ROCm kernels and cross-vendor tasks, plus community submissions.\n\nA quick takeaway for practitioners\n- If you have a kernel-shaped need in Triton, GEAK can bootstrap you from a short description to a correct, competitive kernel and iterate toward better speed automatically.\n- Use 1-shot retrieval, enable reflexion, keep a cap on debugging loops to avoid traps, and run parallel candidates to harvest diversity. Measure with pass@k.\n\nPointers\n- Paper: arXiv:2507.23194\n- Agent code: github.com/AMD-AIG-AIMA/GEAK-agent\n- Evaluation: github.com/AMD-AIG-AIMA/GEAK-eval\n- Overview blog (ROCm): “GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks”\n\nIf you’d like, tell me a kernel you care about (e.g., attention softmax variant, fused norm+activation), and I’ll sketch how GEAK would approach generating and tuning it.","created_at":1759178035377,"messageId":"39f40bff-a265-48d9-8f74-0e196745b60b","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":null,"queries":["arXiv 2507.23194 Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","\"Geak\" Triton Kernel AI Agent Evaluation Benchmarks arXiv 2025-07-??","Geak Triton kernel AI agent benchmarks GitHub"]},"result":[{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.23194","id":"https://arxiv.org/abs/2507.23194","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses the implementation of a reCAPTCHA system for the arXiv platform. The paper outlines the motivations behind integrating reCAPTCHA, such as preventing automated submissions and ensuring the integrity of the submission process. It details the methodology used in developing the system, including the challenges faced and the solutions implemented. Additionally, the authors acknowledge support from the Simons Foundation and member institutions. For further insights, readers can access the full paper through the provided URL.","title":"Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/abs/2507.23194"},{"content":"# GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks\n\nJianghui Wang111Primary authors\\\\AndVinay Joshi111Primary authors\\\\AndSaptarshi Majumder\n\\\\AndXu Chao\n\\\\AndBin Ding\n\\\\AndZiqiong Liu\n\\\\AndPratik Prabhanjan Brahma\n\\\\AndDong Li\n\\\\AndZicheng Liu\n\\\\AndEmad Barsoum\n\\\\ANDAdvanced Micro Devices, Inc. (AMD)\n\n###### Abstract\n\nThe demand for AI-generated GPU kernels is rapidly growing, influenced by the need for scalable, hardware-optimized solutions in both industry and academia. As deep learning workloads grow in complexity and diversity, it is imperative to automate low-level kernel development to meet performance and productivity demands. Major cloud providers, semiconductor companies, and research institutions are now investing heavily in AI-driven code generation for GPUs, aiming to reduce manual optimization efforts while achieving near-expert performance on hardware like AMD Instinct™ MI300X. The Triton language, a Python-based DSL for GPU programming, has emerged as a popular target for such AI-generated kernels due to its balance of performance and ease-of-coding. In this work, we present an evaluation suite for Triton-based GPU kernels and GEAK (Generating Efficient AI-centric GPU Kernels)—a framework that leverages cutting-edge LLMs to generate performant Triton code specifically for AMD GPUs, including the AMD Instinct™ MI300X and MI250. GEAK leverages inference-time compute scaling to produce Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style feedback mechanisms. On two evaluation benchmarks, GEAK significantly outperformed the baselines of directly prompting frontier LLMs as well as Reflexion-based generation pipelines by achieving correctness up to 636363% and execution speed up of up to 2.59X. These results highlight the promise of GEAK-like agentic code generation for accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance.\n\n_K_ eywords Triton language  ⋅\\\\cdot⋅\nGPU kernel generation  ⋅\\\\cdot⋅\nAutomated Code Generation  ⋅\\\\cdot⋅\nInference time scaling\n\n## 1 Introduction\n\nFigure 1: Illustration of GEAK pipeline.\n\nAs AI workloads scale in both complexity and hardware diversity, there is a growing demand for intelligent systems that can generate high-performance GPU kernels without manual tuning. This demand is especially critical in environments where tight coupling between software and hardware efficiency determines practical feasibility—ranging from hyperscaler data centers used for serving frontier LLMs to academic HPC clusters. Recent advances in large language models (LLMs) have shown promising capabilities in code generation tasks, but achieving correct and efficient GPU code remains an open challenge. AI-assisted GPU kernel development, particularly for emerging hardware like AMD Instinct™ MI300X GPUs, can substantially improve productivity and deployment speed.\n\nTo this end, we introduce GEAK (Generating Efficient AI-centric GPU Kernels), AMD’s new agentic framework for automatic Triton kernel generation targeting AMD Instinct™ GPUs. GEAK is built to push the frontier of AI-assisted code generation by combining state-of-the-art LLMs with a structured reasoning and feedback loop. Alongside GEAK, we release two benchmark suites for evaluating AI-generated Triton kernels, enabling measurement of both execution correctness and runtime performance of generated kernels.\n\nKey Contributions:\n\n1. 1.\n\n\nWe introduce GEAK framework, a modular, agent-based system that uses inference-time compute scaling with frontier LLMs to generate Triton GPU kernels from minimal task descriptions. GEAK incorporates multiple agents—generation, evaluation, reflection, and optimization—to iteratively refine code quality and execution performance.\n\n2. 2.\n\n\nWe establish two benchmark suites for evaluating Triton kernel generation. The TritonBench-revised is a revised subset of 184 kernels adapted from TritonBench-G\\[ [1](https://arxiv.org/html/2507.23194v1#bib.bib1)\\] with stricter testing harnesses. In addition, we release the ROCm Triton Benchmark, a new suite of 30 real-world kernels sampled from open-source AMD ROCm repositories.\n\n3. 3.\n\n\nWe demonstrate that GEAK significantly outperforms direct prompting of state-of-the-art LLMs, achieving correct kernel generation rate up to 54.8954.8954.89% on TritonBench-revised and 63.3363.3363.33% on the ROCm Triton benchmark—compared to less than 151515% when directly prompting strong LLMs without agentic feedback.\n\n4. 4.\n\n\nWe also present a detailed study on one of the performant kernels from ROCm Triton Benchmark (please see Appendix [A](https://arxiv.org/html/2507.23194v1#A1)).\n\n5. 5.\n\n\nWe open source our GEAK agent implementation ( [github link](https://github.com/AMD-AIG-AIMA/GEAK-agent)) and GEAK evaluation framework ( [github link](https://github.com/AMD-AIG-AIMA/GEAK-eval)).\n\n\nFurthermore, GEAK-generated kernels demonstrate an average speedup of up to 2.592.592.59× over their reference counterparts on the TritonBench-revised set.\n\n## 2 Related Work\n\n##### Benchmarks\n\nAs program synthesis research advances, the need for rigorous and multidimensional benchmarks has become increasingly important. Most existing coding benchmarks concentrate on functional correctness, often assessed through manually crafted test cases and isolated execution environments. For instance, HumanEval provides expert-written programming tasks paired with curated test suites, while MBPP\\[ [2](https://arxiv.org/html/2507.23194v1#bib.bib2)\\] collects problems through crowdsourcing. To improve coverage and scalability, more recent approaches have turned to automated test generation\\[ [3](https://arxiv.org/html/2507.23194v1#bib.bib3)\\], enabling broader application in areas such as software engineering and developer assistance\\[ [4](https://arxiv.org/html/2507.23194v1#bib.bib4)\\].\n\nIn addition to correctness, performance profiling is gaining traction as a crucial axis in benchmarking (\\[ [5](https://arxiv.org/html/2507.23194v1#bib.bib5)\\];\\[ [3](https://arxiv.org/html/2507.23194v1#bib.bib3)\\];\\[ [6](https://arxiv.org/html/2507.23194v1#bib.bib6)\\];\\[ [7](https://arxiv.org/html/2507.23194v1#bib.bib7)\\]). However, most existing frameworks emphasize competition-style tasks or sequential execution and provide limited support for parallel or hardware-specific evaluation. While a few benchmarks address parallel CPU programming (\\[ [8](https://arxiv.org/html/2507.23194v1#bib.bib8)\\];\\[ [9](https://arxiv.org/html/2507.23194v1#bib.bib9)\\]), GPU-targeted benchmarks remain scarce, despite the growing importance of hardware efficiency in deploying large-scale deep learning models.\n\nTo address this gap, benchmarks like KernelBench \\[ [10](https://arxiv.org/html/2507.23194v1#bib.bib10)\\] and TritonBench \\[ [1](https://arxiv.org/html/2507.23194v1#bib.bib1)\\] have emerged, offering domain-specific assessments of LLM performance on GPU kernel generation. KernelBench \\[ [10](https://arxiv.org/html/2507.23194v1#bib.bib10)\\] focuses on correctness and runtime efficiency across diverse workloads using metrics like \"fast\\_p\", while TritonBench \\[ [1](https://arxiv.org/html/2507.23194v1#bib.bib1)\\] highlights the challenges LLMs face in generating performant code for the Triton DSL. These benchmarks reveal a substantial gap between general-purpose code generation and the demands of high-performance, hardware-aware synthesis, underscoring the need for specialized, compute-oriented evaluation frameworks.\n\n##### LLMs for Code Generation\n\nMore recent work has explored the use of large language models for code generation, but often in general-purpose contexts with limited success on performance-critical low level code, such as DeepSeek-Coder\\[ [11](https://arxiv.org/html/2507.23194v1#bib.bib11)\\] and Qwen-Coder\\[ [12](https://arxiv.org/html/2507.23194v1#bib.bib12)\\], which have achieved strong performance on broad coding benchmarks.\n\nThere are also LLMs specifically trained for the kernel generation task, such as KernelLLM. However, its capability is limited to translating PyTorch code into Triton kernels, restricting its general applicability. In contrast, our agent can generate Triton code based on both natural language instructions and reference code, allowing for more flexible and context-aware code synthesis. Furthermore, specialized models like KernelLLM often lack the broader reasoning and problem-solving capabilities exhibited by larger state-of-the-art LLMs such as GPT-4.1 and Claude 4, which are critical for tackling complex optimization tasks involving multiple constraints and trade-offs.\n\n##### Inference-Time Compute Scaling\n\nInference-time compute scaling refers to the strategy of allocating increased computational resources—such as longer context windows, multiple reasoning passes, or parallel generations — during the inference phase of machine learning models to boost performance on complex tasks.\nNotably, this approach enables significant improvements in large language model (LLM) performance without requiring additional training.\nFor example, the Chain-of-Thought \\[ [13](https://arxiv.org/html/2507.23194v1#bib.bib13)\\] prompting technique enhances reasoning by encouraging models to answer questions in a step-by-step manner. Similarly, the Reflexion \\[ [13](https://arxiv.org/html/2507.23194v1#bib.bib13)\\] framework improves agent performance by integrating verbalized feedback into an episodic memory buffer, which guides more informed decision-making in future attempts.\nOur proposed GEAK agent also leverages inference-time compute scaling to improve code generation and optimization quality. In addition, we incorporate a Reflexion-style module for debugging, enabling the agent to iteratively refine its outputs based on past execution feedback.\n\n## 3 Benchmarks\n\nWe experiment with Triton kernel evaluation benchmarks, consisting of a modified set of 184 kernels originally sourced from TritonBench-G and an additional set of","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2507.23194v1","id":"https://arxiv.org/html/2507.23194v1","publishedDate":null,"summary":"The webpage introduces GEAK (Generating Efficient AI-centric GPU Kernels), a framework developed by AMD for automating the generation of high-performance GPU kernels using the Triton programming language. As the demand for AI-generated GPU kernels increases due to the complexity of deep learning workloads, GEAK aims to streamline kernel development by leveraging advanced large language models (LLMs) to produce optimized Triton code specifically for AMD GPUs, including the MI300X and MI250.\n\nThe framework employs a structured reasoning loop and inference-time compute scaling to enhance the quality and performance of generated kernels. GEAK has demonstrated significant improvements over existing methods, achieving up to 636363% correctness and a 2.59X increase in execution speed on evaluation benchmarks. Additionally, the authors provide two benchmark suites for assessing the performance and correctness of AI-generated Triton kernels, highlighting the potential of GEAK to democratize access to expert-level GPU performance and accelerate the adoption of diverse hardware platforms.","title":"GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/html/2507.23194v1"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.arxiv.org%2Fpdf%2F2507.23194","id":"https://www.arxiv.org/pdf/2507.23194","publishedDate":null,"summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses the implementation and implications of a reCAPTCHA system on the arXiv platform. It explores how this system helps in preventing automated submissions and ensuring the integrity of the submission process. The paper acknowledges support from the Simons Foundation and member institutions, indicating collaboration in the research. For detailed insights, readers can access the full paper through the provided URL.","title":"[PDF] Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://www.arxiv.org/pdf/2507.23194"},{"content":"[Skip to main content](https://rocm.blogs.amd.com/rocm.blogs.amd.com#main-content)\nBack to top\n\n`Ctrl` + `K`\n\n[ROCm blogs](https://rocm.blogs.amd.com/)\n\n# GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks\n\n## Contents\n\n# GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#geak-introducing-triton-kernel-ai-agent-evaluation-benchmarks)\n\nAugust 01, 2025 by [Jianghui Wang](https://rocm.blogs.amd.com/authors/jianghui-wang.html), [Vinay Joshi](https://rocm.blogs.amd.com/authors/vinay-joshi.html), [Saptarshi Majumder](https://rocm.blogs.amd.com/authors/saptarshi-majumder.html), [Chao Xu](https://rocm.blogs.amd.com/authors/chao-xu.html), [Bin Ding](https://rocm.blogs.amd.com/authors/bin-ding.html), [Ziqiong Liu](https://rocm.blogs.amd.com/authors/ziqiong-liu.html), [Pratik Prabhanjan Brahma](https://rocm.blogs.amd.com/authors/pratik-prabhanjan-brahma.html), [Dong Li](https://rocm.blogs.amd.com/authors/dong-li.html), [Zicheng Liu](https://rocm.blogs.amd.com/authors/zicheng-liu.html), [Emad Barsoum](https://rocm.blogs.amd.com/authors/emad-barsoum.html).\n\n10 min read. \\| 2414 total words.\n\n[Software tools & optimizations](https://rocm.blogs.amd.com/blog/category/software-tools-optimizations.html)\n\n[AI/ML](https://rocm.blogs.amd.com/blog/tag/aiml.html)\n\n[Developers](https://rocm.blogs.amd.com/developers.html), [AI](https://rocm.blogs.amd.com/ai.html)\n\nAt AMD, we are pioneering ways to accelerate AI development using AI itself, by generating accurate and efficient GPU kernels. Specifically, we are starting with the automatic generation of kernels in [Triton](https://triton-lang.org/main/index.html), an open-source Python-like language for writing parallel programming code for GPUs. Today, AMD is excited to announce (a) [Generating Efficient AI-centric Kernels (GEAK)](https://github.com/AMD-AIG-AIMA/GEAK-Agent) for AMD GPUs, and results on (b) two Triton kernel evaluation benchmarks, where we show how AI agents can perform inference-time scaling with frontier LLMs to generate accurate and efficient kernels for AMD Instinct™ GPUs like MI250X and MI300X.\n\n**Key Takeaways:**\n\n- Announcing [GEAK Triton kernel generation framework](https://github.com/AMD-AIG-AIMA/GEAK-Agent), a library of various types of agentic AI and scaling methods to automatically produce Triton kernels from just brief instructions of its intended functionality.\n\n- Announcing corresponding [GEAK Triton Kernel Evaluation Benchmarks](https://github.com/AMD-AIG-AIMA/GEAK-eval), consisting of a modified set of 184 kernels originally sourced from [TritonBench-G](https://arxiv.org/abs/2502.14752) \\[1\\] and an additional set of 30 kernels from various open [ROCm repositories](https://github.com/ROCm).\n\n- Execution accuracy up to 54.89% on TritonBench-modified and 63.33% on ROCm benchmark (as compared to around 15% when directly prompting frontier LLMs) in producing correct kernels that pass all the unit tests.\n\n- Average Speedup of up to 2.59X with the AI-generated kernels over reference kernels from the TritonBench-modified benchmark.\n\n\nFigure 1. Overview of the GEAK kernel code generation AI agent [#](https://rocm.blogs.amd.com/rocm.blogs.amd.com#id1)\n\nAs shown in Figure 1, the agentic AI system comprises four core modules: a **generator**, a **reflector**, an **evaluator**, and an **optimizer**.\n\n- The **generator** produces code based on user query and contextual information.\n\n- The **evaluator** follows a cascaded design: it first performs a functionality test to verify correctness. If the code fails this test, the corresponding error trace is fed back to the reflector for further analysis. If the code passes, the evaluator proceeds to assess its performance, including latency and memory efficiency.\n\n- If the generated code fails to execute correctly, the **reflector** analyzes both the code and the resulting error trace to identify potential issues.\n\n- Finally, the **optimizer** takes as input the functionally correct code and formulates strategies to enhance its performance with respect to latency and computational efficiency.\n\n\nTo improve the performance of our agent, we employ some composable and configurable techniques as mentioned below:\n\n- **1-shot Prompting**\n\n\nTo enable 1-shot prompting, the most similar Triton code from existing datasets is used in the prompt. The datasets used for 1-shot prompt do not overlap with the benchmark. We’ve observed that 1-shot sample retrieval from datasets is most effective when using code similarity rather than instruction similarity.\n\n- **Knowledge Injection**\n\n\nWe enhance the prompt with domain-specific knowledge on writing efficient Triton kernels, including detailed hardware specifications. This incorporation of low-level optimization principles significantly improves the accuracy and quality of LLM-generated code.\n\n- **Reflexion**\n\n\nTo enhance the system’s self-correcting capabilities, we leverage a reflexion \\[3\\] module that supports introspective debugging and iterative refinement. When an LLM-generated kernel fails the functionality test, the resulting error trace is provided as feedback to the reflector for further analysis and correction. The agent is tasked with analyzing the cause of failure and proposing a corresponding solution. We scale the number of iterations to improve kernel generation.\n\n- **LLM selection for kernel agent**\n\n\nWe have explored the use of multiple LLMs like GPT-4.1, GPT-o1, and Gemini 2.5 Pro. We observed notable variations in the output produced by different models, indicating that the capability of the underlying LLM can substantially influence the results. In this blog, we present results with GPT-4.1 only but the GEAK codebase allows developers to try different LLMs as per their choice.\n\n- **LLM as Optimizer**\n\n\nThe optimizer is tasked with identifying potential optimization directions based on previous code generations and their corresponding performance metrics. These historical records, which include the generated code and associated performance results, are sorted in ascending order of performance. This structured presentation \\[2\\] helps guide the LLM toward proposing more effective optimization strategies.\n\n- **Debugging Trap**\n\n\nWhen LLM’s generated code has bugs, the error trace is provided to the Reflector for feedback generation for further correction. However, we’ve observed that sometimes code can undergo several reflection cycles while still being plagued by the same bug, which is what we refer to as the debugging trap. To prevent the agent from getting stuck in a debugging trap, we impose a limit on the number of debugging attempts per code snippet using a parameter max\\_perf\\_debug\\_num. If the code continues to fail after reaching this threshold, the agent is required to discard the current approach and generate a new strategy along with fresh code.\n\n- **Parallel Scaling**\n\n\nTo ensure reliable kernel generation, we run multiple instances of GEAK multiple times in parallel and independently. To introduce diversity in the generated code, the LLM output is sampled with the parameter temperature set to 1. Our experiments show that such diversity in generation can yield correct and faster kernels that are otherwise underexplored. Moreover, further experiments indicate that combining sequential and parallel scaling yields additional performance improvements. To obtain unbiased estimates of accuracy, we use the pass@k metric to estimate it \\[4\\].\n\n## Triton Kernel Evaluation Benchmark [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#triton-kernel-evaluation-benchmark)\n\nWe conducted experiments using two benchmarks methods:\n\n**1\\. TritonBench-Modified Benchmark (184 Kernels):**\n\nThe Triton kernels and corresponding evaluation codebase were adapted from [TritonBench](https://github.com/thunlp/TritonBench) \\[1\\] with several modifications made:\n\n- Out of 184 kernels in TritonBench-G, we identified and corrected errors in 37 kernels that were encountered when running on AMD GPUs. These errors include shared memory errors, invalid HIP arguments, and ModuleNotFound errors.\n\n- We found that several kernels in the original TritonBench repository missed necessary call functions, which resulted in the accuracy checking code comparing empty strings as output. We corrected this and ensured that the correct common seed was used during test.\n\n- For some kernels, testbenches were written inconsistently that produced unexpected outputs. We have fixed these kernels to have consistent testbenches.\n\n\n**2\\. ROCm Repository Triton Benchmark (30 Kernels):**\n\nThe benchmark contains 30 kernels chosen from various open ROCm repositories that have been written by Triton expert engineers and released publicly to help enable the ecosystem around running AI workloads efficiently on AMD GPUs. The entire list of kernels is outlined in Table 1. We also took assistance from frontier LLMs to put these kernels, as well as their corresponding unit tests, in the same format as the TritonBench-modified framework. It allows us to compare the accuracy and efficiency of our agent consistently.\n\n| **ROCm Kernel** | **Repository link** |\n| --- | --- |\n| 1\\. test\\_tma\\_store\\_gemm 2. moe-gemm 3. layernorm 4. triton\\_multreduce\\_matmul\\_kernel.py 5. multreduce\\_matmul\\_dot\\_kernel.py 6. gemm 7. test\\_block\\_pointer\\_matmul 8. test\\_block\\_copy 9. test\\_add\\_kernel 10. softmax | [ROCm/triton](https://github.com/ROCm/triton/tree/main_perf) |\n| 1\\. mnsnorm\\_fwd 2. rmsnorm\\_bwd 3. test\\_chained\\_dot\\_fp8 4. test\\_matmul\\_MXFP 5. test\\_gemm\\_fusion | [ROCm/aiter](https://github.com/ROCm/aiter) |\n| 1\\. test\\_triton\\_swizzle2d 2. test\\_triton\\_sort 3. test\\_triton\\_flip 4. test\\_reverse\\_range | [ROCm/aotriton](https://github.com/ROCm/aotriton) |\n| 1\\. test\\_load\\_reduce.py 2. test\\_chained\\_matmul | [ROCm/vllm](https://github.com/ROCm/vllm) |\n| 1\\. test\\_random\\_int 2. test\\_randn 3. test\\_kernel\\_sub 4. test\\_kernel\\_dot 5. test\\_batched","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Frocm.blogs.amd.com%2Fsoftware-tools-optimization%2Ftriton-kernel-ai%2FREADME.html","id":"https://rocm.blogs.amd.com/software-tools-optimization/triton-kernel-ai/README.html","publishedDate":"2025-08-01T00:00:00.000Z","summary":"The webpage introduces GEAK, a new AI agent framework developed by AMD for generating efficient GPU kernels using Triton, an open-source programming language. The framework aims to enhance AI development by automating the creation of kernels from brief functional instructions. Key features include:\n\n- **GEAK Triton Kernel Generation Framework**: A library that utilizes various AI methods to produce Triton kernels efficiently.\n- **Evaluation Benchmarks**: The framework includes two benchmark sets, with execution accuracy rates of 54.89% and 63.33% on modified Triton and ROCm benchmarks, respectively, significantly outperforming traditional methods.\n- **Performance Improvements**: AI-generated kernels show an average speedup of 2.59 times compared to reference kernels.\n\nThe system consists of four modules: a generator for code creation, an evaluator for testing functionality and performance, a reflector for analyzing errors, and an optimizer for refining the code. This innovative approach aims to streamline the development of AI applications on AMD GPUs.","title":"GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://rocm.blogs.amd.com/software-tools-optimization/triton-kernel-ai/README.html"},{"content":"Agree & Join LinkedIn\n\nBy clicking Continue to join or sign in, you agree to LinkedIn’s [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n\n [Skip to main content](https://www.linkedin.com/www.linkedin.com#main-content)\n\n# Introducing GEAK: Triton Kernel AI Agent and Benchmarks\n\nThis title was summarized by AI from the post below.\n\n[Zicheng Liu](https://www.linkedin.com/in/zicheng-liu?trk=public_post_feed-actor-name)\n\n1mo\n\n- [Report this post](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nI'm thrilled to share that we've released GEAK: our Triton Kernel AI Agent and Evaluation Benchmarks. We achieve an average execution accuracy of 54.89% on TritonBench-modified and 63.33% ROCm benchmark as compared to 15% if directly prompting frontier models. The generated kernels have an average speedup of 2.59X over the baseline.\n[#AMD](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Famd&trk=public_post-text) [#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllm&trk=public_post-text) [#ML](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fml&trk=public_post-text) [#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&trk=public_post-text) [#HW](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fhw&trk=public_post-text) [#MI300X](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmi300x&trk=public_post-text) [https://lnkd.in/gMcx6BBz](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgMcx6BBz&urlhash=IKK1&trk=public_post-text)\n\n[141](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_social-actions-reactions) [3 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_social-actions-comments)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_like-cta) [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment-cta)\n\nShare\n\n- Copy\n- LinkedIn\n- Facebook\n- Twitter\n\n['YZ' Yezhou Yang](https://www.linkedin.com/in/yz-yezhou-yang-6a02462a?trk=public_post_comment_actor-name)\n\nProf. YZ at School of Computing and #AI;\nAmazon Scholar at Prime Video;\nActive and Interactive Perception Solutions for #AI and #GenAI tech Deployment and Automated Mobility.\n\n1mo\n\n- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\nCongratulations 🎉🎉🎉\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reply) [1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reactions) 2 Reactions\n\n[Stefanos Damianakis](https://www.linkedin.com/in/damianakis?trk=public_post_comment_actor-name)\n\nSoftware Executive \\| Enterprise Software \\| Board Director \\| Advisor\n\n1mo\n\n- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\nBravo [Zicheng](https://www.linkedin.com/in/zicheng-liu?trk=public_post_comment-text) and team! 👏🏻👏🏻👏🏻\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reply) 1 Reaction\n\n[Vijay Krishnan](https://www.linkedin.com/in/vijay0?trk=public_post_comment_actor-name)\n\nFounder & CTO, Turing.com\n\n1mo\n\n- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\nCongrats, [Zicheng](https://www.linkedin.com/in/zicheng-liu?trk=public_post_comment-text)!\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reply) [1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reactions) 2 Reactions\n\n[See more comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_see-more-comments)\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_feed-cta-banner-cta)\n\n6,279 followers\n\n- [25 Posts](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Fzicheng-liu%2Frecent-activity%2F&trk=public_post_follow-posts)\n\n[View Profile](https://www.linkedin.com/in/zicheng-liu?trk=public_post_follow-view-profile) [Connect](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7357054030113656833&trk=public_post_follow)\n\n## Explore content categories\n\n- [Career](https://www.linkedin.com/top-content/career/)\n- [Productivity](https://www.linkedin.com/top-content/productivity/)\n- [Finance](https://www.linkedin.com/top-content/finance/)\n- [Soft Skills & Emotional Intelligence](https://www.linkedin.com/top-content/soft-skills-emotional-intelligence/)\n- [Project Management](https://www.linkedin.com/top-content/project-management/)\n- [Education](https://www.linkedin.com/top-content/education/)\n- [Technology](https://www.linkedin.com/top-content/technology/)\n- [Leadership](https://www.linkedin.com/top-content/leadership/)\n- [Ecommerce](https://www.linkedin.com/top-content/ecommerce/)\n\nShow more\n\nShow less\n\nLinkedIn\n\nNever miss a beat on the app\n\nDon’t have the app? Get it in the Microsoft Store.\n\n[Open the app](ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&mode=mini&cid=guest_desktop_upsell_post2)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v","id":"https://www.linkedin.com/posts/zicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v","publishedDate":"2025-08-01T00:00:00.000Z","summary":"Zicheng Liu announced the release of GEAK, a Triton Kernel AI Agent along with evaluation benchmarks. The new system achieves an average execution accuracy of 54.89% on the TritonBench-modified benchmark and 63.33% on the ROCm benchmark, significantly outperforming the 15% accuracy when directly prompting frontier models. Additionally, the generated kernels demonstrate an average speedup of 2.59 times over the baseline.","title":"Introducing GEAK: Triton Kernel AI Agent and Benchmarks - LinkedIn","url":"https://www.linkedin.com/posts/zicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.arxiv.org%2Flist%2Fcs%2F2025-07%3Fskip%3D10225%26show%3D2000","id":"https://www.arxiv.org/list/cs/2025-07?skip=10225&show=2000","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage from arXiv lists recent submissions in the computer science category, specifically from July 2025. It provides a collection of research papers and articles, showcasing the latest developments and findings in the field. The page acknowledges support from the Simons Foundation and member institutions, emphasizing the collaborative nature of the research shared. For detailed insights, users can explore individual papers linked on the page.","title":"Computer Science Jul 2025","url":"https://www.arxiv.org/list/cs/2025-07?skip=10225&show=2000"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.05687","id":"https://arxiv.org/abs/2507.05687","publishedDate":"2025-07-08T00:00:00.000Z","summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses a method for improving the arXiv preprint repository's submission process by integrating reCAPTCHA technology. The paper outlines the challenges of spam submissions and proposes a solution to enhance the quality of submissions while maintaining user accessibility. The authors acknowledge support from the Simons Foundation and member institutions. For detailed insights, readers can access the full paper through the provided link.","title":"AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs","url":"https://arxiv.org/abs/2507.05687"},{"content":"# Computer Science > Artificial Intelligence\n\n**arXiv:2507.11633** (cs)\n\n\\[Submitted on 15 Jul 2025\\]\n\n# Title:General Modular Harness for LLM Agents in Multi-Turn Gaming Environments\n\nAuthors: [Yuxuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+Y), [Haoyang Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+H), [Lanxiang Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu,+L), [Haojian Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin,+H), [Hao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+H)\n\nView a PDF of the paper titled General Modular Harness for LLM Agents in Multi-Turn Gaming Environments, by Yuxuan Zhang and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2507.11633) [HTML (experimental)](https://arxiv.org/html/2507.11633v1)\n\n> Abstract:We introduce a modular harness design for LLM agents that composes of perception, memory, and reasoning components, enabling a single LLM or VLM backbone to tackle a wide spectrum of multi turn gaming environments without domain-specific engineering. Using classic and modern game suites as low-barrier, high-diversity testbeds, our framework provides a unified workflow for analyzing how each module affects performance across dynamic interactive settings. Extensive experiments demonstrate that the harness lifts gameplay performance consistently over un-harnessed baselines and reveals distinct contribution patterns, for example, memory dominates in long-horizon puzzles while perception is critical in vision noisy arcades. These findings highlight the effectiveness of our modular harness design in advancing general-purpose agent, given the familiarity and ubiquity of games in everyday human experience.\n\n| | |\n| --- | --- |\n| Comments: | 8 pages, ICML MAS workshop |\n| Subjects: | Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2507.11633](https://arxiv.org/abs/2507.11633) \\[cs.AI\\] |\n| (or [arXiv:2507.11633v1](https://arxiv.org/abs/2507.11633v1) \\[cs.AI\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2507.11633](https://doi.org/10.48550/arXiv.2507.11633) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Haoyang Yu \\[ [view email](https://arxiv.org/show-email/df428258/2507.11633)\\] **\\[v1\\]**\nTue, 15 Jul 2025 18:13:04 UTC (2,392 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled General Modular Harness for LLM Agents in Multi-Turn Gaming Environments, by Yuxuan Zhang and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2507.11633)\n- [HTML (experimental)](https://arxiv.org/html/2507.11633v1)\n- [TeX Source](https://arxiv.org/src/2507.11633)\n- [Other Formats](https://arxiv.org/format/2507.11633)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.AI\n\n[< prev](https://arxiv.org/prevnext?id=2507.11633&function=prev&context=cs.AI)  \\|  [next >](https://arxiv.org/prevnext?id=2507.11633&function=next&context=cs.AI)\n\n[new](https://arxiv.org/list/cs.AI/new) \\| [recent](https://arxiv.org/list/cs.AI/recent) \\| [2025-07](https://arxiv.org/list/cs.AI/2025-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2507.11633?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2507.11633)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2507.11633)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2507.11633)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2507.11633) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.11633","id":"https://arxiv.org/abs/2507.11633","publishedDate":"2025-07-15T00:00:00.000Z","summary":"The paper titled \"General Modular Harness for LLM Agents in Multi-Turn Gaming Environments\" introduces a modular design for large language model (LLM) agents that integrates perception, memory, and reasoning components. This framework allows a single LLM or vision-language model (VLM) to effectively engage in various multi-turn gaming scenarios without requiring specific adaptations for each domain. The authors conducted extensive experiments using diverse game environments, demonstrating that their modular harness significantly enhances gameplay performance compared to unmodified models. Key findings indicate that memory is crucial for solving long-term puzzles, while perception plays a vital role in visually complex games. This research underscores the potential of modular designs in developing versatile AI agents capable of navigating dynamic interactive settings.","title":"General Modular Harness for LLM Agents in Multi-Turn Gaming Environments","url":"https://arxiv.org/abs/2507.11633"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.16280","id":"https://arxiv.org/abs/2507.16280","publishedDate":"2025-07-22T00:00:00.000Z","summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses the implementation and implications of using reCAPTCHA technology on the arXiv platform. It explores how reCAPTCHA can help prevent spam submissions while maintaining user accessibility. The paper details the methodology, results, and potential future applications of this technology in academic publishing. Additionally, it acknowledges support from the Simons Foundation and member institutions. For further insights, readers can access the full paper through the provided URL.","title":"ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of Scientific Inquiry","url":"https://arxiv.org/abs/2507.16280"},{"content":"[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[AMD-AGI](https://github.com/AMD-AGI)/ **[GEAK-agent](https://github.com/AMD-AGI/GEAK-agent)** Public\n\n- [Notifications](https://github.com/login?return_to=%2FAMD-AGI%2FGEAK-agent) You must be signed in to change notification settings\n- [Fork\\\n6](https://github.com/login?return_to=%2FAMD-AGI%2FGEAK-agent)\n- [Star\\\n31](https://github.com/login?return_to=%2FAMD-AGI%2FGEAK-agent)\n\n\nIt is an LLM-based AI agent, which can write correct and efficient gpu kernels automatically.\n\n### License\n\n[View license](https://github.com/AMD-AGI/GEAK-agent/blob/main/LICENSE.md)\n\n[31\\\nstars](https://github.com/AMD-AGI/GEAK-agent/stargazers) [6\\\nforks](https://github.com/AMD-AGI/GEAK-agent/forks) [Branches](https://github.com/AMD-AGI/GEAK-agent/branches) [Tags](https://github.com/AMD-AGI/GEAK-agent/tags) [Activity](https://github.com/AMD-AGI/GEAK-agent/activity)\n\n[Star](https://github.com/login?return_to=%2FAMD-AGI%2FGEAK-agent)\n\n[Notifications](https://github.com/login?return_to=%2FAMD-AGI%2FGEAK-agent) You must be signed in to change notification settings\n\n# AMD-AGI/GEAK-agent\n\nmain\n\n[Branches](https://github.com/AMD-AGI/GEAK-agent/branches) [Tags](https://github.com/AMD-AGI/GEAK-agent/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit ## History [10 Commits](https://github.com/AMD-AGI/GEAK-agent/commits/main/) |\n| [src](https://github.com/AMD-AGI/GEAK-agent/tree/main/src) | [src](https://github.com/AMD-AGI/GEAK-agent/tree/main/src) |\n| [.gitignore](https://github.com/AMD-AGI/GEAK-agent/blob/main/.gitignore) | [.gitignore](https://github.com/AMD-AGI/GEAK-agent/blob/main/.gitignore) |\n| [LICENSE.md](https://github.com/AMD-AGI/GEAK-agent/blob/main/LICENSE.md) | [LICENSE.md](https://github.com/AMD-AGI/GEAK-agent/blob/main/LICENSE.md) |\n| [README.md](https://github.com/AMD-AGI/GEAK-agent/blob/main/README.md) | [README.md](https://github.com/AMD-AGI/GEAK-agent/blob/main/README.md) |\n| [requirements.txt](https://github.com/AMD-AGI/GEAK-agent/blob/main/requirements.txt) | [requirements.txt](https://github.com/AMD-AGI/GEAK-agent/blob/main/requirements.txt) |\n| View all files |\n\n## Repository files navigation\n\n## Technical report\n\n## Introduction\n\nThis is an LLM-based multi-agent framework, which can generate functional and efficient gpu kernels automatically.\n\nThe framework is extendable and flexible. You can easily make you own coding agent and test it on our GEAK-eval [https://github.com/AMD-AIG-AIMA/GEAK-eval](https://github.com/AMD-AIG-AIMA/GEAK-eval).\nWe provide a baseline agent to let you run directly.\n\n## GEAK-agent\n\nIt contains a Generator, a Reflector, an Evaluator and an Optimizer. The actor generates codes according to the query and context information. The Reflector is responsible for reflecting on the generated code and the error trace if the code failed to run. The Evaluator has a cascade structure. It tests the generated code for the functionality first. If the generated code doesn't pass the functionality test, the error trace will be fedback to the Reflector. Otherwise, the Evaluator will evaluate the performance including latency and efficiency. The Optimizer gets the generated codes, which pass the evaluator's tests, and gives a strategy to optimize the code in terms of latency and efficiency.\n\n### the Optimizer\n\nWe provide previous generated codes as reference codes with their corresponding performance to the Optimizer. The number of reference codes is controlled by the arg `ancestor_num`. The reference codes are arranged in ascending order to help the Optimizer LLM find the optimization direction. We don't ask the LLM to generate new codes directly from the reference codes, instead we ask the Optimizer to analyze the reference codes first and to generate a promising strategy to optimize the code. Then we feed the generated optimization stratgey to the Generator to generate new codes.\n\n### debugging trap\n\nLLMs frequently get caught in debugging traps. When their generated code has bugs, we provide the error trace to the Reflector correction. However, we've observed that sometimes code can undergo several reflection cycles while still being plagued by the same bug. We refer to this as a debugging trap.\n\nTo prevent the LLM from getting stuck in a debugging trap, we limit debugging attempts per code snippet using `max_perf_debug_num`. If the code fails after this many fixes, the agent must abandon the current approach and generate a fresh strategy and code.\n\n## Run the Agents\n\n1. prepare the environment\n\n\n\n```\npython3 -m pip install -r requirements.txt\n\n```\n\n2. go to the src/ folder\n\n\n\n```\ncd src\n\n```\n\n3. edit config file. You need to give your API key and TritonBench data path in your config file.\n\n\n\n```\ncp configs/tritonbench_optimagent_config.yaml configs/tritonbench_optimagent_config_new.yaml\n\n```\n\n4. put the config file in the main\\_optimagent.py and run the script\n\n\n\n```\npython main_optimagent.py\n\n```\n\n\n### Resuming from Checkpoints\n\nResult and memories will be stored in the `output_path` specified in the config file for each iteration. You can resume from any iter you want by specifying the `result_file`, `mem_file` and `start_iter` in the config file. For example:\n\n```\nresult_path: \"../outputs/optimagent_10.jsonl\"\nmem_file: \"../outputs/optimagent_mem_10.json\"\nstart_iter: 11\n\n```\n\n## How to use your own data\n\n1. create a new file for your own dataloader in dataloaders\n\n\n\n```\ntouch dataloaders/OnlineData.py\n\n```\n\n2. In your own dataloader, define a new data class\n\n\n\n```\nclass OnlineData:\n\n```\n\n3. In the OnlineData, you need load `problem_states`, which is a list of ProblemState instances. To meet the minimum requirement, each ProblemState instance should include the following fields: `instruction`, `filename`, `test_code`.\n\n4. In order to use OptimAgent, the OnlineData class must implement the following methods:\n\n\n\n```\n__len__() -> int\ntest_opt_correctness(code, filename, tmp_dir, exe_dir) -> pass_call, pass_exe, call_stdout, call_stderr, exe_stdout, exe_stderr\nwrite_perf_file(input_folder_path, results_path, tmp_dir) -> None\nrun_perf_scripts(gpu_id, script_dir, log_dir) -> None\nwrite_file(file_path) -> None\n\n\n```\n\n\n\n\n\n### Method Description\n\n\n\n - \\_\\_len\\_\\_()\n\n Returns the number of problem\\_states in the dataset.\n\n - test\\_opt\\_correctness(code, filename, tmp\\_dir, exe\\_dir)\n\n Tests whether the generated code is functionally correct.\n\n Parameters:\n\n\n\n ```\n code: The generated code to be tested.\n\n filename: Name of the test script file.\n\n tmp_dir: Directory to save the script (generated code + unit test).\n\n exe_dir: Directory to store scripts that pass execution tests.\n\n ```\n\n\n\n Returns:\n\n\n\n ```\n ass_call: True if the script runs without errors.\n\n pass_exe: True if the script produces the correct output.\n\n call_stdout, call_stderr: Stdout and stderr from the initial script execution.\n\n exe_stdout, exe_stderr: Stdout and stderr from the execution verification step.\n\n ```\n\n\n\n Note:\n\n\n\n ```\n You might need to provide a Python file containing both the golden code and unit test code, separated by a line of exactly 146 hash (#) symbols. The unit test will be appended to the generated code for validation.\n\n ```\n\n - write\\_perf\\_file(input\\_folder\\_path, results\\_path, tmp\\_dir)\n\n Generates performance test scripts using the code in input\\_folder\\_path (which is typically the same as exe\\_dir from test\\_opt\\_correctness). The function will append unit tests for performance tests to the generated codes and store the scripts in tmp\\_dir. The performance test results will be saved in results\\_path.\n\n - run\\_perf\\_scripts(gpu\\_id, script\\_dir, log\\_dir)\n\n Executes the performance test scripts found in script\\_dir on the specified GPU (gpu\\_id) and stores the logs in log\\_dir.\n\n - write\\_file(file\\_path)\n\n Serializes and saves the current result to the specified file path.\n\n#### Citation\n\nIf you find this work useful in your research or applications, please consider citing:\n\n```\n@misc{wang2025geakintroducingtritonkernel,\n title={Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks},\n author={Jianghui Wang and Vinay Joshi and Saptarshi Majumder and Xu Chao and Bin Ding and Ziqiong Liu and Pratik Prabhanjan Brahma and Dong Li and Zicheng Liu and Emad Barsoum},\n year={2025},\n eprint={2507.23194},\n archivePrefix={arXiv},\n primaryClass={cs.CL},\n url={https://arxiv.org/abs/2507.23194},\n}\n```\n\n## About\n\nIt is an LLM-based AI agent, which can write correct and efficient gpu kernels automatically.\n\n### Resources\n\n[Readme](https://github.com/github.com#readme-ov-file)\n\n### License\n\n[View license](https://github.com/github.com#License-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](https://github.com/AMD-AGI/GEAK-agent/activity)\n\n[Custom properties](https://github.com/AMD-AGI/GEAK-agent/custom-properties)\n\n### Stars\n\n[**31**\\\nstars](https://github.com/AMD-AGI/GEAK-agent/stargazers)\n\n### Watchers\n\n[**0**\\\nwatching](https://github.com/AMD-AGI/GEAK-agent/watchers)\n\n### Forks\n\n[**6**\\\nforks](https://github.com/AMD-AGI/GEAK-agent/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FAMD-AGI%2FGEAK-agent&report=AMD-AGI+%28user%29)\n\n## [Releases](https://github.com/AMD-AGI/GEAK-agent/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/orgs/AMD-AGI/packages?repo_name=GEAK-agent)\n\nNo packages published\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## [Contributors\\ 3](https://github.com/AMD-AGI/GEAK-agent/graphs/contributors)\n\n### Uh oh!\n\nThere was an error while loadi","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fgithub.com%2FAMD-AIG-AIMA%2FGEAK-Agent","id":"https://github.com/AMD-AIG-AIMA/GEAK-Agent","publishedDate":null,"summary":"The GitHub repository for **GEAK-agent** by AMD-AGI features an LLM-based AI framework designed to automatically generate functional and efficient GPU kernels. The framework includes components such as a Generator, Reflector, Evaluator, and Optimizer, allowing users to create and test their own coding agents. It provides a baseline agent for direct use and supports performance evaluation and optimization of generated code. The repository also includes documentation and resources for further exploration. For more details, visit the [GEAK-agent GitHub page](https://github.com/AMD-AGI/GEAK-agent).","title":"AMD-AGI/GEAK-agent: It is an LLM-based AI agent, which ... - GitHub","url":"https://github.com/AMD-AIG-AIMA/GEAK-Agent"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2507.23194","id":"https://arxiv.org/pdf/2507.23194","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage titled \"arXiv reCAPTCHA\" presents a research paper discussing the implementation and implications of reCAPTCHA technology on the arXiv platform. It explores how reCAPTCHA helps prevent automated submissions and enhances the integrity of the submission process. The paper likely includes details on the methodology, effectiveness, and potential challenges associated with using reCAPTCHA in academic settings. Additionally, it acknowledges support from the Simons Foundation and member institutions, indicating collaboration and funding for the research. For a more detailed understanding, readers are encouraged to access the full paper through the provided URL.","title":"[PDF] Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/pdf/2507.23194"},{"content":"Agree & Join LinkedIn\n\nBy clicking Continue to join or sign in, you agree to LinkedIn’s [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n\n [Skip to main content](https://www.linkedin.com/www.linkedin.com#main-content)\n\n# Pratik Prabhanjan Brahma’s Post\n\n[Pratik Prabhanjan Brahma](https://in.linkedin.com/in/pratik-p-brahma?trk=public_post_feed-actor-name)\n\nLeading AI Models Research at AMD India\n\n1mo\n\n- [Report this post](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fpratik-p-brahma_geak-introducing-triton-kernel-ai-agent-activity-7357773030414036992-mf1w&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nCan AI be used to accelerate AI? Our first step towards answering it is 𝗚𝗘𝗔𝗞 — an agentic framework that creates and optimizes GPU kernels in Triton.\nWe observe :\n• up to 63.33% accuracy in generating correct kernels (vs. ~15% from directly prompting frontier LLMs)\n• average speedup of up to 2.59X over reference kernels\n• log-linear scaling law during inference time, i.e. accuracy of GEAK increases with more runs\nWe are open-sourcing our code, test sets, etc. to support developers in building faster and smarter AI systems.\n• Blog - [https://lnkd.in/g6azcsgg](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fg6azcsgg&urlhash=oB6_&trk=public_post-text)\n• Arxiv paper - [https://lnkd.in/gn6UG4rX](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fgn6UG4rX&urlhash=nkZY&trk=public_post-text)\n• Github -  [https://lnkd.in/gw-vB-3t](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2Fgw-vB-3t&urlhash=gCbB&trk=public_post-text) and [https://lnkd.in/ggurR3er](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FggurR3er&urlhash=lqc3&trk=public_post-text) [Jianghui Wang](https://de.linkedin.com/in/jianghui-wang-657a53141?trk=public_post-text) , [Vinay Joshi](https://in.linkedin.com/in/vinaymjoshi?trk=public_post-text), [Saptarshi Majumder](https://in.linkedin.com/in/saptarshi-majumder-62251369?trk=public_post-text), Chao Xu, Bin Ding, [Ziqiong Liu](https://cn.linkedin.com/in/ziqiong-liu-9b421789?trk=public_post-text), [Pratik Prabhanjan Brahma](https://in.linkedin.com/in/pratik-p-brahma?trk=public_post-text), [Dong Li](https://cn.linkedin.com/in/dong-li-701005361?trk=public_post-text), [Zicheng Liu](https://www.linkedin.com/in/zicheng-liu?trk=public_post-text), [Emad Barsoum](https://www.linkedin.com/in/ebarsoum?trk=public_post-text) [#AMD](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Famd&trk=public_post-text) [#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&trk=public_post-text) [#AgenticAI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fagenticai&trk=public_post-text) [#GPU](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fgpu&trk=public_post-text) [#MachineLearning](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmachinelearning&trk=public_post-text) [#DeveloperTools](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fdevelopertools&trk=public_post-text) [#Triton](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Ftriton&trk=public_post-text) [#ROCm](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Frocm&trk=public_post-text) [#MI300X](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmi300x&trk=public_post-text)\n\n[Zicheng Liu](https://www.linkedin.com/in/zicheng-liu?trk=public_post_reshare_feed-actor-name)\n\n1mo\n\nI'm thrilled to share that we've released GEAK: our Triton Kernel AI Agent and Evaluation Benchmarks. We achieve an average execution accuracy of 54.89% on TritonBench-modified and 63.33% ROCm benchmark as compared to 15% if directly prompting frontier models. The generated kernels have an average speedup of 2.59X over the baseline.\n[#AMD](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Famd&trk=public_post_reshare-text) [#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllm&trk=public_post_reshare-text) [#ML](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fml&trk=public_post_reshare-text) [#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&trk=public_post_reshare-text) [#HW](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fhw&trk=public_post_reshare-text) [#MI300X](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmi300x&trk=public_post_reshare-text) [https://lnkd.in/gMcx6BBz](https://lnkd.in/gMcx6BBz?trk=public_post_reshare-text)\n\n[29](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fpratik-p-brahma_geak-introducing-triton-kernel-ai-agent-activity-7357773030414036992-mf1w&trk=public_post_social-actions-reactions) [1 Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fpratik-p-brahma_geak-introducing-triton-kernel-ai-agent-activity-7357773030414036992-mf1w&trk=public_post_social-actions-comments)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fpratik-p-brahma_geak-introducing-triton-kernel-ai-agent-activity-7357773030414036992-mf1w&trk=public_post_like-cta) [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fpratik-p-brahma_geak-introducing-triton-kernel-ai-agent-activity-7357773030414036992-mf1w&trk=public_post_comment-cta)\n\nShare\n\n- Copy\n- LinkedIn\n- Facebook\n- Twitter\n\n[Aritra Roy Gosthipaty](https://in.linkedin.com/in/arig23498?trk=public_post_comment_actor-name) 1mo\n\n- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fpratik-p-brahma_geak-introducing-triton-kernel-ai-agent-activity-7357773030414036992-mf1w&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\nIt would actually be awesome to integrate the kernels into Hugging Face.\nHere is how we support community driven kernels on the Hub:\n[https://huggingface.co/blog/hello-hf-kernels](https://huggingface.co/blog/hello-hf-kernels?trk=public_post_comment-text) [https://huggingface.co/blog/kernel-builder](https://huggingface.co/blog/kernel-builder?trk=public_post_comment-text)\nAlso tagging [Arthur Zucker](https://fr.linkedin.com/in/arthur-zucker-8a0445144?trk=public_post_comment-text) for more details.\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fpratik-p-brahma_geak-introducing-triton-kernel-ai-agent-activity-7357773030414036992-mf1w&trk=public_post_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fpratik-p-brahma_geak-introducing-triton-kernel-ai-agent-activity-7357773030414036992-mf1w&trk=public_post_comment_reply) 1 Reaction\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fpratik-p-brahma_geak-introducing-triton-kernel-ai-agent-activity-7357773030414036992-mf1w&trk=public_post_feed-cta-banner-cta)\n\n2,538 followers\n\n- [28 Posts](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Fpratik-p-brahma%2Frecent-activity%2F&trk=public_post_follow-posts)\n\n[View Profile](https://in.linkedin.com/in/pratik-p-brahma?trk=public_post_follow-view-profile) [Connect](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7357773030414036992&trk=public_post_follow)\n\n## Explore content categories\n\n- [Career](https://www.linkedin.com/top-content/career/)\n- [Productivity](https://www.linkedin.com/top-content/productivity/)\n- [Finance](https://www.linkedin.com/top-content/finance/)\n- [Soft Skills & Emotional Intelligence](https://www.linkedin.com/top-content/soft-skills-emotional-intelligence/)\n- [Project Management](https://www.linkedin.com/top-content/project-management/)\n- [Education](https://www.linkedin.com/top-content/education/)\n- [Technology](https://www.linkedin.com/top-content/technology/)\n- [Leadership](https://www.linkedin.com/top-content/leadership/)\n- [Ecommerce](https://www.linkedin.com/top-content/ecommerce/)\n\nShow more\n\nShow less\n\nLinkedIn\n\nNever miss a beat on the app\n\nDon’t have the app? Get it in the Microsoft Store.\n\n[Open the app](ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&mode=mini&cid=guest_desktop_upsell_post2)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fpratik-p-brahma_geak-introducing-triton-kernel-ai-agent-activity-7357773030414036992-mf1w","id":"https://www.linkedin.com/posts/pratik-p-brahma_geak-introducing-triton-kernel-ai-agent-activity-7357773030414036992-mf1w","publishedDate":"2025-08-03T00:00:00.000Z","summary":"The LinkedIn post by Pratik Prabhanjan Brahma discusses the potential of using AI to enhance AI development through a framework called GEAK. This framework focuses on creating and optimizing GPU kernels in Triton, achieving a notable accuracy of 63.33% in generating correct kernels, compared to approximately 15% from traditional large language models. Additionally, GEAK demonstrates an average speedup of 2.59 times over reference kernels and shows improved accuracy with increased runs, following a log-linear scaling law. The post also mentions that the code, test sets, and related resources are being open-sourced to assist developers in building more efficient AI systems. Links to a blog, an Arxiv paper, and GitHub repositories are provided for further exploration.","title":"How GEAK accelerates AI with GPU kernels in Triton - LinkedIn","url":"https://www.linkedin.com/posts/pratik-p-brahma_geak-introducing-triton-kernel-ai-agent-activity-7357773030414036992-mf1w"}],"status":"completed","toolCallId":"call_crK8S7W9XPnEYligRWIPvLdf","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Sure—let me walk you through Geak (GEAK) as if I were the first author, using the Feynman technique: explain it simply, build intuition, then add detail, and finally show evidence and limitations.\n\nWhat problem are we solving?\n- Writing fast GPU kernels is hard. You typically need a GPU expert to pick tile sizes, manage memory hierarchy, avoid bank conflicts, coalesce loads, and adapt to each GPU architecture. It’s time-consuming and doesn’t scale with the explosion of AI workloads.\n- LLMs can write code, but “correct and fast Triton kernels” is a much higher bar than, say, Python scripts. General LLMs tend to produce kernels that don’t compile, don’t pass tests, or run slowly.\n- We want an automated system that can: from a brief description, generate Triton kernels that are both correct and performant on AMD Instinct GPUs (MI300X, MI250), then verify them, and iteratively improve them—without a human in the loop.\n\nOur idea in one sentence\n- Build an agent around a strong LLM that uses a structured loop—generate, test, reflect, and optimize—plus inference-time compute scaling (multiple tries, feedback, and parallelization) to steadily turn rough kernel drafts into correct, fast kernels.\n\nWhy Triton, and why AMD GPUs?\n- Triton is a Python-like DSL designed for kernels; it balances control and readability and is widely adopted.\n- AMD Instinct GPUs power many modern AI stacks; to democratize performance across diverse hardware, we need automated kernel generation beyond a single vendor.\n\nWhat is GEAK (the system)?\nThink of GEAK as four cooperating roles around an LLM:\n\n1) Generator\n- Takes a short task description (e.g., “fused softmax over rows with dropout mask”) and any helpful context (a 1-shot similar kernel retrieved by code similarity, and a “knowledge pack” with Triton and hardware tips).\n- Produces an initial Triton kernel.\n\n2) Evaluator\n- Stage 1: Functional tests. We compile and run with unit tests and fixtures. If it fails, we collect the error trace and observable outputs.\n- Stage 2: If it passes functionally, we run performance tests (latency, memory behavior) against a reference.\n\n3) Reflector\n- Reads the failing code plus the error trace and explains what went wrong in plain language (e.g., “mismatched block pointer strides”, “out-of-bounds due to tile size vs. shape”).\n- Proposes a concrete fix strategy. This guards against repeating the same mistake and helps the LLM escape the “debugging trap” where it keeps making the same fix.\n\n4) Optimizer\n- When code is correct, we shift to speed. The optimizer sees a ranked history of previous candidates and their timings and proposes a new optimization plan (not code yet): try different BLOCK sizes, vectorized loads, shared memory tiling, software pipelining, persistent threads, avoiding register spills, etc.\n- The Generator then implements that plan into a new kernel variant.\n\nKey techniques that make this work\n- Inference-time compute scaling: we allocate more effort at inference: multiple iterations, parallel runs (temperature=1 for diversity), and pass@k selection. Accuracy improves roughly log-linearly with more runs.\n- Reflexion-style loop: we write down “lessons learned” from crashes and errors into a short-term memory for the next attempt. This dramatically improves debuggability.\n- 1-shot retrieval: we retrieve the most similar kernel by code similarity, not instruction similarity. Seeing a concrete, structurally similar Triton kernel is more helpful than a stylistically similar prompt.\n- Knowledge injection: we augment prompts with targeted Triton and AMD ROCm/Mi300X tips (e.g., alignment constraints, wavefront sizes, LDS usage patterns, TMA/block pointer idioms, masking semantics).\n- Anti-debugging-trap: if an attempt exceeds a max number of reflection cycles without progress, we force a fresh strategy to avoid looping.\n\nWhat do we evaluate on?\nWe needed rigorous, GPU-targeted benchmarks that score both correctness and speed. We release two suites:\n\n- TritonBench-revised (184 kernels)\n  - Adapted from TritonBench-G; we fixed issues discovered on AMD GPUs (e.g., shared memory errors, HIP argument mismatches, missing function calls, inconsistent test harnesses). This yields a cleaner, stricter correctness signal.\n- ROCm Triton Benchmark (30 kernels)\n  - Real kernels from open-source AMD ROCm repos (e.g., Triton-based GEMM variants, layernorm, softmax, random ops, sorting/swizzle tests). These reflect practical, expert-authored patterns.\n\nHow do we measure?\n- Correctness: unit tests must pass end-to-end. We also use pass@k to fairly account for multiple candidates.\n- Performance: latency vs. reference on the same shapes/hardware; we report average speedups for correct kernels.\n\nMain results\n- Correctness:\n  - On TritonBench-revised: up to 54.89% of tasks produce a fully correct kernel (vs. ~15% when directly prompting frontier LLMs without an agent loop).\n  - On ROCm Triton Benchmark: up to 63.33% correctness.\n- Performance:\n  - On TritonBench-revised, GEAK-generated correct kernels achieve average speedup up to 2.59× over reference baselines.\n- Scaling:\n  - We observe a log-linear scaling law with more inference-time attempts: more parallel/sequential runs → higher pass rates.\n\nWhat changed vs. naïve “LLM writes code”?\n- The failure mode of direct prompting is “compiles rarely; passes tests even more rarely; performance is an afterthought.” GEAK’s loop changes the game:\n  - Errors are treated as training signals at inference time.\n  - We deliberately separate “fix bugs” (reflector) from “get faster” (optimizer), so we don’t prematurely tune performance on broken code.\n  - We guide the optimizer with sorted historical candidates, which focuses the LLM on productive directions instead of random tweaks.\n\nA concrete example of the loop\n- Task: write a block-pointer matmul variant using Triton for MI300X with FP16 inputs.\n  1) Generator drafts a tiling (e.g., BLOCK_M/N/K = 128/128/32) with prefetching and masks for edge tiles.\n  2) Evaluator: tests fail—incorrect accumulation shape on fringe tiles.\n  3) Reflector: identifies incorrect stride usage and masking in the last K-tile; proposes a corrected mask computation and pointer arithmetic.\n  4) Generator applies fix → tests pass.\n  5) Optimizer: proposes trying different BLOCK_K and vectorized loads (ldmatrix-like patterns), adjusts num_warps, adds software pipelining hints.\n  6) Evaluator times variants; the fastest passing kernel is selected.\n  7) Parallel runs add diversity; pass@k increases the chance we find both correct and fast solutions.\n\nWhat we open-sourced\n- GEAK-agent: the multi-agent framework with Generator/Reflector/Evaluator/Optimizer, configs, and scripts to run on your data.\n- GEAK-eval: the evaluation harnesses for the two benchmarks, including our fixes and standardized tests/perf runners.\n\nWhy the results matter\n- Portable performance: With an agentic system, teams can target new hardware (here, AMD Instinct GPUs) without paying the full expert-optimization cost each time.\n- Productivity: Instead of weeks per kernel, you describe intent and let the agent grind through the debugging and tuning loop automatically.\n- Research direction: It shows that inference-time scaling and agentic feedback can substitute for specialized training in many cases, which is promising for “AI that accelerates AI.”\n\nLimitations and what’s next\n- Not all kernels pass yet. Control-flow-heavy or numerically fragile kernels still fail at notable rates.\n- Performance tuning is good but not always expert-level. Truly bleeding-edge performance (e.g., micro-architectural tricks, exotic prefetching) may need custom guidance or additional domain tools.\n- Model dependence: stronger LLMs help. We experimented with multiple frontier models; results vary by backbone.\n- Generalization: we focused on Triton on AMD GPUs; extending to CUDA, HIP bare-metal, or embedded accelerators will need adapted knowledge packs and evaluators.\n\nPlanned extensions\n- Richer retrieval: structural program analysis to find not only similar code, but similar performance bottlenecks.\n- Learned optimizers: have the optimizer learn which strategies worked historically for a given pattern/hardware.\n- Multi-objective tuning: jointly optimize for speed, memory footprint, numerical stability, and energy.\n- Broader benchmarks: more ROCm kernels and cross-vendor tasks, plus community submissions.\n\nA quick takeaway for practitioners\n- If you have a kernel-shaped need in Triton, GEAK can bootstrap you from a short description to a correct, competitive kernel and iterate toward better speed automatically.\n- Use 1-shot retrieval, enable reflexion, keep a cap on debugging loops to avoid traps, and run parallel candidates to harvest diversity. Measure with pass@k.\n\nPointers\n- Paper: arXiv:2507.23194\n- Agent code: github.com/AMD-AIG-AIMA/GEAK-agent\n- Evaluation: github.com/AMD-AIG-AIMA/GEAK-eval\n- Overview blog (ROCm): “GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks”\n\nIf you’d like, tell me a kernel you care about (e.g., attention softmax variant, fused norm+activation), and I’ll sketch how GEAK would approach generating and tuning it.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0a553a7eac3ac7a50068daed4022bc81a198e9ecfe42c28785"}},"resumableStreamId":"STREAM:4bb035ca-d9db-4e9d-b1d2-a7c3442365cb","role":"assistant","spanId":"c0925eed9566510e","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","timeToFirstToken":0.96,"tokens":2021,"tokensPerSecond":32.64574280776002,"traceId":"8daf1257f2264c047c28eefca8c3f93a","updated_at":1759178035377,"userId":"google:104458561188320708185","id":"j977d00g5h25mpr7a7fg92ksqh7rhq7j"},{"_creationTime":1759178140704.0264,"_id":"j97d6zd82gf6y9phh97tx4c9hd7rgwvr","attachmentIds":[],"content":"what is the evaluation pipeline of GEAK","created_at":1759178140704,"messageId":"f3f30336-e6ca-494c-baca-156c234b541f","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","spanId":"ae157cdb3be0fab9","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","traceId":"8e267a21d9b6f6bd82ba7c38deb73287","updated_at":1759178140704,"userId":"google:104458561188320708185","id":"j97d6zd82gf6y9phh97tx4c9hd7rgwvr","providerMetadata":{}},{"_creationTime":1759178140704.0266,"_id":"j9717dbttf369v4mh8wvkk2zzx7rg73g","attachmentIds":[],"byok":false,"content":"Here’s the GEAK evaluation pipeline, step by step, focusing on what is measured, how it’s executed, and how results are aggregated.\n\nInputs\n- Task spec: brief natural-language description of the kernel plus shapes/dtypes.\n- Reference implementation: PyTorch or Triton baseline for functional parity and timing.\n- Hardware: AMD Instinct GPUs (e.g., MI300X/MI250) under ROCm.\n- Benchmarks:\n  - TritonBench-revised (184 tasks; fixes for AMD-specific issues).\n  - ROCm Triton Benchmark (30 real kernels from ROCm repos).\n\nPipeline stages\n1) Candidate generation (pass@k setting)\n- The agent produces multiple kernel candidates per task using its generate–reflect–optimize loop.\n- Diversity via different temperatures/seeds and strategy plans.\n- Each candidate is independently evaluated (below). Final scoring uses pass@k.\n\n2) Functional verification\n- Compile: Try to JIT-compile the Triton kernel. Capture compile-time errors.\n- Run unit tests: Deterministic test harness executes the kernel across prescribed shapes.\n- Correctness checks:\n  - Numerical: compare outputs vs. reference using dtype-appropriate tolerances (e.g., FP16/FP32 epsilons) and mask handling on edge tiles.\n  - Shape/stride/layout: ensure correct output shapes, strides, and no OOB accesses (masking).\n- Result: candidate is marked functionally correct only if all tests pass without runtime errors.\n\n3) Performance evaluation (only for functionally correct candidates)\n- Warmup: multiple warmup runs to stabilize caches and clocks.\n- Timing: multiple measured runs; report median or trimmed mean latency to reduce noise.\n- Baseline comparison: compute speedup versus the reference implementation for the same shapes/dtypes.\n- Resource sanity checks: optional counters to detect register spill anomalies or pathological LDS use when available.\n\n4) Iterative reflection and optimization (within an evaluation budget)\n- If a candidate fails:\n  - Error triage: collect compiler/runtime traces and mismatched tensor stats.\n  - Reflection note: summarize root cause (e.g., incorrect pointer arithmetic, mask, stride).\n  - Regenerate with a targeted fix; repeat functional verification.\n- If a candidate passes but is slow:\n  - Optimization plan: propose concrete tweaks (BLOCK sizes, num_warps, vectorized loads, pipelining, tiling).\n  - Generate new variants and re-time them.\n- Anti-stall guard: cap iterations per task; if no progress, force a fresh strategy or stop.\n\n5) Aggregation and metrics\n- Correctness:\n  - pass@1 and pass@k: fraction of tasks for which at least one of k candidates is functionally correct.\n- Performance:\n  - For tasks with at least one correct candidate, report best-achieved speedup over baseline.\n  - Summaries: average and median speedup across solved tasks; optionally success-weighted averages.\n- Scaling curves:\n  - Plot correctness vs. inference-time budget (number of attempts/parallelism) to show scaling behavior.\n\nReproducibility controls\n- Fixed seeds per attempt class; logged prompts, retrieved exemplars, and reflection notes.\n- Pinned ROCm/Triton versions, kernel launch configs, and benchmark inputs.\n- Standardized harness:\n  - Identical tensor initializers and dtype handling.\n  - Consistent tolerance thresholds per op/dtype.\n  - Uniform timing protocol across tasks.\n\nOutputs per benchmark\n- TritonBench-revised:\n  - pass@k correctness rates.\n  - Speedup distribution for solved tasks.\n- ROCm Triton Benchmark:\n  - Same metrics, highlighting real-world kernels.\n\nIn short: GEAK evaluates each generated kernel through compile → unit tests → timed runs; uses failures as feedback to produce improved candidates; and reports pass@k correctness and speedups against reference baselines under a controlled ROCm/Triton harness.","created_at":1759178140705,"messageId":"026099bc-6755-4d6a-85ae-2221369e6005","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"news","domains":null,"queries":["GEAK Triton Kernel AI Agent evaluation pipeline GEAK-eval arXiv:2507.23194 'Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks' evaluation pipeline details 2025-07-01","GEAK agent evaluator correctness performance pass@k ROCm Triton Benchmark TritonBench-revised evaluation harness 2025"]},"result":[{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.23194","id":"https://arxiv.org/abs/2507.23194","publishedDate":null,"summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses a system designed to improve the submission process on the arXiv platform. The paper details the methodology and implications of implementing reCAPTCHA to enhance user verification while maintaining accessibility. It acknowledges support from the Simons Foundation and member institutions, indicating collaborative efforts in the research. For a comprehensive understanding, readers are encouraged to explore the full paper linked on the page.","title":"Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/abs/2507.23194"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2507.23194v1","id":"https://arxiv.org/html/2507.23194v1","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage titled \"arXiv reCAPTCHA\" discusses a specific implementation of reCAPTCHA technology on the arXiv platform, which is a repository for research papers. It acknowledges support from the Simons Foundation and member institutions. The content likely includes technical details about the reCAPTCHA system, its purpose in preventing spam and automated submissions, and its integration into the arXiv submission process. For a more detailed understanding, users may need to explore the full text.","title":"GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/html/2507.23194v1"},{"content":"## AI-Powered Library for Researchers\n\n### Track research interests, scroll daily paper feeds with AI summary, and chat with bulk of files.\n\n1. [Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks](https://arxiv.org/abs/2507.23194)\n\ncs.CLcs.LGcs.AI01 Aug 2025\n\nJianghui Wang, Vinay Joshi, Saptarshi Majumder, Xu Chao, Bin Ding, Ziqiong Liu, Pratik Prabhanjan Brahma, Dong Li, Zicheng Liu, Emad Barsoum\n\nAdvanced Micro Devices, Inc. (AMD)\n\nThe demand for AI-generated GPU kernels is rapidly growing, influenced by the need for scalable, hardware-optimized solutions in both industry and academia. As deep learning workloads grow in complexity and diversity, it is imperative to automate low-level kernel development to meet performance and productivity demands. Major cloud providers, semiconductor companies, and research institutions are now investing heavily in AI-driven code generation for GPUs, aiming to reduce manual optimization efforts while achieving near-expert performance on hardware like AMD MI300X. The Triton language, a Python-based DSL for GPU programming, has emerged as a popular target for such AI-generated kernels due to its balance of performance and ease-of-coding. In this work, we present an evaluation suite for Triton-based GPU kernels and GEAK (Generating Efficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs to generate performant Triton code specifically for AMD GPUs, including the AMD MI300X and MI250. GEAK leverages inference-time compute scaling to produce Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style feedback mechanisms. On two evaluation benchmarks, GEAK significantly outperformed the baselines of directly prompting frontier LLMs as well as Reflexion-based generation pipelines by achieving correctness up to $63$% and execution speed up of up to $2.59$X. These results highlight the promise of GEAK-like agentic code generation for accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance.\n\nIllustration of GEAK pipeline.\n\n# Why ChatPaper\n\n- Interest-Driven Paper Curation\n\nDescribe your interests as you like, be it keywords or sentence, you’ll get relevant papers every day via AI semantic matching.\n\n- Top Conferences, One-Stop\n\nIJCAI, ICML, CVPR, KDD... Access papers from top AI conferences, all in one tool, for free.\n\n- Paper Management Made Easy\n\nKeep your research organized with our bookmarking feature. Build a well-structured knowledge hub at your fingertips.\n\n- Chat With Any Paper\n\nGot questions? Ask in ChatDOC with one single click. Use AI to pull data, clarify terms, and verify facts with our precise word-level tracing feature.\n\n\n# See how it works\n\n## Watch now 1 min\n\nGet Started","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fchatpaper.com%2Fpaper%2F172041","id":"https://chatpaper.com/paper/172041","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage introduces GEAK (Generating Efficient AI-centric GPU Kernels), a framework designed to automate the generation of GPU kernels using the Triton programming language, particularly for AMD GPUs like the MI300X and MI250. As the demand for AI-generated GPU kernels increases, GEAK utilizes advanced large language models (LLMs) to produce efficient Triton code, significantly improving performance and correctness compared to traditional methods. The evaluation benchmarks show that GEAK achieves up to 63% correctness and 2.59 times faster execution speed than baseline approaches. The page also highlights features of ChatPaper, an AI-powered library for researchers that curates papers based on user interests, organizes research, and facilitates interaction with academic papers.","title":"Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://chatpaper.com/paper/172041"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2507.23194","id":"https://arxiv.org/pdf/2507.23194","publishedDate":null,"summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses the implementation and implications of a reCAPTCHA system for the arXiv preprint repository. The paper explores how this system helps in preventing automated submissions and ensuring the integrity of the submission process. It highlights the technical aspects of the reCAPTCHA integration, its effectiveness, and the potential impact on user experience. The authors acknowledge support from the Simons Foundation and member institutions, indicating collaborative efforts in this research. For a detailed understanding, readers are encouraged to refer to the full paper available at the provided URL.","title":"[PDF] Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/pdf/2507.23194"},{"content":"Agree & Join LinkedIn\n\nBy clicking Continue to join or sign in, you agree to LinkedIn’s [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n\n [Skip to main content](https://www.linkedin.com/www.linkedin.com#main-content)\n\n# Introducing GEAK: Triton Kernel AI Agent and Benchmarks\n\nThis title was summarized by AI from the post below.\n\n[Zicheng Liu](https://www.linkedin.com/in/zicheng-liu?trk=public_post_feed-actor-name)\n\n1mo\n\n- [Report this post](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nI'm thrilled to share that we've released GEAK: our Triton Kernel AI Agent and Evaluation Benchmarks. We achieve an average execution accuracy of 54.89% on TritonBench-modified and 63.33% ROCm benchmark as compared to 15% if directly prompting frontier models. The generated kernels have an average speedup of 2.59X over the baseline.\n[#AMD](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Famd&trk=public_post-text) [#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllm&trk=public_post-text) [#ML](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fml&trk=public_post-text) [#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&trk=public_post-text) [#HW](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fhw&trk=public_post-text) [#MI300X](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmi300x&trk=public_post-text) [https://lnkd.in/gMcx6BBz](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgMcx6BBz&urlhash=IKK1&trk=public_post-text)\n\n[141](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_social-actions-reactions) [3 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_social-actions-comments)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_like-cta) [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment-cta)\n\nShare\n\n- Copy\n- LinkedIn\n- Facebook\n- Twitter\n\n['YZ' Yezhou Yang](https://www.linkedin.com/in/yz-yezhou-yang-6a02462a?trk=public_post_comment_actor-name)\n\nProf. YZ at School of Computing and #AI;\nAmazon Scholar at Prime Video;\nActive and Interactive Perception Solutions for #AI and #GenAI tech Deployment and Automated Mobility.\n\n1mo\n\n- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\nCongratulations 🎉🎉🎉\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reply) [1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reactions) 2 Reactions\n\n[Stefanos Damianakis](https://www.linkedin.com/in/damianakis?trk=public_post_comment_actor-name)\n\nSoftware Executive \\| Enterprise Software \\| Board Director \\| Advisor\n\n1mo\n\n- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\nBravo [Zicheng](https://www.linkedin.com/in/zicheng-liu?trk=public_post_comment-text) and team! 👏🏻👏🏻👏🏻\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reply) 1 Reaction\n\n[Vijay Krishnan](https://www.linkedin.com/in/vijay0?trk=public_post_comment_actor-name)\n\nFounder & CTO, Turing.com\n\n1mo\n\n- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\nCongrats, [Zicheng](https://www.linkedin.com/in/zicheng-liu?trk=public_post_comment-text)!\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reply) [1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reactions) 2 Reactions\n\n[See more comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_see-more-comments)\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_feed-cta-banner-cta)\n\n6,279 followers\n\n- [25 Posts](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Fzicheng-liu%2Frecent-activity%2F&trk=public_post_follow-posts)\n\n[View Profile](https://www.linkedin.com/in/zicheng-liu?trk=public_post_follow-view-profile) [Connect](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7357054030113656833&trk=public_post_follow)\n\n## Explore content categories\n\n- [Career](https://www.linkedin.com/top-content/career/)\n- [Productivity](https://www.linkedin.com/top-content/productivity/)\n- [Finance](https://www.linkedin.com/top-content/finance/)\n- [Soft Skills & Emotional Intelligence](https://www.linkedin.com/top-content/soft-skills-emotional-intelligence/)\n- [Project Management](https://www.linkedin.com/top-content/project-management/)\n- [Education](https://www.linkedin.com/top-content/education/)\n- [Technology](https://www.linkedin.com/top-content/technology/)\n- [Leadership](https://www.linkedin.com/top-content/leadership/)\n- [Ecommerce](https://www.linkedin.com/top-content/ecommerce/)\n\nShow more\n\nShow less\n\nLinkedIn\n\nNever miss a beat on the app\n\nDon’t have the app? Get it in the Microsoft Store.\n\n[Open the app](ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&mode=mini&cid=guest_desktop_upsell_post2)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v","id":"https://www.linkedin.com/posts/zicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v","publishedDate":"2025-08-01T00:00:00.000Z","summary":"Zicheng Liu announces the release of GEAK, a Triton Kernel AI Agent and Evaluation Benchmarks. The new system achieves an average execution accuracy of 54.89% on the TritonBench-modified benchmark and 63.33% on the ROCm benchmark, significantly outperforming the 15% accuracy of directly prompting frontier models. Additionally, the generated kernels demonstrate an average speedup of 2.59 times over the baseline. The post highlights the advancements in AI performance and efficiency brought by GEAK.","title":"Introducing GEAK: Triton Kernel AI Agent and Benchmarks - LinkedIn","url":"https://www.linkedin.com/posts/zicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2509.14279","id":"https://arxiv.org/abs/2509.14279","publishedDate":"2025-09-16T00:00:00.000Z","summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses a method for improving the arXiv preprint repository's submission process using reCAPTCHA technology. The authors explore how this approach can help prevent spam submissions while maintaining accessibility for legitimate users. The paper acknowledges support from the Simons Foundation and member institutions, indicating collaboration and backing for the research. For detailed insights, including methodologies and findings, refer to the full paper linked on the page.","title":"Towards Robust Agentic CUDA Kernel Benchmarking, Verification, and Optimization","url":"https://arxiv.org/abs/2509.14279"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2502.14752","id":"https://arxiv.org/abs/2502.14752","publishedDate":"2025-02-20T00:00:00.000Z","summary":"The webpage titled \"arXiv reCAPTCHA\" discusses a research paper that explores the implementation and implications of reCAPTCHA systems on the arXiv platform. It highlights the challenges and solutions related to automated submissions and spam prevention. The authors acknowledge support from the Simons Foundation and member institutions, emphasizing the collaborative effort behind the research. The paper likely includes technical details and findings relevant to improving user verification processes in online repositories. For a more in-depth understanding, readers are encouraged to access the full paper through the provided link.","title":"TritonBench: Benchmarking Large Language Model Capabilities for Generating Triton Operators","url":"https://arxiv.org/abs/2502.14752"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2509.14647","id":"https://arxiv.org/abs/2509.14647","publishedDate":"2025-09-18T00:00:00.000Z","summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses a system designed to improve the submission process on the arXiv platform by implementing a reCAPTCHA mechanism. The authors explore the challenges of distinguishing between human users and automated bots, aiming to enhance the integrity of submissions. The paper acknowledges support from the Simons Foundation and member institutions, indicating collaborative efforts in this research. For a detailed understanding, readers are encouraged to access the full paper through the provided link.","title":"AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production","url":"https://arxiv.org/abs/2509.14647"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2509.14477","id":"https://arxiv.org/abs/2509.14477","publishedDate":"2025-09-17T00:00:00.000Z","summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses the implementation of a reCAPTCHA system for the arXiv preprint repository. The paper outlines the challenges of spam and automated submissions in online platforms and proposes a solution to enhance the integrity of submissions. It details the methodology, results, and implications of the reCAPTCHA system, emphasizing its effectiveness in distinguishing between human users and bots. The authors acknowledge support from the Simons Foundation and member institutions, highlighting the collaborative nature of the research. For further details, the full paper can be accessed through the provided URL.","title":"Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation","url":"https://arxiv.org/abs/2509.14477"}],"status":"completed","toolCallId":"call_RFAifAmxKHuJXXB8UIlsNLHM","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Here’s the GEAK evaluation pipeline, step by step, focusing on what is measured, how it’s executed, and how results are aggregated.\n\nInputs\n- Task spec: brief natural-language description of the kernel plus shapes/dtypes.\n- Reference implementation: PyTorch or Triton baseline for functional parity and timing.\n- Hardware: AMD Instinct GPUs (e.g., MI300X/MI250) under ROCm.\n- Benchmarks:\n  - TritonBench-revised (184 tasks; fixes for AMD-specific issues).\n  - ROCm Triton Benchmark (30 real kernels from ROCm repos).\n\nPipeline stages\n1) Candidate generation (pass@k setting)\n- The agent produces multiple kernel candidates per task using its generate–reflect–optimize loop.\n- Diversity via different temperatures/seeds and strategy plans.\n- Each candidate is independently evaluated (below). Final scoring uses pass@k.\n\n2) Functional verification\n- Compile: Try to JIT-compile the Triton kernel. Capture compile-time errors.\n- Run unit tests: Deterministic test harness executes the kernel across prescribed shapes.\n- Correctness checks:\n  - Numerical: compare outputs vs. reference using dtype-appropriate tolerances (e.g., FP16/FP32 epsilons) and mask handling on edge tiles.\n  - Shape/stride/layout: ensure correct output shapes, strides, and no OOB accesses (masking).\n- Result: candidate is marked functionally correct only if all tests pass without runtime errors.\n\n3) Performance evaluation (only for functionally correct candidates)\n- Warmup: multiple warmup runs to stabilize caches and clocks.\n- Timing: multiple measured runs; report median or trimmed mean latency to reduce noise.\n- Baseline comparison: compute speedup versus the reference implementation for the same shapes/dtypes.\n- Resource sanity checks: optional counters to detect register spill anomalies or pathological LDS use when available.\n\n4) Iterative reflection and optimization (within an evaluation budget)\n- If a candidate fails:\n  - Error triage: collect compiler/runtime traces and mismatched tensor stats.\n  - Reflection note: summarize root cause (e.g., incorrect pointer arithmetic, mask, stride).\n  - Regenerate with a targeted fix; repeat functional verification.\n- If a candidate passes but is slow:\n  - Optimization plan: propose concrete tweaks (BLOCK sizes, num_warps, vectorized loads, pipelining, tiling).\n  - Generate new variants and re-time them.\n- Anti-stall guard: cap iterations per task; if no progress, force a fresh strategy or stop.\n\n5) Aggregation and metrics\n- Correctness:\n  - pass@1 and pass@k: fraction of tasks for which at least one of k candidates is functionally correct.\n- Performance:\n  - For tasks with at least one correct candidate, report best-achieved speedup over baseline.\n  - Summaries: average and median speedup across solved tasks; optionally success-weighted averages.\n- Scaling curves:\n  - Plot correctness vs. inference-time budget (number of attempts/parallelism) to show scaling behavior.\n\nReproducibility controls\n- Fixed seeds per attempt class; logged prompts, retrieved exemplars, and reflection notes.\n- Pinned ROCm/Triton versions, kernel launch configs, and benchmark inputs.\n- Standardized harness:\n  - Identical tensor initializers and dtype handling.\n  - Consistent tolerance thresholds per op/dtype.\n  - Uniform timing protocol across tasks.\n\nOutputs per benchmark\n- TritonBench-revised:\n  - pass@k correctness rates.\n  - Speedup distribution for solved tasks.\n- ROCm Triton Benchmark:\n  - Same metrics, highlighting real-world kernels.\n\nIn short: GEAK evaluates each generated kernel through compile → unit tests → timed runs; uses failures as feedback to produce improved candidates; and reports pass@k correctness and speedups against reference baselines under a controlled ROCm/Triton harness.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0eba799e979ca6080068daeda880b4819cb4366942453edab6"}},"resumableStreamId":"STREAM:1132913b-9dd3-4613-99dc-954d1a938690","role":"assistant","spanId":"ae157cdb3be0fab9","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","timeToFirstToken":1.604,"tokens":805,"tokensPerSecond":31.341249756667313,"traceId":"8e267a21d9b6f6bd82ba7c38deb73287","updated_at":1759178140705,"userId":"google:104458561188320708185","id":"j9717dbttf369v4mh8wvkk2zzx7rg73g"},{"_creationTime":1759178190751.431,"_id":"j97ff260x27rfav345c6wna8g17rg1hz","attachmentIds":[],"content":"now do they use LLM as optimizer","created_at":1759178190751,"messageId":"36cca6f5-cb09-4fcb-996c-87f8df7d2e46","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","spanId":"3afab23b7f77deb7","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","traceId":"1aafd235606862d8c521c109b4cf050e","updated_at":1759178190751,"userId":"google:104458561188320708185","id":"j97ff260x27rfav345c6wna8g17rg1hz","providerMetadata":{}},{"_creationTime":1759178190751.4312,"_id":"j97bvs4dp3chvsrentmev6fswx7rgf45","attachmentIds":[],"byok":false,"content":"Yes. In GEAK, the optimizer is an LLM-driven role.\n\n- Separation of concerns: GEAK splits “make it correct” and “make it fast.” Once a candidate passes functional tests, the Optimizer role kicks in.\n- Optimizer’s job: The LLM reads the history of prior candidates (configs, perf measurements, error notes) and proposes a concrete optimization plan, not just vague advice. Typical levers:\n  - Tile sizes: BLOCK_M/BLOCK_N/BLOCK_K, num_warps, num_stages\n  - Memory: vectorized/global loads-stores, shared/LDS tiling, block pointers/TMA patterns, prefetch distance\n  - Execution: software pipelining, persistent threads, unrolling, avoiding register spill\n  - Edge handling: masking strategy for fringe tiles to reduce divergence\n- Loop mechanics: The plan is handed back to the Generator (also LLM-guided) to implement as a new kernel variant, which is then re-benchmarked. The Optimizer uses the ranked timings of recent variants to steer the next plan toward promising regions rather than random tweaks.\n- Inference-time scaling: Multiple optimizer-guided variants are explored in parallel (different seeds/temperatures), and the best passing kernel is selected (pass@k and best speedup).\n\nSo, the optimization phase is agentic and LLM-led: the LLM reasons about performance trade-offs and proposes targeted Triton-level changes, guided by measured feedback.","created_at":1759178190752,"messageId":"023277cf-bcc4-4cad-b195-a1a3cd39f30b","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Yes. In GEAK, the optimizer is an LLM-driven role.\n\n- Separation of concerns: GEAK splits “make it correct” and “make it fast.” Once a candidate passes functional tests, the Optimizer role kicks in.\n- Optimizer’s job: The LLM reads the history of prior candidates (configs, perf measurements, error notes) and proposes a concrete optimization plan, not just vague advice. Typical levers:\n  - Tile sizes: BLOCK_M/BLOCK_N/BLOCK_K, num_warps, num_stages\n  - Memory: vectorized/global loads-stores, shared/LDS tiling, block pointers/TMA patterns, prefetch distance\n  - Execution: software pipelining, persistent threads, unrolling, avoiding register spill\n  - Edge handling: masking strategy for fringe tiles to reduce divergence\n- Loop mechanics: The plan is handed back to the Generator (also LLM-guided) to implement as a new kernel variant, which is then re-benchmarked. The Optimizer uses the ranked timings of recent variants to steer the next plan toward promising regions rather than random tweaks.\n- Inference-time scaling: Multiple optimizer-guided variants are explored in parallel (different seeds/temperatures), and the best passing kernel is selected (pass@k and best speedup).\n\nSo, the optimization phase is agentic and LLM-led: the LLM reasons about performance trade-offs and proposes targeted Triton-level changes, guided by measured feedback.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_04f9b7e83242a9520068daedcf52d08190bd2a15206e69e39a"}},"resumableStreamId":"STREAM:ad6b0447-895b-46ab-b362-bb13084fab24","role":"assistant","spanId":"3afab23b7f77deb7","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","timeToFirstToken":1.357,"tokens":307,"tokensPerSecond":50.336120675520576,"traceId":"1aafd235606862d8c521c109b4cf050e","updated_at":1759178190752,"userId":"google:104458561188320708185","id":"j97bvs4dp3chvsrentmev6fswx7rgf45"},{"_creationTime":1759178237822.2441,"_id":"j977nmgqm9p4d0czyd19z03yh97rhtqb","attachmentIds":[],"content":"what is the limit of debugging attempts?","created_at":1759178237822,"messageId":"373cb44d-231e-4738-a11c-13c05831d182","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","spanId":"04638ce20048a802","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","traceId":"5dbc0bed8c981bf1811a2a5431ba5b7e","updated_at":1759178237822,"userId":"google:104458561188320708185","id":"j977nmgqm9p4d0czyd19z03yh97rhtqb","providerMetadata":{}},{"_creationTime":1759178237822.2444,"_id":"j97179mb2abk3yh9xrd6q957v17rh3ka","attachmentIds":[],"byok":false,"content":"They cap it with a configurable parameter to avoid the “debugging trap.”\n\n- Name: max_perf_debug_num\n- Meaning: maximum number of reflection/debugging attempts allowed for a given code path before the agent must abandon that approach.\n- Behavior: if a kernel keeps failing after hitting this limit, GEAK discards the current strategy and forces the LLM to generate a fresh plan and new code, rather than looping on the same bug.\n\nThe exact numeric value isn’t fixed by the paper; it’s a tunable setting in the framework/configs.","created_at":1759178237823,"messageId":"e6bac911-85b5-4ec6-843d-bfc35d1078c7","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"news","domains":null,"queries":["GEAK Triton Kernel AI Agent debugging attempts limit reflection cycles paper arXiv:2507.23194","Geak Introducing Triton Kernel AI Agent evaluation benchmarks max iterations cap anti-debugging-trap limit attempts"]},"result":[{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.23194","id":"https://arxiv.org/abs/2507.23194","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses a system designed to improve the submission process on the arXiv platform by implementing a reCAPTCHA mechanism. This mechanism aims to differentiate between human users and automated bots, enhancing the integrity and security of submissions. The paper outlines the methodology, results, and implications of this approach, contributing to the ongoing efforts to maintain the quality and reliability of academic submissions. For further details, the full paper can be accessed through the provided URL.","title":"Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/abs/2507.23194"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2507.23194","id":"https://arxiv.org/pdf/2507.23194","publishedDate":null,"summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses the implementation and implications of reCAPTCHA technology on the arXiv platform. It explores how reCAPTCHA helps in preventing automated submissions and ensuring the integrity of user interactions. The paper likely includes methodologies, results, and discussions on the effectiveness of this technology in maintaining the quality of submissions to arXiv. Additionally, it acknowledges support from the Simons Foundation and member institutions. For a detailed understanding, readers are encouraged to refer to the full paper.","title":"[PDF] Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/pdf/2507.23194"},{"content":"[Skip to main content](https://rocm.blogs.amd.com/rocm.blogs.amd.com#main-content)\nBack to top\n\n`Ctrl` + `K`\n\n[ROCm blogs](https://rocm.blogs.amd.com/)\n\n# GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks\n\n## Contents\n\n# GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#geak-introducing-triton-kernel-ai-agent-evaluation-benchmarks)\n\nAugust 01, 2025 by [Jianghui Wang](https://rocm.blogs.amd.com/authors/jianghui-wang.html), [Vinay Joshi](https://rocm.blogs.amd.com/authors/vinay-joshi.html), [Saptarshi Majumder](https://rocm.blogs.amd.com/authors/saptarshi-majumder.html), [Chao Xu](https://rocm.blogs.amd.com/authors/chao-xu.html), [Bin Ding](https://rocm.blogs.amd.com/authors/bin-ding.html), [Ziqiong Liu](https://rocm.blogs.amd.com/authors/ziqiong-liu.html), [Pratik Prabhanjan Brahma](https://rocm.blogs.amd.com/authors/pratik-prabhanjan-brahma.html), [Dong Li](https://rocm.blogs.amd.com/authors/dong-li.html), [Zicheng Liu](https://rocm.blogs.amd.com/authors/zicheng-liu.html), [Emad Barsoum](https://rocm.blogs.amd.com/authors/emad-barsoum.html).\n\n10 min read. \\| 2414 total words.\n\n[Software tools & optimizations](https://rocm.blogs.amd.com/blog/category/software-tools-optimizations.html)\n\n[AI/ML](https://rocm.blogs.amd.com/blog/tag/aiml.html)\n\n[Developers](https://rocm.blogs.amd.com/developers.html), [AI](https://rocm.blogs.amd.com/ai.html)\n\nAt AMD, we are pioneering ways to accelerate AI development using AI itself, by generating accurate and efficient GPU kernels. Specifically, we are starting with the automatic generation of kernels in [Triton](https://triton-lang.org/main/index.html), an open-source Python-like language for writing parallel programming code for GPUs. Today, AMD is excited to announce (a) [Generating Efficient AI-centric Kernels (GEAK)](https://github.com/AMD-AIG-AIMA/GEAK-Agent) for AMD GPUs, and results on (b) two Triton kernel evaluation benchmarks, where we show how AI agents can perform inference-time scaling with frontier LLMs to generate accurate and efficient kernels for AMD Instinct™ GPUs like MI250X and MI300X.\n\n**Key Takeaways:**\n\n- Announcing [GEAK Triton kernel generation framework](https://github.com/AMD-AIG-AIMA/GEAK-Agent), a library of various types of agentic AI and scaling methods to automatically produce Triton kernels from just brief instructions of its intended functionality.\n\n- Announcing corresponding [GEAK Triton Kernel Evaluation Benchmarks](https://github.com/AMD-AIG-AIMA/GEAK-eval), consisting of a modified set of 184 kernels originally sourced from [TritonBench-G](https://arxiv.org/abs/2502.14752) \\[1\\] and an additional set of 30 kernels from various open [ROCm repositories](https://github.com/ROCm).\n\n- Execution accuracy up to 54.89% on TritonBench-modified and 63.33% on ROCm benchmark (as compared to around 15% when directly prompting frontier LLMs) in producing correct kernels that pass all the unit tests.\n\n- Average Speedup of up to 2.59X with the AI-generated kernels over reference kernels from the TritonBench-modified benchmark.\n\n\nFigure 1. Overview of the GEAK kernel code generation AI agent [#](https://rocm.blogs.amd.com/rocm.blogs.amd.com#id1)\n\nAs shown in Figure 1, the agentic AI system comprises four core modules: a **generator**, a **reflector**, an **evaluator**, and an **optimizer**.\n\n- The **generator** produces code based on user query and contextual information.\n\n- The **evaluator** follows a cascaded design: it first performs a functionality test to verify correctness. If the code fails this test, the corresponding error trace is fed back to the reflector for further analysis. If the code passes, the evaluator proceeds to assess its performance, including latency and memory efficiency.\n\n- If the generated code fails to execute correctly, the **reflector** analyzes both the code and the resulting error trace to identify potential issues.\n\n- Finally, the **optimizer** takes as input the functionally correct code and formulates strategies to enhance its performance with respect to latency and computational efficiency.\n\n\nTo improve the performance of our agent, we employ some composable and configurable techniques as mentioned below:\n\n- **1-shot Prompting**\n\n\nTo enable 1-shot prompting, the most similar Triton code from existing datasets is used in the prompt. The datasets used for 1-shot prompt do not overlap with the benchmark. We’ve observed that 1-shot sample retrieval from datasets is most effective when using code similarity rather than instruction similarity.\n\n- **Knowledge Injection**\n\n\nWe enhance the prompt with domain-specific knowledge on writing efficient Triton kernels, including detailed hardware specifications. This incorporation of low-level optimization principles significantly improves the accuracy and quality of LLM-generated code.\n\n- **Reflexion**\n\n\nTo enhance the system’s self-correcting capabilities, we leverage a reflexion \\[3\\] module that supports introspective debugging and iterative refinement. When an LLM-generated kernel fails the functionality test, the resulting error trace is provided as feedback to the reflector for further analysis and correction. The agent is tasked with analyzing the cause of failure and proposing a corresponding solution. We scale the number of iterations to improve kernel generation.\n\n- **LLM selection for kernel agent**\n\n\nWe have explored the use of multiple LLMs like GPT-4.1, GPT-o1, and Gemini 2.5 Pro. We observed notable variations in the output produced by different models, indicating that the capability of the underlying LLM can substantially influence the results. In this blog, we present results with GPT-4.1 only but the GEAK codebase allows developers to try different LLMs as per their choice.\n\n- **LLM as Optimizer**\n\n\nThe optimizer is tasked with identifying potential optimization directions based on previous code generations and their corresponding performance metrics. These historical records, which include the generated code and associated performance results, are sorted in ascending order of performance. This structured presentation \\[2\\] helps guide the LLM toward proposing more effective optimization strategies.\n\n- **Debugging Trap**\n\n\nWhen LLM’s generated code has bugs, the error trace is provided to the Reflector for feedback generation for further correction. However, we’ve observed that sometimes code can undergo several reflection cycles while still being plagued by the same bug, which is what we refer to as the debugging trap. To prevent the agent from getting stuck in a debugging trap, we impose a limit on the number of debugging attempts per code snippet using a parameter max\\_perf\\_debug\\_num. If the code continues to fail after reaching this threshold, the agent is required to discard the current approach and generate a new strategy along with fresh code.\n\n- **Parallel Scaling**\n\n\nTo ensure reliable kernel generation, we run multiple instances of GEAK multiple times in parallel and independently. To introduce diversity in the generated code, the LLM output is sampled with the parameter temperature set to 1. Our experiments show that such diversity in generation can yield correct and faster kernels that are otherwise underexplored. Moreover, further experiments indicate that combining sequential and parallel scaling yields additional performance improvements. To obtain unbiased estimates of accuracy, we use the pass@k metric to estimate it \\[4\\].\n\n## Triton Kernel Evaluation Benchmark [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#triton-kernel-evaluation-benchmark)\n\nWe conducted experiments using two benchmarks methods:\n\n**1\\. TritonBench-Modified Benchmark (184 Kernels):**\n\nThe Triton kernels and corresponding evaluation codebase were adapted from [TritonBench](https://github.com/thunlp/TritonBench) \\[1\\] with several modifications made:\n\n- Out of 184 kernels in TritonBench-G, we identified and corrected errors in 37 kernels that were encountered when running on AMD GPUs. These errors include shared memory errors, invalid HIP arguments, and ModuleNotFound errors.\n\n- We found that several kernels in the original TritonBench repository missed necessary call functions, which resulted in the accuracy checking code comparing empty strings as output. We corrected this and ensured that the correct common seed was used during test.\n\n- For some kernels, testbenches were written inconsistently that produced unexpected outputs. We have fixed these kernels to have consistent testbenches.\n\n\n**2\\. ROCm Repository Triton Benchmark (30 Kernels):**\n\nThe benchmark contains 30 kernels chosen from various open ROCm repositories that have been written by Triton expert engineers and released publicly to help enable the ecosystem around running AI workloads efficiently on AMD GPUs. The entire list of kernels is outlined in Table 1. We also took assistance from frontier LLMs to put these kernels, as well as their corresponding unit tests, in the same format as the TritonBench-modified framework. It allows us to compare the accuracy and efficiency of our agent consistently.\n\n| **ROCm Kernel** | **Repository link** |\n| --- | --- |\n| 1\\. test\\_tma\\_store\\_gemm 2. moe-gemm 3. layernorm 4. triton\\_multreduce\\_matmul\\_kernel.py 5. multreduce\\_matmul\\_dot\\_kernel.py 6. gemm 7. test\\_block\\_pointer\\_matmul 8. test\\_block\\_copy 9. test\\_add\\_kernel 10. softmax | [ROCm/triton](https://github.com/ROCm/triton/tree/main_perf) |\n| 1\\. mnsnorm\\_fwd 2. rmsnorm\\_bwd 3. test\\_chained\\_dot\\_fp8 4. test\\_matmul\\_MXFP 5. test\\_gemm\\_fusion | [ROCm/aiter](https://github.com/ROCm/aiter) |\n| 1\\. test\\_triton\\_swizzle2d 2. test\\_triton\\_sort 3. test\\_triton\\_flip 4. test\\_reverse\\_range | [ROCm/aotriton](https://github.com/ROCm/aotriton) |\n| 1\\. test\\_load\\_reduce.py 2. test\\_chained\\_matmul | [ROCm/vllm](https://github.com/ROCm/vllm) |\n| 1\\. test\\_random\\_int 2. test\\_randn 3. test\\_kernel\\_sub 4. test\\_kernel\\_dot 5. test\\_batched","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Frocm.blogs.amd.com%2Fsoftware-tools-optimization%2Ftriton-kernel-ai%2FREADME.html","id":"https://rocm.blogs.amd.com/software-tools-optimization/triton-kernel-ai/README.html","publishedDate":"2025-08-01T00:00:00.000Z","summary":"The webpage introduces the GEAK (Generating Efficient AI-centric Kernels) framework, developed by AMD, which utilizes AI to automatically generate efficient GPU kernels in Triton, a parallel programming language. Key highlights include:\n\n- **GEAK Framework**: A library designed to produce Triton kernels from brief functional instructions, enhancing AI development for AMD GPUs.\n- **Evaluation Benchmarks**: The framework includes two benchmark sets, with execution accuracy reaching 54.89% on modified TritonBench and 63.33% on ROCm benchmarks, significantly improving upon the 15% accuracy when using frontier LLMs directly.\n- **Performance Improvements**: AI-generated kernels show an average speedup of up to 2.59 times compared to reference kernels.\n- **Core Modules**: The system consists of a generator, evaluator, reflector, and optimizer, working together to produce and refine kernel code based on user input and performance metrics.\n\nOverall, GEAK aims to streamline the process of kernel generation, making it more efficient and accessible for developers working with AMD's GPU architecture.","title":"GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://rocm.blogs.amd.com/software-tools-optimization/triton-kernel-ai/README.html"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2507.23194v1","id":"https://arxiv.org/html/2507.23194v1","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage titled \"arXiv reCAPTCHA\" discusses a specific implementation of reCAPTCHA technology on the arXiv platform. It outlines the purpose of using reCAPTCHA to prevent automated submissions and ensure that users are human. The document may also include acknowledgments for support from the Simons Foundation and member institutions. For detailed technical insights or specific methodologies related to reCAPTCHA on arXiv, further exploration of the content may be necessary.","title":"GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/html/2507.23194v1"},{"content":"## AI-Powered Library for Researchers\n\n### Track research interests, scroll daily paper feeds with AI summary, and chat with bulk of files.\n\n1. [Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks](https://arxiv.org/abs/2507.23194)\n\ncs.CLcs.LGcs.AI01 Aug 2025\n\nJianghui Wang, Vinay Joshi, Saptarshi Majumder, Xu Chao, Bin Ding, Ziqiong Liu, Pratik Prabhanjan Brahma, Dong Li, Zicheng Liu, Emad Barsoum\n\nAdvanced Micro Devices, Inc. (AMD)\n\nThe demand for AI-generated GPU kernels is rapidly growing, influenced by the need for scalable, hardware-optimized solutions in both industry and academia. As deep learning workloads grow in complexity and diversity, it is imperative to automate low-level kernel development to meet performance and productivity demands. Major cloud providers, semiconductor companies, and research institutions are now investing heavily in AI-driven code generation for GPUs, aiming to reduce manual optimization efforts while achieving near-expert performance on hardware like AMD MI300X. The Triton language, a Python-based DSL for GPU programming, has emerged as a popular target for such AI-generated kernels due to its balance of performance and ease-of-coding. In this work, we present an evaluation suite for Triton-based GPU kernels and GEAK (Generating Efficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs to generate performant Triton code specifically for AMD GPUs, including the AMD MI300X and MI250. GEAK leverages inference-time compute scaling to produce Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style feedback mechanisms. On two evaluation benchmarks, GEAK significantly outperformed the baselines of directly prompting frontier LLMs as well as Reflexion-based generation pipelines by achieving correctness up to $63$% and execution speed up of up to $2.59$X. These results highlight the promise of GEAK-like agentic code generation for accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance.\n\nIllustration of GEAK pipeline.\n\n# Why ChatPaper\n\n- Interest-Driven Paper Curation\n\nDescribe your interests as you like, be it keywords or sentence, you’ll get relevant papers every day via AI semantic matching.\n\n- Top Conferences, One-Stop\n\nIJCAI, ICML, CVPR, KDD... Access papers from top AI conferences, all in one tool, for free.\n\n- Paper Management Made Easy\n\nKeep your research organized with our bookmarking feature. Build a well-structured knowledge hub at your fingertips.\n\n- Chat With Any Paper\n\nGot questions? Ask in ChatDOC with one single click. Use AI to pull data, clarify terms, and verify facts with our precise word-level tracing feature.\n\n\n# See how it works\n\n## Watch now 1 min\n\nGet Started","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fchatpaper.com%2Fpaper%2F172041","id":"https://chatpaper.com/paper/172041","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage introduces GEAK (Generating Efficient AI-centric GPU Kernels), a framework designed to automate the generation of GPU kernels using the Triton programming language, particularly for AMD GPUs like the MI300X and MI250. With the increasing complexity of deep learning workloads, GEAK aims to enhance performance and productivity by leveraging large language models (LLMs) to produce efficient Triton code. The framework employs a reasoning loop inspired by Reflexion-style feedback mechanisms, achieving notable improvements in correctness (up to 63%) and execution speed (up to 2.59 times faster) compared to traditional methods. The evaluation suite for Triton-based GPU kernels demonstrates the potential of GEAK to democratize access to expert-level kernel performance and accelerate the adoption of diverse hardware platforms. Additionally, the webpage highlights features of ChatPaper, an AI-powered library for researchers that offers paper curation, management, and interactive capabilities.","title":"Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://chatpaper.com/paper/172041"},{"content":"Agree & Join LinkedIn\n\nBy clicking Continue to join or sign in, you agree to LinkedIn’s [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n\n [Skip to main content](https://www.linkedin.com/www.linkedin.com#main-content)\n\n# Emad Barsoum’s Post\n\n[Emad Barsoum](https://www.linkedin.com/in/ebarsoum?trk=public_post_feed-actor-name)\n\n1mo\n\n- [Report this post](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nThe future of optimization and GPU kernel writing, proud of the team!!! Generating Efficient AI-centric Kernels (GEAK) is our OpenAI Triton Kernel agent using inference-time scaling to generate a high performance numerically accurate kernel and two Triton evaluation benchmarks. Looking forward for the community to try it. [#AMD](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Famd&trk=public_post-text) [#Triton](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Ftriton&trk=public_post-text) [#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&trk=public_post-text) [#ML](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fml&trk=public_post-text) [#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllm&trk=public_post-text) [#AGI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fagi&trk=public_post-text) [#MI300X](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmi300x&trk=public_post-text)\n\n[Zicheng Liu](https://www.linkedin.com/in/zicheng-liu?trk=public_post_reshare_feed-actor-name)\n\n1mo\n\nI'm thrilled to share that we've released GEAK: our Triton Kernel AI Agent and Evaluation Benchmarks. We achieve an average execution accuracy of 54.89% on TritonBench-modified and 63.33% ROCm benchmark as compared to 15% if directly prompting frontier models. The generated kernels have an average speedup of 2.59X over the baseline.\n[#AMD](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Famd&trk=public_post_reshare-text) [#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllm&trk=public_post_reshare-text) [#ML](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fml&trk=public_post_reshare-text) [#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&trk=public_post_reshare-text) [#HW](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fhw&trk=public_post_reshare-text) [#MI300X](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmi300x&trk=public_post_reshare-text) [https://lnkd.in/gMcx6BBz](https://lnkd.in/gMcx6BBz?trk=public_post_reshare-text)\n\n[53](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_social-actions-reactions) [1 Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_social-actions-comments)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_like-cta) [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment-cta)\n\nShare\n\n- Copy\n- LinkedIn\n- Facebook\n- Twitter\n\n[Waleed Atallah](https://www.linkedin.com/in/waleedatallah?trk=public_post_comment_actor-name)\n\nmaking AI go fast\n\n1mo\n\n- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\nAmazing work! The LLM generated kernels will change how we access GPUs, no doubt\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment_reply) [3 Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment_reactions) 4 Reactions\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_feed-cta-banner-cta)\n\n3,240 followers\n\n- [675 Posts](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Febarsoum%2Frecent-activity%2F&trk=public_post_follow-posts)\n\n[View Profile](https://www.linkedin.com/in/ebarsoum?trk=public_post_follow-view-profile) [Connect](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7357062890551496704&trk=public_post_follow)\n\n## Explore content categories\n\n- [Career](https://www.linkedin.com/top-content/career/)\n- [Productivity](https://www.linkedin.com/top-content/productivity/)\n- [Finance](https://www.linkedin.com/top-content/finance/)\n- [Soft Skills & Emotional Intelligence](https://www.linkedin.com/top-content/soft-skills-emotional-intelligence/)\n- [Project Management](https://www.linkedin.com/top-content/project-management/)\n- [Education](https://www.linkedin.com/top-content/education/)\n- [Technology](https://www.linkedin.com/top-content/technology/)\n- [Leadership](https://www.linkedin.com/top-content/leadership/)\n- [Ecommerce](https://www.linkedin.com/top-content/ecommerce/)\n\nShow more\n\nShow less","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO","id":"https://www.linkedin.com/posts/ebarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO","publishedDate":"2025-08-01T00:00:00.000Z","summary":"The LinkedIn post by Emad Barsoum announces the release of the Generating Efficient AI-centric Kernels (GEAK), an OpenAI Triton Kernel agent designed for optimizing GPU kernel writing. The GEAK utilizes inference-time scaling to produce high-performance, numerically accurate kernels, achieving significant improvements in execution accuracy and speed. Specifically, it reports an average execution accuracy of 54.89% on modified Triton benchmarks and 63.33% on ROCm benchmarks, with a speedup of 2.59 times over baseline performance. The post expresses pride in the team's achievements and invites the community to try out the new tool.","title":"Introducing GEAK: Efficient AI-centric Kernels for OpenAI Triton","url":"https://www.linkedin.com/posts/ebarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO"},{"content":"Agree & Join LinkedIn\n\nBy clicking Continue to join or sign in, you agree to LinkedIn’s [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n\n [Skip to main content](https://www.linkedin.com/www.linkedin.com#main-content)\n\n# Zicheng Liu’s Post\n\n[Zicheng Liu](https://www.linkedin.com/in/zicheng-liu?trk=public_post_feed-actor-name)\n\n1mo\n\n- [Report this post](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nI'm thrilled to share that we've released GEAK: our Triton Kernel AI Agent and Evaluation Benchmarks. We achieve an average execution accuracy of 54.89% on TritonBench-modified and 63.33% ROCm benchmark as compared to 15% if directly prompting frontier models. The generated kernels have an average speedup of 2.59X over the baseline.\n[#AMD](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Famd&trk=public_post-text) [#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllm&trk=public_post-text) [#ML](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fml&trk=public_post-text) [#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&trk=public_post-text) [#HW](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fhw&trk=public_post-text) [#MI300X](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmi300x&trk=public_post-text) [https://lnkd.in/gMcx6BBz](https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Flnkd%2Ein%2FgMcx6BBz&urlhash=IKK1&trk=public_post-text)\n\n[141](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_social-actions-reactions) [3 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_social-actions-comments)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_like-cta) [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment-cta)\n\nShare\n\n- Copy\n- LinkedIn\n- Facebook\n- Twitter\n\n['YZ' Yezhou Yang](https://www.linkedin.com/in/yz-yezhou-yang-6a02462a?trk=public_post_comment_actor-name)\n\nProf. YZ at School of Computing and #AI;\nAmazon Scholar at Prime Video;\nActive and Interactive Perception Solutions for #AI and #GenAI tech Deployment and Automated Mobility.\n\n1mo\n\n- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\nCongratulations 🎉🎉🎉\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reply) [1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reactions) 2 Reactions\n\n[Stefanos Damianakis](https://www.linkedin.com/in/damianakis?trk=public_post_comment_actor-name)\n\nSoftware Executive \\| Enterprise Software \\| Board Director \\| Advisor\n\n1mo\n\n- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\nBravo [Zicheng](https://www.linkedin.com/in/zicheng-liu?trk=public_post_comment-text) and team! 👏🏻👏🏻👏🏻\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reply) 1 Reaction\n\n[Vijay Krishnan](https://www.linkedin.com/in/vijay0?trk=public_post_comment_actor-name)\n\nFounder & CTO, Turing.com\n\n1mo\n\n- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\nCongrats, [Zicheng](https://www.linkedin.com/in/zicheng-liu?trk=public_post_comment-text)!\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reply) [1 Reaction](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_comment_reactions) 2 Reactions\n\n[See more comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_see-more-comments)\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v&trk=public_post_feed-cta-banner-cta)\n\n6,279 followers\n\n- [25 Posts](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Fzicheng-liu%2Frecent-activity%2F&trk=public_post_follow-posts)\n\n[View Profile](https://www.linkedin.com/in/zicheng-liu?trk=public_post_follow-view-profile) [Connect](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7357054030113656833&trk=public_post_follow)\n\n## Explore content categories\n\n- [Career](https://www.linkedin.com/top-content/career/)\n- [Productivity](https://www.linkedin.com/top-content/productivity/)\n- [Finance](https://www.linkedin.com/top-content/finance/)\n- [Soft Skills & Emotional Intelligence](https://www.linkedin.com/top-content/soft-skills-emotional-intelligence/)\n- [Project Management](https://www.linkedin.com/top-content/project-management/)\n- [Education](https://www.linkedin.com/top-content/education/)\n- [Technology](https://www.linkedin.com/top-content/technology/)\n- [Leadership](https://www.linkedin.com/top-content/leadership/)\n- [Ecommerce](https://www.linkedin.com/top-content/ecommerce/)\n\nShow more\n\nShow less\n\nLinkedIn\n\nNever miss a beat on the app\n\nDon’t have the app? Get it in the Microsoft Store.\n\n[Open the app](ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&mode=mini&cid=guest_desktop_upsell_post2)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fzicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v","id":"https://www.linkedin.com/posts/zicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v","publishedDate":"2025-08-01T00:00:00.000Z","summary":"Zicheng Liu announced the release of GEAK, a Triton Kernel AI Agent along with evaluation benchmarks. The new system achieves an average execution accuracy of 54.89% on the TritonBench-modified benchmark and 63.33% on the ROCm benchmark, significantly outperforming the 15% accuracy of directly prompting frontier models. Additionally, the generated kernels demonstrate an average speedup of 2.59 times over the baseline. The post highlights advancements in AI and machine learning capabilities.","title":"Introducing GEAK: Triton Kernel AI Agent and Benchmarks - LinkedIn","url":"https://www.linkedin.com/posts/zicheng-liu_geak-introducing-triton-kernel-ai-agent-activity-7357054030113656833-DW3v"}],"status":"completed","toolCallId":"call_nGs3BTfaQURI24YIDQEAQrgt","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"They cap it with a configurable parameter to avoid the “debugging trap.”\n\n- Name: max_perf_debug_num\n- Meaning: maximum number of reflection/debugging attempts allowed for a given code path before the agent must abandon that approach.\n- Behavior: if a kernel keeps failing after hitting this limit, GEAK discards the current strategy and forces the LLM to generate a fresh plan and new code, rather than looping on the same bug.\n\nThe exact numeric value isn’t fixed by the paper; it’s a tunable setting in the framework/configs.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0e6830ac4fc12a900068daee08471481a387c12d1c86430a38"}},"resumableStreamId":"STREAM:b7d5a1b3-367e-429d-8964-0032792caf0f","role":"assistant","spanId":"04638ce20048a802","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","timeToFirstToken":1.159,"tokens":118,"tokensPerSecond":20.081688223281144,"traceId":"5dbc0bed8c981bf1811a2a5431ba5b7e","updated_at":1759178237823,"userId":"google:104458561188320708185","id":"j97179mb2abk3yh9xrd6q957v17rh3ka"},{"_creationTime":1759178261732.9844,"_id":"j97ccvm2npsv7wxkksyje3t6s17rh19d","attachmentIds":[],"content":"what would happen if the maximum was hit?","created_at":1759178261733,"messageId":"99ff229e-0cd0-4a2d-8b76-c0bb2aa1153b","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","spanId":"0c667a31369904d8","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","traceId":"03c15b436511d1cf300aec1020c49c30","updated_at":1759178261733,"userId":"google:104458561188320708185","id":"j97ccvm2npsv7wxkksyje3t6s17rh19d","providerMetadata":{}},{"_creationTime":1759178261732.9846,"_id":"j977e5ne5eeyp86qftekp2ws6x7rhk96","attachmentIds":[],"byok":false,"content":"When the max debugging-attempts threshold is reached (max_perf_debug_num), GEAK deliberately “cuts the cord” on the current line of attack and pivots. Concretely:\n\n- Stop the current fix loop: The reflector-led debug cycle for that code path is terminated. No more retries applying incremental patches to the same buggy kernel.\n- Discard the stalled strategy: The agent treats the current approach (its plan, tiling choices, masking logic, etc.) as exhausted.\n- Force a fresh restart with a new plan: The system prompts the LLM to produce a new high-level strategy and regenerate code from scratch (often with altered tiling, memory layout, masking, or algorithmic choices), then resumes evaluate → reflect/optimize on that new branch.\n- Budget-aware progression: If the overall per-task attempt budget is still available, the new branch is evaluated like any other candidate. If the global budget is exhausted, the task is marked unsolved for that run.\n\nWhy this matters: it prevents the “debugging trap,” where the agent keeps cycling on the same bug without making structural changes. By resetting the plan, GEAK increases diversity and the chance of escaping a local failure mode.","created_at":1759178261734,"messageId":"14379840-6eef-4ff8-85f4-93441912d7b6","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"news","domains":null,"queries":["GEAK Triton agent max_perf_debug_num what happens when limit hit site:github.com","Geak Introducing Triton Kernel AI Agent evaluation benchmarks debugging trap max attempts behavior arXiv:2507.23194","GEAK-agent config max_perf_debug_num description"]},"result":[{"content":"[Skip to main content](https://pypi.org/pypi.org#content) Switch to mobile version\n\nSearch PyPISearch\n\n# triton-runner 0.2.2\n\npip install triton-runner==0.2.2Copy PIP instructions\n\n[Latest version](https://pypi.org/project/triton-runner/)\n\nReleased: Sep 20, 2025\n\nTriton multi-level runner, include IR/PTX/cubin.\n\n### Navigation\n\n### Verified details\n\n_These details have been [verified by PyPI](https://docs.pypi.org/project_metadata/#verified-details)_\n\n###### Maintainers\n\n[BobHuang](https://pypi.org/user/BobHuang/)\n\n### Unverified details\n\n_These details have **not** been verified by PyPI_\n\n###### Project links\n\n- [homepage](https://triton-runner.org)\n- [repository](https://github.com/OpenMLIR/triton_runner)\n\n###### Meta\n\n- **License Expression:** MIT\n_[SPDX](https://spdx.org/licenses/) [License Expression](https://spdx.github.io/spdx-spec/v2-draft/SPDX-license-expressions/)_\n- **Author:** [Bob Huang](mailto:git@bobhuang.xyz)\n\n[Report project as malware](https://pypi.org/project/triton-runner/submit-malware-report/)\n\n## Project description\n\n### Multi-Level Triton Runner Tool 🔧\n\n[**Documentation**](https://pypi.org/doc/) ｜ [**🔗 triton-runner.org**](https://triton-runner.org)\n\n**English** \\| [**中文**](https://pypi.org/README.zh.md)\n\ntriton-runner is a lightweight, multi-level execution engine for [OpenAI/Triton](https://github.com/triton-lang/triton), designed to support IR/PTX/cubin launches in complex pass pipelines.\n\ntriton-runner is compatible with **Triton v3.4.0 (primary), v3.3.x, v3.2.0, v3.1.0 or v3.0.0**, and may not work with other versions.\n\n## ✨ Features\n\n- [I. multi-level execution](https://pypi.org/pypi.org#i-multi-level-execution)\n- [II. 💡use cubin runner to solve Triton issue](https://pypi.org/pypi.org#ii-use-cubin-runner-to-solve-triton-issue)\n- [III. TTIR Debug](https://pypi.org/pypi.org#iii-ttir-debug)\n- [IV. Benchmark performance](https://pypi.org/pypi.org#iv-benchmarks)\n\n## 📦 Installation\n\n### Quick Installation\n\nYou can install the latest stable release of Triton from pip.\n\n```\npipinstalltriton-runner\n\n```\n\n### Install from source\n\nYou can install from source to access the latest features and developments.\n\n```\ngitclonehttps://github.com/OpenMLIR/triton-runner\ncdtriton-runner\n\npipinstall-e.\n\n```\n\n## 🚀 Quick Start\n\nSee the provided examples in the [triton-runner.org](https://triton-runner.org) repository for your first run.\n\n### I. multi-level execution\n\nAlmost all of Triton’s compilation levels are supported by triton-runner. [Gluon](https://github.com/triton-lang/triton/tree/main/python/tutorials/gluon) will be supported soon.\n\n```\n---\ntitle: Triton Compilation Pipeline\n---\nflowchart LR\n\n subgraph Triton\n A[\"Python Triton\"]:::supported --> B[\"TTIR Triton IR\"]:::supported\n B --> C[\"TTGIR Triton GPU IR\"]:::supported\n C --> D[\"LLIR LLVM IR\"]:::supported\n\n Gluon[\"Python Gluon\"]:::unsupported --> C\n end\n\n subgraph Backend\n D --> E[\"PTX\"]:::supported\n E --> F[\"cubin CUDA Binary\"]:::supported\n end\n\n classDef supported fill:#AED6F1,stroke:#2E86C1,stroke-width:2px,color:#000000;\n classDef unsupported fill:#F5B7B1,stroke:#C0392B,stroke-width:2px,color:#000000;\n\n```\n\n#### 1\\. Python runner\n\nYou can run your Triton code using `@triton_runner.jit` instead of `@triton.jit`. See an example in [examples/python\\_runner/matmul.py](https://github.com/OpenMLIR/triton-runner/blob/main/examples/python_runner/matmul.py#L12)\n\nYou can run the example with `python examples/python_runner/matmul.py`. After running successfully, you should see output like `[triton-runner] Triton cache saved`.\n\n#### 2\\. TTIR runner\n\nIn addition to using `@triton_runner.jit` instead of `@triton.jit`, you also need to provide the TTIR file. You can place it in the same directory as the current Python file and use `ttir_dir=triton_runner.get_file_dir(__file__)`. See an example in [examples/ttir\\_runner/matmul.py](https://github.com/OpenMLIR/triton-runner/blob/main/examples/ttir_runner/matmul.py#L67). Alternatively, you can use the Triton cache directory generated by the Python runner(previous step).\n\nYou can run the example with `python examples/ttir_runner/matmul.py`.\n\n#### 3\\. TTGIR/LLIR/PTX/cubin runner\n\nIn addition to using `@triton_runner.jit` instead of `@triton.jit`, you also need to provide the corresponding file. Like the TTGIR runner, You can place it in the same directory as the current Python file and use `ttgir_dir=triton_runner.get_file_dir(__file__)`. Since all of them are architecture-specific, be sure to use the corresponding metadata JSON file. See an example in [examples/ttgir\\_runner/sm90/matmul-with-tma-v4.py](https://github.com/OpenMLIR/triton-runner/blob/main/examples/ttgir_runner/sm90/matmul-with-tma-v4.py#L76).\n\nIf your architecture is `sm90`(Hopper), you can run the example using the TTGIR runner with `python examples/ttgir_runner/sm90/matmul-with-tma-v4.py`.\n\n#### 4\\. Hopper examples with Triton v3.4.0\n\nI provide examples for different architectures and Triton versions. Here's example commands for multi-level targeting `sm90 (H100, H200, H20, etc.)` with Triton v3.4.0.\n\n```\npythonexamples/python_runner/matmul.py\n\npythonexamples/ttir_runner/matmul.py\n\npythonexamples/ttgir_runner/sm90/matmul-with-tma-v4.py\n\npythonexamples/llir_runner/sm90/matmul-with-tma-v4.py\n\npythonexamples/ptx_runner/sm90/matmul-with-tma-v4.py\n\npythonexamples/cubin_runner/sm90/matmul-with-tma-v4.py\n\n```\n\n#### 5\\. More architectures examples\n\nNow provide examples for architectures include `sm90 (H100, H200, H20, etc.)`, `sm80 (A100, A30)`, `sm120 (RTX PRO 6000, RTX 5090, etc.)`, `sm86 (A10, RTX 3090, etc.)` or `sm75 (T4, RTX 2080, etc.)`. For these targets, please refer to [examples](https://pypi.org/doc/examples_v3.4.0.md).\n\nIf your GPU does not have one of the above compute capabilities, you can use `TRITON_CACHE_DIR=$PWD/.cache` to output the Triton cache to the current directory, and use this kernel cache directory to run your program.\n\n#### 6\\. More Triton version examples\n\nIf your Triton version is v3.3.1 or v3.3.0, please refer to [examples\\_v3.3.x](https://pypi.org/doc/examples_v3.3.x.md) for example commands. If your Triton version is v3.2.0, please refer to [examples\\_v3.2.0](https://pypi.org/doc/examples_v3.2.0.md) for example commands. If your Triton version is v3.1.0, please refer to [examples\\_v3.1.0](https://pypi.org/doc/examples_v3.1.0.md) for example commands. If your Triton version is v3.0.0, please refer to [examples\\_v3.0.0](https://pypi.org/doc/examples_v3.0.0.md) for example commands.\n\n### II. use cubin runner to solve Triton issue\n\nTo solve Triton’s performance and shared memory issues as shown in the [triton\\_issue](https://pypi.org/triton_issue) folder, we use the cubin runner.\n\n### III. TTIR Debug\n\nDebugging is supported for TTIR ops like `tt.load`, `arith.addf`, and `tt.trans` in Triton v3.4.0. Here are some example commands for debugging.\n\n```\npythondebug_tool/ttir/01-vector_add/debug_load.py\npythondebug_tool/ttir/01-vector_add/debug_addf.py\n\npythondebug_tool/ttir/02-matrix_transpose/debug_2d_load.py\npythondebug_tool/ttir/02-matrix_transpose/debug_2d_trans.py\n\npythondebug_tool/ttir/03-matrix_multiplication/debug_acc.py\n\npythondebug_tool/ttir/04-softmax/debug_maxnumf.py\npythondebug_tool/ttir/04-softmax/debug_addf-sum.py\npythondebug_tool/ttir/04-softmax/debug_subf.py\npythondebug_tool/ttir/04-softmax/debug_exp-exp_shifted.py\npythondebug_tool/ttir/04-softmax/debug_divf-normalize_by_sum.py\n\npythondebug_tool/ttir/05-softmax_lse/debug_log_acc.py\npythondebug_tool/ttir/05-softmax_lse/debug_max_acc.py\npythondebug_tool/ttir/05-softmax_lse/debug_more.py\n\npythondebug_tool/ttir/06-attention/debug_out.py\n\npythondebug_tool/ttir/07-debug_not_f32/debug_bf16.py\n\n```\n\n### IV. Benchmarks\n\nBenchmarks Referencing [TritonBench](https://github.com/pytorch-labs/tritonbench)\n\n- `launch_latency`: Measures kernel launch overhead.\n- `matmul`: Provides a benchmark for matrix multiplication performance.\n\n```\npythonbenchmark/launch_latency/bench.py\n\npythonbenchmark/static_shape/matmul.py\n\n```\n\n## 📄 License\n\nThis project is licensed under the **MIT License**.\nSee the [LICENSE](https://pypi.org/LICENSE) file for more details.\n\n## Project details\n\n### Verified details\n\n_These details have been [verified by PyPI](https://docs.pypi.org/project_metadata/#verified-details)_\n\n###### Maintainers\n\n[BobHuang](https://pypi.org/user/BobHuang/)\n\n### Unverified details\n\n_These details have **not** been verified by PyPI_\n\n###### Project links\n\n- [homepage](https://triton-runner.org)\n- [repository](https://github.com/OpenMLIR/triton_runner)\n\n###### Meta\n\n- **License Expression:** MIT\n_[SPDX](https://spdx.org/licenses/) [License Expression](https://spdx.github.io/spdx-spec/v2-draft/SPDX-license-expressions/)_\n- **Author:** [Bob Huang](mailto:git@bobhuang.xyz)\n\n## Release history[Release notifications](https://pypi.org/help/\\#project-release-notifications) \\| [RSS feed](https://pypi.org/rss/project/triton-runner/releases.xml)\n\nThis version\n\n[0.2.2\\\n\\\nSep 20, 2025](https://pypi.org/project/triton-runner/0.2.2/)\n\n[0.2.0\\\n\\\nSep 15, 2025](https://pypi.org/project/triton-runner/0.2.0/)\n\n[0.1.9\\\n\\\nAug 16, 2025](https://pypi.org/project/triton-runner/0.1.9/)\n\n## Download files\n\nDownload the file for your platform. If you're not sure which to choose, learn more about [installing packages](https://packaging.python.org/tutorials/installing-packages/).\n\n### Source Distributions\n\nNo source distribution files available for this release.See tutorial on [generating distribution archives](https://packaging.python.org/tutorials/packaging-projects/#generating-distribution-archives).\n\n### Built Distribution\n\nFilter files by name, interpreter, ABI, and platform.\n\nIf you're not sure about the file name format, learn more about [wheel file names](https://packaging.python.org/en/latest/specifications/binary-distribution-format/).\n\nCopy a direct link to the current filters\n\nCopy\n\nFile name\n\nInterpreterInterpreterpy3\n\nABIABInone\n\nPlatformPlatformany\n\n[triton\\_runner-0.2.2-py3-none-any.whl","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpypi.org%2Fproject%2Ftriton-runner%2F0.2.2%2F","id":"https://pypi.org/project/triton-runner/0.2.2/","publishedDate":"2025-09-20T00:00:00.000Z","summary":"**Triton Runner (Version 0.2.2)** is a lightweight, multi-level execution engine designed for OpenAI's Triton, facilitating the launch of Intermediate Representation (IR), PTX, and CUDA binary (cubin) in complex pass pipelines. Released on September 20, 2025, it is compatible with Triton versions 3.0.0 to 3.4.0. \n\n### Key Features:\n- **Multi-Level Execution**: Supports various compilation levels of Triton.\n- **Cubin Runner**: Addresses specific Triton issues.\n- **TTIR Debugging**: Offers tools for debugging TTIR.\n- **Performance Benchmarking**: Includes features for benchmarking performance.\n\n### Installation:\n- Install via pip: `pip install triton-runner==0.2.2`\n- For the latest features, install from source using Git.\n\n### Quick Start:\nUsers can run Triton code with `@triton_runner.jit` and access examples from the project's repository.\n\nFor more information, visit the [official documentation](https://triton-runner.org) or the [GitHub repository](https://github.com/OpenMLIR/triton_runner).","title":"triton-runner","url":"https://pypi.org/project/triton-runner/0.2.2/"},{"content":"A required part of this site couldn’t load. This may be due to a browser\nextension, network issues, or browser settings. Please check your\nconnection, disable any ad blockers, or try using a different browser.","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpypi.org%2Fproject%2Ftritonparse%2F0.2.4.dev20250927071447%2F","id":"https://pypi.org/project/tritonparse/0.2.4.dev20250927071447/","publishedDate":"2025-09-27T00:00:00.000Z","summary":"The webpage for \"Client Challenge\" on PyPI appears to be experiencing loading issues, preventing access to its content. Users are advised to check their internet connection, disable ad blockers, or try a different browser to resolve the problem.","title":"tritonparse","url":"https://pypi.org/project/tritonparse/0.2.4.dev20250927071447/"},{"content":"A required part of this site couldn’t load. This may be due to a browser\nextension, network issues, or browser settings. Please check your\nconnection, disable any ad blockers, or try using a different browser.","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpypi.org%2Fproject%2Ftritonparse%2F0.2.1.dev20250915071616%2F","id":"https://pypi.org/project/tritonparse/0.2.1.dev20250915071616/","publishedDate":"2025-09-15T00:00:00.000Z","summary":"The webpage for \"Client Challenge\" on PyPI is currently experiencing loading issues, possibly due to browser extensions, network problems, or settings. Users are advised to check their internet connection, disable ad blockers, or try a different browser to access the content.","title":"tritonparse","url":"https://pypi.org/project/tritonparse/0.2.1.dev20250915071616/"},{"content":"[Skip Navigation](https://nvidia.custhelp.com/euf/generated/optimized/1756083284/themes/standard/#rn_MainContent)\n\n## NVIDIA SUPPORT\n\n[NVIDIA Home](http://www.nvidia.com/page/home.html) > [Support Home Page](http://www.nvidia.com/page/support.html) > [Knowledgebase Home Page](https://nvidia.custhelp.com/app/home/) >  Security Bulletin: NVIDIA Triton Inference Server - February 2025\n\n[Log In](https://login.nvidia.com/authorize?response_type=code&client_id=qtGdYjahOV9NjYJ6iTPXfYDu_Z20k-ualaLp5b2vNVg&redirect_uri=https%3A%2F%2Fnvidia.custhelp.com%2Fapp%2Futils%2Fstarfleet_login&scope=openid+profile+email&state=%2Fapp%2Fanswers%2Fdetail%2Fa_id%2F5612%2F%7E%2Fsecurity-bulletin%253A-nvidia-triton-inference-server---february-2025) \\| [Sign Up](https://login.nvidia.com/authorize?response_type=code&client_id=qtGdYjahOV9NjYJ6iTPXfYDu_Z20k-ualaLp5b2vNVg&redirect_uri=https%3A%2F%2Fnvidia.custhelp.com%2Fapp%2Futils%2Fstarfleet_login&scope=openid+profile+email&state=%2Fapp%2Fanswers%2Fdetail%2Fa_id%2F5612%2F%7E%2Fsecurity-bulletin%253A-nvidia-triton-inference-server---february-2025)\n\n# Security Bulletin: NVIDIA Triton Inference Server - February 2025\n\nUpdated 02/10/2025 06:44 PM\n\nNVIDIA has released a software update for NVIDIA® Triton Inference Server.\n\nTo protect your system, download and install the latest release from the [Triton Inference Server Releases](https://github.com/triton-inference-server/server/releases) page on GitHub and view the [Secure Deployment Considerations Guide](https://github.com/triton-inference-server/server/blob/main/docs/customization_guide/deploy.md).\n\nGo to [NVIDIA Product Security](https://www.nvidia.com/security/).\n\n### Details\n\nThis section provides a summary of potential vulnerabilities that this security update addresses and their impact. Descriptions use [CWE™](https://cwe.mitre.org/), and base scores and vectors use [CVSS v3.1](https://www.first.org/cvss/specification-document) standards.\n\n| **CVE ID** | **Description** | **Vector** | **Base Score** | **Severity** | **CWE** | **Impacts** |\n| --- | --- | --- | --- | --- | --- | --- |\n| CVE‑2024‑53880 | NVIDIA Triton Inference Server contains a vulnerability in the model loading API, where a user could cause an integer overflow or wraparound error by loading a model with an extra-large file size that overflows an internal variable. A successful exploit of this vulnerability might lead to denial of service. | [AV:N/AC:L/PR:H/UI:N/S:U/C:N/I:N/A:H](https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:N/AC:L/PR:H/UI:N/S:U/C:N/I:N/A:H&version=3.1) | 4.9 | Medium | [CWE‑190](https://cwe.mitre.org/data/definitions/190.html) | Denial of service |\n| --- | --- | --- | --- | --- | --- | --- |\n\nThe NVIDIA risk assessment is based on an average of risk across a diverse set of installed systems and may not represent the true risk to your local installation. NVIDIA recommends evaluating the risk to your specific configuration.\n\n### Security Updates\n\nThe following table lists the software products and versions affected, and the updated version available from nvidia.com that includes this security update.\n\n| **CVE IDs Addressed** | **Affected Products** | **Platform or OS** | **Affected Versions** | **Updated Version** |\n| --- | --- | --- | --- | --- |\n| CVE‑2024-53880 | Triton Inference Server | Windows, Linux | 24.11 | 24.12 |\n| --- | --- | --- | --- | --- |\n\n### Notes\n\nEarlier software branch releases that support this product are also affected. If you are using an earlier branch release, upgrade to the latest branch release.\n\n### Acknowledgements\n\nNVIDIA thanks 7resp4ss and Guang Gong - 360 Vulnerability Research Institute for reporting issue CVE‑2024-53880.\n\n### Get the Most Up-to-Date Product Security Information\n\nVisit the [NVIDIA Product Security](https://www.nvidia.com/security) page to\n\n- Subscribe to security bulletin notifications\n- See the current list of NVIDIA security bulletins\n- Report a potential security issue in any NVIDIA supported product\n- Learn more about the vulnerability management process followed by the NVIDIA Product Security Incident Response Team (PSIRT)\n\n### Revision History\n\n| **Revision** | **Date** | **Description** |\n| --- | --- | --- |\n| 1.0 | 11 February 2025 | Initial release |\n\n### Support\n\nIf you have any questions about this security bulletin, contact [NVIDIA Support](https://www.nvidia.com/object/support.html).\n\n##### Disclaimer\n\nALL NVIDIA INFORMATION, DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES, INCLUDING ANY IMPLIED WARRANTY OR CONDITION OF TITLE, MERCHANTABILITY, SATISFACTORY QUALITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT, ARE HEREBY EXCLUDED TO THE MAXIMUM EXTENT PERMITTED BY LAW.\n\nInformation is believed to be accurate and reliable at the time it is furnished. However, NVIDIA Corporation assumes no responsibility for the consequences of use of such information or for any infringement of patents or other rights of third parties that may result from its use. No license is granted by implication or otherwise under any patent or patent rights of NVIDIA Corporation. Specifications mentioned in this publication are subject to change without notice. This publication supersedes and replaces all information previously supplied. NVIDIA Corporation products are not authorized for use as critical components in life support devices or systems without express written approval of NVIDIA Corporation.\n\n## Is this answer helpful?\n\nYesNo\n\n## Answers others found helpful\n\n- [Security Notice: NVIDIA Response to OpenSSL Vulnerabilities - November 2022](https://nvidia.custhelp.com/app/answers/detail/a_id/5405/related/1/session/L2F2LzEvdGltZS8xNzU5MDQ3MjY1L2dlbi8xNzU5MDQ3MjY1L3NpZC9mVThIRUNTUFRWaWN1cUtUS05BVW00aGNwQlVvcjRUa2h2OWI3WHEzNEl4NEdmcTFaQUNiSXlEdm1CaDdqMk1MTjVyaDFCbXd2cElFV3B6Q3VuUllXTFNfM3FGaEhVQWRZQ1pybElLWVBaNEQ5RCU3RW1OdWFDMTRXZyUyMSUyMQ==)\n- [Security Bulletin: NVIDIA DGX H100 - August 2023](https://nvidia.custhelp.com/app/answers/detail/a_id/5473/related/1/session/L2F2LzEvdGltZS8xNzU5MDQ3MjY1L2dlbi8xNzU5MDQ3MjY1L3NpZC9mVThIRUNTUFRWaWN1cUtUS05BVW00aGNwQlVvcjRUa2h2OWI3WHEzNEl4NEdmcTFaQUNiSXlEdm1CaDdqMk1MTjVyaDFCbXd2cElFV3B6Q3VuUllXTFNfM3FGaEhVQWRZQ1pybElLWVBaNEQ5RCU3RW1OdWFDMTRXZyUyMSUyMQ==)\n- [Security Notice: NVIDIA Response to Security Incident - March 2022](https://nvidia.custhelp.com/app/answers/detail/a_id/5333/related/1/session/L2F2LzEvdGltZS8xNzU5MDQ3MjY1L2dlbi8xNzU5MDQ3MjY1L3NpZC9mVThIRUNTUFRWaWN1cUtUS05BVW00aGNwQlVvcjRUa2h2OWI3WHEzNEl4NEdmcTFaQUNiSXlEdm1CaDdqMk1MTjVyaDFCbXd2cElFV3B6Q3VuUllXTFNfM3FGaEhVQWRZQ1pybElLWVBaNEQ5RCU3RW1OdWFDMTRXZyUyMSUyMQ==)\n- [Security Bulletin: NVIDIA GeForce NOW for Android - August 2023](https://nvidia.custhelp.com/app/answers/detail/a_id/5476/related/1/session/L2F2LzEvdGltZS8xNzU5MDQ3MjY1L2dlbi8xNzU5MDQ3MjY1L3NpZC9mVThIRUNTUFRWaWN1cUtUS05BVW00aGNwQlVvcjRUa2h2OWI3WHEzNEl4NEdmcTFaQUNiSXlEdm1CaDdqMk1MTjVyaDFCbXd2cElFV3B6Q3VuUllXTFNfM3FGaEhVQWRZQ1pybElLWVBaNEQ5RCU3RW1OdWFDMTRXZyUyMSUyMQ==)\n- [Security Bulletin: NVIDIA License System - February 2022](https://nvidia.custhelp.com/app/answers/detail/a_id/5319/related/1/session/L2F2LzEvdGltZS8xNzU5MDQ3MjY1L2dlbi8xNzU5MDQ3MjY1L3NpZC9mVThIRUNTUFRWaWN1cUtUS05BVW00aGNwQlVvcjRUa2h2OWI3WHEzNEl4NEdmcTFaQUNiSXlEdm1CaDdqMk1MTjVyaDFCbXd2cElFV3B6Q3VuUllXTFNfM3FGaEhVQWRZQ1pybElLWVBaNEQ5RCU3RW1OdWFDMTRXZyUyMSUyMQ==)\n\n### Live Chat\n\n#### Chat online with one of our support agents\n\n[CHAT NOW](https://nvidia.custhelp.com/app/chat/chat_launch/)\n\n### ASK US A QUESTION\n\n#### Contact Support for assistance\n\n#### 800.797.6530\n\n[Ask a Question](https://nvidia.custhelp.com/app/ask)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fnvidia.custhelp.com%2Fapp%2Fanswers%2Fdetail%2Fa_id%2F5612%2F~%2Fsecurity-bulletin%253A-nvidia-triton-inference-server---february-2025","id":"https://nvidia.custhelp.com/app/answers/detail/a_id/5612/~/security-bulletin%3A-nvidia-triton-inference-server---february-2025","publishedDate":"2025-10-02T00:00:00.000Z","summary":"The NVIDIA Support webpage provides important information regarding a security bulletin for the NVIDIA Triton Inference Server, updated on February 10, 2025. It announces a software update to address a vulnerability (CVE-2024-53880) related to the model loading API, which could lead to a denial of service if exploited. Users are advised to download the latest version (24.12) from the Triton Inference Server Releases page on GitHub and to consult the Secure Deployment Considerations Guide. The page also includes a risk assessment and encourages users to evaluate the specific risks to their installations. For ongoing updates, users can visit the NVIDIA Product Security page to subscribe to notifications and report potential security issues.","title":"NVIDIA Support","url":"https://nvidia.custhelp.com/app/answers/detail/a_id/5612/~/security-bulletin%3A-nvidia-triton-inference-server---february-2025"},{"content":"[Back to changelog](https://github.blog/changelog/)\n\nCopilot coding agent, our asynchronous, autonomous developer agent, is now **generally available** for all paid Copilot subscribers.\n\nDelegate a task to Copilot, and Copilot will open a draft pull request and work in the background in its own development environment. It does this through the power of GitHub Actions. Once Copilot is done, it will request a review from you, and you can ask Copilot to make changes by leaving comments on the pull request.\n\nCopilot coding agent can complete a wide range of software development tasks including:\n\n- Implementing new features\n- Fixing bugs\n- Addressing technical debt\n- Improving test coverage\n- Updating documentation\n\nThere are many ways to hand tasks to Copilot, including assigning it an issue, using the agents panel accessible on every page on GitHub, or using the **Delegate to coding agent** button in Visual Studio Code.\n\nTo learn more about Copilot coding agent, head to [our documentation](https://docs.github.com/enterprise-cloud@latest/copilot/concepts/agents/coding-agent/about-coding-agent).\n\nIf you’re a Copilot Business or Copilot Enterprise subscriber, an administrator will have to [enable Copilot coding agent from the “Policies” page](https://docs.github.com/enterprise-cloud@latest/copilot/concepts/agents/coding-agent/coding-agent-for-business-and-enterprise) before you can use it.\n\n## Related Posts\n\n### Sep.29Release\n\n[Anthropic Claude Sonnet 4.5 is in public preview for GitHub Copilot](https://github.blog/changelog/2025-09-29-anthropic-claude-sonnet-4-5-is-in-public-preview-for-github-copilot)\n\n[copilot](https://github.blog/changelog/2025/?label=copilot)\n\n### Sep.26Release\n\n[Kick off and track Copilot coding agent sessions from the GitHub CLI](https://github.blog/changelog/2025-09-25-kick-off-and-track-copilot-coding-agent-sessions-from-the-github-cli)\n\n[client apps](https://github.blog/changelog/2025/?label=client-apps) [copilot](https://github.blog/changelog/2025/?label=copilot)...\n+1\n\n### Sep.25Improvement\n\n[Copilot can create issues with code snippets in public preview](https://github.blog/changelog/2025-09-25-copilot-can-create-issues-with-code-snippets-in-public-preview)\n\n[copilot](https://github.blog/changelog/2025/?label=copilot)\n\n### Sep.25Retired\n\n[Upcoming deprecation of gh-copilot CLI extension](https://github.blog/changelog/2025-09-25-upcoming-deprecation-of-gh-copilot-cli-extension)\n\n[client apps](https://github.blog/changelog/2025/?label=client-apps) [copilot](https://github.blog/changelog/2025/?label=copilot)...\n+1\n\n### Sep.25Release\n\n[GitHub Copilot CLI is now in public preview](https://github.blog/changelog/2025-09-25-github-copilot-cli-is-now-in-public-preview)\n\n[client apps](https://github.blog/changelog/2025/?label=client-apps) [copilot](https://github.blog/changelog/2025/?label=copilot)...\n+1\n\n### Sep.24Release\n\n[Copilot Spaces is now generally available](https://github.blog/changelog/2025-09-24-copilot-spaces-is-now-generally-available)\n\n[copilot](https://github.blog/changelog/2025/?label=copilot)\n\n### Sep.24Release\n\n[Start and track Copilot coding agent tasks in GitHub Mobile](https://github.blog/changelog/2025-09-24-start-and-track-copilot-coding-agent-tasks-in-github-mobile)\n\n[client apps](https://github.blog/changelog/2025/?label=client-apps) [copilot](https://github.blog/changelog/2025/?label=copilot)...\n+1\n\n### Sep.24Improvement\n\n[Spark updates: automatic error fixes, faster publishing, and reliability improvements](https://github.blog/changelog/2025-09-24-progressive-error-handling-speedier-publishing-and-reliability-fixes)\n\n[copilot](https://github.blog/changelog/2025/?label=copilot)\n\n### Sep.24Retired\n\n[Sunset notice: GitHub App-based Copilot Extensions](https://github.blog/changelog/2025-09-24-deprecate-github-copilot-extensions-github-apps)\n\n[copilot](https://github.blog/changelog/2025/?label=copilot)\n\n## Subscribe to our developer newsletter\n\nDiscover tips, technical guides, and best practices in our biweekly newsletter just for devs.\n\nEnter your email\\*\nSubscribe\n\nBy submitting, I agree to let GitHub and its affiliates use my information for personalized communications, targeted advertising, and campaign effectiveness. See the [GitHub Privacy Statement](https://github.com/site/privacy) for more details.\n\n[Back to top](https://github.blog/github.blog#start-of-content)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fgithub.blog%2Fchangelog%2F2025-09-25-copilot-coding-agent-is-now-generally-available%2F","id":"https://github.blog/changelog/2025-09-25-copilot-coding-agent-is-now-generally-available/","publishedDate":"2025-09-25T00:00:00.000Z","summary":"The GitHub Copilot coding agent is now generally available for all paid subscribers. This autonomous developer agent can handle various software development tasks such as implementing features, fixing bugs, addressing technical debt, improving test coverage, and updating documentation. Users can delegate tasks to Copilot through GitHub's interface or Visual Studio Code. Once a task is completed, Copilot will create a draft pull request and request a review. For Copilot Business or Enterprise subscribers, administrators must enable the coding agent feature. For more details, users can refer to the official documentation linked in the announcement.","title":"Copilot coding agent is now generally available","url":"https://github.blog/changelog/2025-09-25-copilot-coding-agent-is-now-generally-available/"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.23194","id":"https://arxiv.org/abs/2507.23194","publishedDate":null,"summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses a system designed to enhance the arXiv repository's submission process by integrating a reCAPTCHA mechanism. This system aims to prevent automated submissions while ensuring a smooth user experience for legitimate contributors. The paper outlines the methodology, implementation, and potential impacts of this approach on the arXiv platform. Additionally, it acknowledges support from the Simons Foundation and member institutions. For further details, you can access the full paper at the provided URL.","title":"Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/abs/2507.23194"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2507.23194v1","id":"https://arxiv.org/html/2507.23194v1","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage titled \"arXiv reCAPTCHA\" discusses a specific implementation of reCAPTCHA technology on the arXiv platform, which is a repository for research papers. It highlights the importance of this technology in preventing automated submissions and ensuring the integrity of the submission process. The page acknowledges support from the Simons Foundation and member institutions, indicating collaborative efforts in enhancing the platform's security measures. Overall, the content focuses on the role of reCAPTCHA in maintaining the quality and reliability of submissions to arXiv.","title":"GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/html/2507.23194v1"},{"content":"[Skip to main content](https://rocm.blogs.amd.com/rocm.blogs.amd.com#main-content)\nBack to top\n\n`Ctrl` + `K`\n\n[ROCm blogs](https://rocm.blogs.amd.com/)\n\n# GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks\n\n## Contents\n\n# GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#geak-introducing-triton-kernel-ai-agent-evaluation-benchmarks)\n\nAugust 01, 2025 by [Jianghui Wang](https://rocm.blogs.amd.com/authors/jianghui-wang.html), [Vinay Joshi](https://rocm.blogs.amd.com/authors/vinay-joshi.html), [Saptarshi Majumder](https://rocm.blogs.amd.com/authors/saptarshi-majumder.html), [Chao Xu](https://rocm.blogs.amd.com/authors/chao-xu.html), [Bin Ding](https://rocm.blogs.amd.com/authors/bin-ding.html), [Ziqiong Liu](https://rocm.blogs.amd.com/authors/ziqiong-liu.html), [Pratik Prabhanjan Brahma](https://rocm.blogs.amd.com/authors/pratik-prabhanjan-brahma.html), [Dong Li](https://rocm.blogs.amd.com/authors/dong-li.html), [Zicheng Liu](https://rocm.blogs.amd.com/authors/zicheng-liu.html), [Emad Barsoum](https://rocm.blogs.amd.com/authors/emad-barsoum.html).\n\n10 min read. \\| 2414 total words.\n\n[Software tools & optimizations](https://rocm.blogs.amd.com/blog/category/software-tools-optimizations.html)\n\n[AI/ML](https://rocm.blogs.amd.com/blog/tag/aiml.html)\n\n[Developers](https://rocm.blogs.amd.com/developers.html), [AI](https://rocm.blogs.amd.com/ai.html)\n\nAt AMD, we are pioneering ways to accelerate AI development using AI itself, by generating accurate and efficient GPU kernels. Specifically, we are starting with the automatic generation of kernels in [Triton](https://triton-lang.org/main/index.html), an open-source Python-like language for writing parallel programming code for GPUs. Today, AMD is excited to announce (a) [Generating Efficient AI-centric Kernels (GEAK)](https://github.com/AMD-AIG-AIMA/GEAK-Agent) for AMD GPUs, and results on (b) two Triton kernel evaluation benchmarks, where we show how AI agents can perform inference-time scaling with frontier LLMs to generate accurate and efficient kernels for AMD Instinct™ GPUs like MI250X and MI300X.\n\n**Key Takeaways:**\n\n- Announcing [GEAK Triton kernel generation framework](https://github.com/AMD-AIG-AIMA/GEAK-Agent), a library of various types of agentic AI and scaling methods to automatically produce Triton kernels from just brief instructions of its intended functionality.\n\n- Announcing corresponding [GEAK Triton Kernel Evaluation Benchmarks](https://github.com/AMD-AIG-AIMA/GEAK-eval), consisting of a modified set of 184 kernels originally sourced from [TritonBench-G](https://arxiv.org/abs/2502.14752) \\[1\\] and an additional set of 30 kernels from various open [ROCm repositories](https://github.com/ROCm).\n\n- Execution accuracy up to 54.89% on TritonBench-modified and 63.33% on ROCm benchmark (as compared to around 15% when directly prompting frontier LLMs) in producing correct kernels that pass all the unit tests.\n\n- Average Speedup of up to 2.59X with the AI-generated kernels over reference kernels from the TritonBench-modified benchmark.\n\n\nFigure 1. Overview of the GEAK kernel code generation AI agent [#](https://rocm.blogs.amd.com/rocm.blogs.amd.com#id1)\n\nAs shown in Figure 1, the agentic AI system comprises four core modules: a **generator**, a **reflector**, an **evaluator**, and an **optimizer**.\n\n- The **generator** produces code based on user query and contextual information.\n\n- The **evaluator** follows a cascaded design: it first performs a functionality test to verify correctness. If the code fails this test, the corresponding error trace is fed back to the reflector for further analysis. If the code passes, the evaluator proceeds to assess its performance, including latency and memory efficiency.\n\n- If the generated code fails to execute correctly, the **reflector** analyzes both the code and the resulting error trace to identify potential issues.\n\n- Finally, the **optimizer** takes as input the functionally correct code and formulates strategies to enhance its performance with respect to latency and computational efficiency.\n\n\nTo improve the performance of our agent, we employ some composable and configurable techniques as mentioned below:\n\n- **1-shot Prompting**\n\n\nTo enable 1-shot prompting, the most similar Triton code from existing datasets is used in the prompt. The datasets used for 1-shot prompt do not overlap with the benchmark. We’ve observed that 1-shot sample retrieval from datasets is most effective when using code similarity rather than instruction similarity.\n\n- **Knowledge Injection**\n\n\nWe enhance the prompt with domain-specific knowledge on writing efficient Triton kernels, including detailed hardware specifications. This incorporation of low-level optimization principles significantly improves the accuracy and quality of LLM-generated code.\n\n- **Reflexion**\n\n\nTo enhance the system’s self-correcting capabilities, we leverage a reflexion \\[3\\] module that supports introspective debugging and iterative refinement. When an LLM-generated kernel fails the functionality test, the resulting error trace is provided as feedback to the reflector for further analysis and correction. The agent is tasked with analyzing the cause of failure and proposing a corresponding solution. We scale the number of iterations to improve kernel generation.\n\n- **LLM selection for kernel agent**\n\n\nWe have explored the use of multiple LLMs like GPT-4.1, GPT-o1, and Gemini 2.5 Pro. We observed notable variations in the output produced by different models, indicating that the capability of the underlying LLM can substantially influence the results. In this blog, we present results with GPT-4.1 only but the GEAK codebase allows developers to try different LLMs as per their choice.\n\n- **LLM as Optimizer**\n\n\nThe optimizer is tasked with identifying potential optimization directions based on previous code generations and their corresponding performance metrics. These historical records, which include the generated code and associated performance results, are sorted in ascending order of performance. This structured presentation \\[2\\] helps guide the LLM toward proposing more effective optimization strategies.\n\n- **Debugging Trap**\n\n\nWhen LLM’s generated code has bugs, the error trace is provided to the Reflector for feedback generation for further correction. However, we’ve observed that sometimes code can undergo several reflection cycles while still being plagued by the same bug, which is what we refer to as the debugging trap. To prevent the agent from getting stuck in a debugging trap, we impose a limit on the number of debugging attempts per code snippet using a parameter max\\_perf\\_debug\\_num. If the code continues to fail after reaching this threshold, the agent is required to discard the current approach and generate a new strategy along with fresh code.\n\n- **Parallel Scaling**\n\n\nTo ensure reliable kernel generation, we run multiple instances of GEAK multiple times in parallel and independently. To introduce diversity in the generated code, the LLM output is sampled with the parameter temperature set to 1. Our experiments show that such diversity in generation can yield correct and faster kernels that are otherwise underexplored. Moreover, further experiments indicate that combining sequential and parallel scaling yields additional performance improvements. To obtain unbiased estimates of accuracy, we use the pass@k metric to estimate it \\[4\\].\n\n## Triton Kernel Evaluation Benchmark [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#triton-kernel-evaluation-benchmark)\n\nWe conducted experiments using two benchmarks methods:\n\n**1\\. TritonBench-Modified Benchmark (184 Kernels):**\n\nThe Triton kernels and corresponding evaluation codebase were adapted from [TritonBench](https://github.com/thunlp/TritonBench) \\[1\\] with several modifications made:\n\n- Out of 184 kernels in TritonBench-G, we identified and corrected errors in 37 kernels that were encountered when running on AMD GPUs. These errors include shared memory errors, invalid HIP arguments, and ModuleNotFound errors.\n\n- We found that several kernels in the original TritonBench repository missed necessary call functions, which resulted in the accuracy checking code comparing empty strings as output. We corrected this and ensured that the correct common seed was used during test.\n\n- For some kernels, testbenches were written inconsistently that produced unexpected outputs. We have fixed these kernels to have consistent testbenches.\n\n\n**2\\. ROCm Repository Triton Benchmark (30 Kernels):**\n\nThe benchmark contains 30 kernels chosen from various open ROCm repositories that have been written by Triton expert engineers and released publicly to help enable the ecosystem around running AI workloads efficiently on AMD GPUs. The entire list of kernels is outlined in Table 1. We also took assistance from frontier LLMs to put these kernels, as well as their corresponding unit tests, in the same format as the TritonBench-modified framework. It allows us to compare the accuracy and efficiency of our agent consistently.\n\n| **ROCm Kernel** | **Repository link** |\n| --- | --- |\n| 1\\. test\\_tma\\_store\\_gemm 2. moe-gemm 3. layernorm 4. triton\\_multreduce\\_matmul\\_kernel.py 5. multreduce\\_matmul\\_dot\\_kernel.py 6. gemm 7. test\\_block\\_pointer\\_matmul 8. test\\_block\\_copy 9. test\\_add\\_kernel 10. softmax | [ROCm/triton](https://github.com/ROCm/triton/tree/main_perf) |\n| 1\\. mnsnorm\\_fwd 2. rmsnorm\\_bwd 3. test\\_chained\\_dot\\_fp8 4. test\\_matmul\\_MXFP 5. test\\_gemm\\_fusion | [ROCm/aiter](https://github.com/ROCm/aiter) |\n| 1\\. test\\_triton\\_swizzle2d 2. test\\_triton\\_sort 3. test\\_triton\\_flip 4. test\\_reverse\\_range | [ROCm/aotriton](https://github.com/ROCm/aotriton) |\n| 1\\. test\\_load\\_reduce.py 2. test\\_chained\\_matmul | [ROCm/vllm](https://github.com/ROCm/vllm) |\n| 1\\. test\\_random\\_int 2. test\\_randn 3. test\\_kernel\\_sub 4. test\\_kernel\\_dot 5. test\\_batched","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Frocm.blogs.amd.com%2Fsoftware-tools-optimization%2Ftriton-kernel-ai%2FREADME.html","id":"https://rocm.blogs.amd.com/software-tools-optimization/triton-kernel-ai/README.html","publishedDate":"2025-08-01T00:00:00.000Z","summary":"The webpage introduces GEAK, an AI-driven framework for generating efficient GPU kernels using Triton, a parallel programming language. AMD's initiative aims to enhance AI development by automating kernel generation for AMD Instinct™ GPUs, specifically the MI250X and MI300X models. Key features include:\n\n- **GEAK Framework**: A library that enables the automatic production of Triton kernels from brief functional instructions.\n- **Evaluation Benchmarks**: Two sets of benchmarks are presented, showing significant improvements in execution accuracy (up to 63.33%) and speed (average speedup of 2.59X) compared to traditional methods.\n- **Core Modules**: The system consists of a generator, evaluator, reflector, and optimizer, each playing a role in producing, testing, and refining the generated code.\n\nOverall, GEAK represents a significant advancement in leveraging AI for efficient GPU programming.","title":"GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://rocm.blogs.amd.com/software-tools-optimization/triton-kernel-ai/README.html"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.arxiv.org%2Fpdf%2F2507.23194","id":"https://www.arxiv.org/pdf/2507.23194","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses the implementation and implications of a reCAPTCHA system on the arXiv platform. The paper explores how this system helps in preventing automated submissions while ensuring accessibility for human users. It also acknowledges support from the Simons Foundation and member institutions. The document likely includes technical details, methodologies, and findings related to the effectiveness and user experience of the reCAPTCHA integration on arXiv.","title":"[PDF] Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://www.arxiv.org/pdf/2507.23194"},{"content":"Agree & Join LinkedIn\n\nBy clicking Continue to join or sign in, you agree to LinkedIn’s [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n\n [Skip to main content](https://www.linkedin.com/www.linkedin.com#main-content)\n\n# Emad Barsoum’s Post\n\n[Emad Barsoum](https://www.linkedin.com/in/ebarsoum?trk=public_post_feed-actor-name)\n\n1mo\n\n- [Report this post](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nThe future of optimization and GPU kernel writing, proud of the team!!! Generating Efficient AI-centric Kernels (GEAK) is our OpenAI Triton Kernel agent using inference-time scaling to generate a high performance numerically accurate kernel and two Triton evaluation benchmarks. Looking forward for the community to try it. [#AMD](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Famd&trk=public_post-text) [#Triton](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Ftriton&trk=public_post-text) [#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&trk=public_post-text) [#ML](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fml&trk=public_post-text) [#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllm&trk=public_post-text) [#AGI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fagi&trk=public_post-text) [#MI300X](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmi300x&trk=public_post-text)\n\n[Zicheng Liu](https://www.linkedin.com/in/zicheng-liu?trk=public_post_reshare_feed-actor-name)\n\n1mo\n\nI'm thrilled to share that we've released GEAK: our Triton Kernel AI Agent and Evaluation Benchmarks. We achieve an average execution accuracy of 54.89% on TritonBench-modified and 63.33% ROCm benchmark as compared to 15% if directly prompting frontier models. The generated kernels have an average speedup of 2.59X over the baseline.\n[#AMD](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Famd&trk=public_post_reshare-text) [#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllm&trk=public_post_reshare-text) [#ML](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fml&trk=public_post_reshare-text) [#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&trk=public_post_reshare-text) [#HW](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fhw&trk=public_post_reshare-text) [#MI300X](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmi300x&trk=public_post_reshare-text) [https://lnkd.in/gMcx6BBz](https://lnkd.in/gMcx6BBz?trk=public_post_reshare-text)\n\n[53](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_social-actions-reactions) [1 Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_social-actions-comments)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_like-cta) [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment-cta)\n\nShare\n\n- Copy\n- LinkedIn\n- Facebook\n- Twitter\n\n[Waleed Atallah](https://www.linkedin.com/in/waleedatallah?trk=public_post_comment_actor-name)\n\nmaking AI go fast\n\n1mo\n\n- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\nAmazing work! The LLM generated kernels will change how we access GPUs, no doubt\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment_reply) [3 Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment_reactions) 4 Reactions\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_feed-cta-banner-cta)\n\n3,240 followers\n\n- [675 Posts](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Febarsoum%2Frecent-activity%2F&trk=public_post_follow-posts)\n\n[View Profile](https://www.linkedin.com/in/ebarsoum?trk=public_post_follow-view-profile) [Connect](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7357062890551496704&trk=public_post_follow)\n\n## Explore content categories\n\n- [Career](https://www.linkedin.com/top-content/career/)\n- [Productivity](https://www.linkedin.com/top-content/productivity/)\n- [Finance](https://www.linkedin.com/top-content/finance/)\n- [Soft Skills & Emotional Intelligence](https://www.linkedin.com/top-content/soft-skills-emotional-intelligence/)\n- [Project Management](https://www.linkedin.com/top-content/project-management/)\n- [Education](https://www.linkedin.com/top-content/education/)\n- [Technology](https://www.linkedin.com/top-content/technology/)\n- [Leadership](https://www.linkedin.com/top-content/leadership/)\n- [Ecommerce](https://www.linkedin.com/top-content/ecommerce/)\n\nShow more\n\nShow less","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO","id":"https://www.linkedin.com/posts/ebarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO","publishedDate":"2025-08-01T00:00:00.000Z","summary":"The LinkedIn post by Emad Barsoum announces the release of the Generating Efficient AI-centric Kernels (GEAK), an OpenAI Triton Kernel agent designed for optimizing GPU kernel writing. The GEAK utilizes inference-time scaling to produce high-performance, numerically accurate kernels and includes two Triton evaluation benchmarks. Zicheng Liu highlights that the generated kernels achieve an average execution accuracy of 54.89% on modified TritonBench and 63.33% on the ROCm benchmark, significantly outperforming direct prompts to frontier models, which only achieved 15%. Additionally, the generated kernels provide an average speedup of 2.59 times over baseline performance. The post encourages the community to try out this innovative tool.","title":"Introducing GEAK: Efficient AI-centric Kernels for OpenAI Triton","url":"https://www.linkedin.com/posts/ebarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO"},{"content":"[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[AMD-AGI](https://github.com/AMD-AGI)/ **[GEAK-agent](https://github.com/AMD-AGI/GEAK-agent)** Public\n\n- [Notifications](https://github.com/login?return_to=%2FAMD-AGI%2FGEAK-agent) You must be signed in to change notification settings\n- [Fork\\\n6](https://github.com/login?return_to=%2FAMD-AGI%2FGEAK-agent)\n- [Star\\\n31](https://github.com/login?return_to=%2FAMD-AGI%2FGEAK-agent)\n\n\nIt is an LLM-based AI agent, which can write correct and efficient gpu kernels automatically.\n\n### License\n\n[View license](https://github.com/AMD-AGI/GEAK-agent/blob/main/LICENSE.md)\n\n[31\\\nstars](https://github.com/AMD-AGI/GEAK-agent/stargazers) [6\\\nforks](https://github.com/AMD-AGI/GEAK-agent/forks) [Branches](https://github.com/AMD-AGI/GEAK-agent/branches) [Tags](https://github.com/AMD-AGI/GEAK-agent/tags) [Activity](https://github.com/AMD-AGI/GEAK-agent/activity)\n\n[Star](https://github.com/login?return_to=%2FAMD-AGI%2FGEAK-agent)\n\n[Notifications](https://github.com/login?return_to=%2FAMD-AGI%2FGEAK-agent) You must be signed in to change notification settings\n\n# AMD-AGI/GEAK-agent\n\nmain\n\n[Branches](https://github.com/AMD-AGI/GEAK-agent/branches) [Tags](https://github.com/AMD-AGI/GEAK-agent/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit ## History [10 Commits](https://github.com/AMD-AGI/GEAK-agent/commits/main/) |\n| [src](https://github.com/AMD-AGI/GEAK-agent/tree/main/src) | [src](https://github.com/AMD-AGI/GEAK-agent/tree/main/src) |\n| [.gitignore](https://github.com/AMD-AGI/GEAK-agent/blob/main/.gitignore) | [.gitignore](https://github.com/AMD-AGI/GEAK-agent/blob/main/.gitignore) |\n| [LICENSE.md](https://github.com/AMD-AGI/GEAK-agent/blob/main/LICENSE.md) | [LICENSE.md](https://github.com/AMD-AGI/GEAK-agent/blob/main/LICENSE.md) |\n| [README.md](https://github.com/AMD-AGI/GEAK-agent/blob/main/README.md) | [README.md](https://github.com/AMD-AGI/GEAK-agent/blob/main/README.md) |\n| [requirements.txt](https://github.com/AMD-AGI/GEAK-agent/blob/main/requirements.txt) | [requirements.txt](https://github.com/AMD-AGI/GEAK-agent/blob/main/requirements.txt) |\n| View all files |\n\n## Repository files navigation\n\n## Technical report\n\n## Introduction\n\nThis is an LLM-based multi-agent framework, which can generate functional and efficient gpu kernels automatically.\n\nThe framework is extendable and flexible. You can easily make you own coding agent and test it on our GEAK-eval [https://github.com/AMD-AIG-AIMA/GEAK-eval](https://github.com/AMD-AIG-AIMA/GEAK-eval).\nWe provide a baseline agent to let you run directly.\n\n## GEAK-agent\n\nIt contains a Generator, a Reflector, an Evaluator and an Optimizer. The actor generates codes according to the query and context information. The Reflector is responsible for reflecting on the generated code and the error trace if the code failed to run. The Evaluator has a cascade structure. It tests the generated code for the functionality first. If the generated code doesn't pass the functionality test, the error trace will be fedback to the Reflector. Otherwise, the Evaluator will evaluate the performance including latency and efficiency. The Optimizer gets the generated codes, which pass the evaluator's tests, and gives a strategy to optimize the code in terms of latency and efficiency.\n\n### the Optimizer\n\nWe provide previous generated codes as reference codes with their corresponding performance to the Optimizer. The number of reference codes is controlled by the arg `ancestor_num`. The reference codes are arranged in ascending order to help the Optimizer LLM find the optimization direction. We don't ask the LLM to generate new codes directly from the reference codes, instead we ask the Optimizer to analyze the reference codes first and to generate a promising strategy to optimize the code. Then we feed the generated optimization stratgey to the Generator to generate new codes.\n\n### debugging trap\n\nLLMs frequently get caught in debugging traps. When their generated code has bugs, we provide the error trace to the Reflector correction. However, we've observed that sometimes code can undergo several reflection cycles while still being plagued by the same bug. We refer to this as a debugging trap.\n\nTo prevent the LLM from getting stuck in a debugging trap, we limit debugging attempts per code snippet using `max_perf_debug_num`. If the code fails after this many fixes, the agent must abandon the current approach and generate a fresh strategy and code.\n\n## Run the Agents\n\n1. prepare the environment\n\n\n\n```\npython3 -m pip install -r requirements.txt\n\n```\n\n2. go to the src/ folder\n\n\n\n```\ncd src\n\n```\n\n3. edit config file. You need to give your API key and TritonBench data path in your config file.\n\n\n\n```\ncp configs/tritonbench_optimagent_config.yaml configs/tritonbench_optimagent_config_new.yaml\n\n```\n\n4. put the config file in the main\\_optimagent.py and run the script\n\n\n\n```\npython main_optimagent.py\n\n```\n\n\n### Resuming from Checkpoints\n\nResult and memories will be stored in the `output_path` specified in the config file for each iteration. You can resume from any iter you want by specifying the `result_file`, `mem_file` and `start_iter` in the config file. For example:\n\n```\nresult_path: \"../outputs/optimagent_10.jsonl\"\nmem_file: \"../outputs/optimagent_mem_10.json\"\nstart_iter: 11\n\n```\n\n## How to use your own data\n\n1. create a new file for your own dataloader in dataloaders\n\n\n\n```\ntouch dataloaders/OnlineData.py\n\n```\n\n2. In your own dataloader, define a new data class\n\n\n\n```\nclass OnlineData:\n\n```\n\n3. In the OnlineData, you need load `problem_states`, which is a list of ProblemState instances. To meet the minimum requirement, each ProblemState instance should include the following fields: `instruction`, `filename`, `test_code`.\n\n4. In order to use OptimAgent, the OnlineData class must implement the following methods:\n\n\n\n```\n__len__() -> int\ntest_opt_correctness(code, filename, tmp_dir, exe_dir) -> pass_call, pass_exe, call_stdout, call_stderr, exe_stdout, exe_stderr\nwrite_perf_file(input_folder_path, results_path, tmp_dir) -> None\nrun_perf_scripts(gpu_id, script_dir, log_dir) -> None\nwrite_file(file_path) -> None\n\n\n```\n\n\n\n\n\n### Method Description\n\n\n\n - \\_\\_len\\_\\_()\n\n Returns the number of problem\\_states in the dataset.\n\n - test\\_opt\\_correctness(code, filename, tmp\\_dir, exe\\_dir)\n\n Tests whether the generated code is functionally correct.\n\n Parameters:\n\n\n\n ```\n code: The generated code to be tested.\n\n filename: Name of the test script file.\n\n tmp_dir: Directory to save the script (generated code + unit test).\n\n exe_dir: Directory to store scripts that pass execution tests.\n\n ```\n\n\n\n Returns:\n\n\n\n ```\n ass_call: True if the script runs without errors.\n\n pass_exe: True if the script produces the correct output.\n\n call_stdout, call_stderr: Stdout and stderr from the initial script execution.\n\n exe_stdout, exe_stderr: Stdout and stderr from the execution verification step.\n\n ```\n\n\n\n Note:\n\n\n\n ```\n You might need to provide a Python file containing both the golden code and unit test code, separated by a line of exactly 146 hash (#) symbols. The unit test will be appended to the generated code for validation.\n\n ```\n\n - write\\_perf\\_file(input\\_folder\\_path, results\\_path, tmp\\_dir)\n\n Generates performance test scripts using the code in input\\_folder\\_path (which is typically the same as exe\\_dir from test\\_opt\\_correctness). The function will append unit tests for performance tests to the generated codes and store the scripts in tmp\\_dir. The performance test results will be saved in results\\_path.\n\n - run\\_perf\\_scripts(gpu\\_id, script\\_dir, log\\_dir)\n\n Executes the performance test scripts found in script\\_dir on the specified GPU (gpu\\_id) and stores the logs in log\\_dir.\n\n - write\\_file(file\\_path)\n\n Serializes and saves the current result to the specified file path.\n\n#### Citation\n\nIf you find this work useful in your research or applications, please consider citing:\n\n```\n@misc{wang2025geakintroducingtritonkernel,\n title={Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks},\n author={Jianghui Wang and Vinay Joshi and Saptarshi Majumder and Xu Chao and Bin Ding and Ziqiong Liu and Pratik Prabhanjan Brahma and Dong Li and Zicheng Liu and Emad Barsoum},\n year={2025},\n eprint={2507.23194},\n archivePrefix={arXiv},\n primaryClass={cs.CL},\n url={https://arxiv.org/abs/2507.23194},\n}\n```\n\n## About\n\nIt is an LLM-based AI agent, which can write correct and efficient gpu kernels automatically.\n\n### Resources\n\n[Readme](https://github.com/github.com#readme-ov-file)\n\n### License\n\n[View license](https://github.com/github.com#License-1-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](https://github.com/AMD-AGI/GEAK-agent/activity)\n\n[Custom properties](https://github.com/AMD-AGI/GEAK-agent/custom-properties)\n\n### Stars\n\n[**31**\\\nstars](https://github.com/AMD-AGI/GEAK-agent/stargazers)\n\n### Watchers\n\n[**0**\\\nwatching](https://github.com/AMD-AGI/GEAK-agent/watchers)\n\n### Forks\n\n[**6**\\\nforks](https://github.com/AMD-AGI/GEAK-agent/forks)\n\n[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FAMD-AGI%2FGEAK-agent&report=AMD-AGI+%28user%29)\n\n## [Releases](https://github.com/AMD-AGI/GEAK-agent/releases)\n\nNo releases published\n\n## [Packages\\ 0](https://github.com/orgs/AMD-AGI/packages?repo_name=GEAK-agent)\n\nNo packages published\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n## [Contributors\\ 3](https://github.com/AMD-AGI/GEAK-agent/graphs/contributors)\n\n### Uh oh!\n\nThere was an error while loadi","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fgithub.com%2FAMD-AIG-AIMA%2FGEAK-Agent","id":"https://github.com/AMD-AIG-AIMA/GEAK-Agent","publishedDate":null,"summary":"The GitHub repository for **GEAK-agent** by AMD-AGI presents an LLM-based AI framework designed to automatically generate functional and efficient GPU kernels. The framework is modular, consisting of a Generator, Reflector, Evaluator, and Optimizer, allowing users to create custom coding agents and evaluate their performance. The Evaluator tests the generated code for functionality and performance, while the Optimizer refines successful outputs based on reference codes. The repository includes essential files like a README, requirements, and a license. For more details, visit the [repository](https://github.com/AMD-AGI/GEAK-agent).","title":"AMD-AGI/GEAK-agent: It is an LLM-based AI agent, which ... - GitHub","url":"https://github.com/AMD-AIG-AIMA/GEAK-Agent"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F394175397_Geak_Introducing_Triton_Kernel_AI_Agent_Evaluation_Benchmarks","id":"https://www.researchgate.net/publication/394175397_Geak_Introducing_Triton_Kernel_AI_Agent_Evaluation_Benchmarks","publishedDate":"2025-08-03T00:00:00.000Z","summary":"Failed to fetch content","title":"Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://www.researchgate.net/publication/394175397_Geak_Introducing_Triton_Kernel_AI_Agent_Evaluation_Benchmarks"},{"content":"Introduction\nWhen I first read “Building and Optimizing Multi-Agent RAG Systems with DSPy and GEPA” by Isaac Kargar, I was struck by how practical and grounded the tutorial is. It walks the reader through using DSPy to build multiple agents (subagents) specializing in different domains (e.g. diabetes, COPD), optimizing them with GEPA (a reflective prompt evolution optimizer), and then assembling a lead agent. In my recent work trying to build reliable RAG (Retrieval-Augmented Generation) pipelines, following many of the lessons in that article strongly improved both accuracy and robustness.\nIn this write-up I’ll share my experience replicating and extending parts of that work, some pitfalls, plus what’s new in the research landscape as of late 2025. If you’re planning to build multi-agent RAG systems, I think you’ll find some of these takeaways helpful.\n\nRecap: DSPy + GEPA Setup\nFirst, a quick refresher to set the scene:\n\n- DSPy is a declarative framework for composing LM modules, tools, agents, etc. It lets you write more structured “modules” (subagents, tool wrappers, ReAct modules, etc.) rather than ad-hoc prompt engineering. (DSPy)\n- GEPA (Genetic-Pareto Prompt Optimizer) is a key optimizer in DSPy. It uses evolutionary ideas + reflective feedback (via a stronger “reflection LM”) to evolve prompt components. It outperforms or competes with older prompt optimizers and RL-based methods in many settings. (DSPy)\nIn Kargar’s tutorial, two subagents are built: one specialized in diabetes, one in COPD; each has its own vector search tool over disease-specific documents. The ReAct pattern is used. Then each agent is optimized (via GEPA) using a dataset of QA pairs, and finally a lead agent orchestrates among subagents. The tutorial shows substantial gains in evaluation metrics after GEPA. (Medium)\n\nWhat I Tried: Extending / Adapting\nIn my recent experiments I followed a similar architecture, but in a new domain (legal documents + regulatory guidance). Here are things I tried / lessons I learned.\n\n01. Domain-Specific Retrieval Tool Setup\n02. Building strong vector search tools matters. In the original case, disease documents are well-structured, fairly clean. In my domain, legal texts are noisy, have cross-references, ambiguous terms. I found that:\n - Better embedding models (fine-tuned or domain-adapted) led to improvements in how the subagent fetched relevant docs.\n - Using metadata filtering (e.g. date, jurisdiction) before or during retrieval helped reduce noise.\n03. Prompt & Instruction Design for ReAct Subagents\n04. The instructions given to agents (what they are and what tools they have) and their “thoughts / tool choices” structure have a huge effect on behavior. In Kargar’s work, the ReAct agent’s template includes next\\_thought, next\\_tool\\_name, next\\_tool\\_args, finish markers, etc. (Medium)\n05. In my trials, I discovered:\n - Being explicit about what constitutes a “good” tool call helps. Example: ask agent to “explain why you chose this tool” (or include a reasoning step) sometimes helps avoid useless or redundant searches.\n - Providing a few examples (even if synthetic) during prompt optimization helps GEPA more quickly understand the domain-specific ways queries should map to retrieval or finish. However, too many examples can “overload” the prompt and slow inference or lower generalization.\n06. GEPA Tuning\n07. GEPA has several knobs. From what the documentation shows:\n - You can choose auto modes (light / medium / heavy), set max\\_full\\_evals, reflection\\_lm, candidate selection strategy, etc. (DSPy)\n - The choice of reflection LM was important: a more capable model gives more useful feedback, but is more costly.\n08. In my domain, using a smaller reflection LM (for cost) sometimes produced feedback that was too generic; using a bigger model occasionally fixed that. But there's diminishing return at some point.\n09. Joint vs Pipeline Optimization\n10. One thing Kargar’s tutorial does is optimize subagents separately, then the lead agent. In my work, I noticed that optimizing the pipeline holistically (lead agent + subagents together, with mixed questions that force coordination) sometimes leaves hidden failure modes. For example, subagents may be individually good, but the lead agent fails to decide which to use properly in abstract or ambiguous queries.\n11. So I recommend including “mixed / joint” datasets during optimization, not only “pure” domain queries, for lead agent evaluation. Kargar does something similar when combining subdatasets. (Medium)\n\nWhat’s New / Recent Related Research (mid-2025)\nTo know where this field is going, here are a few recent works and trends that relate closely. Some validate parts of the DSPy+GEPA approach; others suggest extensions or things to watch out for.\n\n- MAO-ARAG: Multi-Agent Orchestration for Adaptive RAG (Aug 2025). This introduces a planner agent that selects among executor agents (like query reformulation, document selection, generation) depending on the query, balancing quality vs cost. Similar in spirit to having a lead agent; shows that adaptivity (deciding pipeline dynamically per query) yields good tradeoffs. (arXiv)\n- Maestro: Joint Graph & Config Optimization for Reliable AI Agents (Sep 2025). Maestro goes beyond prompt-only optimization: it searches both over how modules are wired (graph structure) and how each is configured. On standard benchmarks, it improves on GEPA or GEPA+Merge. This suggests that in addition to optimizing prompts, reconsidering the structure of your multi-agent graph (which agents exist, how they communicate) can unlock further gains. (arXiv)\n- ReSo: Reward-driven Self-organizing LLM-based Multi-Agent Systems. This focuses on flexibility and scalability: letting agents self-organize (choose responsibilities), and generating fine-grained reward signals. It points to a growing trend: moving from manually designed multi-agent systems + prompt tuning to systems that adapt more autonomously. (arXiv)\nThese and others mean that while GEPA + DSPy are excellent tools now, the frontier is shifting toward joint structure + dynamic workflows + efficient feedback signals.\n\nPitfalls & What to Be Careful About\nFrom my hands-on work (and reading) I want to share what tripped me (so you don’t repeat):\n\n- Overfitting to your prompt dataset: If your evaluation or dev set is too similar or narrow, GEPA optimizes prompts that work well there but fail in real, out-of-distribution usage.\n- Cost & latency: Using large reflection LMs, many full evaluations, or heavy budget modes of GEPA can make the pipeline pricey. Also, large prompt sizes (from many examples or big tool descriptions) slow down inference.\n- Trace & feedback quality: GEPA depends on rich traces (what the agent did step-by-step), and meaningful feedback metrics. If these are weak (e.g. only scalar accuracy), the optimizer may make \"safe\" but minimal improvements rather than addressing real failure modes.\n- Graph structure limitations: If your system’s architecture (which agents, tools, what's allowed) is too constrained, prompt optimization alone may not fix issues. For example, suppose the lead agent can only call subagent A or B but sometimes what is needed is a new kind of subagent; no amount of prompt tweaking will help.\n\nRecommendations / Best Practices\nPutting all that together, here are my recommendations if you’re building multi-agent RAG systems and using DSPy+GEPA (or planning to):\n\n1. Start with modularity: design subagents around domain or function early, with clear tool interfaces.\n2. Prepare diverse training / dev data: include both “pure domain” queries, cross-domain or ambiguous ones, and mixed ones that test coordination.\n3. Choose a capable reflection LM for GEPA, balanced vs cost. Maybe begin with light mode, then a heavier mode once you have a baseline.\n4. Iterate structure with configuration: don’t assume your initial agent graph is “correct” forever; explore different workflows or module graphs, perhaps inspired by newer tools like Maestro.\n5. Use interpretable/feedback metrics that go beyond accuracy: e.g. evaluate whether the agent selected the right tool, whether retrievals are relevant, whether reasoning steps are logical. These help GEPA's reflection step.\n6. Monitor generalization: test on out-of-domain or real user queries so that improvements aren’t just overfitted to your dataset.\n\nConclusion\nMy journey working with the ideas in Building and Optimizing Multi-Agent RAG Systems with DSPy and GEPA has reinforced that structured prompt design + optimizer feedback loops are powerful. GEPA, in particular, seems to hit a sweet spot: far more sample-efficient and often more robust than naive prompt engineering or even some RL methods, while DSPy gives you the scaffolding to build agents in a modular, maintainable way.\nAs the field moves forward — with works like MAO-ARAG and Maestro — I expect systems will become better at dynamically adapting workflow, optimizing both structure and prompts, and doing so with less human intervention.\nIf you’re looking to explore or experiment more deeply, I’ve also documented part of my pipeline (legal-domain experiments) in more detail over at [https://iacommunidad.com/](https://iacommunidad.com/) — feel free to check it out.\n\n[Create template](https://dev.to/settings/response-templates)\n\nTemplates let you quickly answer FAQs or store snippets for re-use.\n\nSubmitPreview [Dismiss](https://dev.to/404.html)\n\nAre you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's [permalink](https://dev.to/dev.to).\n\nHide child comments as well\n\nConfirm\n\nFor further actions, you may consider blocking this person and/or [reporting abuse](https://dev.to/report-abuse)\n\nWe're a place where coders share, stay up-to-date and grow their careers.\n\n[Log in](https://dev.to/enter?signup_subforem=1) [Create account](https://dev.to/enter?signup_subforem=1&state=new-user)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdev.to%2Fsky_yv_11b3d5d44877d27276%2Flessons-practices-for-building-and-optimizing-multi-agent-rag-systems-with-dspy-and-gepa-2lh7","id":"https://dev.to/sky_yv_11b3d5d44877d27276/lessons-practices-for-building-and-optimizing-multi-agent-rag-systems-with-dspy-and-gepa-2lh7","publishedDate":"2025-09-12T00:00:00.000Z","summary":"The article discusses practical insights and lessons learned from building and optimizing multi-agent Retrieval-Augmented Generation (RAG) systems using DSPy and GEPA, based on Isaac Kargar's tutorial. It highlights the use of DSPy for creating specialized subagents (e.g., for diabetes and COPD) and GEPA for optimizing their performance through evolutionary feedback mechanisms.\n\nThe author shares their experience in adapting this framework to a new domain involving legal documents, emphasizing the importance of domain-specific retrieval tools and the challenges posed by noisy legal texts. Key takeaways include the necessity of strong vector search tools, effective prompt design for ReAct subagents, and careful tuning of GEPA parameters. The author notes that providing explicit instructions and examples can enhance agent performance, while the choice of reflection language model impacts optimization outcomes.\n\nOverall, the article serves as a guide for those looking to implement multi-agent RAG systems, offering practical strategies to improve accuracy and robustness in various domains.","title":"Lessons & Practices for Building and Optimizing Multi-Agent RAG Systems with DSPy and GEPA","url":"https://dev.to/sky_yv_11b3d5d44877d27276/lessons-practices-for-building-and-optimizing-multi-agent-rag-systems-with-dspy-and-gepa-2lh7"},{"content":"When you're building AI agents in PHP, one question keeps surfacing in production environments: what happens when your agent gets stuck in a loop? Whether it's an external API that's down, an LLM that's having an off day, or a tool that's returning unexpected responses, runaway tool calls can quickly turn a helpful agent into a resource-draining problem.\n\nNeuron V2 introduces [Tool Max Tries](https://docs.neuron-ai.dev/getting-started/tools#max-tries), a straightforward guardrail that puts you back in control of your agent's behavior. This feature sets a hard limit on how many times an agent can invoke any single tool during a conversation, preventing the cascading failures that can occur when AI reasoning goes sideways.\n\n## The Problem: When AI Gets Stuck\n\nPicture this scenario: you've built an agent that helps customers track their orders. It uses a GetOrderStatus tool to fetch information from your backend API. Everything works perfectly in testing, but in production, your API occasionally returns a 500 error. Instead of gracefully handling the failure, your agent decides the best course of action is to keep trying the same tool call, over and over.\n\nWithout proper constraints, you might see something like this in your logs:\n\n```\n[2024-09-04 10:15:23] Tool call: getOrderStatus(order_id: 12345)\n[2024-09-04 10:15:24] API Error: Internal Server Error\n[2024-09-04 10:15:25] Tool call: getOrderStatus(order_id: 12345)\n[2024-09-04 10:15:26] API Error: Internal Server Error\n[2024-09-04 10:15:27] Tool call: getOrderStatus(order_id: 12345)\n// ... continues indefinitely\n\n```\n\nThis isn't just annoying—it's dangerous. Each failed tool call still consumes tokens, and if you're paying per API call to your LLM provider, those costs add up quickly. More importantly, it creates a terrible user experience as customers wait for responses that never come.\n\n## Simple but Effective Solution\n\nTool Max Tries addresses this by letting you set a maximum number of attempts for each tool in your agent's toolkit. Here's how you configure it:\n\n```\ntry {\n\n $result = YouTubeAgent::make()\n ->toolMaxTries(5) // Max number of calls for each tool\n ->addTool(\n // It takes precedence over the global setting\n GetOrderStatus::make()->setMaxTries(2)\n )\n ->chat(...);\n\n} catch (ToolMaxTriesException $exception) {\n // do something\n}\n```\n\nBy default the limit is 5 calls, and it count for each tool individually. You can customize this value with the `toolMaxTries()` method at agent level, or use setMaxTries() on the tool level. Setting max tries on single tool takes precedence over the global setting.\n\nNow when your agent encounters that problematic API, it will attempt the call up to three times before giving up and informing the user that the service is currently unavailable. The conversation continues, but within reasonable bounds.\n\nYou can also apply this configuration for tools included in toolkit using the with method:\n\n```\nclass MyAgent extends Agent\n{\n ...\n\n public function tools(): array\n {\n return [\n MySQLToolkit::make()\n ->with(\n MySQLSchemaTool::class,\n fn (ToolInterface $tool) => $tool->setMaxTries(1)\n ),\n ];\n }\n}\n```\n\n## Getting Started\n\nSetting up Tool Max Tries is straightforward, and you can configure different limits for different tools based on their risk profiles and expected behavior. A tool that performs simple calculations might never need retries, while one that calls external APIs might benefit from two or three attempts.\n\nThe beauty of this approach is that it's opt-in and granular. You can set limits only where you need them, and adjust them based on your actual usage patterns. If you find that five attempts aren't enough for a particular tool, you can increase the limit. If you want to be more aggressive about preventing loops, you can set it to one or two.\n\nThe framework handles the operational complexities while letting you focus on solving business problems.\n\nIf you want to learn more about how to get started your AI journey in PHP check out the learning section in the documentation.\n\n## Monitoring & Debugging\n\nMany of the Agents you build with Neuron will contain multiple steps with multiple invocations of LLM calls, tool usage, access to external memories, etc. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly your agent is doing and why.\n\nThe Deep Research Agent integrates with [Inspector](https://inspector.dev) for comprehensive observability:\n\n```\nINSPECTOR_INGESTION_KEY=fwe45gtxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n```\n\nThis monitoring reveals the complete execution timeline, showing which agents made decisions, how long each phase took, and where any issues occurred. For multi-agent systems, this visibility proves essential for optimization and debugging.\n\n[Create template](https://dev.to/settings/response-templates)\n\nTemplates let you quickly answer FAQs or store snippets for re-use.\n\nSubmitPreview [Dismiss](https://dev.to/404.html)\n\nAre you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's [permalink](https://dev.to/dev.to).\n\nHide child comments as well\n\nConfirm\n\nFor further actions, you may consider blocking this person and/or [reporting abuse](https://dev.to/report-abuse)\n\nWe're a place where coders share, stay up-to-date and grow their careers.\n\n[Log in](https://dev.to/enter?signup_subforem=1) [Create account](https://dev.to/enter?signup_subforem=1&state=new-user)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdev.to%2Finspector%2Fkeeping-your-ai-agents-under-control-tool-max-tries-in-neuron-v2-5m7","id":"https://dev.to/inspector/keeping-your-ai-agents-under-control-tool-max-tries-in-neuron-v2-5m7","publishedDate":"2025-09-08T00:00:00.000Z","summary":"The article discusses the introduction of the \"Tool Max Tries\" feature in Neuron V2, designed to prevent AI agents from getting stuck in infinite loops when external tools fail. It highlights the problem of runaway tool calls, which can lead to excessive costs and poor user experiences. The Tool Max Tries feature allows developers to set a maximum number of attempts for each tool, ensuring that agents stop trying after a specified number of failures. This can be configured globally or on a per-tool basis, providing flexibility based on the tool's behavior. The article provides code examples for implementing this feature, emphasizing its ease of use and the ability to customize limits according to the needs of different tools. Overall, Tool Max Tries serves as an effective guardrail to maintain control over AI agent behavior in production environments.","title":"Keeping Your AI Agents Under Control: Tool Max Tries in Neuron V2","url":"https://dev.to/inspector/keeping-your-ai-agents-under-control-tool-max-tries-in-neuron-v2-5m7"},{"content":"## Other Sites\n\n### [Ubuntu Buzz !](http://feeds.feedburner.com/Ubuntubuzz)\n\n##### [Collection of Ubuntu 24.04 Tutorials Part II: CHM, AppImages, Telegram and Synaptic](https://www.ubuntubuzz.com/2025/09/collection-of-ubuntu-2404-tutorials-part-ii.html)\n\n> This is the second compilation of our Ubuntu 24.04 Noble Numbat tutorials which have been published here at The Ubuntu Buzz since 2024. In last compilation, we talked about downloading Ubuntu up to selecting our repository server among others. In this episode, we will remind you about CHM ebooks, AppImages, Telegram Desktop as well as the classic but powerful Synaptic Package Manager with a short review for each. We hope this compilation series will be useful to you and everyone. This marks our last article of that year in the month May. Please enjoy!\n\n### [LinuxGizmos.com](https://linuxgizmos.com/feed/)\n\n##### [Tiny RISC-V Development Board with WCH CH32V317WCU6 Available from $6.80](https://linuxgizmos.com/tiny-risc-v-development-board-with-wch-ch32v317wcu6-available-from-6-80/)\n\n> The board is powered by the WCH CH32V317WCU6, a RISC-V microcontroller capable of running at up to 144MHz. The chip features a USB 2.0 high-speed controller alongside a full-speed controller, a 10/100M Ethernet MAC with a physical layer transceiver, SDIO support, DVP for digital video input, and advanced motor PWM timers.\n\n##### [DietPi September 2025 Update Brings Faster Backups and Roon Server Early Access](https://linuxgizmos.com/dietpi-september-2025-update-brings-faster-backups-and-roon-server-early-access/)\n\n> The September 20th release of DietPi v9.17 introduces smaller and more efficient system images, faster backups with reduced disk usage, and a new toggle for Roon Server’s early access builds. The update also addresses SPI bootloader flashing issues on Rockchip devices, improves Raspberry Pi sound card handling, and includes multiple bug fixes across tools and software.\n\n##### [MSI EdgeXpert Compact AI Supercomputer Based on NVIDIA DGX Spark](https://linuxgizmos.com/msi-edgexpert-compact-ai-supercomputer-based-on-nvidia-dgx-spark/)\n\n> The EdgeXpert achieves up to 1000 AI TOPS at FP4 precision with sparsity and can scale higher using FP8 data formats. It integrates 128 GB of unified LPDDR5x memory on a 256-bit interface, offering 273 GB/s bandwidth shared across the CPU and GPU.\n\n##### [ESP32 Bus Pirate Turns Low-Cost Boards into Multi-Protocol Debugging Tools](https://linuxgizmos.com/esp32-bus-pirate-turns-low-cost-boards-into-multi-protocol-debugging-tools/)\n\n> The firmware supports protocols such as I²C, SPI, UART, 1-Wire, CAN, and JTAG, as well as infrared, smartcards, and USB device emulation. It also extends to wireless communication, enabling interaction with Sub-GHz radios, Bluetooth, Wi-Fi, RFID, and RF24 devices. A built-in scripting engine allows automation using Bus Pirate-style bytecode instructions or Python.\n\n- [This Is My Favorite Email Client on Linux (And It’s Not Thunderbird)](https://tuxmachines.org/n/2025/09/23/This_Is_My_Favorite_Email_Client_on_Linux_And_It_s_Not_Thunderb.shtml)\n- [Elementary OS 8.0.2 Brings Kernel 6.14, Better Accessibility](https://tuxmachines.org/n/2025/09/23/Elementary_OS_8_0_2_Brings_Kernel_6_14_Better_Accessibility.shtml)\n\n## news\n\n# 5 Linux features I rely on for maximum performance\n\nposted by Rianne Schestowitz on Sep 23, 2025\n\nQuoting: 5 Linux performance features to optimize your system —\n\n> Most distributions ship with a conservative CPU governor that raises the CPU speed only after it detects load. That saves energy, but it means there is always a micro pause while the processor ramps up. On my plugged-in desktop, it makes the interface feel less snappy. I prefer switching to the “performance” governor that keeps the CPU at its highest frequency all the time.\n>\n> The difference is huge. I see windows open quickly, apps launch as if they are preloaded, and heavy tasks no longer stutter at the beginning. It draws more power and generates more heat, but on a desktop, that trade-off is worth it. When I am on a laptop, I script a switch back to the on-demand governor on battery and return to performance mode when I plug in, which gives me the best of both worlds.\n\n[Read on](https://www.xda-developers.com/linux-features-for-maximum-performance/)\n\n# Other Recent Tux Machines' Posts\n\n[Fedora Linux 43 Beta Released with Linux 6.17, GNOME 49, and KDE Plasma 6.4](https://tuxmachines.org/n/2025/09/16/Fedora_Linux_43_Beta_Released_with_Linux_6_17_GNOME_49_and_KDE_.shtml)The Fedora Project released today the beta version of the upcoming Fedora Linux 43 for public testing to give us a glimpse of the new features and report potential bugs.[Linux 6.17-rc7](https://tuxmachines.org/n/2025/09/22/Linux_6_17_rc7.shtml)\"Let's keep the testing going, and we'll have the final 6.17 in a week\"[Tails 7.0 Anonymous Linux OS Released, Based on Debian 13 “Trixie”](https://tuxmachines.org/n/2025/09/18/Tails_7_0_Anonymous_Linux_OS_Released_Based_on_Debian_13_Trixie.shtml)Tails 7.0 is out today as a major update to this portable Linux OS based on the Debian GNU/Linux operating system that protects users against surveillance and censorship.[KDE-Centric KaOS 2025.9 Released with Plasma 6.4.5](https://tuxmachines.org/n/2025/09/27/KDE_Centric_KaOS_2025_9_Released_with_Plasma_6_4_5.shtml)KaOS 2025.9 arrives with Plasma 6.4.5, a new Midna theme[System76 Releases COSMIC Beta Desktop Environment and Pop!\\_OS 24.04 LTS Beta](https://tuxmachines.org/n/2025/09/26/System76_Releases_COSMIC_Beta_Desktop_Environment_and_Pop_OS_24.shtml)Linux hardware vendor System76 announced today the general availability of the beta version of their up-and-coming COSMIC desktop environment written in Rust, along with the beta version of Pop!\\_OS 24.04 LTS.[OBS Studio 32.0 Adds PipeWire Video Capture Improvements, Basic Plugin Manager](https://tuxmachines.org/n/2025/09/23/OBS_Studio_32_0_Adds_PipeWire_Video_Capture_Improvements_Basic_.shtml)OBS Studio 32.0 has been released today for this powerful, open-source, cross-platform, and free software for video recording and live streaming on Linux, macOS, and Windows systems.[SysLinuxOS 13 Mate released](https://tuxmachines.org/n/2025/09/28/SysLinuxOS_13_Mate_released.shtml)SysLinuxOS 13 is now available with several enhancements and new features. Some of these improvements are under the hood, providing additional functionality[Free, Libre, and Open Source Software Leftovers](https://tuxmachines.org/n/2025/09/28/Free_Libre_and_Open_Source_Software_Leftovers.shtml)FOSS picks[Programming Leftovers](https://tuxmachines.org/n/2025/09/28/Programming_Leftovers.shtml)Development picks[Applications and Games on GNU/Linux](https://tuxmachines.org/n/2025/09/28/Applications_and_Games_on_GNU_Linux.shtml)mixture of 4 stories[Highlights and Talks at Akademy 2025](https://tuxmachines.org/n/2025/09/28/Highlights_and_Talks_at_Akademy_2025.shtml)a pair of belated or related blog posts[Open Hardware/Modding: Router, Raspberry Pi, and RISC-V](https://tuxmachines.org/n/2025/09/28/Open_Hardware_Modding_Router_Raspberry_Pi_and_RISC_V.shtml)Hardware picks[today's howtos](https://tuxmachines.org/n/2025/09/28/today_s_howtos.shtml)idroot and more[FreeBSD 15.0-ALPHA4 Now Available](https://tuxmachines.org/n/2025/09/28/FreeBSD_15_0_ALPHA4_Now_Available.shtml)The fourth alpha build of the 15.0-RELEASE release cycle is now available[Android Leftovers](https://tuxmachines.org/n/2025/09/28/Android_Leftovers.shtml)I use these Android apps for creating content instead of a laptop[This tool helped me create a USB image with everything I need for the perfect Linux setup](https://tuxmachines.org/n/2025/09/28/This_tool_helped_me_create_a_USB_image_with_everything_I_need_f.shtml)The endless customizability is one of my favorite features of the Linux ecosystem[5 Linux Distros That Play Nice With Brand-New Hardware](https://tuxmachines.org/n/2025/09/28/5_Linux_Distros_That_Play_Nice_With_Brand_New_Hardware.shtml)Are you building a new PC and want to install Linux on it[Free and Open Source Software](https://tuxmachines.org/n/2025/09/28/Free_and_Open_Source_Software.shtml)This is free and open source software[DietPi September 2025 Update Brings Faster Backups and Roon Server Early Access](https://tuxmachines.org/n/2025/09/28/DietPi_September_2025_Update_Brings_Faster_Backups_and_Roon_Ser.shtml)DietPi is a lightweight, Debian-based operating system optimized for single-board computers and embedded devices[Independent Distro KaOS Linux 2025.09 Arrives with Linux 6.16, KDE Gear 25.08](https://tuxmachines.org/n/2025/09/28/Independent_Distro_KaOS_Linux_2025_09_Arrives_with_Linux_6_16_K.shtml)KaOS Linux 2025.09 has been released as the latest ISO snapshot for this independent Linux distribution built on top of the latest KDE software and using Arch Linux’s pacman package manager.[GNU/Linux Leftovers](https://tuxmachines.org/n/2025/09/28/GNU_Linux_Leftovers.shtml)several more stories[Distributions and Operating Systems: EasyOS and ZimaOS](https://tuxmachines.org/n/2025/09/28/Distributions_and_Operating_Systems_EasyOS_and_ZimaOS.shtml)mostly EasyOS today[Open Hardware/Modding: Raspberry Pi, ESP32, and More](https://tuxmachines.org/n/2025/09/28/Open_Hardware_Modding_Raspberry_Pi_ESP32_and_More.shtml)Devices and more[Today in Techrights](https://tuxmachines.org/n/2025/09/28/Today_in_Techrights.shtml)Some of the latest articles[GNU/Linux and BSD Leftovers](https://tuxmachines.org/n/2025/09/27/GNU_Linux_and_BSD_Leftovers.shtml)mostly GNU/Linux[Free, Libre, and Open Source Software Leftovers](https://tuxmachines.org/n/2025/09/27/Free_Libre_and_Open_Source_Software_Leftovers.shtml)FOSS news[Fedora and Red Hat Leftovers](https://tuxmachines.org/n/2025/09/27/Fedora_and_Red_Hat_Leftovers.shtml)Fedora and more[Programming Leftovers](https://tuxmachines.org/n/2025/09/27/Programming_Leftovers.shtml)Development picks[Security Patches and Incidents](https://tuxmachines.org/n/2025/09/27/Security_Patch","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Ftuxmachines.org%2Fn%2F2025%2F09%2F23%2F5_Linux_features_I_rely_on_for_maximum_performance.shtml","id":"https://tuxmachines.org/n/2025/09/23/5_Linux_features_I_rely_on_for_maximum_performance.shtml","publishedDate":"2025-09-23T00:00:00.000Z","summary":"The article \"5 Linux features I rely on for maximum performance\" discusses key optimizations for enhancing system performance on Linux. The author emphasizes the importance of switching from the default conservative CPU governor to the \"performance\" governor, which keeps the CPU at its highest frequency, resulting in faster application launches and a more responsive interface. Other features mentioned include optimizing system settings to reduce latency and improve overall efficiency. The article serves as a guide for users looking to maximize their Linux system's performance.","title":"5 Linux features I rely on for maximum performance","url":"https://tuxmachines.org/n/2025/09/23/5_Linux_features_I_rely_on_for_maximum_performance.shtml"}],"status":"completed","toolCallId":"call_Yi0D0DSwKGErdtabHUsHDkod","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"When the max debugging-attempts threshold is reached (max_perf_debug_num), GEAK deliberately “cuts the cord” on the current line of attack and pivots. Concretely:\n\n- Stop the current fix loop: The reflector-led debug cycle for that code path is terminated. No more retries applying incremental patches to the same buggy kernel.\n- Discard the stalled strategy: The agent treats the current approach (its plan, tiling choices, masking logic, etc.) as exhausted.\n- Force a fresh restart with a new plan: The system prompts the LLM to produce a new high-level strategy and regenerate code from scratch (often with altered tiling, memory layout, masking, or algorithmic choices), then resumes evaluate → reflect/optimize on that new branch.\n- Budget-aware progression: If the overall per-task attempt budget is still available, the new branch is evaluated like any other candidate. If the global budget is exhausted, the task is marked unsolved for that run.\n\nWhy this matters: it prevents the “debugging trap,” where the agent keeps cycling on the same bug without making structural changes. By resetting the plan, GEAK increases diversity and the chance of escaping a local failure mode.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0ad1b6a62c7d6b300068daee22538881939ee029487bf9dc8b"}},"resumableStreamId":"STREAM:b689aa17-18a9-4d41-9118-70f886310859","role":"assistant","spanId":"0c667a31369904d8","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","timeToFirstToken":1.306,"tokens":250,"tokensPerSecond":20.193861066235865,"traceId":"03c15b436511d1cf300aec1020c49c30","updated_at":1759178261734,"userId":"google:104458561188320708185","id":"j977e5ne5eeyp86qftekp2ws6x7rhk96"},{"_creationTime":1759178295391.821,"_id":"j9742gbdhm889w304m82kpeh197rgaff","attachmentIds":[],"content":"how were the results","created_at":1759178295391,"messageId":"ec57d2f0-26c1-44be-aa1b-55a29f4510b4","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","spanId":"df54306d5cfb0e21","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","traceId":"9f2f8fc5e0b95ba9259122fe622815ca","updated_at":1759178295391,"userId":"google:104458561188320708185","id":"j9742gbdhm889w304m82kpeh197rgaff","providerMetadata":{}},{"_creationTime":1759178295391.8213,"_id":"j97bnxtn7t0a4zksr8pvbt0m197rhfgz","attachmentIds":[],"byok":false,"content":"Headline results (from the paper/blog):\n\n- Correctness\n  - TritonBench-modified (184 tasks): up to 54.89% tasks solved (all unit tests pass), versus ~15% when directly prompting frontier LLMs without the agent loop.\n  - ROCm Triton Benchmark (30 real kernels): up to 63.33% solved.\n\n- Performance\n  - On TritonBench-modified, among tasks with at least one correct GEAK kernel, the best candidate per task achieves an average speedup up to 2.59× over the provided reference implementations.\n\n- Scaling behavior\n  - Increasing inference-time budget (more parallel/sequential attempts with diversity) improves pass rates roughly log-linearly; combining parallel and sequential scaling helps further.\n\n- Ablations/techniques that mattered\n  - 1-shot retrieval (code-similarity based) and knowledge injection improved both correctness and speed.\n  - Reflexion-style debugging and the “debugging trap” guard (max_perf_debug_num) were important for escaping loops.\n  - Using the LLM as an Optimizer, guided by ranked historical timings, outperformed naïve tweak-and-retry.\n\nIf you want, I can break this down by pass@1 vs. pass@k, or by kernel category (e.g., GEMM, softmax, norms) if you’re targeting specific ops.","created_at":1759178295392,"messageId":"96831e19-3ed8-4854-8b45-5cc7096ccf01","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"news","domains":null,"queries":["Geak Triton Kernel AI Agent evaluation results pass@k speedup arXiv:2507.23194 2025-07-01","GEAK AMD Triton agent benchmark results TritonBench-revised ROCm Triton Benchmark 2025"]},"result":[{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.23194","id":"https://arxiv.org/abs/2507.23194","publishedDate":null,"summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses the implementation of a reCAPTCHA system for the arXiv platform. The paper outlines the motivations behind using reCAPTCHA, its design, and its impact on user experience and submission processes. It emphasizes the importance of balancing security with accessibility for researchers submitting papers. The authors acknowledge support from the Simons Foundation and member institutions. For more detailed insights, readers can refer to the full paper on the arXiv website.","title":"Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/abs/2507.23194"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2507.23194v1","id":"https://arxiv.org/html/2507.23194v1","publishedDate":null,"summary":"The webpage titled \"arXiv reCAPTCHA\" discusses a specific implementation of reCAPTCHA technology on the arXiv platform. It outlines the purpose of using reCAPTCHA to prevent automated submissions and ensure that users are human. The document may include technical details about the integration of reCAPTCHA, its benefits for maintaining the integrity of submissions, and acknowledgments of support from the Simons Foundation and member institutions. For a comprehensive understanding, readers are encouraged to explore the full text on the provided URL.","title":"GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/html/2507.23194v1"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2507.23194","id":"https://arxiv.org/pdf/2507.23194","publishedDate":null,"summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses a novel approach to improving the security and accessibility of the arXiv preprint repository. The authors explore the implementation of reCAPTCHA technology to prevent automated submissions while ensuring that human users can easily access and submit papers. The paper outlines the methodology, results, and implications of this approach, emphasizing the balance between security and user experience. The research is supported by the Simons Foundation and affiliated institutions. For further details, the full paper can be accessed via the provided URL.","title":"[PDF] Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/pdf/2507.23194"},{"content":"## AI-Powered Library for Researchers\n\n### Track research interests, scroll daily paper feeds with AI summary, and chat with bulk of files.\n\n1. [Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks](https://arxiv.org/abs/2507.23194)\n\ncs.CLcs.LGcs.AI01 Aug 2025\n\nJianghui Wang, Vinay Joshi, Saptarshi Majumder, Xu Chao, Bin Ding, Ziqiong Liu, Pratik Prabhanjan Brahma, Dong Li, Zicheng Liu, Emad Barsoum\n\nAdvanced Micro Devices, Inc. (AMD)\n\nThe demand for AI-generated GPU kernels is rapidly growing, influenced by the need for scalable, hardware-optimized solutions in both industry and academia. As deep learning workloads grow in complexity and diversity, it is imperative to automate low-level kernel development to meet performance and productivity demands. Major cloud providers, semiconductor companies, and research institutions are now investing heavily in AI-driven code generation for GPUs, aiming to reduce manual optimization efforts while achieving near-expert performance on hardware like AMD MI300X. The Triton language, a Python-based DSL for GPU programming, has emerged as a popular target for such AI-generated kernels due to its balance of performance and ease-of-coding. In this work, we present an evaluation suite for Triton-based GPU kernels and GEAK (Generating Efficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs to generate performant Triton code specifically for AMD GPUs, including the AMD MI300X and MI250. GEAK leverages inference-time compute scaling to produce Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style feedback mechanisms. On two evaluation benchmarks, GEAK significantly outperformed the baselines of directly prompting frontier LLMs as well as Reflexion-based generation pipelines by achieving correctness up to $63$% and execution speed up of up to $2.59$X. These results highlight the promise of GEAK-like agentic code generation for accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance.\n\nIllustration of GEAK pipeline.\n\n# Why ChatPaper\n\n- Interest-Driven Paper Curation\n\nDescribe your interests as you like, be it keywords or sentence, you’ll get relevant papers every day via AI semantic matching.\n\n- Top Conferences, One-Stop\n\nIJCAI, ICML, CVPR, KDD... Access papers from top AI conferences, all in one tool, for free.\n\n- Paper Management Made Easy\n\nKeep your research organized with our bookmarking feature. Build a well-structured knowledge hub at your fingertips.\n\n- Chat With Any Paper\n\nGot questions? Ask in ChatDOC with one single click. Use AI to pull data, clarify terms, and verify facts with our precise word-level tracing feature.\n\n\n# See how it works\n\n## Watch now 1 min\n\nGet Started","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fchatpaper.com%2Fpaper%2F172041","id":"https://chatpaper.com/paper/172041","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage introduces GEAK (Generating Efficient AI-centric GPU Kernels), a framework designed to automate the generation of GPU kernels using the Triton programming language. Developed by a team from Advanced Micro Devices (AMD), GEAK leverages large language models (LLMs) to create optimized Triton code for AMD GPUs, including the MI300X and MI250. The framework employs a reasoning loop inspired by Reflexion-style feedback mechanisms, resulting in significant performance improvements—up to 63% correctness and 2.59 times faster execution compared to traditional methods. The article emphasizes the growing demand for AI-generated GPU kernels in both industry and academia, highlighting the potential of GEAK to enhance kernel performance and accessibility across diverse hardware platforms. Additionally, the webpage features tools for researchers, including AI-driven paper curation and management.","title":"Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://chatpaper.com/paper/172041"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.paperreading.club%2Fpage%3Fid%3D327242","id":"https://www.paperreading.club/page?id=327242","publishedDate":"2025-07-31T00:00:00.000Z","summary":"Failed to fetch content","title":"Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://www.paperreading.club/page?id=327242"},{"content":"[Skip to main content](https://rocm.blogs.amd.com/rocm.blogs.amd.com#main-content)\nBack to top\n\n`Ctrl` + `K`\n\n[ROCm blogs](https://rocm.blogs.amd.com/)\n\n# GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks\n\n## Contents\n\n# GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#geak-introducing-triton-kernel-ai-agent-evaluation-benchmarks)\n\nAugust 01, 2025 by [Jianghui Wang](https://rocm.blogs.amd.com/authors/jianghui-wang.html), [Vinay Joshi](https://rocm.blogs.amd.com/authors/vinay-joshi.html), [Saptarshi Majumder](https://rocm.blogs.amd.com/authors/saptarshi-majumder.html), [Chao Xu](https://rocm.blogs.amd.com/authors/chao-xu.html), [Bin Ding](https://rocm.blogs.amd.com/authors/bin-ding.html), [Ziqiong Liu](https://rocm.blogs.amd.com/authors/ziqiong-liu.html), [Pratik Prabhanjan Brahma](https://rocm.blogs.amd.com/authors/pratik-prabhanjan-brahma.html), [Dong Li](https://rocm.blogs.amd.com/authors/dong-li.html), [Zicheng Liu](https://rocm.blogs.amd.com/authors/zicheng-liu.html), [Emad Barsoum](https://rocm.blogs.amd.com/authors/emad-barsoum.html).\n\n10 min read. \\| 2414 total words.\n\n[Software tools & optimizations](https://rocm.blogs.amd.com/blog/category/software-tools-optimizations.html)\n\n[AI/ML](https://rocm.blogs.amd.com/blog/tag/aiml.html)\n\n[Developers](https://rocm.blogs.amd.com/developers.html), [AI](https://rocm.blogs.amd.com/ai.html)\n\nAt AMD, we are pioneering ways to accelerate AI development using AI itself, by generating accurate and efficient GPU kernels. Specifically, we are starting with the automatic generation of kernels in [Triton](https://triton-lang.org/main/index.html), an open-source Python-like language for writing parallel programming code for GPUs. Today, AMD is excited to announce (a) [Generating Efficient AI-centric Kernels (GEAK)](https://github.com/AMD-AIG-AIMA/GEAK-Agent) for AMD GPUs, and results on (b) two Triton kernel evaluation benchmarks, where we show how AI agents can perform inference-time scaling with frontier LLMs to generate accurate and efficient kernels for AMD Instinct™ GPUs like MI250X and MI300X.\n\n**Key Takeaways:**\n\n- Announcing [GEAK Triton kernel generation framework](https://github.com/AMD-AIG-AIMA/GEAK-Agent), a library of various types of agentic AI and scaling methods to automatically produce Triton kernels from just brief instructions of its intended functionality.\n\n- Announcing corresponding [GEAK Triton Kernel Evaluation Benchmarks](https://github.com/AMD-AIG-AIMA/GEAK-eval), consisting of a modified set of 184 kernels originally sourced from [TritonBench-G](https://arxiv.org/abs/2502.14752) \\[1\\] and an additional set of 30 kernels from various open [ROCm repositories](https://github.com/ROCm).\n\n- Execution accuracy up to 54.89% on TritonBench-modified and 63.33% on ROCm benchmark (as compared to around 15% when directly prompting frontier LLMs) in producing correct kernels that pass all the unit tests.\n\n- Average Speedup of up to 2.59X with the AI-generated kernels over reference kernels from the TritonBench-modified benchmark.\n\n\nFigure 1. Overview of the GEAK kernel code generation AI agent [#](https://rocm.blogs.amd.com/rocm.blogs.amd.com#id1)\n\nAs shown in Figure 1, the agentic AI system comprises four core modules: a **generator**, a **reflector**, an **evaluator**, and an **optimizer**.\n\n- The **generator** produces code based on user query and contextual information.\n\n- The **evaluator** follows a cascaded design: it first performs a functionality test to verify correctness. If the code fails this test, the corresponding error trace is fed back to the reflector for further analysis. If the code passes, the evaluator proceeds to assess its performance, including latency and memory efficiency.\n\n- If the generated code fails to execute correctly, the **reflector** analyzes both the code and the resulting error trace to identify potential issues.\n\n- Finally, the **optimizer** takes as input the functionally correct code and formulates strategies to enhance its performance with respect to latency and computational efficiency.\n\n\nTo improve the performance of our agent, we employ some composable and configurable techniques as mentioned below:\n\n- **1-shot Prompting**\n\n\nTo enable 1-shot prompting, the most similar Triton code from existing datasets is used in the prompt. The datasets used for 1-shot prompt do not overlap with the benchmark. We’ve observed that 1-shot sample retrieval from datasets is most effective when using code similarity rather than instruction similarity.\n\n- **Knowledge Injection**\n\n\nWe enhance the prompt with domain-specific knowledge on writing efficient Triton kernels, including detailed hardware specifications. This incorporation of low-level optimization principles significantly improves the accuracy and quality of LLM-generated code.\n\n- **Reflexion**\n\n\nTo enhance the system’s self-correcting capabilities, we leverage a reflexion \\[3\\] module that supports introspective debugging and iterative refinement. When an LLM-generated kernel fails the functionality test, the resulting error trace is provided as feedback to the reflector for further analysis and correction. The agent is tasked with analyzing the cause of failure and proposing a corresponding solution. We scale the number of iterations to improve kernel generation.\n\n- **LLM selection for kernel agent**\n\n\nWe have explored the use of multiple LLMs like GPT-4.1, GPT-o1, and Gemini 2.5 Pro. We observed notable variations in the output produced by different models, indicating that the capability of the underlying LLM can substantially influence the results. In this blog, we present results with GPT-4.1 only but the GEAK codebase allows developers to try different LLMs as per their choice.\n\n- **LLM as Optimizer**\n\n\nThe optimizer is tasked with identifying potential optimization directions based on previous code generations and their corresponding performance metrics. These historical records, which include the generated code and associated performance results, are sorted in ascending order of performance. This structured presentation \\[2\\] helps guide the LLM toward proposing more effective optimization strategies.\n\n- **Debugging Trap**\n\n\nWhen LLM’s generated code has bugs, the error trace is provided to the Reflector for feedback generation for further correction. However, we’ve observed that sometimes code can undergo several reflection cycles while still being plagued by the same bug, which is what we refer to as the debugging trap. To prevent the agent from getting stuck in a debugging trap, we impose a limit on the number of debugging attempts per code snippet using a parameter max\\_perf\\_debug\\_num. If the code continues to fail after reaching this threshold, the agent is required to discard the current approach and generate a new strategy along with fresh code.\n\n- **Parallel Scaling**\n\n\nTo ensure reliable kernel generation, we run multiple instances of GEAK multiple times in parallel and independently. To introduce diversity in the generated code, the LLM output is sampled with the parameter temperature set to 1. Our experiments show that such diversity in generation can yield correct and faster kernels that are otherwise underexplored. Moreover, further experiments indicate that combining sequential and parallel scaling yields additional performance improvements. To obtain unbiased estimates of accuracy, we use the pass@k metric to estimate it \\[4\\].\n\n## Triton Kernel Evaluation Benchmark [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#triton-kernel-evaluation-benchmark)\n\nWe conducted experiments using two benchmarks methods:\n\n**1\\. TritonBench-Modified Benchmark (184 Kernels):**\n\nThe Triton kernels and corresponding evaluation codebase were adapted from [TritonBench](https://github.com/thunlp/TritonBench) \\[1\\] with several modifications made:\n\n- Out of 184 kernels in TritonBench-G, we identified and corrected errors in 37 kernels that were encountered when running on AMD GPUs. These errors include shared memory errors, invalid HIP arguments, and ModuleNotFound errors.\n\n- We found that several kernels in the original TritonBench repository missed necessary call functions, which resulted in the accuracy checking code comparing empty strings as output. We corrected this and ensured that the correct common seed was used during test.\n\n- For some kernels, testbenches were written inconsistently that produced unexpected outputs. We have fixed these kernels to have consistent testbenches.\n\n\n**2\\. ROCm Repository Triton Benchmark (30 Kernels):**\n\nThe benchmark contains 30 kernels chosen from various open ROCm repositories that have been written by Triton expert engineers and released publicly to help enable the ecosystem around running AI workloads efficiently on AMD GPUs. The entire list of kernels is outlined in Table 1. We also took assistance from frontier LLMs to put these kernels, as well as their corresponding unit tests, in the same format as the TritonBench-modified framework. It allows us to compare the accuracy and efficiency of our agent consistently.\n\n| **ROCm Kernel** | **Repository link** |\n| --- | --- |\n| 1\\. test\\_tma\\_store\\_gemm 2. moe-gemm 3. layernorm 4. triton\\_multreduce\\_matmul\\_kernel.py 5. multreduce\\_matmul\\_dot\\_kernel.py 6. gemm 7. test\\_block\\_pointer\\_matmul 8. test\\_block\\_copy 9. test\\_add\\_kernel 10. softmax | [ROCm/triton](https://github.com/ROCm/triton/tree/main_perf) |\n| 1\\. mnsnorm\\_fwd 2. rmsnorm\\_bwd 3. test\\_chained\\_dot\\_fp8 4. test\\_matmul\\_MXFP 5. test\\_gemm\\_fusion | [ROCm/aiter](https://github.com/ROCm/aiter) |\n| 1\\. test\\_triton\\_swizzle2d 2. test\\_triton\\_sort 3. test\\_triton\\_flip 4. test\\_reverse\\_range | [ROCm/aotriton](https://github.com/ROCm/aotriton) |\n| 1\\. test\\_load\\_reduce.py 2. test\\_chained\\_matmul | [ROCm/vllm](https://github.com/ROCm/vllm) |\n| 1\\. test\\_random\\_int 2. test\\_randn 3. test\\_kernel\\_sub 4. test\\_kernel\\_dot 5. test\\_batched","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Frocm.blogs.amd.com%2Fsoftware-tools-optimization%2Ftriton-kernel-ai%2FREADME.html","id":"https://rocm.blogs.amd.com/software-tools-optimization/triton-kernel-ai/README.html","publishedDate":"2025-08-01T00:00:00.000Z","summary":"The webpage introduces the GEAK (Generating Efficient AI-centric Kernels) framework developed by AMD, which utilizes AI to automatically generate efficient GPU kernels in Triton, a parallel programming language. Key highlights include:\n\n- **GEAK Framework**: A library that enables the automatic production of Triton kernels from brief functional instructions.\n- **Evaluation Benchmarks**: The framework includes two sets of benchmarks: a modified set of 184 kernels from TritonBench-G and an additional 30 kernels from various ROCm repositories.\n- **Performance Metrics**: The AI-generated kernels achieved execution accuracy rates of 54.89% and 63.33% on the respective benchmarks, significantly outperforming the 15% accuracy when using frontier LLMs directly. Additionally, these kernels provided an average speedup of up to 2.59 times compared to reference kernels.\n- **Core Modules**: The AI system consists of four modules: a generator for code creation, an evaluator for functionality and performance testing, a reflector for error analysis, and an optimizer for refining the code.\n\nOverall, the GEAK framework represents a significant advancement in leveraging AI for efficient GPU programming, particularly for AMD's Instinct GPUs.","title":"GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://rocm.blogs.amd.com/software-tools-optimization/triton-kernel-ai/README.html"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F394175397_Geak_Introducing_Triton_Kernel_AI_Agent_Evaluation_Benchmarks","id":"https://www.researchgate.net/publication/394175397_Geak_Introducing_Triton_Kernel_AI_Agent_Evaluation_Benchmarks","publishedDate":"2025-08-03T00:00:00.000Z","summary":"Failed to fetch content","title":"Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://www.researchgate.net/publication/394175397_Geak_Introducing_Triton_Kernel_AI_Agent_Evaluation_Benchmarks"},{"content":"Agree & Join LinkedIn\n\nBy clicking Continue to join or sign in, you agree to LinkedIn’s [User Agreement](https://www.linkedin.com/legal/user-agreement?trk=linkedin-tc_auth-button_user-agreement), [Privacy Policy](https://www.linkedin.com/legal/privacy-policy?trk=linkedin-tc_auth-button_privacy-policy), and [Cookie Policy](https://www.linkedin.com/legal/cookie-policy?trk=linkedin-tc_auth-button_cookie-policy).\n\n [Skip to main content](https://www.linkedin.com/www.linkedin.com#main-content)\n\n# Emad Barsoum’s Post\n\n[Emad Barsoum](https://www.linkedin.com/in/ebarsoum?trk=public_post_feed-actor-name)\n\n1mo\n\n- [Report this post](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=POST&_f=guest-reporting)\n\nThe future of optimization and GPU kernel writing, proud of the team!!! Generating Efficient AI-centric Kernels (GEAK) is our OpenAI Triton Kernel agent using inference-time scaling to generate a high performance numerically accurate kernel and two Triton evaluation benchmarks. Looking forward for the community to try it. [#AMD](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Famd&trk=public_post-text) [#Triton](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Ftriton&trk=public_post-text) [#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&trk=public_post-text) [#ML](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fml&trk=public_post-text) [#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllm&trk=public_post-text) [#AGI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fagi&trk=public_post-text) [#MI300X](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmi300x&trk=public_post-text)\n\n[Zicheng Liu](https://www.linkedin.com/in/zicheng-liu?trk=public_post_reshare_feed-actor-name)\n\n1mo\n\nI'm thrilled to share that we've released GEAK: our Triton Kernel AI Agent and Evaluation Benchmarks. We achieve an average execution accuracy of 54.89% on TritonBench-modified and 63.33% ROCm benchmark as compared to 15% if directly prompting frontier models. The generated kernels have an average speedup of 2.59X over the baseline.\n[#AMD](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Famd&trk=public_post_reshare-text) [#LLM](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllm&trk=public_post_reshare-text) [#ML](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fml&trk=public_post_reshare-text) [#AI](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fai&trk=public_post_reshare-text) [#HW](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fhw&trk=public_post_reshare-text) [#MI300X](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fmi300x&trk=public_post_reshare-text) [https://lnkd.in/gMcx6BBz](https://lnkd.in/gMcx6BBz?trk=public_post_reshare-text)\n\n[53](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_social-actions-reactions) [1 Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_social-actions-comments)\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_like-cta) [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment-cta)\n\nShare\n\n- Copy\n- LinkedIn\n- Facebook\n- Twitter\n\n[Waleed Atallah](https://www.linkedin.com/in/waleedatallah?trk=public_post_comment_actor-name)\n\nmaking AI go fast\n\n1mo\n\n- [Report this comment](https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment_ellipsis-menu-semaphore-sign-in-redirect&guestReportContentType=COMMENT&_f=guest-reporting)\n\nAmazing work! The LLM generated kernels will change how we access GPUs, no doubt\n\n[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment_like) [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment_reply) [3 Reactions](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_comment_reactions) 4 Reactions\n\nTo view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO&trk=public_post_feed-cta-banner-cta)\n\n3,240 followers\n\n- [675 Posts](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fin%2Febarsoum%2Frecent-activity%2F&trk=public_post_follow-posts)\n\n[View Profile](https://www.linkedin.com/in/ebarsoum?trk=public_post_follow-view-profile) [Connect](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Ffeed%2Fupdate%2Furn%3Ali%3Aactivity%3A7357062890551496704&trk=public_post_follow)\n\n## Explore topics\n\n- [Sales](https://www.linkedin.com/pulse/topics/sales-s5/)\n- [Marketing](https://www.linkedin.com/pulse/topics/marketing-s2461/)\n- [IT Services](https://www.linkedin.com/pulse/topics/it-services-s57547/)\n- [Business Administration](https://www.linkedin.com/pulse/topics/business-administration-s50111/)\n- [HR Management](https://www.linkedin.com/pulse/topics/hr-management-s50359/)\n- [Engineering](https://www.linkedin.com/pulse/topics/engineering-s166/)\n- [Soft Skills](https://www.linkedin.com/pulse/topics/soft-skills-s2976/)\n- [See All](https://www.linkedin.com/pulse/topics/home/)\n\nLinkedIn\n\nNever miss a beat on the app\n\nDon’t have the app? Get it in the Microsoft Store.\n\n[Open the app](ms-windows-store://pdp/?ProductId=9WZDNCRFJ4Q7&mode=mini&cid=guest_desktop_upsell_post2)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Febarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO","id":"https://www.linkedin.com/posts/ebarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO","publishedDate":"2025-08-01T00:00:00.000Z","summary":"The LinkedIn post by Emad Barsoum announces the release of the Generating Efficient AI-centric Kernels (GEAK), an OpenAI Triton Kernel agent designed for optimizing GPU kernel writing. The GEAK utilizes inference-time scaling to produce high-performance, numerically accurate kernels, achieving significant improvements in execution accuracy and speed. Specifically, it reports an average execution accuracy of 54.89% on modified Triton benchmarks and 63.33% on ROCm benchmarks, compared to just 15% when using traditional prompting methods. The generated kernels also demonstrate an average speedup of 2.59 times over baseline performance. The post encourages the community to try out this new technology.","title":"Introducing GEAK: Efficient AI-centric Kernels for OpenAI Triton","url":"https://www.linkedin.com/posts/ebarsoum_geak-introducing-triton-kernel-ai-agent-activity-7357062890551496704-TxhO"}],"status":"completed","toolCallId":"call_R8Nbud9jkXz4xApgHL0ZkFYn","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Headline results (from the paper/blog):\n\n- Correctness\n  - TritonBench-modified (184 tasks): up to 54.89% tasks solved (all unit tests pass), versus ~15% when directly prompting frontier LLMs without the agent loop.\n  - ROCm Triton Benchmark (30 real kernels): up to 63.33% solved.\n\n- Performance\n  - On TritonBench-modified, among tasks with at least one correct GEAK kernel, the best candidate per task achieves an average speedup up to 2.59× over the provided reference implementations.\n\n- Scaling behavior\n  - Increasing inference-time budget (more parallel/sequential attempts with diversity) improves pass rates roughly log-linearly; combining parallel and sequential scaling helps further.\n\n- Ablations/techniques that mattered\n  - 1-shot retrieval (code-similarity based) and knowledge injection improved both correctness and speed.\n  - Reflexion-style debugging and the “debugging trap” guard (max_perf_debug_num) were important for escaping loops.\n  - Using the LLM as an Optimizer, guided by ranked historical timings, outperformed naïve tweak-and-retry.\n\nIf you want, I can break this down by pass@1 vs. pass@k, or by kernel category (e.g., GEMM, softmax, norms) if you’re targeting specific ops.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0449ef6a4b3bd38f0068daee3f41e4819c9726802acb626726"}},"resumableStreamId":"STREAM:7978d104-a7bf-4c64-a4b3-99ef03cbb0f7","role":"assistant","spanId":"df54306d5cfb0e21","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","timeToFirstToken":1.093,"tokens":287,"tokensPerSecond":33.944411590774685,"traceId":"9f2f8fc5e0b95ba9259122fe622815ca","updated_at":1759178295392,"userId":"google:104458561188320708185","id":"j97bnxtn7t0a4zksr8pvbt0m197rhfgz"},{"_creationTime":1759178345486.176,"_id":"j978j5y68t7n5b9g5n9ca19zen7rg7j7","attachmentIds":[],"content":"what is the effectiveness of geak against different hardwares","created_at":1759178345486,"messageId":"509a46c3-5678-4267-ab2f-9c3c4a329913","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","spanId":"a76781ef4eecf40e","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","traceId":"f6348099e78c9e83a2096fdc61c28dcf","updated_at":1759178345486,"userId":"google:104458561188320708185","id":"j978j5y68t7n5b9g5n9ca19zen7rg7j7","providerMetadata":{}},{"_creationTime":1759178345486.1763,"_id":"j972n822ymxaxfnjxqjznddd7n7rgyr0","attachmentIds":[],"byok":false,"content":"Short answer: GEAK is evaluated and tuned for AMD Instinct GPUs and Triton. It shows strong effectiveness on AMD MI300X/MI250, but the paper does not report cross-vendor results (e.g., NVIDIA). Portability to other hardware is plausible in principle but would require adapting the “knowledge pack,” retrieval set, and evaluator.\n\nDetails:\n- Hardware covered: AMD Instinct (MI300X, MI250/MI250X) under ROCm with Triton. All reported correctness and speedup numbers are on AMD GPUs.\n- Why it works well there:\n  - Prompts are injected with AMD-specific Triton tips (wavefronts, LDS usage, MFMA/WMMA layouts, block-pointer/TMA idioms, alignment constraints).\n  - Benchmarks include 30 real ROCm Triton kernels authored for AMD plus a revised TritonBench set fixed for AMD-specific issues.\n  - The optimizer/reflector reason with performance and error signals observed on AMD backends.\n\n- What’s not shown:\n  - No results on NVIDIA (CUDA/Triton-NVIDIA backend), Intel, or other accelerators.\n  - No HIP/CUDA bare-metal kernels; GEAK targets Triton kernels, not vendor-native kernels.\n\n- How portable is GEAK?\n  - Conceptually portable: the agent loop (generate → test → reflect → optimize) is hardware-agnostic.\n  - Practically requires:\n    - Swapping in hardware-specific knowledge (e.g., warps vs. wavefronts, shared memory bank rules, async copy/TMA equivalents).\n    - Updating retrieval corpus to examples optimized for the target backend.\n    - Adapting the evaluator (compiler toolchain, unit tests, timing, tolerances).\n  - Expect drop-in performance if you use AMD-tuned prompts on other hardware; effectiveness improves once you tailor those components.\n\nBottom line: GEAK is effective on AMD Instinct GPUs with Triton, achieving up to ~55–63% task pass rates and up to ~2.6× speedups over baselines there. The paper doesn’t claim results on other hardware; extending GEAK beyond AMD would need backend-specific adaptation and re-evaluation.","created_at":1759178345487,"messageId":"2fda9d1c-e4b3-42a7-931d-6c49b2e82b35","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"news","domains":null,"queries":["GEAK Triton Agent hardware generalization AMD Instinct MI300X MI250 NVIDIA H100 results 2025-07-01 arXiv:2507.23194","Geak Introducing Triton Kernel AI Agent & Evaluation Benchmarks hardware portability AMD vs NVIDIA 2025"]},"result":[{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.23194","id":"https://arxiv.org/abs/2507.23194","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses the implementation and implications of using reCAPTCHA technology on the arXiv platform. It highlights the need for such measures to prevent automated submissions and ensure the integrity of the submission process. The paper acknowledges support from the Simons Foundation and member institutions, indicating collaborative efforts in the research. For a detailed understanding, readers are encouraged to explore the full paper available on the arXiv website.","title":"Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/abs/2507.23194"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2507.23194v1","id":"https://arxiv.org/html/2507.23194v1","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage discusses the arXiv reCAPTCHA, a system designed to prevent automated submissions to the arXiv preprint repository. It outlines the challenges posed by bots and the importance of maintaining the integrity of the submission process. The document likely includes technical details about the implementation of reCAPTCHA, its effectiveness, and potential improvements. Additionally, it acknowledges support from the Simons Foundation and member institutions. For a comprehensive understanding, users are encouraged to explore the full text on the arXiv website.","title":"GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/html/2507.23194v1"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2507.23194","id":"https://arxiv.org/pdf/2507.23194","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage presents a research paper titled \"arXiv reCAPTCHA,\" which discusses the implementation and implications of reCAPTCHA technology on the arXiv platform. It explores how reCAPTCHA helps in preventing automated submissions and ensuring the integrity of user interactions. The paper likely includes technical details, methodologies, and findings related to the effectiveness of reCAPTCHA in maintaining the quality of submissions on arXiv. Additionally, it acknowledges support from the Simons Foundation and member institutions, indicating collaboration in the research. For a comprehensive understanding, readers are encouraged to access the full paper through the provided link.","title":"[PDF] Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://arxiv.org/pdf/2507.23194"},{"content":"[Skip to main content](https://rocm.blogs.amd.com/rocm.blogs.amd.com#main-content)\nBack to top\n\n`Ctrl` + `K`\n\n[ROCm blogs](https://rocm.blogs.amd.com/)\n\n# GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks\n\n## Contents\n\n# GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#geak-introducing-triton-kernel-ai-agent-evaluation-benchmarks)\n\nAugust 01, 2025 by [Jianghui Wang](https://rocm.blogs.amd.com/authors/jianghui-wang.html), [Vinay Joshi](https://rocm.blogs.amd.com/authors/vinay-joshi.html), [Saptarshi Majumder](https://rocm.blogs.amd.com/authors/saptarshi-majumder.html), [Chao Xu](https://rocm.blogs.amd.com/authors/chao-xu.html), [Bin Ding](https://rocm.blogs.amd.com/authors/bin-ding.html), [Ziqiong Liu](https://rocm.blogs.amd.com/authors/ziqiong-liu.html), [Pratik Prabhanjan Brahma](https://rocm.blogs.amd.com/authors/pratik-prabhanjan-brahma.html), [Dong Li](https://rocm.blogs.amd.com/authors/dong-li.html), [Zicheng Liu](https://rocm.blogs.amd.com/authors/zicheng-liu.html), [Emad Barsoum](https://rocm.blogs.amd.com/authors/emad-barsoum.html).\n\n10 min read. \\| 2414 total words.\n\n[Software tools & optimizations](https://rocm.blogs.amd.com/blog/category/software-tools-optimizations.html)\n\n[AI/ML](https://rocm.blogs.amd.com/blog/tag/aiml.html)\n\n[Developers](https://rocm.blogs.amd.com/developers.html), [AI](https://rocm.blogs.amd.com/ai.html)\n\nAt AMD, we are pioneering ways to accelerate AI development using AI itself, by generating accurate and efficient GPU kernels. Specifically, we are starting with the automatic generation of kernels in [Triton](https://triton-lang.org/main/index.html), an open-source Python-like language for writing parallel programming code for GPUs. Today, AMD is excited to announce (a) [Generating Efficient AI-centric Kernels (GEAK)](https://github.com/AMD-AIG-AIMA/GEAK-Agent) for AMD GPUs, and results on (b) two Triton kernel evaluation benchmarks, where we show how AI agents can perform inference-time scaling with frontier LLMs to generate accurate and efficient kernels for AMD Instinct™ GPUs like MI250X and MI300X.\n\n**Key Takeaways:**\n\n- Announcing [GEAK Triton kernel generation framework](https://github.com/AMD-AIG-AIMA/GEAK-Agent), a library of various types of agentic AI and scaling methods to automatically produce Triton kernels from just brief instructions of its intended functionality.\n\n- Announcing corresponding [GEAK Triton Kernel Evaluation Benchmarks](https://github.com/AMD-AIG-AIMA/GEAK-eval), consisting of a modified set of 184 kernels originally sourced from [TritonBench-G](https://arxiv.org/abs/2502.14752) \\[1\\] and an additional set of 30 kernels from various open [ROCm repositories](https://github.com/ROCm).\n\n- Execution accuracy up to 54.89% on TritonBench-modified and 63.33% on ROCm benchmark (as compared to around 15% when directly prompting frontier LLMs) in producing correct kernels that pass all the unit tests.\n\n- Average Speedup of up to 2.59X with the AI-generated kernels over reference kernels from the TritonBench-modified benchmark.\n\n\nFigure 1. Overview of the GEAK kernel code generation AI agent [#](https://rocm.blogs.amd.com/rocm.blogs.amd.com#id1)\n\nAs shown in Figure 1, the agentic AI system comprises four core modules: a **generator**, a **reflector**, an **evaluator**, and an **optimizer**.\n\n- The **generator** produces code based on user query and contextual information.\n\n- The **evaluator** follows a cascaded design: it first performs a functionality test to verify correctness. If the code fails this test, the corresponding error trace is fed back to the reflector for further analysis. If the code passes, the evaluator proceeds to assess its performance, including latency and memory efficiency.\n\n- If the generated code fails to execute correctly, the **reflector** analyzes both the code and the resulting error trace to identify potential issues.\n\n- Finally, the **optimizer** takes as input the functionally correct code and formulates strategies to enhance its performance with respect to latency and computational efficiency.\n\n\nTo improve the performance of our agent, we employ some composable and configurable techniques as mentioned below:\n\n- **1-shot Prompting**\n\n\nTo enable 1-shot prompting, the most similar Triton code from existing datasets is used in the prompt. The datasets used for 1-shot prompt do not overlap with the benchmark. We’ve observed that 1-shot sample retrieval from datasets is most effective when using code similarity rather than instruction similarity.\n\n- **Knowledge Injection**\n\n\nWe enhance the prompt with domain-specific knowledge on writing efficient Triton kernels, including detailed hardware specifications. This incorporation of low-level optimization principles significantly improves the accuracy and quality of LLM-generated code.\n\n- **Reflexion**\n\n\nTo enhance the system’s self-correcting capabilities, we leverage a reflexion \\[3\\] module that supports introspective debugging and iterative refinement. When an LLM-generated kernel fails the functionality test, the resulting error trace is provided as feedback to the reflector for further analysis and correction. The agent is tasked with analyzing the cause of failure and proposing a corresponding solution. We scale the number of iterations to improve kernel generation.\n\n- **LLM selection for kernel agent**\n\n\nWe have explored the use of multiple LLMs like GPT-4.1, GPT-o1, and Gemini 2.5 Pro. We observed notable variations in the output produced by different models, indicating that the capability of the underlying LLM can substantially influence the results. In this blog, we present results with GPT-4.1 only but the GEAK codebase allows developers to try different LLMs as per their choice.\n\n- **LLM as Optimizer**\n\n\nThe optimizer is tasked with identifying potential optimization directions based on previous code generations and their corresponding performance metrics. These historical records, which include the generated code and associated performance results, are sorted in ascending order of performance. This structured presentation \\[2\\] helps guide the LLM toward proposing more effective optimization strategies.\n\n- **Debugging Trap**\n\n\nWhen LLM’s generated code has bugs, the error trace is provided to the Reflector for feedback generation for further correction. However, we’ve observed that sometimes code can undergo several reflection cycles while still being plagued by the same bug, which is what we refer to as the debugging trap. To prevent the agent from getting stuck in a debugging trap, we impose a limit on the number of debugging attempts per code snippet using a parameter max\\_perf\\_debug\\_num. If the code continues to fail after reaching this threshold, the agent is required to discard the current approach and generate a new strategy along with fresh code.\n\n- **Parallel Scaling**\n\n\nTo ensure reliable kernel generation, we run multiple instances of GEAK multiple times in parallel and independently. To introduce diversity in the generated code, the LLM output is sampled with the parameter temperature set to 1. Our experiments show that such diversity in generation can yield correct and faster kernels that are otherwise underexplored. Moreover, further experiments indicate that combining sequential and parallel scaling yields additional performance improvements. To obtain unbiased estimates of accuracy, we use the pass@k metric to estimate it \\[4\\].\n\n## Triton Kernel Evaluation Benchmark [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#triton-kernel-evaluation-benchmark)\n\nWe conducted experiments using two benchmarks methods:\n\n**1\\. TritonBench-Modified Benchmark (184 Kernels):**\n\nThe Triton kernels and corresponding evaluation codebase were adapted from [TritonBench](https://github.com/thunlp/TritonBench) \\[1\\] with several modifications made:\n\n- Out of 184 kernels in TritonBench-G, we identified and corrected errors in 37 kernels that were encountered when running on AMD GPUs. These errors include shared memory errors, invalid HIP arguments, and ModuleNotFound errors.\n\n- We found that several kernels in the original TritonBench repository missed necessary call functions, which resulted in the accuracy checking code comparing empty strings as output. We corrected this and ensured that the correct common seed was used during test.\n\n- For some kernels, testbenches were written inconsistently that produced unexpected outputs. We have fixed these kernels to have consistent testbenches.\n\n\n**2\\. ROCm Repository Triton Benchmark (30 Kernels):**\n\nThe benchmark contains 30 kernels chosen from various open ROCm repositories that have been written by Triton expert engineers and released publicly to help enable the ecosystem around running AI workloads efficiently on AMD GPUs. The entire list of kernels is outlined in Table 1. We also took assistance from frontier LLMs to put these kernels, as well as their corresponding unit tests, in the same format as the TritonBench-modified framework. It allows us to compare the accuracy and efficiency of our agent consistently.\n\n| **ROCm Kernel** | **Repository link** |\n| --- | --- |\n| 1\\. test\\_tma\\_store\\_gemm 2. moe-gemm 3. layernorm 4. triton\\_multreduce\\_matmul\\_kernel.py 5. multreduce\\_matmul\\_dot\\_kernel.py 6. gemm 7. test\\_block\\_pointer\\_matmul 8. test\\_block\\_copy 9. test\\_add\\_kernel 10. softmax | [ROCm/triton](https://github.com/ROCm/triton/tree/main_perf) |\n| 1\\. mnsnorm\\_fwd 2. rmsnorm\\_bwd 3. test\\_chained\\_dot\\_fp8 4. test\\_matmul\\_MXFP 5. test\\_gemm\\_fusion | [ROCm/aiter](https://github.com/ROCm/aiter) |\n| 1\\. test\\_triton\\_swizzle2d 2. test\\_triton\\_sort 3. test\\_triton\\_flip 4. test\\_reverse\\_range | [ROCm/aotriton](https://github.com/ROCm/aotriton) |\n| 1\\. test\\_load\\_reduce.py 2. test\\_chained\\_matmul | [ROCm/vllm](https://github.com/ROCm/vllm) |\n| 1\\. test\\_random\\_int 2. test\\_randn 3. test\\_kernel\\_sub 4. test\\_kernel\\_dot 5. test\\_batched","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Frocm.blogs.amd.com%2Fsoftware-tools-optimization%2Ftriton-kernel-ai%2FREADME.html","id":"https://rocm.blogs.amd.com/software-tools-optimization/triton-kernel-ai/README.html","publishedDate":"2025-08-01T00:00:00.000Z","summary":"The webpage introduces GEAK, a new AI agent framework developed by AMD for generating efficient GPU kernels using the Triton programming language. The framework aims to enhance AI development by automating kernel generation from brief functional instructions. Key highlights include:\n\n- **GEAK Framework**: A library that utilizes various AI methods to produce Triton kernels efficiently.\n- **Evaluation Benchmarks**: The framework includes two benchmark sets, demonstrating significant improvements in execution accuracy (up to 63.33%) and speed (average speedup of 2.59X) compared to traditional methods.\n- **Core Modules**: The AI system consists of four modules: a generator for code creation, an evaluator for functionality and performance testing, a reflector for error analysis, and an optimizer for refining the code.\n\nOverall, GEAK represents a significant advancement in automating GPU kernel development, particularly for AMD's Instinct GPUs.","title":"GEAK: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://rocm.blogs.amd.com/software-tools-optimization/triton-kernel-ai/README.html"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fartificialanalysis.ai%2Farticles%2Findependent-analysis-of-leading-gpus-amd-nvidia","id":"https://artificialanalysis.ai/articles/independent-analysis-of-leading-gpus-amd-nvidia","publishedDate":null,"summary":"Failed to fetch content","title":"AMD MI300X, NVIDIA H100, NVIDIA H200 - Artificial Analysis","url":"https://artificialanalysis.ai/articles/independent-analysis-of-leading-gpus-amd-nvidia"},{"content":"## AI-Powered Library for Researchers\n\n### Track research interests, scroll daily paper feeds with AI summary, and chat with bulk of files.\n\n1. [Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks](https://arxiv.org/abs/2507.23194)\n\ncs.CLcs.LGcs.AI01 Aug 2025\n\nJianghui Wang, Vinay Joshi, Saptarshi Majumder, Xu Chao, Bin Ding, Ziqiong Liu, Pratik Prabhanjan Brahma, Dong Li, Zicheng Liu, Emad Barsoum\n\nAdvanced Micro Devices, Inc. (AMD)\n\nThe demand for AI-generated GPU kernels is rapidly growing, influenced by the need for scalable, hardware-optimized solutions in both industry and academia. As deep learning workloads grow in complexity and diversity, it is imperative to automate low-level kernel development to meet performance and productivity demands. Major cloud providers, semiconductor companies, and research institutions are now investing heavily in AI-driven code generation for GPUs, aiming to reduce manual optimization efforts while achieving near-expert performance on hardware like AMD MI300X. The Triton language, a Python-based DSL for GPU programming, has emerged as a popular target for such AI-generated kernels due to its balance of performance and ease-of-coding. In this work, we present an evaluation suite for Triton-based GPU kernels and GEAK (Generating Efficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs to generate performant Triton code specifically for AMD GPUs, including the AMD MI300X and MI250. GEAK leverages inference-time compute scaling to produce Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style feedback mechanisms. On two evaluation benchmarks, GEAK significantly outperformed the baselines of directly prompting frontier LLMs as well as Reflexion-based generation pipelines by achieving correctness up to $63$% and execution speed up of up to $2.59$X. These results highlight the promise of GEAK-like agentic code generation for accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance.\n\nIllustration of GEAK pipeline.\n\n# Why ChatPaper\n\n- Interest-Driven Paper Curation\n\nDescribe your interests as you like, be it keywords or sentence, you’ll get relevant papers every day via AI semantic matching.\n\n- Top Conferences, One-Stop\n\nIJCAI, ICML, CVPR, KDD... Access papers from top AI conferences, all in one tool, for free.\n\n- Paper Management Made Easy\n\nKeep your research organized with our bookmarking feature. Build a well-structured knowledge hub at your fingertips.\n\n- Chat With Any Paper\n\nGot questions? Ask in ChatDOC with one single click. Use AI to pull data, clarify terms, and verify facts with our precise word-level tracing feature.\n\n\n# See how it works\n\n## Watch now 1 min\n\nGet Started","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fchatpaper.com%2Fpaper%2F172041","id":"https://chatpaper.com/paper/172041","publishedDate":"2025-07-31T00:00:00.000Z","summary":"The webpage introduces GEAK (Generating Efficient AI-centric GPU Kernels), a framework designed to automate the generation of GPU kernels using the Triton programming language, particularly for AMD GPUs like the MI300X and MI250. As the demand for AI-generated GPU kernels increases, GEAK utilizes advanced large language models (LLMs) to produce efficient Triton code, significantly outperforming traditional methods in terms of correctness and execution speed. The evaluation benchmarks demonstrate GEAK's effectiveness, achieving up to 63% correctness and a speedup of 2.59 times compared to baseline approaches. The page also highlights features of ChatPaper, an AI-powered library for researchers that curates papers based on user interests, provides access to top AI conference papers, and offers tools for paper management and interaction.","title":"Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks","url":"https://chatpaper.com/paper/172041"},{"content":"[Skip to main content](https://rocm.blogs.amd.com/rocm.blogs.amd.com#main-content)\nBack to top\n\n`Ctrl` + `K`\n\n[ROCm blogs](https://rocm.blogs.amd.com/)\n\n# Unlock Peak Performance on AMD GPUs with Triton Kernel Optimizations\n\n## Contents\n\n# Unlock Peak Performance on AMD GPUs with Triton Kernel Optimizations [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#unlock-peak-performance-on-amd-gpus-with-triton-kernel-optimizations)\n\nApril 10, 2025 by [Ning Zhang](https://rocm.blogs.amd.com/authors/ning-zhang.html), [George Wang](https://rocm.blogs.amd.com/authors/george-wang.html).\n\n4 min read. \\| 1007 total words.\n\n[Software tools & optimizations](https://rocm.blogs.amd.com/blog/category/software-tools-optimizations.html)\n\n[AI/ML](https://rocm.blogs.amd.com/blog/tag/aiml.html)\n\n[AI](https://rocm.blogs.amd.com/ai.html)\n\n[Triton](https://github.com/triton-lang/triton) is a domain-specific programming language designed to simplify GPU programming for high-performance tasks, particularly in AI applications. It provides an open-source environment that enables users to write high-level Triton code with greater productivity compared to Nvidia CUDA or AMD HIP. The Triton compiler translates Triton code into optimized GPUs instructions, effectively compiling tensor operations into low-level GPU code. It achieves high efficiency through multiple optimizations passes and leverages the underlying architecture of the GPU. To optimize GPU performance, it is important to have a solid understanding of the Triton compiler and the role it plays in kernel performance. In this blog, we will deep dive into the AMD Triton compiler, introduce Triton kernel compilation, and provide insights on how to create an efficient Triton kernel code.\n\n## AMD Triton Compilation Flow [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#amd-triton-compilation-flow)\n\nTriton compiler includes 3 key modules, Frontend, Optimizer, and Backend machine code generation, as shown in Figure 1:\n\nFigure 1. Triton Complier Block Diagram [#](https://rocm.blogs.amd.com/rocm.blogs.amd.com#id1)\n\nFrontend module walks the abstract syntax tree (AST) of python Triton kernel function, which includes the @triton.jit decorator to create the Triton Intermediate Representation (Triton-IR). The Optimizer module applies multiple optimizations and converts the Triton-IR into Triton-GPU IR (Triton-TTGIR) and then LLVM-IR. At this stage, optimization includes two core components: layout and passes, which are hardware dependent.\n\n1. When generating Triton GPU IR, the hardware compute capability is included along with layout. Layout represents memory optimizations details on how the tensors are distributed to cores and warps for AMD GPUs on wavefronts. Currently, Triton supports multiple layouts like blocked (each warp owns a contiguous portion of the tensor), slice (restructures and distributes a tensor along a dimension), dot\\_op (optimized layout for block matrix product), shared (indicates GPU shared memory, amd\\_mfma (for AMD MFMA matrix core), amd\\_wmma (for AMD WMMA matrix core), and linear layout (to unify layouts within and across backends).\n\n2. Various optimization passes are applied to the IRs. These optimizations include loop unrolling, memory access optimizations, constant folding, CSE, etc. to improve the performance of the generated codes. Optimization passes can be divided into 3 categories:\n\n1. MLIR general optimization passes, like CSE, DCE, Inlining, etc.\n\n2. GPU specific optimization passes, like Pipeline, Prefetch, Matmul accelerate, Coalesce, etc.\n\n3. Vender specific GPU optimization passes, for example Nvidia provides TMA, Async Dot, etc. AMD provides OptimizeLDSUsage, BlockPingpong, etc.\n\nThe final stages of the Triton compilation process lower LLVM-IR to a device specific binary, which is done by backend machine code generation modules. Triton leverages LLVM to perform further optimizations and facilitate the generation of machine code. On AMD platforms, the AMD LLVM compiler module first translates the LLVM-IR to AMDGCN assembly codes then AMDGCN codes compile into a hsaco binary, which uses AMD JIT compiler.\n\n## AMD Triton Compilation Flow Example [\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\#amd-triton-compilation-flow-example)\n\nLet’s walk the triton compilation flow implementation on AMD GPUs using an example of [Vector Add](https://github.com/triton-lang/triton/tree/main/python/tutorials)\n\n1. Frontend module of AMD Triton Compiler\n\n\nIn the frontend module codes of Triton compiler, the Trition Kernel function converts into Triton-IR. In this example, Triton kernel function is add\\_kernel, which is marked by @triton.jit decorator. [JIT decorator function](https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L782) first checks the TRITON\\_INTERPRET environment variable , if True, InterpretedFunction will be invoked, which is used to run Triton kernel in Interpreter mode; otherwise, it will run into JITFunction, where we will compile and run Triton kernel on real device.\n\nThe kernel compilation entry point is [Triton compile function](https://github.com/triton-lang/triton/blob/main/python/triton/compiler/compiler.py#L214), which is invoked with the target device and compilation options information. It will create the kernel cache manager, launch the compilation pipeline, and populate the kernel meta data. It also loads the backend specific dialects, e.g. TritonAMDGPUDialect on the AMD platform, and backend specific LLVM modules to handle LLVM-IR compilation. When all are ready, it will invoke ast\\_to\\_ttir function to generate Triton-IR file of this kernel.\n\n1. Optimizer module of AMD Triton Compiler\n\n\n```\n i. Triton-IR optimization\n\n```\n\nOn AMD platforms, Triton-IR optimization passes are defined in [make\\_ttir function](https://github.com/triton-lang/triton/blob/main/third_party/amd/backend/compiler.py.).The optimization passes at this stage are hardware independent, like inline optimization,\nCSE(Common subexpression elimination), Canonicalization, DCE(Dead code elimination), LICM(Loop Invariant Code Motion), and Loop Unrolling.\n\n```\n ii. Triton-GPU IR optimization\n\n```\n\nOn AMD platforms, Triton-IR optimization passes are defined in make\\_ttgir function , and are designed for GPU platform to boost its performance. Based on AMD GPU hardware features and kernel optimization experience, AMD also developed some AMD GPU specific optimization passes for this stage, which are listed in table 1, below.\n\n| TTGIR AMD Specific Optimization Pass | Description |\n| --- | --- |\n| AMD GPU accelerate Matmul | Optimize the input/output layout of \\`dot\\` instruction to make them compatible AMD matrix cores |\n| AMD GPU Optimize Epilogue | Store accumulators directly without going through SMEM in epilogue. |\n| AMD GPU Stream Pipeline | Pipeline global loads through registers to shared memory while computing on previous tile |\n| AMD GPU insert instruction sched hints | Insert instruction scheduling hints after the dot ops in the main loop |\n| AMD GPU Reorder Instructions | This pass reorder instructions, so as to (1) decrease register pressure (e.g., by moving conversions from shared memory before their first use) and (2) promote LLVM instruction order more friendly to AMDGCN assembly generation. |\n| AMD GPU Block Ping pong | This pass reorder instructions to interleave instructions from two warps on the same SIMD unit. We call this a ping-pong scheduling pattern, where two warps run concurrently in the synchronized fashion.This block ping-pong pattern could be beneficial under few conditions including occupancy and number of warps |\n| AMD GPU Canonicalize Pointers | Canonicalize pointers means to rewrite pointers passed to load/store operation as a \\` \\` pair.This pass pushes all the constant pointer arithmetic on a scalar basePtr, while all the vector pointer arithmetic to a vector offset.when we load the data, we splat the scalar pointer, add the “variable” offset and then issue the load |\n| AMD GPU Convert To Buffer Ops | This pass converts memory and atomic operations (e.g., tt.load/tt.store/tt.atomic\\_rmw) to amdgpu buffer operations, if possible |\n\nTable 1. AMD GPU-IR specific optimization\n\nThese optimization passes first convert Triton-IR to Triton-GPU IR. At this step, layout information is added into the IR. In this example, the tensors are represented as a #blocked layout.\n\nIf we try another [Triton Matrix Multiplication](https://github.com/ROCm/triton/blob/main_perf/python/tutorials/03-matrix-multiplication.py) example, the shared memory access is introduced by the above optimization passes to boost performance, which is a popular optimization solution for Matrix Multiplication, and amd\\_mfma layout is also used, which is designed for AMD MFMA accelerator.\n\n```\n iii. LLVM-IR optimization\n\n```\n\nOn AMD platform, optimization passes are defined in make\\_llir function. This function includes two components: IR level optimization and AMD GPU LLVM compiler configuration.\nFor IR-level optimization, AMD GPU specific optimization passes are included, like LDS/shared memory related optimization, and LLVM-IR level generic optimization, as shown in table 2.\n\n| LLVM-IR AMD Specific Optimization Pass | Description |\n| --- | --- |\n| Decompose Unsupported AMD Conversions | Decompose conversions that are not supported by AMD GPU during the step of converting TritonGPU-IR to LLVM-IR. |\n| Optimize AMD lds/shared memory usage | Minimize LDS usage: find operations with peak LDS consumption , and then try to transform candidate operations to fit them into LDS |\n| AMD GPU Lower Instruction Sched Hints | Lower instruction scheduling hints to LLVM intrinsics |\n| AMD GPU Convert Builtin Func To LLVM | Convert the Builtin Functions to LLVM |\n\nTable 2. LLVM-IR AMD Specific Optimization Pass\n\nFor AMD GPU LLVM compiler configuration, the LLVM target library and context are initialized, sets various control constants on the LLVM module, sets calling convention for A","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Frocm.blogs.amd.com%2Fsoftware-tools-optimization%2Fkernel-development-optimizations-with-triton-on-%2FREADME.html","id":"https://rocm.blogs.amd.com/software-tools-optimization/kernel-development-optimizations-with-triton-on-/README.html","publishedDate":"2025-04-10T00:00:00.000Z","summary":"The webpage discusses optimizing performance on AMD GPUs using the Triton programming language, which simplifies GPU programming for high-performance tasks, especially in AI applications. It highlights the Triton compiler's three main modules: Frontend, Optimizer, and Backend, which work together to convert high-level Triton code into optimized GPU instructions. Key features include the creation of Triton Intermediate Representation (Triton-IR), various memory layouts for tensor distribution, and multiple optimization passes that enhance performance. The article emphasizes the importance of understanding the Triton compiler for effective kernel performance and provides insights into creating efficient Triton kernel code tailored for AMD GPUs.","title":"Unlock Peak Performance on AMD GPUs with Triton Kernel ...","url":"https://rocm.blogs.amd.com/software-tools-optimization/kernel-development-optimizations-with-triton-on-/README.html"}],"status":"completed","toolCallId":"call_uB59Dq91qMDCC0aT0sYVHJ6D","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Short answer: GEAK is evaluated and tuned for AMD Instinct GPUs and Triton. It shows strong effectiveness on AMD MI300X/MI250, but the paper does not report cross-vendor results (e.g., NVIDIA). Portability to other hardware is plausible in principle but would require adapting the “knowledge pack,” retrieval set, and evaluator.\n\nDetails:\n- Hardware covered: AMD Instinct (MI300X, MI250/MI250X) under ROCm with Triton. All reported correctness and speedup numbers are on AMD GPUs.\n- Why it works well there:\n  - Prompts are injected with AMD-specific Triton tips (wavefronts, LDS usage, MFMA/WMMA layouts, block-pointer/TMA idioms, alignment constraints).\n  - Benchmarks include 30 real ROCm Triton kernels authored for AMD plus a revised TritonBench set fixed for AMD-specific issues.\n  - The optimizer/reflector reason with performance and error signals observed on AMD backends.\n\n- What’s not shown:\n  - No results on NVIDIA (CUDA/Triton-NVIDIA backend), Intel, or other accelerators.\n  - No HIP/CUDA bare-metal kernels; GEAK targets Triton kernels, not vendor-native kernels.\n\n- How portable is GEAK?\n  - Conceptually portable: the agent loop (generate → test → reflect → optimize) is hardware-agnostic.\n  - Practically requires:\n    - Swapping in hardware-specific knowledge (e.g., warps vs. wavefronts, shared memory bank rules, async copy/TMA equivalents).\n    - Updating retrieval corpus to examples optimized for the target backend.\n    - Adapting the evaluator (compiler toolchain, unit tests, timing, tolerances).\n  - Expect drop-in performance if you use AMD-tuned prompts on other hardware; effectiveness improves once you tailor those components.\n\nBottom line: GEAK is effective on AMD Instinct GPUs with Triton, achieving up to ~55–63% task pass rates and up to ~2.6× speedups over baselines there. The paper doesn’t claim results on other hardware; extending GEAK beyond AMD would need backend-specific adaptation and re-evaluation.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0ce994d5b6c96fcf0068daee74e54c819493ec08cda9dd9bcb"}},"resumableStreamId":"STREAM:de7848a6-990d-43dd-b643-1a9acc99696c","role":"assistant","spanId":"a76781ef4eecf40e","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","timeToFirstToken":1.335,"tokens":452,"tokensPerSecond":31.44129104062326,"traceId":"f6348099e78c9e83a2096fdc61c28dcf","updated_at":1759178345487,"userId":"google:104458561188320708185","id":"j972n822ymxaxfnjxqjznddd7n7rgyr0"},{"_creationTime":1759178512497.3096,"_id":"j97eh0nva9q7c51r31gr159arx7rhqb1","attachmentIds":[],"content":"how does it compare the manually coded kernels?","created_at":1759178512497,"messageId":"ee370b40-6b62-4180-a745-dbc2cf250e31","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","spanId":"cf1066e2b570414a","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","traceId":"40b8f74a5075002d4c3d45a65798bc07","updated_at":1759178512497,"userId":"google:104458561188320708185","id":"j97eh0nva9q7c51r31gr159arx7rhqb1","providerMetadata":{}},{"_creationTime":1759178512497.3098,"_id":"j975gjqwh29xghfy7y4per35597rhafx","attachmentIds":[],"byok":false,"content":"Short answer: When GEAK produces a correct kernel, its performance is often competitive with strong baselines but does not universally beat expert hand-tuned kernels. It can match or exceed reference implementations on many tasks (average best-case speedups up to ~2.6× over provided baselines in their benchmark), but the paper does not claim consistent wins over top-tier, manually optimized expert kernels across the board.\n\nMore detail:\n- Baselines used: The paper compares against reference implementations bundled with the benchmarks (PyTorch/Triton baselines and ROCm Triton kernels). These are reasonable, often solid, but not necessarily the absolute fastest hand-tuned kernels possible for each task and GPU.\n- Outcomes:\n  - Correctness: GEAK reaches ~55–63% pass rates on the two suites, substantially higher than prompting an LLM directly without the agent loop.\n  - Performance among solved tasks: The best GEAK variant per task achieves notable speedups over the references (average up to ~2.59× on TritonBench-revised). This shows the agent can find meaningful optimizations beyond straightforward baselines.\n- Versus expert kernels: Truly expert-crafted kernels (deeply tuned for microarchitecture, occupancy, bank conflicts, async pipelines, etc.) are a high bar. The paper does not provide a head-to-head against such state-of-the-art, hand-optimized kernels for each operation; consequently, it does not claim systematic superiority there. In practice:\n  - GEAK sometimes approaches expert-level performance on structurally simpler ops (e.g., certain matmul variants, pointwise fusions, reductions).\n  - It tends to lag experts on control-flow-heavy, numerically delicate, or microarchitecturally intricate kernels that rely on specialized tricks (e.g., fine-grained prefetch/async orchestration, register-pressure choreography).\n- Practical takeaway:\n  - GEAK can get you from a spec to a correct, competitive Triton kernel quickly, and often beats “reference” code.\n  - If you need absolute peak performance, an expert pass may still squeeze out more, but GEAK can serve as a strong starting point or even a final solution for many workloads.\n\nIf you share a specific op (e.g., layernorm, softmax, attention, custom fused op), I can outline where GEAK is likely to match references, where it might trail an expert kernel, and what optimization levers to try next.","created_at":1759178512498,"messageId":"5dbfdc2f-d1e1-4e0e-817d-04d1ed0a0f7f","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Short answer: When GEAK produces a correct kernel, its performance is often competitive with strong baselines but does not universally beat expert hand-tuned kernels. It can match or exceed reference implementations on many tasks (average best-case speedups up to ~2.6× over provided baselines in their benchmark), but the paper does not claim consistent wins over top-tier, manually optimized expert kernels across the board.\n\nMore detail:\n- Baselines used: The paper compares against reference implementations bundled with the benchmarks (PyTorch/Triton baselines and ROCm Triton kernels). These are reasonable, often solid, but not necessarily the absolute fastest hand-tuned kernels possible for each task and GPU.\n- Outcomes:\n  - Correctness: GEAK reaches ~55–63% pass rates on the two suites, substantially higher than prompting an LLM directly without the agent loop.\n  - Performance among solved tasks: The best GEAK variant per task achieves notable speedups over the references (average up to ~2.59× on TritonBench-revised). This shows the agent can find meaningful optimizations beyond straightforward baselines.\n- Versus expert kernels: Truly expert-crafted kernels (deeply tuned for microarchitecture, occupancy, bank conflicts, async pipelines, etc.) are a high bar. The paper does not provide a head-to-head against such state-of-the-art, hand-optimized kernels for each operation; consequently, it does not claim systematic superiority there. In practice:\n  - GEAK sometimes approaches expert-level performance on structurally simpler ops (e.g., certain matmul variants, pointwise fusions, reductions).\n  - It tends to lag experts on control-flow-heavy, numerically delicate, or microarchitecturally intricate kernels that rely on specialized tricks (e.g., fine-grained prefetch/async orchestration, register-pressure choreography).\n- Practical takeaway:\n  - GEAK can get you from a spec to a correct, competitive Triton kernel quickly, and often beats “reference” code.\n  - If you need absolute peak performance, an expert pass may still squeeze out more, but GEAK can serve as a strong starting point or even a final solution for many workloads.\n\nIf you share a specific op (e.g., layernorm, softmax, attention, custom fused op), I can outline where GEAK is likely to match references, where it might trail an expert kernel, and what optimization levers to try next.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_04b109a02a6b25560068daef111c4081a1931389dae3a6ac96"}},"resumableStreamId":"STREAM:128255ac-d455-466e-81cb-79ba1bcd4ffb","role":"assistant","spanId":"cf1066e2b570414a","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","timeToFirstToken":0.985,"tokens":498,"tokensPerSecond":55.99910041605757,"traceId":"40b8f74a5075002d4c3d45a65798bc07","updated_at":1759178512498,"userId":"google:104458561188320708185","id":"j975gjqwh29xghfy7y4per35597rhafx"},{"_creationTime":1759178538854.4492,"_id":"j9794310s49snsm7evzmdkxw117rh9qy","attachmentIds":[],"content":"a basic matmult","created_at":1759178538854,"messageId":"c610e65a-3c28-4a4c-8854-a74ef5f58e81","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","spanId":"3998a81a75f18809","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","traceId":"1cb5765fc92d468cef89a8b0caadf04e","updated_at":1759178538854,"userId":"google:104458561188320708185","id":"j9794310s49snsm7evzmdkxw117rh9qy","providerMetadata":{}},{"_creationTime":1759178538854.4495,"_id":"j979r46rx9rtfdfnfny7hqk5hn7rga6f","attachmentIds":[],"byok":false,"content":"For a basic matmul, here’s what to expect from GEAK versus manually coded kernels:\n\n- Correctness likelihood\n  - High. Matmul is structurally regular and well-covered in Triton examples and retrieval corpora, so GEAK’s generate→test→reflect loop typically converges.\n\n- Performance versus reference baselines\n  - GEAK often matches or exceeds straightforward baselines (e.g., simple Triton or PyTorch fallback) by selecting good tile sizes and enabling vectorized loads, pipelining, and appropriate num_warps/num_stages.\n  - In the paper’s aggregate, GEAK’s best passing variants achieved up to ~2.6× average speedup over provided references across solved tasks; matmul-like kernels are a strong category for the agent.\n\n- Versus expert hand-tuned matmul\n  - Close but not guaranteed to win. Expert kernels that are meticulously tuned for a specific AMD Instinct GPU (e.g., MFMA-optimized tile shapes, carefully balanced register pressure, LDS bank-aware layouts, and well-orchestrated async copies/persistent threads) can still edge out GEAK.\n  - GEAK can approach expert performance, especially for common FP16/FP32 GEMM shapes, but may leave a gap for peak, microarchitectural tricks.\n\nWhat GEAK typically does for matmul\n- Chooses tile sizes: tries configurations like BLOCK_M/BLOCK_N/BLOCK_K ∈ {64–256} with adaptive num_warps and num_stages.\n- Vectorizes memory: aligns and uses `tl.load`/`tl.store` with proper masks; encourages contiguous/coalesced access.\n- Uses LDS tiling: stages A/B tiles in shared memory to increase reuse; masks edges correctly.\n- Software pipelining: overlaps loads and compute; sometimes persistent-thread style to reduce launch overhead on many small tiles.\n- Searches the space: runs multiple variants in parallel, keeps the fastest passing one.\n\nWhere experts might still do better\n- MFMA/WMMA pattern alignment and tile shapes tailored to the exact wavefront and matrix instruction shapes.\n- Fine-grained control of register usage to avoid spills at large BLOCK_K.\n- Bank-conflict–free LDS layouts for tricky leading dimensions, and carefully tuned prefetch distances.\n- Highly shape-specialized kernels (e.g., very skinny or tall matrices) or mixed-precision accumulations with numerics constraints.\n\nIf you share your matmul specs (M, N, K sizes; dtype; transposition; batch; target GPU), I can propose a GEAK-style tiling/launch plan and the first set of variants I’d explore, along with which metrics to watch to close the gap to expert levels.","created_at":1759178538855,"messageId":"e4cdc9ba-3cb6-45ba-aaab-dd9b32f6025b","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"For a basic matmul, here’s what to expect from GEAK versus manually coded kernels:\n\n- Correctness likelihood\n  - High. Matmul is structurally regular and well-covered in Triton examples and retrieval corpora, so GEAK’s generate→test→reflect loop typically converges.\n\n- Performance versus reference baselines\n  - GEAK often matches or exceeds straightforward baselines (e.g., simple Triton or PyTorch fallback) by selecting good tile sizes and enabling vectorized loads, pipelining, and appropriate num_warps/num_stages.\n  - In the paper’s aggregate, GEAK’s best passing variants achieved up to ~2.6× average speedup over provided references across solved tasks; matmul-like kernels are a strong category for the agent.\n\n- Versus expert hand-tuned matmul\n  - Close but not guaranteed to win. Expert kernels that are meticulously tuned for a specific AMD Instinct GPU (e.g., MFMA-optimized tile shapes, carefully balanced register pressure, LDS bank-aware layouts, and well-orchestrated async copies/persistent threads) can still edge out GEAK.\n  - GEAK can approach expert performance, especially for common FP16/FP32 GEMM shapes, but may leave a gap for peak, microarchitectural tricks.\n\nWhat GEAK typically does for matmul\n- Chooses tile sizes: tries configurations like BLOCK_M/BLOCK_N/BLOCK_K ∈ {64–256} with adaptive num_warps and num_stages.\n- Vectorizes memory: aligns and uses `tl.load`/`tl.store` with proper masks; encourages contiguous/coalesced access.\n- Uses LDS tiling: stages A/B tiles in shared memory to increase reuse; masks edges correctly.\n- Software pipelining: overlaps loads and compute; sometimes persistent-thread style to reduce launch overhead on many small tiles.\n- Searches the space: runs multiple variants in parallel, keeps the fastest passing one.\n\nWhere experts might still do better\n- MFMA/WMMA pattern alignment and tile shapes tailored to the exact wavefront and matrix instruction shapes.\n- Fine-grained control of register usage to avoid spills at large BLOCK_K.\n- Bank-conflict–free LDS layouts for tricky leading dimensions, and carefully tuned prefetch distances.\n- Highly shape-specialized kernels (e.g., very skinny or tall matrices) or mixed-precision accumulations with numerics constraints.\n\nIf you share your matmul specs (M, N, K sizes; dtype; transposition; batch; target GPU), I can propose a GEAK-style tiling/launch plan and the first set of variants I’d explore, along with which metrics to watch to close the gap to expert levels.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0eb9f660fb6284490068daef2b89f4819ca692573249b14e98"}},"resumableStreamId":"STREAM:7fc65933-f6b1-4dec-a601-4d61316548b3","role":"assistant","spanId":"3998a81a75f18809","status":"done","threadId":"47a05711-92a5-4fe0-aca3-a5a9532b3e87","timeToFirstToken":2.37,"tokens":558,"tokensPerSecond":45.78273711847719,"traceId":"1cb5765fc92d468cef89a8b0caadf04e","updated_at":1759178538855,"userId":"google:104458561188320708185","id":"j979r46rx9rtfdfnfny7hqk5hn7rga6f"}],"version":"6.0.5"}