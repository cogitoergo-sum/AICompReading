{"threads":[{"_creationTime":1758568730153.5679,"_id":"jd742bfgtmxy5fvjjqk6xckn0x7r2j92","createdAt":1758568730153,"generationStatus":"completed","lastMessageAt":1758569376097,"model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"pinned":false,"threadId":"1dec2aab-3170-4af0-b945-458c9d6e802d","title":"PyTorch API Benchmarking Explanation","updatedAt":1758569376097,"userId":"google:104458561188320708185","userSetTitle":false,"visibility":"visible","id":"1dec2aab-3170-4af0-b945-458c9d6e802d","last_message_at":1758569376097,"created_at":1758568730153,"updated_at":1758569376097,"status":"completed","user_edited_title":false}],"messages":[{"_creationTime":1758568730153.568,"_id":"j97av8580hrbw61q224706h28d7r3b8s","attachmentIds":[],"content":"Explain this paper to me in depth using the Feynman technique, as if you were its author.\n\nBenchmarking PyTorch with High API Surface Coverage\n\n\nhttps://arxiv.org/abs/2304.14226","created_at":1758568730153,"messageId":"de0d9d3d-bced-44e3-9de1-6dcf1dc11848","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"1dec2aab-3170-4af0-b945-458c9d6e802d","updated_at":1758568730153,"userId":"google:104458561188320708185","id":"j97av8580hrbw61q224706h28d7r3b8s","providerMetadata":{}},{"_creationTime":1758568730153.5684,"_id":"j97872w17axde8djncg645avnx7r2aed","attachmentIds":[],"byok":false,"content":"Here’s the paper, explained using the Feynman technique, as if I wrote it.\n\nWhat problem were we trying to solve?\n- PyTorch is huge and fast-moving. Its performance depends on three layers working together:\n  1) GPU libraries (like cuDNN, cuBLAS),\n  2) the PyTorch framework itself (operators, autograd, compilers like TorchInductor/TorchDynamo),\n  3) the model code (hundreds of architectures with very different operator mixes).\n- Existing DL benchmarks (e.g., MLPerf) are great for hardware-vs-hardware comparisons, but they cover only a tiny fraction of PyTorch’s APIs and operator behaviors. That means:\n  - many performance bugs stay hidden,\n  - regressions slip in during daily development,\n  - performance “conclusions” vary wildly across papers because the workloads are too narrow.\n\nSo our key question: How do we benchmark PyTorch in a way that actually exercises most of its API surface and reveals real bottlenecks and regressions?\n\nOur idea in one sentence\n- Build TorchBench: a benchmark suite with a broad, representative set of PyTorch models that covers much more of PyTorch’s operator and API surface, plus tools to profile, compare, and continuously guard against performance regressions.\n\nWhat’s different from prior work?\n- Coverage: TorchBench covers about 2.3× more of PyTorch’s API surface than MLPerf. Instead of a handful of canonical models, we include a large, curated mix across domains (CV, NLP, recommendation, generative, classic and modern) and variants (eager vs. compiled paths, mixed precision, quantized, etc.).\n- Actionability: TorchBench isn’t just for pretty charts. It’s wired to:\n  - profile and break down time into meaningful buckets (Python, dispatcher, operators, kernels, data movement),\n  - compare compilers and backends,\n  - run in CI so performance regressions are caught the day they are introduced.\n\nWhy does coverage matter? A concrete bug story\n- PyTorch has an error-handling mechanism (c10_Exception) that, under typical “hot” paths, doesn’t affect performance. But quantized models frequently trigger “Not Implemented Function” paths through torch.ops. That led to heavy string/backtrace overhead, slowing quantized models by about 10× in our tests. Narrow benchmarks didn’t surface this because they didn’t stress those APIs.\n- With TorchBench’s broader coverage, we hit those cold-but-frequent paths, diagnosed the issue, and upstreamed the fix (PyTorch reverted the costly error-handling variant).\n\nHow we built the suite\n- Selection criteria:\n  - Classic, still-relevant models (e.g., ResNet, VGG, MobileNet),\n  - Modern, widely used architectures (e.g., Transformers, BERT-like models),\n  - Varied domains (vision, NLP, recommendation, etc.),\n  - Varied execution modes (eager vs. compiler stacks), data types (FP32, AMP), and quantization.\n- Adaptation and harness:\n  - Standardized inputs, batch sizes, and preprocessing; reproducible seeds;\n  - Configurable toggles for precision, compilers, devices;\n  - Built-in profilers to collect execution time, operator-level stats, kernel-level breakdowns, and memory metrics.\n- Integration:\n  - Designed to be run locally and in CI, with baselining and regression detection (thresholds, alerts).\n\nWhat we measured and why it helps\n- Execution time breakdowns:\n  - How much is Python/dispatcher overhead versus GPU kernel time?\n  - Which operators dominate per model?\n  - Are we bound by compute, memory, or launch overhead?\n- Compiler comparisons:\n  - How do PyTorch compilers (e.g., TorchDynamo+Inductor) change the profile relative to eager?\n  - Do we see benefits consistently across models or only in certain operator patterns?\n- Hardware comparisons:\n  - Comparing GPUs (e.g., NVIDIA vs. AMD), but importantly doing so with enough models to avoid misleading single-workload conclusions.\n\nA few insights we surfaced\n- “Which GPU is faster for PyTorch?” depends on the operator mix and how well each backend path is exercised. Prior dueling claims (AMD beats NVIDIA vs. the reverse) often stemmed from narrow workloads. With broader coverage, you find cases that favor each vendor—and you learn exactly which kernels/operators are tipping the scales.\n- Cold paths matter at scale. If a model’s graph hits “rare” codepaths thousands of times per batch (e.g., through a dispatcher or error handling), that “rare” path is no longer rare for that workload. TorchBench makes those visible.\n\nTwo practical wins (and why they matter)\n1) Finding and fixing PyTorch performance issues\n   - The c10_Exception case: ~10× slowdown in quantized models traced to error-handling overhead. Fix accepted upstream.\n   - Similar cases included unnecessary syncs, suboptimal kernel selection, or Python-layer overheads that compilers could fuse away or avoid.\n2) Continuous integration guardrails\n   - We wired TorchBench into PyTorch’s CI. When a daily commit degrades performance for any covered model/config beyond a threshold, we flag it immediately. This prevents regressions from camping in main.\n\nWhat TorchBench enables users to do\n- PyTorch maintainers: catch regressions as they happen, prioritize real-world hot spots, and validate performance patches across a broad workload set.\n- Researchers and practitioners: evaluate new compiler passes or kernels against realistic, varied models and quickly see where they help or hurt.\n- Hardware/software co-designers: get a more faithful picture of how different operator families stress backends and where investment pays off.\n\nWhat TorchBench is not\n- It’s not a replacement for MLPerf’s standardized, apples-to-apples hardware contests. MLPerf is ideal for vendor comparisons under strict rules. TorchBench is a developer- and maintainer-facing suite to stress the PyTorch stack broadly and continuously, to expose real bottlenecks and protect performance over time.\n\nTakeaways in plain terms\n- If you only test a few models, you’ll miss important performance problems and draw shaky conclusions.\n- Broader API surface coverage reveals the “long tail” of real issues users hit.\n- With TorchBench, we found and fixed problems that genuinely sped up models for users, and we put a safety net in PyTorch CI so they don’t come back.\n\nIf you want to replicate the spirit of our approach for another framework or internal stack:\n- Curate for coverage: include models that light up different operator families, dtypes, shapes, and execution modes.\n- Instrument deeply: collect per-op, per-kernel, Python/dispatcher breakdowns.\n- Automate regression detection: baseline, threshold, alert.\n- Close the loop: file issues, upstream fixes, re-baseline.\n\nReference\n- TorchBench: Benchmarking PyTorch with High API Surface Coverage. Yueming Hao, Xu Zhao, Bin Bao, David Berard, Will Constable, Adnan Aziz, Xu Liu. arXiv:2304.14226. Open-source and evolving.","created_at":1758568730154,"messageId":"d0f2175e-55da-4066-a618-8d8cfb31f51e","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":["arxiv.org","pytorch.org","paperswithcode.com","openreview.net","scholar.google.com"],"queries":["Benchmarking PyTorch with High API Surface Coverage arXiv 2304.14226 summary key contributions results 2025","PyTorch APIBench 2304.14226 High API Surface Coverage authors results arXiv"]},"result":[{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2509.14626","id":"https://arxiv.org/abs/2509.14626","publishedDate":"2025-09-18T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"Evaluating the Effectiveness of Coverage-Guided Fuzzing for Testing Deep Learning Library APIs","url":"https://arxiv.org/abs/2509.14626"},{"content":"# Computer Science > Machine Learning\n\n**arXiv:2508.04035** (cs)\n\n\\[Submitted on 6 Aug 2025\\]\n\n# Title:A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs\n\nAuthors: [Zakariya Ba Alawi](https://arxiv.org/search/cs?searchtype=author&query=Alawi,+Z+B)\n\nView a PDF of the paper titled A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs, by Zakariya Ba Alawi\n\n[View PDF](https://arxiv.org/pdf/2508.04035) [HTML (experimental)](https://arxiv.org/html/2508.04035v1)\n\n> Abstract:This paper presents a comprehensive comparative survey of TensorFlow and PyTorch, the two leading deep learning frameworks, focusing on their usability, performance, and deployment trade-offs. We review each framework's programming paradigm and developer experience, contrasting TensorFlow's graph-based (now optionally eager) approach with PyTorch's dynamic, Pythonic style. We then compare model training speeds and inference performance across multiple tasks and data regimes, drawing on recent benchmarks and studies. Deployment flexibility is examined in depth - from TensorFlow's mature ecosystem (TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript support) to PyTorch's newer production tools (TorchScript compilation, ONNX export, and TorchServe). We also survey ecosystem and community support, including library integrations, industry adoption, and research trends (e.g., PyTorch's dominance in recent research publications versus TensorFlow's broader tooling in enterprise). Applications in computer vision, natural language processing, and other domains are discussed to illustrate how each framework is used in practice. Finally, we outline future directions and open challenges in deep learning framework design, such as unifying eager and graph execution, improving cross-framework interoperability, and integrating compiler optimizations (XLA, JIT) for improved speed. Our findings indicate that while both frameworks are highly capable for state-of-the-art deep learning, they exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored in research, whereas TensorFlow provides a fuller production-ready ecosystem - understanding these trade-offs is key for practitioners selecting the appropriate tool. We include charts, code snippets, and more than 20 references to academic papers and official documentation to support this comparative analysis\n\n| | |\n| --- | --- |\n| Comments: | 14 pages, 15 figures, 43 references |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| MSC classes: | 68T05 |\n| ACM classes: | I.2.6; I.2.10 |\n| Cite as: | [arXiv:2508.04035](https://arxiv.org/abs/2508.04035) \\[cs.LG\\] |\n| (or [arXiv:2508.04035v1](https://arxiv.org/abs/2508.04035v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2508.04035](https://doi.org/10.48550/arXiv.2508.04035) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Zakariya Ba Alawi \\[ [view email](https://arxiv.org/show-email/b0dfae6a/2508.04035)\\] **\\[v1\\]**\nWed, 6 Aug 2025 02:55:57 UTC (1,255 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs, by Zakariya Ba Alawi\n\n- [View PDF](https://arxiv.org/pdf/2508.04035)\n- [HTML (experimental)](https://arxiv.org/html/2508.04035v1)\n- [TeX Source](https://arxiv.org/src/2508.04035)\n- [Other Formats](https://arxiv.org/format/2508.04035)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[< prev](https://arxiv.org/prevnext?id=2508.04035&function=prev&context=cs.LG)  \\|  [next >](https://arxiv.org/prevnext?id=2508.04035&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-08](https://arxiv.org/list/cs.LG/2025-08)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2508.04035?context=cs) [cs.AI](https://arxiv.org/abs/2508.04035?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2508.04035)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2508.04035)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2508.04035)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2508.04035) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2508.04035","id":"https://arxiv.org/abs/2508.04035","publishedDate":"2025-08-06T00:00:00.000Z","summary":"This paper is a comparative survey of TensorFlow and PyTorch, the two leading deep learning frameworks. It focuses on their usability, performance, and deployment trade-offs. The paper contrasts TensorFlow's graph-based approach with PyTorch's dynamic style, compares model training speeds and inference performance, and examines deployment flexibility. It also surveys ecosystem and community support, and discusses applications in computer vision and natural language processing. The findings indicate that PyTorch offers simplicity and flexibility, while TensorFlow provides a fuller production-ready ecosystem.\n","title":"A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs","url":"https://arxiv.org/abs/2508.04035"},{"content":"# Computer Science > Distributed, Parallel, and Cluster Computing\n\n**arXiv:2507.17770** (cs)\n\n\\[Submitted on 17 Jul 2025\\]\n\n# Title:Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO Problems at Scale\n\nAuthors: [Pei-Kun Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+P)\n\nView a PDF of the paper titled Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO Problems at Scale, by Pei-Kun Yang\n\n[View PDF](https://arxiv.org/pdf/2507.17770)\n\n> Abstract:Quadratic Unconstrained Binary Optimization (QUBO) is a versatile framework for modeling combinatorial optimization problems. This study benchmarks five software-based QUBO solvers: Neal, PyTorch (CPU), PyTorch (GPU), JAX, and SciPy, on randomly generated QUBO matrices ranging from 1000x1000 to 45000x45000, under six convergence thresholds from 10^-1 to 10^-6. We evaluate their performance in terms of solution quality (energy) and computational time. Among the solvers tested, Neal achieved the lowest energy values but was limited to problems with up to 6000 variables due to high memory consumption. PyTorch produced slightly higher energy results than Neal but demonstrated superior scalability, solving instances with up to 45000 variables. Its support for GPU acceleration and CPU multi-threading also resulted in significantly shorter runtimes. JAX yielded energy values slightly above those of PyTorch and was limited to 25000 variables, with runtimes comparable to PyTorch on GPU. SciPy was the most constrained solver, handling only up to 6000 variables and consistently producing the highest energy values with the longest computation times. These findings highlight trade-offs between solution quality, scalability, and runtime efficiency, and suggest that PyTorch is the most balanced choice for large-scale QUBO problems when computational resources permit.\n\n| | |\n| --- | --- |\n| Comments: | 14 pages, 5 figures |\n| Subjects: | Distributed, Parallel, and Cluster Computing (cs.DC); Quantum Physics (quant-ph) |\n| Cite as: | [arXiv:2507.17770](https://arxiv.org/abs/2507.17770) \\[cs.DC\\] |\n| (or [arXiv:2507.17770v1](https://arxiv.org/abs/2507.17770v1) \\[cs.DC\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2507.17770](https://doi.org/10.48550/arXiv.2507.17770) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Pei-Kun Yang \\[ [view email](https://arxiv.org/show-email/7b964119/2507.17770)\\] **\\[v1\\]**\nThu, 17 Jul 2025 07:19:53 UTC (1,115 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO Problems at Scale, by Pei-Kun Yang\n\n- [View PDF](https://arxiv.org/pdf/2507.17770)\n- [Other Formats](https://arxiv.org/format/2507.17770)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.DC\n\n[< prev](https://arxiv.org/prevnext?id=2507.17770&function=prev&context=cs.DC)  \\|  [next >](https://arxiv.org/prevnext?id=2507.17770&function=next&context=cs.DC)\n\n[new](https://arxiv.org/list/cs.DC/new) \\| [recent](https://arxiv.org/list/cs.DC/recent) \\| [2025-07](https://arxiv.org/list/cs.DC/2025-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2507.17770?context=cs) [quant-ph](https://arxiv.org/abs/2507.17770?context=quant-ph)\n\n### References & Citations\n\n- [INSPIRE HEP](https://inspirehep.net/arxiv/2507.17770)\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2507.17770)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2507.17770)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2507.17770)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2507.17770) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2507.17770","id":"https://arxiv.org/abs/2507.17770","publishedDate":"2025-07-17T00:00:00.000Z","summary":"This study benchmarks Neal, PyTorch (CPU and GPU), JAX, and SciPy for solving QUBO problems. PyTorch is the most balanced choice for large-scale QUBO problems, offering good solution quality, scalability, and runtime efficiency. Neal achieved the lowest energy values but was limited to problems with up to 6000 variables. JAX yielded energy values slightly above those of PyTorch. SciPy was the most constrained solver.\n","title":"Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO Problems at Scale","url":"https://arxiv.org/abs/2507.17770"},{"content":"# Computer Science > Programming Languages\n\n**arXiv:2509.07003** (cs)\n\n\\[Submitted on 5 Sep 2025\\]\n\n# Title:veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD\n\nAuthors: [Youjie Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+Y), [Cheng Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan,+C), [Zhiqi Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin,+Z), [Hongyu Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu,+H), [Jiacheng Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+J), [Ziang Song](https://arxiv.org/search/cs?searchtype=author&query=Song,+Z), [Xinyi Di](https://arxiv.org/search/cs?searchtype=author&query=Di,+X), [Jiawei Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+J), [Huiyao Shu](https://arxiv.org/search/cs?searchtype=author&query=Shu,+H), [Wenlei Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao,+W), [Yanghua Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng,+Y), [Haibin Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin,+H), [Li-Wen Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang,+L)\n\nView a PDF of the paper titled veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD, by Youjie Li and 12 other authors\n\n[View PDF](https://arxiv.org/pdf/2509.07003) [HTML (experimental)](https://arxiv.org/html/2509.07003v1)\n\n> Abstract:Large Language Models (LLMs) have scaled rapidly in size and complexity, requiring increasingly intricate parallelism for distributed training, such as 3D parallelism. This sophistication motivates a shift toward simpler, more debuggable programming paradigm like Single Program Multiple Data (SPMD). However, SPMD in eager execution introduces two key challenges: ensuring consistency with single-device execution and achieving high performance at scale. In this paper, we introduce veScale, an eager-mode training system that fully embraces SPMD paradigm to democratize distributed tensor programming. veScale addresses the prevalent issue of inconsistent results in systems like PyTorch by introducing a novel algorithm of distributed Random Number Generation (RNG) compatible with arbitrary sharded operators. veScale also significantly boosts training performance by reducing PyTorch primitive's overhead and improving communication efficiency. Evaluations show that veScale delivers up to 2.2x speedup over the state-of-the-art training systems, like TorchTitan, and cuts code complexity by 78.4%, while preserving single-device-equivalent results.\n\n| | |\n| --- | --- |\n| Comments: | 21 pages, 16 figures, 5 tables |\n| Subjects: | Programming Languages (cs.PL); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2509.07003](https://arxiv.org/abs/2509.07003) \\[cs.PL\\] |\n| (or [arXiv:2509.07003v1](https://arxiv.org/abs/2509.07003v1) \\[cs.PL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2509.07003](https://doi.org/10.48550/arXiv.2509.07003) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Cheng Wan \\[ [view email](https://arxiv.org/show-email/cde8931a/2509.07003)\\] **\\[v1\\]**\nFri, 5 Sep 2025 19:29:00 UTC (750 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD, by Youjie Li and 12 other authors\n\n- [View PDF](https://arxiv.org/pdf/2509.07003)\n- [HTML (experimental)](https://arxiv.org/html/2509.07003v1)\n- [TeX Source](https://arxiv.org/src/2509.07003)\n- [Other Formats](https://arxiv.org/format/2509.07003)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.PL\n\n[< prev](https://arxiv.org/prevnext?id=2509.07003&function=prev&context=cs.PL)  \\|  [next >](https://arxiv.org/prevnext?id=2509.07003&function=next&context=cs.PL)\n\n[new](https://arxiv.org/list/cs.PL/new) \\| [recent](https://arxiv.org/list/cs.PL/recent) \\| [2025-09](https://arxiv.org/list/cs.PL/2025-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2509.07003?context=cs) [cs.DC](https://arxiv.org/abs/2509.07003?context=cs.DC) [cs.LG](https://arxiv.org/abs/2509.07003?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2509.07003)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2509.07003)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2509.07003)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2509.07003) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2509.07003","id":"https://arxiv.org/abs/2509.07003","publishedDate":"2025-09-05T00:00:00.000Z","summary":"veScale is an eager-mode training system designed for consistent and efficient tensor programming using the Single Program Multiple Data (SPMD) paradigm. It addresses the challenges of inconsistent results and performance limitations in existing systems like PyTorch. veScale introduces a novel distributed Random Number Generation (RNG) algorithm and improves communication efficiency, achieving up to 2.2x speedup over state-of-the-art systems while reducing code complexity and preserving single-device-equivalent results.\n","title":"veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD","url":"https://arxiv.org/abs/2509.07003"},{"content":"# Electrical Engineering and Systems Science > Audio and Speech Processing\n\n**arXiv:2504.08624** (eess)\n\n\\[Submitted on 11 Apr 2025\\]\n\n# Title:TorchFX: A modern approach to Audio DSP with PyTorch and GPU acceleration\n\nAuthors: [Matteo Spanio](https://arxiv.org/search/eess?searchtype=author&query=Spanio,+M), [Antonio Rodà](https://arxiv.org/search/eess?searchtype=author&query=Rod%C3%A0,+A)\n\nView a PDF of the paper titled TorchFX: A modern approach to Audio DSP with PyTorch and GPU acceleration, by Matteo Spanio and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2504.08624) [HTML (experimental)](https://arxiv.org/html/2504.08624v1)\n\n> Abstract:The burgeoning complexity and real-time processing demands of audio signals necessitate optimized algorithms that harness the computational prowess of Graphics Processing Units (GPUs). Existing Digital Signal Processing (DSP) libraries often fall short in delivering the requisite efficiency and flexibility, particularly in integrating Artificial Intelligence (AI) models. In response, we introduce TorchFX: a GPU-accelerated Python library for DSP, specifically engineered to facilitate sophisticated audio signal processing. Built atop the PyTorch framework, TorchFX offers an Object-Oriented interface that emulates the usability of torchaudio, enhancing functionality with a novel pipe operator for intuitive filter chaining. This library provides a comprehensive suite of Finite Impulse Response (FIR) and Infinite Impulse Response (IIR) filters, with a focus on multichannel audio files, thus facilitating the integration of DSP and AI-based approaches. Our benchmarking results demonstrate significant efficiency gains over traditional libraries like SciPy, particularly in multichannel contexts. Despite current limitations in GPU compatibility, ongoing developments promise broader support and real-time processing capabilities. TorchFX aims to become a useful tool for the community, contributing to innovation and progress in DSP with GPU acceleration. TorchFX is publicly available on GitHub at [this https URL](https://github.com/matteospanio/torchfx).\n\n| | |\n| --- | --- |\n| Comments: | Submitted to DAFx 2025 |\n| Subjects: | Audio and Speech Processing (eess.AS); Performance (cs.PF); Sound (cs.SD); Signal Processing (eess.SP) |\n| ACM classes: | D.2.10; J.5 |\n| Cite as: | [arXiv:2504.08624](https://arxiv.org/abs/2504.08624) \\[eess.AS\\] |\n| (or [arXiv:2504.08624v1](https://arxiv.org/abs/2504.08624v1) \\[eess.AS\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2504.08624](https://doi.org/10.48550/arXiv.2504.08624) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Matteo Spanio \\[ [view email](https://arxiv.org/show-email/03ddf939/2504.08624)\\] **\\[v1\\]**\nFri, 11 Apr 2025 15:26:10 UTC (293 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled TorchFX: A modern approach to Audio DSP with PyTorch and GPU acceleration, by Matteo Spanio and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2504.08624)\n- [HTML (experimental)](https://arxiv.org/html/2504.08624v1)\n- [TeX Source](https://arxiv.org/src/2504.08624)\n- [Other Formats](https://arxiv.org/format/2504.08624)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\neess.AS\n\n[< prev](https://arxiv.org/prevnext?id=2504.08624&function=prev&context=eess.AS)  \\|  [next >](https://arxiv.org/prevnext?id=2504.08624&function=next&context=eess.AS)\n\n[new](https://arxiv.org/list/eess.AS/new) \\| [recent](https://arxiv.org/list/eess.AS/recent) \\| [2025-04](https://arxiv.org/list/eess.AS/2025-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2504.08624?context=cs) [cs.PF](https://arxiv.org/abs/2504.08624?context=cs.PF) [cs.SD](https://arxiv.org/abs/2504.08624?context=cs.SD) [eess](https://arxiv.org/abs/2504.08624?context=eess) [eess.SP](https://arxiv.org/abs/2504.08624?context=eess.SP)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2504.08624)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2504.08624)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2504.08624)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2504.08624) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2504.08624","id":"https://arxiv.org/abs/2504.08624","publishedDate":"2025-04-11T00:00:00.000Z","summary":"TorchFX is a GPU-accelerated Python library for Digital Signal Processing (DSP) built on PyTorch, designed for audio signal processing. It offers an object-oriented interface, including FIR and IIR filters, with a focus on multichannel audio. Benchmarking shows efficiency gains over libraries like SciPy, especially in multichannel contexts. The library is available on GitHub.\n","title":"TorchFX: A modern approach to Audio DSP with PyTorch and GPU acceleration","url":"https://arxiv.org/abs/2504.08624"},{"content":"TorchBench: Benchmarking PyTorch with High API Surface\nCoverage\nYueming Hao\nyhao24@ncsu.edu\nNorth Carolina State University\nRaleigh, North Carolina, USA\nXu Zhao\nxzhao9@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nBin Bao\nbinbao@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nDavid Berard\ndberard@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nWill Constable\nwhc@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nAdnan Aziz\nadnanaziz@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nXu Liu\nxliu88@ncsu.edu\nNorth Carolina State University\nRaleigh, North Carolina, USA\nABSTRACT\nDeep learning (DL) has been a revolutionary technique in vari\u0002ous domains. To facilitate the model development and deployment,\nmany deep learning frameworks are proposed, among which Py\u0002Torch is one of the most popular solutions. The performance of eco\u0002system around PyTorch is critically important, which saves the costs\nof training models and reduces the response time of model infer\u0002ences. In this paper, we propose TorchBench, a novel benchmark\nsuite to study the performance of PyTorch software stack. Unlike\nexisting benchmark suites, TorchBench encloses many represen\u0002tative models, covering a large PyTorch API surface. TorchBench\nis able to comprehensively characterize the performance of the Py\u0002Torch software stack, guiding the performance optimization across\nmodels, PyTorch framework, and GPU libraries. We show two prac\u0002tical use cases of TorchBench. (1) We profile TorchBench to iden\u0002tify GPU performance inefficiencies in PyTorch. We are able to op\u0002timize many performance bugs and upstream patches to the official\nPyTorch repository. (2) We integrate TorchBench into PyTorch\ncontinuous integration system. We are able to identify performance\nregression in multiple daily code checkins to prevent PyTorch repos\u0002itory from introducing performance bugs. TorchBench is open\nsource and keeps evolving.\n1 INTRODUCTION\nDeep learning (DL) has been the most transformative technology\nover the past decade, given its wide applicability in global climate\nprojections [1, 2], image processing [3–5], speech recognition [6, 7],\ncontent recommendation [8–10], and pattern classification [11, 12],\nand great potential in emerging domains such as autonomous driv\u0002ing [13, 14], augmented/virtual reality [15, 16], and sciences [17, 18].\nTo facilitate the development and deployment of DL models, DL\nframework practitioners have proposed a number of DL frame\u0002works, such as Caffe [19], TVM [20], ONNX [21], MXNet [22],\nJAX [23], PyTorch [24], and TensorFlow [25]. These frameworks\nprovide rich APIs and support performance tuning.\nPyTorch is one of the most popular deep learning frameworks.\nAs an open-source project, PyTorch has more than 2000 contrib\u0002utors and has received more than 61,000 stars by the end of 2022.\nPyTorch is still actively evolving; PyTorch has over 110k commits in\n2022 [26]. The machine learning research papers based on PyTorch\nhave increased from 29% to 65% in the past four years [27].\nOne major challenge of deep learning tasks is the extensive\namount of computation for both training and inference, which\ntranslates to huge time and money costs. For example, the best\nversion of AlphaGo [28] needs weeks to train with 280 GPUs, and\nits estimated cost is 35 million dollars [29]. In addition, the real time\ninference of deep learning models on edge devices requires fast\nresponse and low latency. Thus, it is critically important for PyTorch\nto achieve bare-metal performance for boosting productivity and\nreducing costs.\nHowever, understanding and optimizing PyTorch performance is\nchallenging because of PyTorch’s complex and rapidly evolved code\nbases. On the one hand, the PyTorch software stack consists of three\nmajor components: the acceleration libraries (e.g., CuDNN [30]),\nthe PyTorch framework [24] (including optimizers and compilers),\nand deep learning models; performance inefficiencies can exist\nin or across any components. Without a systematic evaluation,\ninefficiencies can be difficult to identify. On the other hand, as an\nactive open-source project, PyTorch receives many patches for new\nfunctionality or performance improvement. Without a thorough\nevaluation, it is difficult to understand how these patches affect\nthe entire PyTorch software stack for various models in different\ndomains.\nBenchmarking is a well-known technique to understand the per\u0002formance of different workloads, programming languages, compil\u0002ers, and architectures. For example, SPEC provides many standard\nbenchmark suites for various purposes; Renassance [31] is a Java\nbenchmark suite; DeathStar [32] is a benchmark suite for microser\u0002vices; NPB [33] is a benchmark suite for different parallel models in\nscientific computing; and Rodinia [34] is a GPU benchmark suite.\nThese benchmarks provide indispensable performance insights for\n1\narXiv:2304.14226v3 [cs.LG] 24 Jun 2023\nYueming Hao, Xu Zhao, Bin Bao, David Berard, Will Constable, Adnan Aziz, and Xu Liu\nboth software and hardware evolution. MLPerf [35] is the state\u0002of-the-art benchmark suite for deep learning workloads. However,\nexisting deep learning benchmark suites, including MLPerf, aim\nto compare the performance of deep learning models running on\ndifferent hardware and frameworks. They usually include a small\nnumber of deep learning models (e.g., MLPerf has eight models\nonly) and cover a small PyTorch API surface, which fails to identify\nmany PyTorch performance bugs or fairly evaluate the performance\nimpact of patches.\n1.1 Motivating Examples\nWe show two examples to motivate the necessity of a comprehen\u0002sive benchmark suite for PyTorch.\nMisleading performance characterization. Understanding the per\u0002formance difference across various architectures is one of the ma\u0002jor tasks for benchmarking. However, without a comprehensive\nbenchmark suite, such characterization can result in misleading\nconclusions. For example, some studies [36] show that AMD GPUs\noutperform NVIDIA GPUs on PyTorch, and some studies [37, 38]\nshow an opposite conclusion. Thus, a comprehensive benchmark\nsuite is necessary for performance characterization. Unlike exist\u0002ing studies, we show a different conclusion and unique insights in\nSection 3.3.\nMissing performance bugs. Performance bugs can be buried deep\nin complex PyTorch code bases, especially in the cold execution\npaths. We identify a recent performance bug in PyTorch, which is\ninappropriate error handling. PyTorch implements an error han\u0002dling mechanism named c10_Exception to deal with runtime errors.\nIt prints out backtraces for all errors and uses std::string to gen\u0002erate error messages. Since error handling is usually in the cold\npath, it does not incur any performance degradation in models of\nexisting benchmark suites. However, we find that this error code\nhandling slows down quantized models by 10×. Our further in\u0002vestigation shows that quantized models heavily call torch.ops\nAPI, which frequently throws a benign error, “Not Implemented\nFunctions”, resulting in a large overhead in error handling. This is\u0002sue has been confirmed by PyTorch team, and c10_Exception has\nbeen reverted to the previous implementation. Thus, a benchmark\nsuite that covers a large PyTorch API surface is necessary to expose\nperformance bugs.\n1.2 Paper Contribution\nIn this paper, we develop TorchBench, a novel benchmark suite\nfor PyTorch to address the aforementioned challenges. Unlike exist\u0002ing approaches, TorchBench embraces a large number of models,\nwhich cover a large PyTorch API surface. Given the voluminous\ndeep learning models, we carefully include the representative mod\u0002els in TorchBench for both fairness and generality. Additionally,\nwe develop a set of tools associated with TorchBench to enable\nTorchBench to (1) run benchmarks with different configurations,\n(2) collect various performance statistics, and (3) be ready for any\ncontinuous integration systems. TorchBench helps fix many per\u0002formance bugs in PyTorch eco-system, and many of them are ac\u0002cepted by the official PyTorch repository. In summary, TorchBench\nmakes the following contributions.\n• TorchBench is the first PyTorch benchmark suite that consists\nof rich models in different domains. It covers 2.3× more PyTorch\nAPI surface, compared to the state-of-the-art MLPerf benchmark\nsuite.\n• TorchBench integrate a set of built-in tools, which config\u0002ure execution environments and collect performance statistics.\nTorchBench is able to report multiple metrics to thoroughly\ncharacterize PyTorch performance.\n• TorchBench demonstrates two use cases in practice. First,\nTorchBench exposes the performance bugs in PyTorch software\nstacks and provides insights for bug fixing. Second, TorchBench\nis configured as the continuous integration of PyTorch repository\nto understand the performance regression of daily committed\npatches.\nThe rest of the paper is organized as follows. Section 2 describes\nthe models, benchmark adaptation, and configurations in our bench\u0002mark suite. Section 3 characterizes TorchBench from different as\u0002pects, including execution time breakdown, different PyTorch com\u0002pilers, and different GPU platforms. Section 4 shows the practical\nusages of TorchBench in detecting performance inefficiencies in\nPyTorch software stack. Section 5 reviews related works and distin\u0002guishes our approach. Section 6 presents some conclusions.\n2 TORCHBENCH SUITE\nThere are a huge amount of deep learning models developed by\nthe PyTorch community. For example, The Hugging Face plat\u0002form has more than 10,000 models [39]. Thus, it is challenging\nfor TorchBench to enclose representative models to cover perfor\u0002mance behaviors of typical PyTorch use cases. We work closely\nwith machine learning engineers and set the following criteria to\nselect models for TorchBench.\n• Classic models. TorchBench includes models that were devel\u0002oped many years ago but have been proven both useful and\nimpactful, such as ResNet [4], VGG16 [40], and MobileNet [5, 41].\nThese models","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2304.14226.pdf","id":"https://arxiv.org/pdf/2304.14226.pdf","publishedDate":"2023-10-26T00:00:00.000Z","summary":"TorchBench is a new benchmark suite designed to evaluate the performance of the PyTorch software stack. It aims to cover a large portion of the PyTorch API surface, unlike existing benchmarks. The authors use TorchBench to identify and fix GPU performance inefficiencies in PyTorch, and also integrate it into the PyTorch continuous integration system to prevent performance regressions.\n","title":"TorchBench: Benchmarking PyTorch with High API Surface Coverage","url":"https://arxiv.org/pdf/2304.14226.pdf"},{"content":"# Computer Science > Machine Learning\n\n**arXiv:2304.14226** (cs)\n\n\\[Submitted on 27 Apr 2023 ( [v1](https://arxiv.org/abs/2304.14226v1)), last revised 24 Jun 2023 (this version, v3)\\]\n\n# Title:TorchBench: Benchmarking PyTorch with High API Surface Coverage\n\nAuthors: [Yueming Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao,+Y), [Xu Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao,+X), [Bin Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao,+B), [David Berard](https://arxiv.org/search/cs?searchtype=author&query=Berard,+D), [Will Constable](https://arxiv.org/search/cs?searchtype=author&query=Constable,+W), [Adnan Aziz](https://arxiv.org/search/cs?searchtype=author&query=Aziz,+A), [Xu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+X)\n\nView a PDF of the paper titled TorchBench: Benchmarking PyTorch with High API Surface Coverage, by Yueming Hao and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/2304.14226)\n\n> Abstract:Deep learning (DL) has been a revolutionary technique in various domains. To facilitate the model development and deployment, many deep learning frameworks are proposed, among which PyTorch is one of the most popular solutions. The performance of ecosystem around PyTorch is critically important, which saves the costs of training models and reduces the response time of model inferences. In this paper, we propose TorchBench, a novel benchmark suite to study the performance of PyTorch software stack. Unlike existing benchmark suites, TorchBench encloses many representative models, covering a large PyTorch API surface. TorchBench is able to comprehensively characterize the performance of the PyTorch software stack, guiding the performance optimization across models, PyTorch framework, and GPU libraries. We show two practical use cases of TorchBench. (1) We profile TorchBench to identify GPU performance inefficiencies in PyTorch. We are able to optimize many performance bugs and upstream patches to the official PyTorch repository. (2) We integrate TorchBench into PyTorch continuous integration system. We are able to identify performance regression in multiple daily code checkins to prevent PyTorch repository from introducing performance bugs. TorchBench is open source and keeps evolving.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Performance (cs.PF) |\n| Cite as: | [arXiv:2304.14226](https://arxiv.org/abs/2304.14226) \\[cs.LG\\] |\n| (or [arXiv:2304.14226v3](https://arxiv.org/abs/2304.14226v3) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2304.14226](https://doi.org/10.48550/arXiv.2304.14226) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Yueming Hao \\[ [view email](https://arxiv.org/show-email/180a277c/2304.14226)\\] **[\\[v1\\]](https://arxiv.org/abs/2304.14226v1)**\nThu, 27 Apr 2023 14:37:05 UTC (772 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2304.14226v2)**\nFri, 28 Apr 2023 19:56:19 UTC (772 KB)\n**\\[v3\\]**\nSat, 24 Jun 2023 16:57:43 UTC (751 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled TorchBench: Benchmarking PyTorch with High API Surface Coverage, by Yueming Hao and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/2304.14226)\n- [TeX Source](https://arxiv.org/src/2304.14226)\n- [Other Formats](https://arxiv.org/format/2304.14226)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[< prev](https://arxiv.org/prevnext?id=2304.14226&function=prev&context=cs.LG)  \\|  [next >](https://arxiv.org/prevnext?id=2304.14226&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2023-04](https://arxiv.org/list/cs.LG/2023-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2304.14226?context=cs) [cs.AI](https://arxiv.org/abs/2304.14226?context=cs.AI) [cs.PF](https://arxiv.org/abs/2304.14226?context=cs.PF)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2304.14226)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2304.14226)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2304.14226)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2304.14226) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2304.14226","id":"https://arxiv.org/abs/2304.14226","publishedDate":"2023-05-16T00:00:00.000Z","summary":"This paper introduces TorchBench, a novel benchmark suite designed to evaluate the performance of the PyTorch software stack. TorchBench covers a large portion of the PyTorch API surface, encompassing various representative models. It aids in identifying performance inefficiencies, optimizing performance, and preventing performance regressions in the PyTorch repository. The paper details two use cases: profiling to identify GPU performance issues and integration into the PyTorch continuous integration system. TorchBench is open source.\n","title":"TorchBench: Benchmarking PyTorch with High API Surface Coverage","url":"https://arxiv.org/abs/2304.14226"},{"content":"# Computer Science > Software Engineering\n\n**arXiv:2310.20078** (cs)\n\n\\[Submitted on 30 Oct 2023\\]\n\n# Title:TorchProbe: Fuzzing Dynamic Deep Learning Compilers\n\nAuthors: [Qidong Su](https://arxiv.org/search/cs?searchtype=author&query=Su,+Q), [Chuqin Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng,+C), [Gennady Pekhimenko](https://arxiv.org/search/cs?searchtype=author&query=Pekhimenko,+G), [Xujie Si](https://arxiv.org/search/cs?searchtype=author&query=Si,+X)\n\nView a PDF of the paper titled TorchProbe: Fuzzing Dynamic Deep Learning Compilers, by Qidong Su and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2310.20078)\n\n> Abstract:Static and dynamic computational graphs represent two distinct approaches to constructing deep learning frameworks. The former prioritizes compiler-based optimizations, while the latter focuses on programmability and user-friendliness. The recent release of PyTorch 2.0, which supports compiling arbitrary deep learning programs in Python, signifies a new direction in the evolution of deep learning infrastructure to incorporate compiler techniques in a more dynamic manner and support more dynamic language features like dynamic control flows and closures. Given PyTorch's seamless integration with Python, its compiler aims to support arbitrary deep learning code written in Python. However, the inherent dynamism of Python poses challenges to the completeness and robustness of the compiler. While recent research has introduced fuzzing to test deep learning compilers, there is still a lack of comprehensive analysis on how to test dynamic features. To address this issue, we propose several code transformations to generate test cases involving dynamic features. These transformations preserve the program's semantics, ensuring that any discrepancy between the transformed and original programs indicates the presence of a bug. Through our approach, we have successfully identified twenty previously unknown bugs in the PyTorch compiler and its underlying tensor compiler Triton.\n\n| | |\n| --- | --- |\n| Subjects: | Software Engineering (cs.SE) |\n| Cite as: | [arXiv:2310.20078](https://arxiv.org/abs/2310.20078) \\[cs.SE\\] |\n| (or [arXiv:2310.20078v1](https://arxiv.org/abs/2310.20078v1) \\[cs.SE\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2310.20078](https://doi.org/10.48550/arXiv.2310.20078) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Qidong Su \\[ [view email](https://arxiv.org/show-email/e6525215/2310.20078)\\] **\\[v1\\]**\nMon, 30 Oct 2023 23:20:47 UTC (702 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled TorchProbe: Fuzzing Dynamic Deep Learning Compilers, by Qidong Su and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2310.20078)\n- [TeX Source](https://arxiv.org/src/2310.20078)\n- [Other Formats](https://arxiv.org/format/2310.20078)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.SE\n\n[< prev](https://arxiv.org/prevnext?id=2310.20078&function=prev&context=cs.SE)  \\|  [next >](https://arxiv.org/prevnext?id=2310.20078&function=next&context=cs.SE)\n\n[new](https://arxiv.org/list/cs.SE/new) \\| [recent](https://arxiv.org/list/cs.SE/recent) \\| [2023-10](https://arxiv.org/list/cs.SE/2023-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2310.20078?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2310.20078)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2310.20078)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2310.20078)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2310.20078) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2310.20078","id":"https://arxiv.org/abs/2310.20078","publishedDate":"2023-12-04T00:00:00.000Z","summary":"This paper, \"TorchProbe: Fuzzing Dynamic Deep Learning Compilers,\" introduces a method for testing dynamic features in deep learning compilers, specifically focusing on the PyTorch compiler. The authors propose code transformations to generate test cases that expose bugs in the compiler. They successfully identified twenty previously unknown bugs in the PyTorch compiler and its underlying tensor compiler Triton.\n","title":"TorchProbe: Fuzzing Dynamic Deep Learning Compilers","url":"https://arxiv.org/abs/2310.20078"},{"content":"\n Download PDF \nIn this work, we set out to conduct the first ground-truth empirical evaluation of state-of-the-art DL fuzzers. Specifically, we first manually created an extensive DL bug benchmark dataset, which includes 627 real-world DL bugs from TensorFlow and PyTorch libraries reported by users between 2020 and 2022. Then we run three state-of-the-art DL fuzzers, i.e., FreeFuzz, DeepRel, and DocTer, on the benchmark by following their instructions. We find that these fuzzers are unable to detect many real bugs collected in our benchmark dataset. Specifically, most (235) of the 257 applicable bugs cannot be detected by any fuzzer.\n Our systematic analysis further identifies four major, broad, and common factors that affect these fuzzers' ability to detect real bugs. These findings present opportunities to improve the performance of the fuzzers in future work. As a proof of concept, we propose a lightweight corner case generator as an extension to the three DL fuzzers, which simply covers several boundary values as well as DL-specific data types. It helps FreeFuzz, DeepRel, and DocTer detect 12, 12, and 14 more bugs, respectively, that were overlooked by the original fuzzers. Overall, this work complements prior studies on DL fuzzers with an extensive performance evaluation and provides a benchmark for future DL library fuzzing studies. Also, our proposed corner case generator proves that the fuzzers can be extended to detect more bugs by extending their internal fuzzing logic based on the insights provided in root cause analysis.\n \n \n Submission history From: Song Wang [ view email] [v1] \n Tue, 10 Oct 2023 18:09:16 UTC (511 KB) \n ||||I|||| Skip to main content\n We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate\n > cs > arXiv:2310.06912\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Software Engineering\n\n arXiv:2310.06912 (cs)\n [Submitted on 10 Oct 2023]\n\n Title: Benchmarking Deep Learning Fuzzers\n\n Authors: Nima Shiri Harzevili, Hung Viet Pham, Song Wang\n Download a PDF of the paper titled Benchmarking Deep Learning Fuzzers, by Nima Shiri Harzevili and 2 other authors\n Download PDF\n Abstract: In this work, we set out to conduct the first ground-truth empirical evaluation of state-of-the-art DL fuzzers. Specifically, we first manually created an extensive DL bug benchmark dataset, which includes 627 real-world DL bugs from TensorFlow and PyTorch libraries reported by users between 2020 and 2022. Then we run three state-of-the-art DL fuzzers, i.e., FreeFuzz, DeepRel, and DocTer, on the benchmark by following their instructions. We find that these fuzzers are unable to detect many real bugs collected in our benchmark dataset. Specifically, most (235) of the 257 applicable bugs cannot be detected by any fuzzer.\n Our systematic analysis further identifies four major, broad, and common factors that affect these fuzzers' ability to detect real bugs. These findings present opportunities to improve the performance of the fuzzers in future work. As a proof of concept, we propose a lightweight corner case generator as an extension to the three DL fuzzers, which simply covers several boundary values as well as DL-specific data types. It helps FreeFuzz, DeepRel, and DocTer detect 12, 12, and 14 more bugs, respectively, that were overlooked by the original fuzzers. Overall, this work complements prior studies on DL fuzzers with an extensive performance evaluation and provides a benchmark for future DL library fuzzing studies. Also, our proposed corner case generator proves that the fuzzers can be extended to detect more bugs by extending their internal fuzzing logic based on the insights provided in root cause analysis.\n Subjects: Software Engineering (cs.SE) \n Cite as: arXiv:2310.06912 [cs.SE] \n (or arXiv:2310.06912v1 [cs.SE] for this version)\n https://doi.org/10.48550/arXiv.2310.06912 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Song Wang [view email]\n [v1] Tue, 10 Oct 2023 18:09:16 UTC (511 KB)\n Full-text links:\n\n Access Paper:\n\n Download a PDF of the paper titled Benchmarking Deep Learning Fuzzers, by Nima Shiri Harzevili and 2 other authors\n * Download PDF\n * PostScript\n * Other Formats\n Current browse context:\n cs.SE\n < prev | next >\n new | recent | 2310\n Change to browse by:\n cs\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export BibTeX citation Loading...\n\n BibTeX formatted citation\n\n ×\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n DagsHub Toggle\n DagsHub (What is DagsHub?)\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Link to Influence Flower\n Influence Flower (What are Influence Flowers?)\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2310.06912","id":"https://arxiv.org/abs/2310.06912","publishedDate":"2023-12-04T00:00:00.000Z","summary":"This paper presents the first empirical evaluation of state-of-the-art deep learning (DL) fuzzers. The authors created a benchmark dataset of 627 real-world DL bugs from TensorFlow and PyTorch libraries reported between 2020 and 2022. They tested three DL fuzzers (FreeFuzz, DeepRel, and DocTer) and found that they failed to detect many of the bugs. The study identifies factors affecting fuzzer performance and proposes a corner case generator that improves bug detection. This work provides a benchmark for future DL library fuzzing studies.\n","title":"Benchmarking Deep Learning Fuzzers","url":"https://arxiv.org/abs/2310.06912"}],"status":"completed","toolCallId":"call_UFASTpTclFWFsVTNWm0vQuLt","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Here’s the paper, explained using the Feynman technique, as if I wrote it.\n\nWhat problem were we trying to solve?\n- PyTorch is huge and fast-moving. Its performance depends on three layers working together:\n  1) GPU libraries (like cuDNN, cuBLAS),\n  2) the PyTorch framework itself (operators, autograd, compilers like TorchInductor/TorchDynamo),\n  3) the model code (hundreds of architectures with very different operator mixes).\n- Existing DL benchmarks (e.g., MLPerf) are great for hardware-vs-hardware comparisons, but they cover only a tiny fraction of PyTorch’s APIs and operator behaviors. That means:\n  - many performance bugs stay hidden,\n  - regressions slip in during daily development,\n  - performance “conclusions” vary wildly across papers because the workloads are too narrow.\n\nSo our key question: How do we benchmark PyTorch in a way that actually exercises most of its API surface and reveals real bottlenecks and regressions?\n\nOur idea in one sentence\n- Build TorchBench: a benchmark suite with a broad, representative set of PyTorch models that covers much more of PyTorch’s operator and API surface, plus tools to profile, compare, and continuously guard against performance regressions.\n\nWhat’s different from prior work?\n- Coverage: TorchBench covers about 2.3× more of PyTorch’s API surface than MLPerf. Instead of a handful of canonical models, we include a large, curated mix across domains (CV, NLP, recommendation, generative, classic and modern) and variants (eager vs. compiled paths, mixed precision, quantized, etc.).\n- Actionability: TorchBench isn’t just for pretty charts. It’s wired to:\n  - profile and break down time into meaningful buckets (Python, dispatcher, operators, kernels, data movement),\n  - compare compilers and backends,\n  - run in CI so performance regressions are caught the day they are introduced.\n\nWhy does coverage matter? A concrete bug story\n- PyTorch has an error-handling mechanism (c10_Exception) that, under typical “hot” paths, doesn’t affect performance. But quantized models frequently trigger “Not Implemented Function” paths through torch.ops. That led to heavy string/backtrace overhead, slowing quantized models by about 10× in our tests. Narrow benchmarks didn’t surface this because they didn’t stress those APIs.\n- With TorchBench’s broader coverage, we hit those cold-but-frequent paths, diagnosed the issue, and upstreamed the fix (PyTorch reverted the costly error-handling variant).\n\nHow we built the suite\n- Selection criteria:\n  - Classic, still-relevant models (e.g., ResNet, VGG, MobileNet),\n  - Modern, widely used architectures (e.g., Transformers, BERT-like models),\n  - Varied domains (vision, NLP, recommendation, etc.),\n  - Varied execution modes (eager vs. compiler stacks), data types (FP32, AMP), and quantization.\n- Adaptation and harness:\n  - Standardized inputs, batch sizes, and preprocessing; reproducible seeds;\n  - Configurable toggles for precision, compilers, devices;\n  - Built-in profilers to collect execution time, operator-level stats, kernel-level breakdowns, and memory metrics.\n- Integration:\n  - Designed to be run locally and in CI, with baselining and regression detection (thresholds, alerts).\n\nWhat we measured and why it helps\n- Execution time breakdowns:\n  - How much is Python/dispatcher overhead versus GPU kernel time?\n  - Which operators dominate per model?\n  - Are we bound by compute, memory, or launch overhead?\n- Compiler comparisons:\n  - How do PyTorch compilers (e.g., TorchDynamo+Inductor) change the profile relative to eager?\n  - Do we see benefits consistently across models or only in certain operator patterns?\n- Hardware comparisons:\n  - Comparing GPUs (e.g., NVIDIA vs. AMD), but importantly doing so with enough models to avoid misleading single-workload conclusions.\n\nA few insights we surfaced\n- “Which GPU is faster for PyTorch?” depends on the operator mix and how well each backend path is exercised. Prior dueling claims (AMD beats NVIDIA vs. the reverse) often stemmed from narrow workloads. With broader coverage, you find cases that favor each vendor—and you learn exactly which kernels/operators are tipping the scales.\n- Cold paths matter at scale. If a model’s graph hits “rare” codepaths thousands of times per batch (e.g., through a dispatcher or error handling), that “rare” path is no longer rare for that workload. TorchBench makes those visible.\n\nTwo practical wins (and why they matter)\n1) Finding and fixing PyTorch performance issues\n   - The c10_Exception case: ~10× slowdown in quantized models traced to error-handling overhead. Fix accepted upstream.\n   - Similar cases included unnecessary syncs, suboptimal kernel selection, or Python-layer overheads that compilers could fuse away or avoid.\n2) Continuous integration guardrails\n   - We wired TorchBench into PyTorch’s CI. When a daily commit degrades performance for any covered model/config beyond a threshold, we flag it immediately. This prevents regressions from camping in main.\n\nWhat TorchBench enables users to do\n- PyTorch maintainers: catch regressions as they happen, prioritize real-world hot spots, and validate performance patches across a broad workload set.\n- Researchers and practitioners: evaluate new compiler passes or kernels against realistic, varied models and quickly see where they help or hurt.\n- Hardware/software co-designers: get a more faithful picture of how different operator families stress backends and where investment pays off.\n\nWhat TorchBench is not\n- It’s not a replacement for MLPerf’s standardized, apples-to-apples hardware contests. MLPerf is ideal for vendor comparisons under strict rules. TorchBench is a developer- and maintainer-facing suite to stress the PyTorch stack broadly and continuously, to expose real bottlenecks and protect performance over time.\n\nTakeaways in plain terms\n- If you only test a few models, you’ll miss important performance problems and draw shaky conclusions.\n- Broader API surface coverage reveals the “long tail” of real issues users hit.\n- With TorchBench, we found and fixed problems that genuinely sped up models for users, and we put a safety net in PyTorch CI so they don’t come back.\n\nIf you want to replicate the spirit of our approach for another framework or internal stack:\n- Curate for coverage: include models that light up different operator families, dtypes, shapes, and execution modes.\n- Instrument deeply: collect per-op, per-kernel, Python/dispatcher breakdowns.\n- Automate regression detection: baseline, threshold, alert.\n- Close the loop: file issues, upstream fixes, re-baseline.\n\nReference\n- TorchBench: Benchmarking PyTorch with High API Surface Coverage. Yueming Hao, Xu Zhao, Bin Bao, David Berard, Will Constable, Adnan Aziz, Xu Liu. arXiv:2304.14226. Open-source and evolving.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d1a125a88c819ea28d7e2f4d22c42d0f386d0d1aa085ca"}},"resumableStreamId":"STREAM:14bd4f79-6618-4e5b-9289-098d6afbef82","role":"assistant","status":"done","threadId":"1dec2aab-3170-4af0-b945-458c9d6e802d","timeToFirstToken":1.32,"tokens":1476,"tokensPerSecond":44.02684563758389,"updated_at":1758568730154,"userId":"google:104458561188320708185","id":"j97872w17axde8djncg645avnx7r2aed"},{"_creationTime":1758568817344.0938,"_id":"j9784k3q3ackeqyg3abf7ty55n7r33jp","attachmentIds":[],"content":"How does it test against different vendirs","created_at":1758568817344,"messageId":"ff1bab9d-2f49-46f0-8cac-9bec3b1b8d32","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"1dec2aab-3170-4af0-b945-458c9d6e802d","updated_at":1758568817344,"userId":"google:104458561188320708185","id":"j9784k3q3ackeqyg3abf7ty55n7r33jp","providerMetadata":{}},{"_creationTime":1758568817344.094,"_id":"j97cec92m362y4k0kaj1mchkps7r2jjn","attachmentIds":[],"byok":false,"content":"Do you mean different hardware/software vendors (e.g., NVIDIA vs. AMD GPUs, different BLAS/DNN libraries, or different compiler backends)? Here’s how TorchBench (from the paper) is set up to test across vendors:\n\n- Model suite is vendor-agnostic\n  - The benchmark models are pure PyTorch; no vendor-specific code paths are hardcoded. This lets the same workloads run on different devices/backends without changes.\n\n- Device/backend selection via configuration\n  - You choose the device (e.g., cuda vs. rocm) and execution mode (eager vs. TorchDynamo+Inductor, etc.) through flags/configs. The harness routes runs to the selected backend, so:\n    - NVIDIA typically exercises CUDA + cuDNN/cuBLAS.\n    - AMD typically exercises ROCm + MIOpen/rocBLAS.\n    - CPU runs exercise MKL/oneDNN (where applicable).\n\n- Multiple precisions and modes\n  - Runs can be toggled across FP32, AMP (automatic mixed precision), and sometimes quantized variants. This matters because vendor kernels may differ in maturity across dtypes, which the suite will expose.\n\n- Uniform inputs and repeatable seeds\n  - Standardized batch sizes, input shapes, and seeds keep comparisons fair across vendors and reduce variance-driven noise.\n\n- Profiling and breakdowns\n  - Built-in profiling attributes time to Python/dispatcher vs. operator vs. kernel time, and captures operator-level hotspots. That helps distinguish:\n    - true kernel/library advantages,\n    - launch/dispatcher overheads,\n    - compiler/codegen differences (e.g., Inductor-generated kernels).\n\n- CI and regression tracking per backend\n  - The same model matrix runs in CI on different backends. If a change in PyTorch, a compiler, or a vendor library regresses performance on, say, ROCm but not CUDA, it’s flagged specifically for that vendor path.\n\n- Coverage-driven insights\n  - Because the suite spans many operator families and shapes, it tends to trigger different backend code paths (e.g., convolution algorithms, attention kernels, normalization ops), revealing where one vendor leads or lags. This avoids single-model bias when comparing vendors.\n\n- Apples-to-apples comparison discipline\n  - The harness encourages consistent:\n    - software stacks (driver/runtime versions pinned per run),\n    - environment flags (e.g., deterministic settings, TF32 on/off),\n    - warmup/iteration counts,\n  so differences are attributable to the backend rather than configuration drift.\n\nWhat you’d do in practice\n- Install the appropriate PyTorch build for each backend (e.g., pip wheels for CUDA vs. ROCm).\n- Run TorchBench with a device/backend flag per matrix entry:\n  - Examples: eager FP32 on CUDA, eager AMP on CUDA, Inductor+AMP on CUDA; replicate the same triplet on ROCm.\n- Collect per-model throughput/latency and profiles, then compare distributions (median, p95) rather than single points.\n\nLimitations to keep in mind\n- Exact feature parity isn’t guaranteed across vendors (e.g., some fused kernels or quantized paths may be missing or experimental on a backend). The suite will surface those gaps; results should note skipped/fallback cases.\n- Kernel algorithm selection heuristics can differ and may require tuning flags for perfectly fair comparisons; TorchBench provides the baseline, and you can add tuned runs if you need best-case vendor comparisons.\n\nIf you want, tell me which vendors/backends you care about (e.g., NVIDIA CUDA 12.4 vs. AMD ROCm 6.x, CPU oneDNN), and I’ll sketch a concrete run matrix and commands.","created_at":1758568817345,"messageId":"a4fe42d5-c1ff-41c0-8f07-975b85fc82e5","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Do you mean different hardware/software vendors (e.g., NVIDIA vs. AMD GPUs, different BLAS/DNN libraries, or different compiler backends)? Here’s how TorchBench (from the paper) is set up to test across vendors:\n\n- Model suite is vendor-agnostic\n  - The benchmark models are pure PyTorch; no vendor-specific code paths are hardcoded. This lets the same workloads run on different devices/backends without changes.\n\n- Device/backend selection via configuration\n  - You choose the device (e.g., cuda vs. rocm) and execution mode (eager vs. TorchDynamo+Inductor, etc.) through flags/configs. The harness routes runs to the selected backend, so:\n    - NVIDIA typically exercises CUDA + cuDNN/cuBLAS.\n    - AMD typically exercises ROCm + MIOpen/rocBLAS.\n    - CPU runs exercise MKL/oneDNN (where applicable).\n\n- Multiple precisions and modes\n  - Runs can be toggled across FP32, AMP (automatic mixed precision), and sometimes quantized variants. This matters because vendor kernels may differ in maturity across dtypes, which the suite will expose.\n\n- Uniform inputs and repeatable seeds\n  - Standardized batch sizes, input shapes, and seeds keep comparisons fair across vendors and reduce variance-driven noise.\n\n- Profiling and breakdowns\n  - Built-in profiling attributes time to Python/dispatcher vs. operator vs. kernel time, and captures operator-level hotspots. That helps distinguish:\n    - true kernel/library advantages,\n    - launch/dispatcher overheads,\n    - compiler/codegen differences (e.g., Inductor-generated kernels).\n\n- CI and regression tracking per backend\n  - The same model matrix runs in CI on different backends. If a change in PyTorch, a compiler, or a vendor library regresses performance on, say, ROCm but not CUDA, it’s flagged specifically for that vendor path.\n\n- Coverage-driven insights\n  - Because the suite spans many operator families and shapes, it tends to trigger different backend code paths (e.g., convolution algorithms, attention kernels, normalization ops), revealing where one vendor leads or lags. This avoids single-model bias when comparing vendors.\n\n- Apples-to-apples comparison discipline\n  - The harness encourages consistent:\n    - software stacks (driver/runtime versions pinned per run),\n    - environment flags (e.g., deterministic settings, TF32 on/off),\n    - warmup/iteration counts,\n  so differences are attributable to the backend rather than configuration drift.\n\nWhat you’d do in practice\n- Install the appropriate PyTorch build for each backend (e.g., pip wheels for CUDA vs. ROCm).\n- Run TorchBench with a device/backend flag per matrix entry:\n  - Examples: eager FP32 on CUDA, eager AMP on CUDA, Inductor+AMP on CUDA; replicate the same triplet on ROCm.\n- Collect per-model throughput/latency and profiles, then compare distributions (median, p95) rather than single points.\n\nLimitations to keep in mind\n- Exact feature parity isn’t guaranteed across vendors (e.g., some fused kernels or quantized paths may be missing or experimental on a backend). The suite will surface those gaps; results should note skipped/fallback cases.\n- Kernel algorithm selection heuristics can differ and may require tuning flags for perfectly fair comparisons; TorchBench provides the baseline, and you can add tuned runs if you need best-case vendor comparisons.\n\nIf you want, tell me which vendors/backends you care about (e.g., NVIDIA CUDA 12.4 vs. AMD ROCm 6.x, CPU oneDNN), and I’ll sketch a concrete run matrix and commands.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d1a171df408192b1ae6c01321abaae03610e26151bd102"}},"resumableStreamId":"STREAM:12fa244d-76f0-4b64-a2fb-dc790a6d4ce8","role":"assistant","status":"done","threadId":"1dec2aab-3170-4af0-b945-458c9d6e802d","timeToFirstToken":0.82,"tokens":757,"tokensPerSecond":62.97313035521171,"updated_at":1758568817345,"userId":"google:104458561188320708185","id":"j97cec92m362y4k0kaj1mchkps7r2jjn"},{"_creationTime":1758568936298.0403,"_id":"j9790v1hh5fmnn0rf9rg8m5gnn7r35x0","attachmentIds":[],"content":"explain the testing pipeline","created_at":1758568936298,"messageId":"8c86861b-5633-4977-bc09-d584bb79d7b4","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"1dec2aab-3170-4af0-b945-458c9d6e802d","updated_at":1758568936298,"userId":"google:104458561188320708185","id":"j9790v1hh5fmnn0rf9rg8m5gnn7r35x0","providerMetadata":{}},{"_creationTime":1758568936298.0405,"_id":"j9763kcc2yytbbes3z0crmbxys7r3p1r","attachmentIds":[],"byok":false,"content":"Here’s the testing pipeline, end to end, the way we designed and operate it for TorchBench in the paper “Benchmarking PyTorch with High API Surface Coverage.”\n\n1) Suite definition and curation\n- Model registry: A curated list of real PyTorch models (vision, NLP, recsys, generative, classic/modern). Each entry defines:\n  - How to construct the model (and variants, e.g., eager vs compiled).\n  - Canonical input shapes and batch sizes.\n  - Optional training/inference steps (forward-only vs forward+backward).\n- Config matrix: A standardized matrix of run modes:\n  - Device/backend: cpu, cuda, rocm (and their pinned versions).\n  - Precision: fp32, amp; sometimes bf16 or quantized variants.\n  - Execution: eager; compilers like TorchDynamo + Inductor; optional graph capture.\n  - Task: inference or training.\n- Repro controls: Fixed seeds, fixed warmup/measurement iters, deterministic toggles when meaningful, and environment pinning (drivers, runtimes, library versions).\n\n2) Environment setup\n- Resolve and pin the software stack per backend:\n  - PyTorch build (e.g., CUDA vs. ROCm wheel), driver/runtime, math libraries (cuDNN/cuBLAS or MIOpen/rocBLAS), compiler toolchain.\n- Machine characterization:\n  - Collect hardware fingerprints (GPU model/SM count, CPU model, memory, PCIe, clocks if fixed).\n  - Record kernel/driver versions for provenance.\n\n3) Model preparation\n- Download weights or instantiate random-initialized models as specified.\n- Apply toggles per run config:\n  - Move to device.\n  - Enable AMP/autocast when selected.\n  - Enable compilers: torch.compile or TorchDynamo+Inductor with chosen settings.\n  - Quantization path if supported by the model.\n- Warmup runs:\n  - N warmup iterations to trigger JIT/compile, autotuning, kernel selection, and cache stabilization.\n  - Optional CUDA graph capture in a dedicated warmup phase when enabled.\n\n4) Measurement loop\n- Timed iterations:\n  - Run M measurement iterations after warmup; synchronize device boundaries (e.g., torch.cuda.synchronize) to ensure accurate wall-time.\n  - Record:\n    - Latency per iter and throughput (items/sec).\n    - Optional p50/p90/p95 timing distribution.\n- Profiling (optional but common in CI/nightly):\n  - PyTorch profiler with CPU/GPU events and operator attribution.\n  - Operator-level stats (self time, total time, call counts).\n  - Kernel-level stats (duration, launches, stream, occupancy proxies if available).\n  - Breakdown buckets: Python/dispatcher, ATen operators, kernels, memory copies, syncs.\n- Memory and stability:\n  - Peak memory usage on device/host.\n  - Check for OOMs, NaNs, numerical anomalies (where applicable).\n\n5) Result normalization and metadata\n- Normalize per-model metric:\n  - Report both end-to-end latency/throughput and per-token/per-image normalized metrics where relevant.\n  - Note skipped ops or fallbacks (e.g., dtype not supported on a backend).\n- Attach metadata:\n  - Exact commit SHAs (PyTorch, TorchInductor), driver/library versions, compiler flags, env variables.\n  - Run config (device, precision, execution mode, batch size, task type).\n\n6) Baseline comparison and regression detection\n- Baselines:\n  - Maintain moving baselines per (model, device/backend, precision, execution mode, task).\n  - Store historical metrics (e.g., last green commit in main; nightly aggregates).\n- Thresholding:\n  - Detect regressions if performance deviates beyond configurable thresholds (e.g., >5% slowdown on median throughput) after accounting for variance.\n  - Filter out noisy models with variance guards (min iterations, coefficient-of-variation checks).\n- Attribution aids:\n  - Automatic diff vs. prior profile: highlight operators/kernels with largest deltas.\n  - Flag common culprits: increased syncs, dispatcher overhead, kernel selection changes, compile cache misses.\n\n7) Reporting and dashboards\n- Per-run artifacts:\n  - JSON/CSV of metrics, profiler traces (Chrome trace), logs.\n- Dashboards:\n  - Trend lines per model/config.\n  - Heatmaps across the config matrix (green = faster, red = slower).\n  - Drill-down views: operator/kernels ranked by time and change vs baseline.\n- Notifications:\n  - CI bots post regressions to PRs or channels with a minimal repro link and suggested owners (based on blame/area labels).\n\n8) CI integration and gating\n- Continuous runs:\n  - Lightweight smoke runs on pull requests (reduced model set, fewer iterations) to catch egregious changes.\n  - Full nightly runs on broader matrix with profiling enabled.\n- Gating policy:\n  - Hard gate for severe regressions on key models; soft alerts for long-tail models.\n- Auto-bisect (optional in infra):\n  - If a nightly detects a regression, a queued bisect narrows to the offending commit using the same harness.\n\n9) Cross-vendor methodology\n- Exact same harness and model configs applied to each backend build (CUDA, ROCm, CPU).\n- Backend-specific knobs kept explicit and logged (e.g., TF32 allow, deterministic algorithms, convolution heuristics).\n- If a model is unsupported on a backend (missing ops/kernels), mark as xfail/skip with reason; do not mix partial results into aggregate vendor comparisons.\n\n10) Feedback loop and fixes\n- Reproduce locally using the stored command line and seed.\n- Use profiler artifacts to trace to:\n  - Dispatcher hot spots, error-handling paths, Python overhead.\n  - Kernel algorithm changes or missing fusions in compilers.\n  - Library/version regressions (e.g., cuDNN algo change).\n- Land fixes upstream, rerun affected benchmarks, and re-baseline once green.\n\nWhat you run as a user\n- Select your run matrix (e.g., models × {cpu, cuda, rocm} × {fp32, amp} × {eager, compile}).\n- Run the harness:\n  - It installs/preps env, warms up, measures, profiles (if chosen), and emits artifacts.\n- Compare against stored baselines or across backends using the provided reports.\n\nCommon gotchas the pipeline guards against\n- Warmup/compilation skew: separates warmup from measurement; records compile time separately when relevant.\n- Async timing mistakes: forces sync at measurement boundaries.\n- Config drift: pins versions and records env details; diffs configs between baseline and test.\n- Variance: uses sufficient iterations and variance-based gating to avoid false positives.\n- Unfair comparisons: normalizes per sample where appropriate; flags unsupported or fallback paths.\n\nIf you tell me your target setup (e.g., NVIDIA A100 CUDA 12.4, AMD MI300 ROCm 6.x, models you care about), I can give you concrete commands and a minimal CI matrix to replicate this pipeline.","created_at":1758568936299,"messageId":"f98e87cf-57ec-4382-9d2d-fe2cc475eea9","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Here’s the testing pipeline, end to end, the way we designed and operate it for TorchBench in the paper “Benchmarking PyTorch with High API Surface Coverage.”\n\n1) Suite definition and curation\n- Model registry: A curated list of real PyTorch models (vision, NLP, recsys, generative, classic/modern). Each entry defines:\n  - How to construct the model (and variants, e.g., eager vs compiled).\n  - Canonical input shapes and batch sizes.\n  - Optional training/inference steps (forward-only vs forward+backward).\n- Config matrix: A standardized matrix of run modes:\n  - Device/backend: cpu, cuda, rocm (and their pinned versions).\n  - Precision: fp32, amp; sometimes bf16 or quantized variants.\n  - Execution: eager; compilers like TorchDynamo + Inductor; optional graph capture.\n  - Task: inference or training.\n- Repro controls: Fixed seeds, fixed warmup/measurement iters, deterministic toggles when meaningful, and environment pinning (drivers, runtimes, library versions).\n\n2) Environment setup\n- Resolve and pin the software stack per backend:\n  - PyTorch build (e.g., CUDA vs. ROCm wheel), driver/runtime, math libraries (cuDNN/cuBLAS or MIOpen/rocBLAS), compiler toolchain.\n- Machine characterization:\n  - Collect hardware fingerprints (GPU model/SM count, CPU model, memory, PCIe, clocks if fixed).\n  - Record kernel/driver versions for provenance.\n\n3) Model preparation\n- Download weights or instantiate random-initialized models as specified.\n- Apply toggles per run config:\n  - Move to device.\n  - Enable AMP/autocast when selected.\n  - Enable compilers: torch.compile or TorchDynamo+Inductor with chosen settings.\n  - Quantization path if supported by the model.\n- Warmup runs:\n  - N warmup iterations to trigger JIT/compile, autotuning, kernel selection, and cache stabilization.\n  - Optional CUDA graph capture in a dedicated warmup phase when enabled.\n\n4) Measurement loop\n- Timed iterations:\n  - Run M measurement iterations after warmup; synchronize device boundaries (e.g., torch.cuda.synchronize) to ensure accurate wall-time.\n  - Record:\n    - Latency per iter and throughput (items/sec).\n    - Optional p50/p90/p95 timing distribution.\n- Profiling (optional but common in CI/nightly):\n  - PyTorch profiler with CPU/GPU events and operator attribution.\n  - Operator-level stats (self time, total time, call counts).\n  - Kernel-level stats (duration, launches, stream, occupancy proxies if available).\n  - Breakdown buckets: Python/dispatcher, ATen operators, kernels, memory copies, syncs.\n- Memory and stability:\n  - Peak memory usage on device/host.\n  - Check for OOMs, NaNs, numerical anomalies (where applicable).\n\n5) Result normalization and metadata\n- Normalize per-model metric:\n  - Report both end-to-end latency/throughput and per-token/per-image normalized metrics where relevant.\n  - Note skipped ops or fallbacks (e.g., dtype not supported on a backend).\n- Attach metadata:\n  - Exact commit SHAs (PyTorch, TorchInductor), driver/library versions, compiler flags, env variables.\n  - Run config (device, precision, execution mode, batch size, task type).\n\n6) Baseline comparison and regression detection\n- Baselines:\n  - Maintain moving baselines per (model, device/backend, precision, execution mode, task).\n  - Store historical metrics (e.g., last green commit in main; nightly aggregates).\n- Thresholding:\n  - Detect regressions if performance deviates beyond configurable thresholds (e.g., >5% slowdown on median throughput) after accounting for variance.\n  - Filter out noisy models with variance guards (min iterations, coefficient-of-variation checks).\n- Attribution aids:\n  - Automatic diff vs. prior profile: highlight operators/kernels with largest deltas.\n  - Flag common culprits: increased syncs, dispatcher overhead, kernel selection changes, compile cache misses.\n\n7) Reporting and dashboards\n- Per-run artifacts:\n  - JSON/CSV of metrics, profiler traces (Chrome trace), logs.\n- Dashboards:\n  - Trend lines per model/config.\n  - Heatmaps across the config matrix (green = faster, red = slower).\n  - Drill-down views: operator/kernels ranked by time and change vs baseline.\n- Notifications:\n  - CI bots post regressions to PRs or channels with a minimal repro link and suggested owners (based on blame/area labels).\n\n8) CI integration and gating\n- Continuous runs:\n  - Lightweight smoke runs on pull requests (reduced model set, fewer iterations) to catch egregious changes.\n  - Full nightly runs on broader matrix with profiling enabled.\n- Gating policy:\n  - Hard gate for severe regressions on key models; soft alerts for long-tail models.\n- Auto-bisect (optional in infra):\n  - If a nightly detects a regression, a queued bisect narrows to the offending commit using the same harness.\n\n9) Cross-vendor methodology\n- Exact same harness and model configs applied to each backend build (CUDA, ROCm, CPU).\n- Backend-specific knobs kept explicit and logged (e.g., TF32 allow, deterministic algorithms, convolution heuristics).\n- If a model is unsupported on a backend (missing ops/kernels), mark as xfail/skip with reason; do not mix partial results into aggregate vendor comparisons.\n\n10) Feedback loop and fixes\n- Reproduce locally using the stored command line and seed.\n- Use profiler artifacts to trace to:\n  - Dispatcher hot spots, error-handling paths, Python overhead.\n  - Kernel algorithm changes or missing fusions in compilers.\n  - Library/version regressions (e.g., cuDNN algo change).\n- Land fixes upstream, rerun affected benchmarks, and re-baseline once green.\n\nWhat you run as a user\n- Select your run matrix (e.g., models × {cpu, cuda, rocm} × {fp32, amp} × {eager, compile}).\n- Run the harness:\n  - It installs/preps env, warms up, measures, profiles (if chosen), and emits artifacts.\n- Compare against stored baselines or across backends using the provided reports.\n\nCommon gotchas the pipeline guards against\n- Warmup/compilation skew: separates warmup from measurement; records compile time separately when relevant.\n- Async timing mistakes: forces sync at measurement boundaries.\n- Config drift: pins versions and records env details; diffs configs between baseline and test.\n- Variance: uses sufficient iterations and variance-based gating to avoid false positives.\n- Unfair comparisons: normalizes per sample where appropriate; flags unsupported or fallback paths.\n\nIf you tell me your target setup (e.g., NVIDIA A100 CUDA 12.4, AMD MI300 ROCm 6.x, models you care about), I can give you concrete commands and a minimal CI matrix to replicate this pipeline.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d1a1e8f0cc819cbf7a61f26e23359d0b5e02f2b8715a82"}},"resumableStreamId":"STREAM:5e436957-7e0e-498a-9be4-c8cf6d712763","role":"assistant","status":"done","threadId":"1dec2aab-3170-4af0-b945-458c9d6e802d","timeToFirstToken":1.164,"tokens":1469,"tokensPerSecond":42.75195716073455,"updated_at":1758568936299,"userId":"google:104458561188320708185","id":"j9763kcc2yytbbes3z0crmbxys7r3p1r"},{"_creationTime":1758569267805.1257,"_id":"j974taxpyr15cz35ccwq5dqz2d7r3rwx","attachmentIds":[],"content":"what APIs did torchbench cover and why they were included","created_at":1758569267805,"messageId":"647b1f97-ef4a-4c21-bec6-eef002a533c1","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"1dec2aab-3170-4af0-b945-458c9d6e802d","updated_at":1758569267805,"userId":"google:104458561188320708185","id":"j974taxpyr15cz35ccwq5dqz2d7r3rwx","providerMetadata":{}},{"_creationTime":1758569267805.126,"_id":"j977zw9z45mrmphhz1cd64zcv57r3yca","attachmentIds":[],"byok":false,"content":"Short answer: TorchBench doesn’t hardcode a flat list of APIs; it achieves high API coverage by selecting real models that naturally exercise broad parts of PyTorch. In the paper, we quantify coverage at the level of ATen operators, dispatcher paths, autocast/AMP, autograd, compilers, and quantization/error paths, and we explain why each matters.\n\nWhat TorchBench covers (by layer) and why\n\n- Frontend model APIs and modules\n  - nn.Modules and layers: Conv/Linear/Embedding/Norm/Activation/Pooling/Dropout/RNN/Transformer blocks; vision backbones (ResNet, VGG, MobileNet), NLP transformers (BERT/GPT-like), recsys components, generative models.\n  - Why: These are the user-facing building blocks; different model families trigger very different operator mixes, tensor shapes, and control flows that stress the runtime differently.\n\n- ATen operators (eager)\n  - Dense linear algebra: matmul/addmm/bmm/gemm; pointwise ops; broadcast semantics.\n  - Convolutions and fused conv+activation variants; pooling; normalization ops (batch/group/layer/instance norm).\n  - Reductions and scans: sum/mean/argmax/topk/softmax/logsumexp.\n  - Tensor shape/stride ops: view/reshape/permute/contiguous/cat/split/gather/scatter/advanced indexing.\n  - Why: ATen ops are the core kernel surface. Different models emphasize different families (e.g., conv-heavy CV vs. matmul-heavy Transformers), revealing backend- and dtype-specific issues.\n\n- Dispatcher and device backends\n  - CPU, CUDA, ROCm registrations; out/inplace/functional variants; tensor iterator and composite kernels.\n  - Why: Ensures we exercise both device-specific kernel registrations and composite fallbacks; catches issues like slow composite paths or missing specialized kernels on a backend.\n\n- Autograd engine\n  - Backward graphs for training configs; first-order gradients across common ops; optimizer step overhead in select training runs.\n  - Why: Backprop adds very different operator mixes (grad formulas, reductions) and stresses the graph engine, dependency tracking, and synchronization.\n\n- Mixed precision and autocast\n  - AMP autocast for CUDA and ROCm; dtype promotion rules (fp32, fp16/bf16) and casting patterns.\n  - Why: Autocast dispatches different kernels and can hit precision-specific fused kernels or fallbacks; many regressions are dtype-specific.\n\n- Compilers and graph capture paths\n  - torch.compile/TorchDynamo front-end; AOTAutograd; TorchInductor codegen; CUDA Graphs where configured.\n  - Why: Compiled paths change the kernel mix (fusion, scheduling) and can expose correctness/perf gaps vs. eager; important to track benefits/regressions per model.\n\n- Quantization and torch.ops fallbacks\n  - Quantized operator use via quantized models; torch.ops custom/extension calls.\n  - Why: Quantization exercises different kernel sets and, critically, error/fallback paths through torch.ops (the paper’s c10_Exception case causing 10× slowdowns).\n\n- Memory and I/O paths\n  - Dataloader-lite inputs; tensor transfers H2D/D2H; pinned memory and stream behavior visible in profiles.\n  - Why: Identifies bandwidth/transfer bottlenecks and inadvertent syncs that show up only in full-model runs.\n\nHow coverage is measured and selected\n\n- Operator/API coverage metric\n  - We count unique ATen operators and dispatcher entries touched by the suite under a standard config matrix (devices, precisions, eager vs. compiled). TorchBench covers about 2.3× more of PyTorch’s API surface than MLPerf in our measurements.\n  - Why: Operator- and path-level coverage better predicts real-world regressions than “model count.”\n\n- Model curation for coverage\n  - Include classic CV (conv-heavy), modern Transformers (matmul/attention-heavy), normalization-heavy nets, sequence models, and models with dynamic shapes or control flow.\n  - Pair each with multiple dtypes and execution modes to light up additional kernels and code paths.\n\nRepresentative API families that TorchBench intentionally lights up\n\n- Convolutional stack: conv{1,2,3}d, depthwise, grouped, transposed; cudnn/miopen algorithm selection.\n- Attention stack: scaled dot-product attention, softmax/logits masking, einsum/bmm, fused attention kernels where available.\n- Normalization: batch/group/layer/instance norms and RMSNorm variants; statistics/reduction corner cases.\n- Linear algebra: addmm/matmul/bmm; batched GEMM shapes from tiny to very large.\n- Elementwise and broadcasts: relu/gelu/silu/tanh/sigmoid; add/mul/div with broadcasting; type promotions.\n- Tensor transforms: reshape/view/permute/transpose/contiguous/expand; cat/stack/split/chunk; gather/scatter/index_add.\n- Reductions: sum/mean/amax/amin/argmax/topk; numerically sensitive softmax/logsumexp.\n- Autograd functions: grad formulas of the above, ensuring backward kernels are exercised.\n- Quantization: quantize/dequantize, per-tensor/per-channel observers in models that support it.\n- Runtime/infra: allocator behavior, stream/record_function events, dispatcher overhead, error-handling paths (c10_Exception), torch.ops not-implemented fallthrough.\n\nWhy each was included\n\n- To represent real workloads, not microbenchmarks, thereby activating long-tail code paths (dispatcher fallbacks, error handling, composite kernels) that typical narrow suites miss.\n- To compare backends fairly: same models stress different vendor libraries in different ways; coverage prevents single-workload bias.\n- To guard PyTorch CI: broad coverage makes regressions visible the day they land, including in “cold” paths that become hot under certain models/dtypes.\n\nIf you want, tell me the specific API areas you care about (e.g., attention kernels, quantization, AMP on ROCm), and I can map which TorchBench models/configs hit them and how to profile their usage.","created_at":1758569267806,"messageId":"bf01b88f-6c81-44c6-9e85-f6ad11e1258e","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":null,"queries":["TorchBench API surface coverage which PyTorch APIs covered and why included site:arxiv.org 2304.14226","\"Benchmarking PyTorch with High API Surface Coverage\" APIs covered operators dispatcher autograd c10_Exception quantization paper details","TorchBench API coverage list of operators domains included paper 2023 arXiv 2304.14226"]},"result":[{"content":"# Computer Science > Machine Learning\n\n**arXiv:2304.14226** (cs)\n\n\\[Submitted on 27 Apr 2023 ( [v1](https://arxiv.org/abs/2304.14226v1)), last revised 24 Jun 2023 (this version, v3)\\]\n\n# Title:TorchBench: Benchmarking PyTorch with High API Surface Coverage\n\nAuthors: [Yueming Hao](https://arxiv.org/search/cs?searchtype=author&query=Hao,+Y), [Xu Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao,+X), [Bin Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao,+B), [David Berard](https://arxiv.org/search/cs?searchtype=author&query=Berard,+D), [Will Constable](https://arxiv.org/search/cs?searchtype=author&query=Constable,+W), [Adnan Aziz](https://arxiv.org/search/cs?searchtype=author&query=Aziz,+A), [Xu Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+X)\n\nView a PDF of the paper titled TorchBench: Benchmarking PyTorch with High API Surface Coverage, by Yueming Hao and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/2304.14226)\n\n> Abstract:Deep learning (DL) has been a revolutionary technique in various domains. To facilitate the model development and deployment, many deep learning frameworks are proposed, among which PyTorch is one of the most popular solutions. The performance of ecosystem around PyTorch is critically important, which saves the costs of training models and reduces the response time of model inferences. In this paper, we propose TorchBench, a novel benchmark suite to study the performance of PyTorch software stack. Unlike existing benchmark suites, TorchBench encloses many representative models, covering a large PyTorch API surface. TorchBench is able to comprehensively characterize the performance of the PyTorch software stack, guiding the performance optimization across models, PyTorch framework, and GPU libraries. We show two practical use cases of TorchBench. (1) We profile TorchBench to identify GPU performance inefficiencies in PyTorch. We are able to optimize many performance bugs and upstream patches to the official PyTorch repository. (2) We integrate TorchBench into PyTorch continuous integration system. We are able to identify performance regression in multiple daily code checkins to prevent PyTorch repository from introducing performance bugs. TorchBench is open source and keeps evolving.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Performance (cs.PF) |\n| Cite as: | [arXiv:2304.14226](https://arxiv.org/abs/2304.14226) \\[cs.LG\\] |\n| (or [arXiv:2304.14226v3](https://arxiv.org/abs/2304.14226v3) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2304.14226](https://doi.org/10.48550/arXiv.2304.14226) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Yueming Hao \\[ [view email](https://arxiv.org/show-email/180a277c/2304.14226)\\] **[\\[v1\\]](https://arxiv.org/abs/2304.14226v1)**\nThu, 27 Apr 2023 14:37:05 UTC (772 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2304.14226v2)**\nFri, 28 Apr 2023 19:56:19 UTC (772 KB)\n**\\[v3\\]**\nSat, 24 Jun 2023 16:57:43 UTC (751 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled TorchBench: Benchmarking PyTorch with High API Surface Coverage, by Yueming Hao and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/2304.14226)\n- [TeX Source](https://arxiv.org/src/2304.14226)\n- [Other Formats](https://arxiv.org/format/2304.14226)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[< prev](https://arxiv.org/prevnext?id=2304.14226&function=prev&context=cs.LG)  \\|  [next >](https://arxiv.org/prevnext?id=2304.14226&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2023-04](https://arxiv.org/list/cs.LG/2023-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2304.14226?context=cs) [cs.AI](https://arxiv.org/abs/2304.14226?context=cs.AI) [cs.PF](https://arxiv.org/abs/2304.14226?context=cs.PF)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2304.14226)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2304.14226)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2304.14226)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2304.14226) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2304.14226","id":"https://arxiv.org/abs/2304.14226","publishedDate":"2023-05-16T00:00:00.000Z","summary":"TorchBench is a novel benchmark suite designed to study the performance of the PyTorch software stack. It covers a large PyTorch API surface, encompassing many representative models. TorchBench helps characterize the performance of the PyTorch software stack, guiding performance optimization across models, the PyTorch framework, and GPU libraries. The authors used TorchBench to identify GPU performance inefficiencies in PyTorch, optimize performance bugs, and integrate it into the PyTorch continuous integration system to prevent performance regressions.\n","title":"TorchBench: Benchmarking PyTorch with High API Surface Coverage","url":"https://arxiv.org/abs/2304.14226"},{"content":"TorchBench: Benchmarking PyTorch with High API Surface\nCoverage\nYueming Hao\nyhao24@ncsu.edu\nNorth Carolina State University\nRaleigh, North Carolina, USA\nXu Zhao\nxzhao9@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nBin Bao\nbinbao@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nDavid Berard\ndberard@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nWill Constable\nwhc@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nAdnan Aziz\nadnanaziz@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nXu Liu\nxliu88@ncsu.edu\nNorth Carolina State University\nRaleigh, North Carolina, USA\nABSTRACT\nDeep learning (DL) has been a revolutionary technique in vari\u0002ous domains. To facilitate the model development and deployment,\nmany deep learning frameworks are proposed, among which Py\u0002Torch is one of the most popular solutions. The performance of eco\u0002system around PyTorch is critically important, which saves the costs\nof training models and reduces the response time of model infer\u0002ences. In this paper, we propose TorchBench, a novel benchmark\nsuite to study the performance of PyTorch software stack. Unlike\nexisting benchmark suites, TorchBench encloses many represen\u0002tative models, covering a large PyTorch API surface. TorchBench\nis able to comprehensively characterize the performance of the Py\u0002Torch software stack, guiding the performance optimization across\nmodels, PyTorch framework, and GPU libraries. We show two prac\u0002tical use cases of TorchBench. (1) We profile TorchBench to iden\u0002tify GPU performance inefficiencies in PyTorch. We are able to op\u0002timize many performance bugs and upstream patches to the official\nPyTorch repository. (2) We integrate TorchBench into PyTorch\ncontinuous integration system. We are able to identify performance\nregression in multiple daily code checkins to prevent PyTorch repos\u0002itory from introducing performance bugs. TorchBench is open\nsource and keeps evolving.\n1 INTRODUCTION\nDeep learning (DL) has been the most transformative technology\nover the past decade, given its wide applicability in global climate\nprojections [1, 2], image processing [3–5], speech recognition [6, 7],\ncontent recommendation [8–10], and pattern classification [11, 12],\nand great potential in emerging domains such as autonomous driv\u0002ing [13, 14], augmented/virtual reality [15, 16], and sciences [17, 18].\nTo facilitate the development and deployment of DL models, DL\nframework practitioners have proposed a number of DL frame\u0002works, such as Caffe [19], TVM [20], ONNX [21], MXNet [22],\nJAX [23], PyTorch [24], and TensorFlow [25]. These frameworks\nprovide rich APIs and support performance tuning.\nPyTorch is one of the most popular deep learning frameworks.\nAs an open-source project, PyTorch has more than 2000 contrib\u0002utors and has received more than 61,000 stars by the end of 2022.\nPyTorch is still actively evolving; PyTorch has over 110k commits in\n2022 [26]. The machine learning research papers based on PyTorch\nhave increased from 29% to 65% in the past four years [27].\nOne major challenge of deep learning tasks is the extensive\namount of computation for both training and inference, which\ntranslates to huge time and money costs. For example, the best\nversion of AlphaGo [28] needs weeks to train with 280 GPUs, and\nits estimated cost is 35 million dollars [29]. In addition, the real time\ninference of deep learning models on edge devices requires fast\nresponse and low latency. Thus, it is critically important for PyTorch\nto achieve bare-metal performance for boosting productivity and\nreducing costs.\nHowever, understanding and optimizing PyTorch performance is\nchallenging because of PyTorch’s complex and rapidly evolved code\nbases. On the one hand, the PyTorch software stack consists of three\nmajor components: the acceleration libraries (e.g., CuDNN [30]),\nthe PyTorch framework [24] (including optimizers and compilers),\nand deep learning models; performance inefficiencies can exist\nin or across any components. Without a systematic evaluation,\ninefficiencies can be difficult to identify. On the other hand, as an\nactive open-source project, PyTorch receives many patches for new\nfunctionality or performance improvement. Without a thorough\nevaluation, it is difficult to understand how these patches affect\nthe entire PyTorch software stack for various models in different\ndomains.\nBenchmarking is a well-known technique to understand the per\u0002formance of different workloads, programming languages, compil\u0002ers, and architectures. For example, SPEC provides many standard\nbenchmark suites for various purposes; Renassance [31] is a Java\nbenchmark suite; DeathStar [32] is a benchmark suite for microser\u0002vices; NPB [33] is a benchmark suite for different parallel models in\nscientific computing; and Rodinia [34] is a GPU benchmark suite.\nThese benchmarks provide indispensable performance insights for\n1\narXiv:2304.14226v3 [cs.LG] 24 Jun 2023\nYueming Hao, Xu Zhao, Bin Bao, David Berard, Will Constable, Adnan Aziz, and Xu Liu\nboth software and hardware evolution. MLPerf [35] is the state\u0002of-the-art benchmark suite for deep learning workloads. However,\nexisting deep learning benchmark suites, including MLPerf, aim\nto compare the performance of deep learning models running on\ndifferent hardware and frameworks. They usually include a small\nnumber of deep learning models (e.g., MLPerf has eight models\nonly) and cover a small PyTorch API surface, which fails to identify\nmany PyTorch performance bugs or fairly evaluate the performance\nimpact of patches.\n1.1 Motivating Examples\nWe show two examples to motivate the necessity of a comprehen\u0002sive benchmark suite for PyTorch.\nMisleading performance characterization. Understanding the per\u0002formance difference across various architectures is one of the ma\u0002jor tasks for benchmarking. However, without a comprehensive\nbenchmark suite, such characterization can result in misleading\nconclusions. For example, some studies [36] show that AMD GPUs\noutperform NVIDIA GPUs on PyTorch, and some studies [37, 38]\nshow an opposite conclusion. Thus, a comprehensive benchmark\nsuite is necessary for performance characterization. Unlike exist\u0002ing studies, we show a different conclusion and unique insights in\nSection 3.3.\nMissing performance bugs. Performance bugs can be buried deep\nin complex PyTorch code bases, especially in the cold execution\npaths. We identify a recent performance bug in PyTorch, which is\ninappropriate error handling. PyTorch implements an error han\u0002dling mechanism named c10_Exception to deal with runtime errors.\nIt prints out backtraces for all errors and uses std::string to gen\u0002erate error messages. Since error handling is usually in the cold\npath, it does not incur any performance degradation in models of\nexisting benchmark suites. However, we find that this error code\nhandling slows down quantized models by 10×. Our further in\u0002vestigation shows that quantized models heavily call torch.ops\nAPI, which frequently throws a benign error, “Not Implemented\nFunctions”, resulting in a large overhead in error handling. This is\u0002sue has been confirmed by PyTorch team, and c10_Exception has\nbeen reverted to the previous implementation. Thus, a benchmark\nsuite that covers a large PyTorch API surface is necessary to expose\nperformance bugs.\n1.2 Paper Contribution\nIn this paper, we develop TorchBench, a novel benchmark suite\nfor PyTorch to address the aforementioned challenges. Unlike exist\u0002ing approaches, TorchBench embraces a large number of models,\nwhich cover a large PyTorch API surface. Given the voluminous\ndeep learning models, we carefully include the representative mod\u0002els in TorchBench for both fairness and generality. Additionally,\nwe develop a set of tools associated with TorchBench to enable\nTorchBench to (1) run benchmarks with different configurations,\n(2) collect various performance statistics, and (3) be ready for any\ncontinuous integration systems. TorchBench helps fix many per\u0002formance bugs in PyTorch eco-system, and many of them are ac\u0002cepted by the official PyTorch repository. In summary, TorchBench\nmakes the following contributions.\n• TorchBench is the first PyTorch benchmark suite that consists\nof rich models in different domains. It covers 2.3× more PyTorch\nAPI surface, compared to the state-of-the-art MLPerf benchmark\nsuite.\n• TorchBench integrate a set of built-in tools, which config\u0002ure execution environments and collect performance statistics.\nTorchBench is able to report multiple metrics to thoroughly\ncharacterize PyTorch performance.\n• TorchBench demonstrates two use cases in practice. First,\nTorchBench exposes the performance bugs in PyTorch software\nstacks and provides insights for bug fixing. Second, TorchBench\nis configured as the continuous integration of PyTorch repository\nto understand the performance regression of daily committed\npatches.\nThe rest of the paper is organized as follows. Section 2 describes\nthe models, benchmark adaptation, and configurations in our bench\u0002mark suite. Section 3 characterizes TorchBench from different as\u0002pects, including execution time breakdown, different PyTorch com\u0002pilers, and different GPU platforms. Section 4 shows the practical\nusages of TorchBench in detecting performance inefficiencies in\nPyTorch software stack. Section 5 reviews related works and distin\u0002guishes our approach. Section 6 presents some conclusions.\n2 TORCHBENCH SUITE\nThere are a huge amount of deep learning models developed by\nthe PyTorch community. For example, The Hugging Face plat\u0002form has more than 10,000 models [39]. Thus, it is challenging\nfor TorchBench to enclose representative models to cover perfor\u0002mance behaviors of typical PyTorch use cases. We work closely\nwith machine learning engineers and set the following criteria to\nselect models for TorchBench.\n• Classic models. TorchBench includes models that were devel\u0002oped many years ago but have been proven both useful and\nimpactful, such as ResNet [4], VGG16 [40], and MobileNet [5, 41].\nThese models","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fexport.arxiv.org%2Fpdf%2F2304.14226v3.pdf","id":"https://export.arxiv.org/pdf/2304.14226v3.pdf","publishedDate":"2023-04-27T00:00:00.000Z","summary":"TorchBench is a new benchmark suite designed to evaluate the performance of the PyTorch software stack. It aims to comprehensively characterize PyTorch's performance by including many representative models and covering a large portion of the PyTorch API. The suite helps identify performance inefficiencies, guide performance optimization across models, the PyTorch framework, and GPU libraries, and prevent performance regressions in the PyTorch repository.\n","title":"TorchBench: Benchmarking PyTorch with High API Surface Coverage","url":"https://export.arxiv.org/pdf/2304.14226v3.pdf"},{"content":"[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2509.14626","id":"https://arxiv.org/abs/2509.14626","publishedDate":"2025-09-18T00:00:00.000Z","summary":"This webpage acknowledges support from the Simons Foundation and member institutions.\n","title":"Evaluating the Effectiveness of Coverage-Guided Fuzzing for Testing Deep Learning Library APIs","url":"https://arxiv.org/abs/2509.14626"},{"content":"Large Language Models are Edge-Case Fuzzers:\nTesting Deep Learning Libraries via FuzzGPT\nYinlin Deng\nUniversity of Illinois\nUrbana-Champaign\nyinlind2@illinois.edu\nChunqiu Steven Xia\nUniversity of Illinois\nUrbana-Champaign\nchunqiu2@illinois.edu\nChenyuan Yang\nUniversity of Illinois\nUrbana-Champaign\ncy54@illinois.edu\nShizhuo Dylan Zhang\nUniversity of Illinois\nUrbana-Champaign\nshizhuo2@illinois.edu\nShujing Yang\nUniversity of Illinois\nUrbana-Champaign\nshujing6@illinois.edu\nLingming Zhang\nUniversity of Illinois\nUrbana-Champaign\nlingming@illinois.edu\nABSTRACT\nBugs in Deep Learning (DL) libraries may affect almost all down\u0002stream DL applications, and it is crucial to ensure the quality of\nsuch systems. It is challenging to generate valid input programs\nfor fuzzing DL libraries, since the input programs need to satisfy\nboth the syntax/semantics of the supported languages (e.g., Python)\nand the tensor/operator constraints for constructing valid compu\u0002tational graphs. Recently, the TitanFuzz work demonstrates that\nmodern Large Language Models (LLMs) can be directly leveraged\nto implicitly learn all the language and DL computation constraints\nto generate valid programs for fuzzing DL libraries. However, LLMs\ntend to generate ordinary programs following similar patterns/-\ntokens with typical programs seen in their massive training cor\u0002pora (e.g., GitHub), while fuzzing favors unusual inputs that cover\nedge cases or are unlikely to be manually produced. To fill this gap,\nthis paper proposes FuzzGPT, the first technique to prime LLMs to\nsynthesize unusual programs for fuzzing. FuzzGPT is built on the\nwell-known hypothesis that historical bug-triggering programs may\ninclude rare/valuable code ingredients important for bug finding.\nMeanwhile, while traditional techniques leveraging such historical\ninformation require intensive human efforts to both design dedicated\ngenerators and ensure the syntactic/semantic validity of generated\nprograms, FuzzGPT demonstrates that this process can be fully au\u0002tomated via the intrinsic capabilities of LLMs (including fine-tuning\nand in-context learning), while being generalizable and applicable\nto challenging domains. While FuzzGPT can be applied with dif\u0002ferent LLMs, this paper focuses on the powerful GPT-style models:\nCodex and CodeGen. Moreover, FuzzGPT also shows the potential\nof directly leveraging the instruct-following capability of the recent\nChatGPT for effective fuzzing. The experimental study on two pop\u0002ular DL libraries (PyTorch and TensorFlow) shows that FuzzGPT\ncan substantially outperform TitanFuzz, detecting 76 bugs, with\n49 already confirmed as previously unknown bugs, including 11\nhigh-priority bugs or security vulnerabilities.\n1 INTRODUCTION\nDeep Learning (DL) has been widely adopted in various applica\u0002tion domains, including scientific discovery [54], healthcare [6],\nfinance [65], and transportation [58]. Such DL applications are com\u0002monly constructed using DL libraries (e.g., PyTorch [51] and Tensor\u0002Flow [64])where developers utilizelibraryAPIs tobuild, train, and de\u0002ploy DL models. Similar to any other complicated software systems,\ntorch.nn.Fold\n(output_size, kernel_size, \ndilation, padding, stride)\nInput Shape Output Shape\noutput_size[d]+2×padding[d]−dilation[d]×(kernel_size[d]−1)-1\nstride[d] ∏⌊ +1⌋\nd\nL =\n(N,C*∏(kernel_size),L) (N,C,output_size[0],..)\nd = 3\nall spatial dimensions\nMUST SATISFY\nFigure 1: Tensor computation with torch.nn.Fold.\nDL libraries can also be buggy. Moreover, bugs in DL libraries can\ncause serious consequences as they can potentially affect almost all\ndownstream DL applications, including safety-critical ones [44, 56].\nAs a result, it is crucial to ensure the quality of DL libraries.\nFuzzing [5, 63, 82], a powerful methodology for bug finding via\nrandom input generation, has been widely studied for testing DL\nlibraries in recent years. Meanwhile, it is extremely challenging to\ngenerate arbitrary input programs for DL libraries, since the pro\u0002grams need to satisfy both the syntax/semantics of the supported\nlanguages (such as Python with dynamic typing) and the tensor/-\noperator constraints for constructing valid computational graphs.\nFor example, Figure 1 shows the complicated constraints for in\u0002put/output tensor shapes and other operator inputs (e.g., dilation)\nto correctly use a PyTorch API torch.nn.Fold: the last shape value\n𝐿 in the input should equal the product of the formula across all\nspatial dimensions 𝑑. To simplify the problem, prior DL library\nfuzzing techniques mainly work on model-level [24, 26, 35, 50, 70]\nor API-level fuzzing [15, 71, 75, 77]. Model-level fuzzers either re\u0002use/mutate existing seed models [26, 50, 70], or generate DL models\nfrom scratch [24, 35]. Due to theintricate tensor/operator constraints,\nsuch model-level fuzzers either only focus on manipulating shape\u0002preserving APIs [70] or require manually-written specifications for\neach API [24, 35] to preserve model validity. As a result, they can\nonly cover limited DL APIs and program patterns. On the other hand,\nAPI-level fuzzers focus on testing each individual API via effective\ninput generation [71, 75] or oracle inference [15, 77].While API-level\nfuzzers can easily cover a large number of APIs, they cannot find\nany bugs arising from interactions of different DL APIs.\nWith the recent enormous advances in Large Language Models\n(LLMs), TitanFuzz [14] has been proposed to directly leverage LLMs\nfor fuzzing DL libraries. The key insight is that LLMs are pre-trained\non billions of code snippets in different languages from open source,\narXiv:2304.02014v1 [cs.SE] 4 Apr 2023\nYinlin Deng, Chunqiu Steven Xia, Chenyuan Yang, Shizhuo Dylan Zhang, Shujing Yang, and Lingming Zhang\nwhich can include numerous valid DL programs for popular DL li\u0002braries; in this way, LLMs can implicitly learn both the language syn\u0002tax/semantics and the tensor/operator constraints for valid DL com\u0002putations. TitanFuzz has been shown effective in generating valid\ninput DL programs and substantially outperforms both traditional\nmodel-level and API-level fuzzers. More importantly, compared with\ntraditional fuzzing techniques that require intensive human efforts\nfor building generation/mutation strategies [17, 29, 32, 33, 62, 78, 83],\nTitanFuzz is fully automated, and can be easily generalized to differ\u0002ent application domains and programming languages. Meanwhile,\nTitanFuzz directly leverages the generative capability of LLMs,\nwhich is based on token naturalness [27] and aims to resemble what\nthey saw in the training corpora. In this way, TitanFuzz can easily\ngenerate human-like DL programs, which definitely can be valid\nprograms for fuzzing. However, given DL libraries have been used\nby developers all over the world, such ordinary input programs can\nhardly help cover additional DL library behaviors/paths, leading to\nlimited fuzzing effectiveness.\nOur Work. This paper proposes FuzzGPT, the first technique to\nguide LLMs to synthesize unusual input programs for effective\nfuzzing. FuzzGPT is built on the well-known hypothesis that his\u0002torical bug-triggering programs may include edge-case/valuable\ncode ingredients important for bug finding. In the literature, re\u0002searchers have proposed various techniques to recompose such code\ningredients or insert them into new code contexts for exposing new\ninteresting bugs/vulnerabilities [10, 29, 61, 84]. However, such tech\u0002niques require intensive human efforts to both design such dedicated\ngenerators and ensure the syntactic/semantic validity of the gener\u0002ated programs. For example, according to prior work [29, 84], even\nresolving the very common undeclared-identifier issue can be non\u0002trivial. Moreover, it is hard to generalize such techniques to different\ndomains, not to mention the challenging DL library fuzzing problem.\nIn contrast, our key insight is that recent advanced LLMs offer a\nnatural, generalizable, and fully automated solution for leveraging\nsuch historical programs – they can be easily prompted [53] or fine\u0002tuned [52] to digest such historical programs, and then generate\nmore unusual programs that resemble the historical ones and ef\u0002fectively exploit their code ingredients. Compared with traditional\ntechniques on leveraging such historical information, FuzzGPT can\nimplicitly learn all the generation constraints (including language\nsyntax/semantics, DL computation constraints, and the new un\u0002usualness constraints) implicitly, and is fully automated. Moreover,\nwhile this paper focuses on the challenging problem of DL library\nfuzzing, the FuzzGPT idea is directly generalizable to other applica\u0002tion domains and programming languages (e.g., fuzzing for various\ncompilers/interpreters, DB systems, SMT solvers, and more).\nToimplementFuzzGPT,we first construct a dataset ofbug-triggering\ncode snippets by mining bug reports from open-source repositories\nof the target DL libraries. Built on this dataset, FuzzGPT includes\nthe following strategies. (1) In-context learning [38]: we provide\nLLMs with either a few examples of historical bug-triggering pro\u0002grams (few-shot learning [8, 53]) or a partial bug-triggering program\n(zero-shot learning [49, 60]) to either generate a new code snip\u0002pet or to autocomplete the partial code. (2) Fine-tuning [52]: we\nmodify the model weights by training on the extracted historical bug\u0002triggering programs to obtain fine-tuned LLMs that are specially\ndesigned to generate similar bug-triggering code snippets. From\nboth learning strategies, FuzzGPT can prime the LLMs to generate\nbug-triggering programs by capturing code ingredients within either\nthe local context examples or fine-tuning dataset.\nTo summarize, this paper makes the following contributions:\n• Dimension This paper opens up a new direction for generating\nunusual input programs for effective fuzzing via LLMs. This paper\nis the first to show that LLMs can be easily prompted/fine-tuned\nto resemble historical bug-triggering programs or even directly\nfollow human instructs to generat","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2304.02014.pdf","id":"https://arxiv.org/pdf/2304.02014.pdf","publishedDate":"2023-10-26T00:00:00.000Z","summary":"This paper introduces FuzzGPT, a technique that uses Large Language Models (LLMs) to generate unusual programs for fuzzing Deep Learning (DL) libraries like PyTorch and TensorFlow. FuzzGPT aims to overcome the limitations of LLMs, which tend to produce common code patterns, by priming them to synthesize more diverse inputs that can uncover edge cases and bugs. The study found that FuzzGPT significantly outperforms existing methods, detecting 76 bugs, including 49 previously unknown ones, with 11 being high-priority or security vulnerabilities.\n","title":"Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT","url":"https://arxiv.org/pdf/2304.02014.pdf"},{"content":"Fuzzing Automatic Differentiation in\nDeep-Learning Libraries\nChenyuan Yang\nUniversity of Illinois\nUrbana-Champaign\ncy54@illinois.edu\nYuxing Tu\nHuazhong University of\nScience and Technology\nyxtu@hust.edu.cn\nYinlin Deng\nUniversity of Illinois\nUrbana-Champaign\nyinlind2@illinois.edu\nHanchi Li\nUniversity of Science\nand Technology of China\nslxiaochi@mail.ustc.edu.cn\nJiayi Yao\nThe Chinese University of\nHong Kong, Shenzhen\njiayiyao@link.cuhk.edu.cn\nLingming Zhang\nUniversity of Illinois\nUrbana-Champaign\nlingming@illinois.edu\nAbstract—Deep learning (DL) has attracted wide attention and\nhas been widely deployed in recent years. As a result, more and\nmore research efforts have been dedicated to testing DL libraries\nand frameworks. However, existing work largely overlooked one\ncrucial component of any DL system, automatic differentiation\n(AD), which is the basis for the recent development of DL.\nTo this end, we propose ∇Fuzz, the first general and practical\napproach specifically targeting the critical AD component in DL\nlibraries. Our key insight is that each DL library API can be\nabstracted into a function processing tensors/vectors, which can\nbe differentially tested under various execution scenarios (for\ncomputing outputs/gradients with different implementations). We\nhave implemented ∇Fuzz as a fully automated API-level fuzzer\ntargeting AD in DL libraries, which utilizes differential testing\non different execution scenarios to test both first-order and high\u0002order gradients, and also includes automated filtering strategies\nto remove false positives caused by numerical instability. We have\nperformed an extensive study on four of the most popular and\nactively-maintained DL libraries, PyTorch, TensorFlow, JAX, and\nOneFlow. The result shows that ∇Fuzz substantially outperforms\nstate-of-the-art fuzzers in terms of both code coverage and bug\ndetection. To date, ∇Fuzz has detected 173 bugs for the studied\nDL libraries, with 144 already confirmed by developers (117 of\nwhich are previously unknown bugs and 107 are related to AD).\nRemarkably, ∇Fuzz contributed 58.3% (7/12) of all high-priority\nAD bugs for PyTorch and JAX during a two-month period.None\nof the confirmed AD bugs were detected by existing fuzzers.\nI. INTRODUCTION\nRecent years have witnessed the rapid advancement of\ndeep learning (DL) research and the wide adoption of DL\nsolutions/technologies in various application domains, e.g.,\nnatural language processing [1], healthcare [2], scientific dis\u0002covery [3], and software engineering [4]–[9]. As a result, there\nis a growing concern about the correctness and reliability of\nsuch systems. For example, for a safety-critical application\ndomain such as autonomous driving, a bug in the DL system\ncan cause serious consequences or even death [10].\nAs it is critical to ensure the quality of increasingly\ninfluential DL systems, much research attention has been\nfocused on testing/verifying DL models [11]–[21] or applica\u0002tion programs [22]–[24]. Recently, testing underlying DL li\u0002braries/frameworks (e.g., PyTorch/TensorFlow) has also drawn\nwide attention, since DL libraries serve as the central infras\u0002tructure for all DL applications.CRADLE [25] is one of the pi\u0002oneering work to perform differential testing on multiple back\u0002ends of Keras [26] using various DL models. AUDEE [27] and\nLEMON [28] further apply search-based mutation strategies\non existing models to generate more diverse test inputs. While\nthese mutation-based techniques heavily rely on seed models,\nMuffin [29] directly synthesizes DL models from DL APIs\nvia a top-down generation approach. Moreover, Muffin can\ndetect inconsistencies in both the model training and inference\nphases across different backends of Keras. Unlike the above\nmodel-level testing techniques, the recent FreeFuzz work [30]\nproposes a fully-automated API-level fuzzing technique via\nmining API inputs from open source. Similarly, another recent\nwork, DocTer [31], directly generates inputs for each API\nbased on DL-specific input constraints extracted from DL\nAPI documentation (assisted with human annotations). Despite\nthe recent advances in DL library testing, existing techniques\nstill suffer from a major limitation: The inference phase of\nDL models or the direct execution of DL APIs has received\nthe most attention, while a crucial component of any DL\nsystem - automatic differentiation (AD) [32] - is still under\u0002studied. Many DL algorithms, notably back-propagation [33],\none of the key algorithms for training feed-forward neural\nnetworks, rely heavily on AD for derivative computation of\narbitrary numerical functions.AD enables the development of\nsophisticated DL models and algorithms since, without AD,\npeople would have to manually/symbolically calculate the\nderivatives of billions of parameters for large DL models [34].\nTo obtain the derivatives automatically, special gradient/Ja\u0002cobian computation operations in DL libraries need to be\nexplicitly triggered (e.g., tf.GradientTape.gradient() in\nTensorFlow).Notably, bugs in AD may cause DL models to fail\nto converge and/or perform poorly in practical deployment,\nwhich is fatal for safety-critical applications. For example,\nsilent AD computation bugs may cause the output of the\ndeployed DL models to diverge significantly from the output\narXiv:2302.04351v1 [cs.SE] 8 Feb 2023\ninput = tensor(shape=[5, 5, 5])\ntarget = tensor(shape=[5])\nRevGrad(KLDivLoss, (input, target)) # crash\nFig. 1: Crash bug in AD\nin the training phase. Besides, AD bugs may also directly\ncrash the entire training process, wasting massive computation\nresources when training recent popular large models and/or\ncausing potential denial-of-service (DoS) attacks [35]. Figure 1\nshows a dangerous crash bug where a widely used PyTorch\nAPI KLDivLoss [36] will crash during AD computation with\na special input shape.However, such AD engines have not been\nthoroughly tested by existing work.\nAlthough the recent Muffin work [29] can potentially test\nthe training phase of DL models, it is still far from practical.\nFirst, Muffin needs to manually annotate the input constraints\nof considered DL APIs and use reshaping operations to ensure\nthe validity of the generated models.As a result, Muffin\ncan only cover a small set of APIs (confirmed in §VI-A).\nSecond, whole model testing is inefficient, especially for\nlarge models/datasets.Third, false positives can originate from\nrandomness, precision loss, and numerical instability, which\nare further amplified in the model training scenario.Fourth,\ndifferential testing of the training phase requires the same\nAPI interfaces across different DL libraries, which further\nlimits its application. Muffin uses Keras and its supported\nbackends TensorFlow, Theano, and CNTK. However, Keras\n2.3.0 in 2019 is the last release supporting backends other\nthan TensorFlow [37].Lastly, Muffin cannot fully test the AD\nengines in DL libraries, as it only covers part of reverse mode\nAD and ignores forward mode AD. In fact, the Muffin paper\ndid not report any confirmed AD bug (confirmed in §VI-A).\nTo thoroughly and automatically test the AD engines in DL\nlibraries, we propose ∇Fuzz, the first general and practical\nframework specifically targeting the crucial AD component.\nOur key insight is that each API in a DL library can be ab\u0002stracted into a function processing tensors/vectors, which can\nbe differentially tested under various execution scenarios (for\ncomputing outputs/gradients with different implementations).\nFor example, the same DL API can be executed without AD\nor with different AD modes, but both the API output and gra\u0002dient should be consistent across different execution scenarios,\nwhich can naturally serve as the oracle for differential testing.\nIn addition, since our test oracle is general at the function level,\nwe can further transform each API into its gradient function\nto test the correctness of high-order gradient computation.\n∇Fuzz can potentially address all the aforementioned limi\u0002tations of Muffin. Through API-level testing, ∇Fuzz no longer\nsuffers from strict input constraints in model-level testing, and\ncan be fully automated. Also, API-level mutation is more\nefficient because it avoids extensive computation on large\nmodels with large datasets. Besides, false positives can be\nsignificantly reduced, since floating-point precision loss will\nnot be accumulated in API-level testing.Lastly, compared to\nMuffin, our technique is also more general, because we utilize\nthe natural AD oracles available in any DL library.\nFig. 2: An example of DL model training and inference\nWe have implemented ∇Fuzz as a fully automated tech\u0002nique for API-level fuzzing with test oracles specifically\ntargeting AD in DL libraries. More precisely, while our\napproach is general and can leverage any existing API-level\nDL library fuzzer for input generation, we build ∇Fuzz on top\nof state-of-the-art FreeFuzz [38] because it is fully automated\nand publicly available. For test oracle, ∇Fuzz automatically\nperforms differential testing of each DL library API (and its\nhigh-order gradients) under different execution scenarios pro\u0002vided by the underlying DL library. ∇Fuzz also incorporates\nautomated filter strategies to further reduce false positives\ncaused by numerical instability issues. We have conducted an\nextensive study of ∇Fuzz on four of the most widely-used\nand actively-maintained DL libraries: PyTorch, TensorFlow,\nJAX, and OneFlow. Our results show that ∇Fuzz substantially\noutperforms state-of-the-art DL library fuzzers (including both\nFreeFuzz and Muffin) in terms of both code coverage and bug\ndetection. In fact, the bug in Figure 1 is detected by ∇Fuzz\nand cannot be detected by any previous techniques.Overall,\nour paper makes the following contributions:\n1) To the best of our knowledge, this is the first work\nspecifically targeting fuzzing the crucial AD component\nin DL libraries with practical and general test oracles.\nOur proposed AD oracles can potentially strengthen and\nimpact all future work on f","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2302.04351.pdf","id":"https://arxiv.org/pdf/2302.04351.pdf","publishedDate":"2023-10-26T00:00:00.000Z","summary":"This paper introduces ∇Fuzz, a novel, automated API-level fuzzer designed to test the automatic differentiation (AD) components within deep learning (DL) libraries. ∇Fuzz utilizes differential testing across different execution scenarios to evaluate both first-order and high-order gradients, incorporating automated filtering to mitigate false positives stemming from numerical instability. The researchers evaluated ∇Fuzz on four popular DL libraries: PyTorch, TensorFlow, JAX, and OneFlow. The results demonstrate that ∇Fuzz surpasses existing fuzzers in code coverage and bug detection, identifying 173 bugs, with 144 confirmed by developers. Notably, ∇Fuzz contributed significantly to the discovery of high-priority AD bugs in PyTorch and JAX.\n","title":"Fuzzing Automatic Differentiation in Deep-Learning Libraries","url":"https://arxiv.org/pdf/2302.04351.pdf"},{"content":"TorchBench: Benchmarking PyTorch with High API Surface\nCoverage\nYueming Hao\nyhao24@ncsu.edu\nNorth Carolina State University\nRaleigh, North Carolina, USA\nXu Zhao\nxzhao9@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nBin Bao\nbinbao@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nDavid Berard\ndberard@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nWill Constable\nwhc@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nAdnan Aziz\nadnanaziz@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nXu Liu\nxliu88@ncsu.edu\nNorth Carolina State University\nRaleigh, North Carolina, USA\nABSTRACT\nDeep learning (DL) has been a revolutionary technique in vari\u0002ous domains. To facilitate the model development and deployment,\nmany deep learning frameworks are proposed, among which Py\u0002Torch is one of the most popular solutions. The performance of eco\u0002system around PyTorch is critically important, which saves the costs\nof training models and reduces the response time of model infer\u0002ences. In this paper, we propose TorchBench, a novel benchmark\nsuite to study the performance of PyTorch software stack. Unlike\nexisting benchmark suites, TorchBench encloses many represen\u0002tative models, covering a large PyTorch API surface. TorchBench\nis able to comprehensively characterize the performance of the Py\u0002Torch software stack, guiding the performance optimization across\nmodels, PyTorch framework, and GPU libraries. We show two prac\u0002tical use cases of TorchBench. (1) We profile TorchBench to iden\u0002tify GPU performance inefficiencies in PyTorch. We are able to op\u0002timize many performance bugs and upstream patches to the official\nPyTorch repository. (2) We integrate TorchBench into PyTorch\ncontinuous integration system. We are able to identify performance\nregression in multiple daily code checkins to prevent PyTorch repos\u0002itory from introducing performance bugs. TorchBench is open\nsource and keeps evolving.\n1 INTRODUCTION\nDeep learning (DL) has been the most transformative technology\nover the past decade, given its wide applicability in global climate\nprojections [1, 2], image processing [3–5], speech recognition [6, 7],\ncontent recommendation [8–10], and pattern classification [11, 12],\nand great potential in emerging domains such as autonomous driv\u0002ing [13, 14], augmented/virtual reality [15, 16], and sciences [17, 18].\nTo facilitate the development and deployment of DL models, DL\nframework practitioners have proposed a number of DL frame\u0002works, such as Caffe [19], TVM [20], ONNX [21], MXNet [22],\nJAX [23], PyTorch [24], and TensorFlow [25]. These frameworks\nprovide rich APIs and support performance tuning.\nPyTorch is one of the most popular deep learning frameworks.\nAs an open-source project, PyTorch has more than 2000 contrib\u0002utors and has received more than 61,000 stars by the end of 2022.\nPyTorch is still actively evolving; PyTorch has over 110k commits in\n2022 [26]. The machine learning research papers based on PyTorch\nhave increased from 29% to 65% in the past four years [27].\nOne major challenge of deep learning tasks is the extensive\namount of computation for both training and inference, which\ntranslates to huge time and money costs. For example, the best\nversion of AlphaGo [28] needs weeks to train with 280 GPUs, and\nits estimated cost is 35 million dollars [29]. In addition, the real time\ninference of deep learning models on edge devices requires fast\nresponse and low latency. Thus, it is critically important for PyTorch\nto achieve bare-metal performance for boosting productivity and\nreducing costs.\nHowever, understanding and optimizing PyTorch performance is\nchallenging because of PyTorch’s complex and rapidly evolved code\nbases. On the one hand, the PyTorch software stack consists of three\nmajor components: the acceleration libraries (e.g., CuDNN [30]),\nthe PyTorch framework [24] (including optimizers and compilers),\nand deep learning models; performance inefficiencies can exist\nin or across any components. Without a systematic evaluation,\ninefficiencies can be difficult to identify. On the other hand, as an\nactive open-source project, PyTorch receives many patches for new\nfunctionality or performance improvement. Without a thorough\nevaluation, it is difficult to understand how these patches affect\nthe entire PyTorch software stack for various models in different\ndomains.\nBenchmarking is a well-known technique to understand the per\u0002formance of different workloads, programming languages, compil\u0002ers, and architectures. For example, SPEC provides many standard\nbenchmark suites for various purposes; Renassance [31] is a Java\nbenchmark suite; DeathStar [32] is a benchmark suite for microser\u0002vices; NPB [33] is a benchmark suite for different parallel models in\nscientific computing; and Rodinia [34] is a GPU benchmark suite.\nThese benchmarks provide indispensable performance insights for\n1\narXiv:2304.14226v3 [cs.LG] 24 Jun 2023\nYueming Hao, Xu Zhao, Bin Bao, David Berard, Will Constable, Adnan Aziz, and Xu Liu\nboth software and hardware evolution. MLPerf [35] is the state\u0002of-the-art benchmark suite for deep learning workloads. However,\nexisting deep learning benchmark suites, including MLPerf, aim\nto compare the performance of deep learning models running on\ndifferent hardware and frameworks. They usually include a small\nnumber of deep learning models (e.g., MLPerf has eight models\nonly) and cover a small PyTorch API surface, which fails to identify\nmany PyTorch performance bugs or fairly evaluate the performance\nimpact of patches.\n1.1 Motivating Examples\nWe show two examples to motivate the necessity of a comprehen\u0002sive benchmark suite for PyTorch.\nMisleading performance characterization. Understanding the per\u0002formance difference across various architectures is one of the ma\u0002jor tasks for benchmarking. However, without a comprehensive\nbenchmark suite, such characterization can result in misleading\nconclusions. For example, some studies [36] show that AMD GPUs\noutperform NVIDIA GPUs on PyTorch, and some studies [37, 38]\nshow an opposite conclusion. Thus, a comprehensive benchmark\nsuite is necessary for performance characterization. Unlike exist\u0002ing studies, we show a different conclusion and unique insights in\nSection 3.3.\nMissing performance bugs. Performance bugs can be buried deep\nin complex PyTorch code bases, especially in the cold execution\npaths. We identify a recent performance bug in PyTorch, which is\ninappropriate error handling. PyTorch implements an error han\u0002dling mechanism named c10_Exception to deal with runtime errors.\nIt prints out backtraces for all errors and uses std::string to gen\u0002erate error messages. Since error handling is usually in the cold\npath, it does not incur any performance degradation in models of\nexisting benchmark suites. However, we find that this error code\nhandling slows down quantized models by 10×. Our further in\u0002vestigation shows that quantized models heavily call torch.ops\nAPI, which frequently throws a benign error, “Not Implemented\nFunctions”, resulting in a large overhead in error handling. This is\u0002sue has been confirmed by PyTorch team, and c10_Exception has\nbeen reverted to the previous implementation. Thus, a benchmark\nsuite that covers a large PyTorch API surface is necessary to expose\nperformance bugs.\n1.2 Paper Contribution\nIn this paper, we develop TorchBench, a novel benchmark suite\nfor PyTorch to address the aforementioned challenges. Unlike exist\u0002ing approaches, TorchBench embraces a large number of models,\nwhich cover a large PyTorch API surface. Given the voluminous\ndeep learning models, we carefully include the representative mod\u0002els in TorchBench for both fairness and generality. Additionally,\nwe develop a set of tools associated with TorchBench to enable\nTorchBench to (1) run benchmarks with different configurations,\n(2) collect various performance statistics, and (3) be ready for any\ncontinuous integration systems. TorchBench helps fix many per\u0002formance bugs in PyTorch eco-system, and many of them are ac\u0002cepted by the official PyTorch repository. In summary, TorchBench\nmakes the following contributions.\n• TorchBench is the first PyTorch benchmark suite that consists\nof rich models in different domains. It covers 2.3× more PyTorch\nAPI surface, compared to the state-of-the-art MLPerf benchmark\nsuite.\n• TorchBench integrate a set of built-in tools, which config\u0002ure execution environments and collect performance statistics.\nTorchBench is able to report multiple metrics to thoroughly\ncharacterize PyTorch performance.\n• TorchBench demonstrates two use cases in practice. First,\nTorchBench exposes the performance bugs in PyTorch software\nstacks and provides insights for bug fixing. Second, TorchBench\nis configured as the continuous integration of PyTorch repository\nto understand the performance regression of daily committed\npatches.\nThe rest of the paper is organized as follows. Section 2 describes\nthe models, benchmark adaptation, and configurations in our bench\u0002mark suite. Section 3 characterizes TorchBench from different as\u0002pects, including execution time breakdown, different PyTorch com\u0002pilers, and different GPU platforms. Section 4 shows the practical\nusages of TorchBench in detecting performance inefficiencies in\nPyTorch software stack. Section 5 reviews related works and distin\u0002guishes our approach. Section 6 presents some conclusions.\n2 TORCHBENCH SUITE\nThere are a huge amount of deep learning models developed by\nthe PyTorch community. For example, The Hugging Face plat\u0002form has more than 10,000 models [39]. Thus, it is challenging\nfor TorchBench to enclose representative models to cover perfor\u0002mance behaviors of typical PyTorch use cases. We work closely\nwith machine learning engineers and set the following criteria to\nselect models for TorchBench.\n• Classic models. TorchBench includes models that were devel\u0002oped many years ago but have been proven both useful and\nimpactful, such as ResNet [4], VGG16 [40], and MobileNet [5, 41].\nThese models","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2304.14226.pdf","id":"https://arxiv.org/pdf/2304.14226.pdf","publishedDate":"2023-10-26T00:00:00.000Z","summary":"TorchBench is a new benchmark suite designed to evaluate the performance of the PyTorch software stack. It includes many representative models, covering a large portion of the PyTorch API surface. The authors use TorchBench to identify and fix GPU performance inefficiencies in PyTorch, and also integrate it into the PyTorch continuous integration system to prevent performance regressions.\n","title":"TorchBench: Benchmarking PyTorch with High API Surface Coverage","url":"https://arxiv.org/pdf/2304.14226.pdf"},{"content":"An Empirical Study on Bugs Inside PyTorch: A\nReplication Study\nSharon Chee Yin Ho∗\nConcordia University\nsharoncheeyin.ho@concordia.ca\nVahid Majdinasab∗\nPolytechnique Montreal ´\nvahid.majdinasab@polymtl.ca\nMohayeminul Islam∗\nUniversity of Alberta\nmohayemin@ualberta.ca\nDiego Elias Costa\nUniversite du Qu ´ ebec ´ a Montr ` eal ´\ncosta.diego@uqam.ca\nEmad Shihab\nConcordia University\nemad.shihab@concordia.ca\nFoutse Khomh\nPolytechnique Montreal ´\nfoutse.khomh@polymtl.ca\nSarah Nadi\nUniversity of Alberta\nnadi@ualberta.ca\nMuhammad Raza\nQueen’s University\nm.raza@queensu.ca\nAbstract—Software systems are increasingly relying on deep\nlearning components, due to their remarkable capability of iden\u0002tifying complex data patterns and powering intelligent behaviour.\nA core enabler of this change in software development is the\navailability of easy-to-use deep learning libraries. Libraries like\nPyTorch and TensorFlow empower a large variety of intelligent\nsystems, offering a multitude of algorithms and configuration\noptions, applicable to numerous domains of systems. However,\nbugs in those popular deep learning libraries also may have dire\nconsequences for the quality of systems they enable; thus, it is\nimportant to understand how bugs are identified and fixed in\nthose libraries.\nInspired by a study of Jia et al., which investigates the bug\nidentification and fixing process at TensorFlow, we characterize\nbugs in the PyTorch library, a very popular deep learning\nframework. We investigate the causes and symptoms of bugs\nidentified during PyTorch’s development, and assess their locality\nwithin the project, and extract patterns of bug fixes. Our results\nhighlight that PyTorch bugs are more like traditional software\nprojects bugs, than related to deep learning characteristics.\nFinally, we also compare our results with the study on Ten\u0002sorFlow, highlighting similarities and differences across the bug\nidentification and fixing process.\nIndex Terms—Deep Learning, Bug Analysis, Software Library\nDefect, PyTorch, Empirical Study\nI. INTRODUCTION\nThe recent advances in deep learning research allied to the\nwide availability of large datasets have pushed deep learn\u0002ing components into our everyday systems. Deep learning\u0002powered systems have successfully been deployed in multiple\ndomains, from medicine [1] to self-driving cars [2], to auto\u0002mate highly complex tasks due to their capabilities of learning\ncomplex data patterns. A major catalyst for this widespread use\nof deep learning components is the availability of mature and\neasy to use deep learning libraries. Libraries like PyTorch [3]\nand TensorFlow [4] abstract much of the complexity of defin\u0002ing, training, and testing deep learning components with a\nhigh-level functional API, and are becoming widely popular.\nIn the Stack Overflow survey of 2022, deep learning libraries\nlike PyTorch were reportedly used by 11% of all participants\nof the survey [5].\n*These authors contributed equally to this work\nWhile very popular DL libraries help empower (artificially)\nintelligent behaviour in a myriad of systems, bugs in those li\u0002braries may have dire consequences. Defects in the underlying\nDL library could eventually propagate as observable failures\nin the systems that depend on it. In fact, many authors in prior\nstudies done on bugs in software systems using DL techniques,\n[6]–[8] suggest that such bugs may be impacted by defects\nfound in the DL libraries used as studied by Tambon et al.\n[9]. Therefore, understanding and identifying defects in DL\nlibraries are essential for ensuring the quality of the systems\nthat rely on them.\nTwo studies from Jia et al. [10], [11] pioneered an inves\u0002tigation on bugs inside the TensorFlow library. The authors\nprovided a comprehensive understanding of potential links\nbetween symptoms and root causes of bugs inside TensorFlow,\nin addition to analyzing which repair patterns were used when\nfixing bugs. To further build and strengthen our empirical\nknowledge in the area, we define symptoms as the faulty\nbehaviour of a software that indicates a possibility of a bug\nin the program, such as program crashes or a functionality\nnot providing expected results, with root causes being the\nreason behind why such a bug has appeared, such as the wrong\nimplementation of an algorithm or a mis-configuration of the\nsoftware. In addition, we define repair patterns as reoccurring\npatterns used to fix a bug, such as adding a value checker or\nthrowing an exception when applicable.\nIn this paper, we replicate the study conducted by Jia et\nal. [10] on the PyTorch library [3], another popular Python\nlibrary for developing deep learning systems. PyTorch is used\nin many critical applications, with Tesla’s Autopilot [12] and\nUber’s Pyro [13] being notable examples. We argue that this\nreplication study on PyTorch which complements the work\ndone by Jia et al. on TensorFlow, will benefit the machine\nlearning community by having a comprehensive overview of\nthe bug characteristics and its corresponding repair patterns\ninside the two most popular DL libraries. In addition, by\nhaving three similar research questions as Jia et al. [10], we\ncan have a direct comparison to see the similarities and differ\u0002ences of bugs in terms of the symptoms, causes, components\naffected, and repair patterns. We find that while some results\n1\narXiv:2307.13777v2 [cs.SE] 1 Aug 2023\nin PyTorch are similar to those found in TensorFlow [10],\nthere are also dissimilarities. For example, we find that\ncrash and functional error are most frequent symptoms of\nbugs, which matches their findings. However, we observe\nprocessing as a low occurring root cause, which is the top\nroot cause in the TensorFlow study.\nOur contributions in this paper are as follows:\n• We study 194 bugs in the popular DL library PyTorch to\nunderstand the bug characteristics and how the bugs are\nfixed.\n• We compare our results with a similar study done on\nTensorFlow to outline the similarities and differences\nbetween these two frameworks.\n• To promote further research in the area, we also share\nour replication package, containing all bugs analyzed, our\ncoding scheme, and more detailed results *.\nThe rest of this paper is structured as follows. In the next\nSection, we discuss the related works. In Section III we outline\nour study’s design, explain our research questions and our data\ncollection and curation method. In Sections IV, V, and VI we\ndescribe our findings for each of our research questions. In\nSection VII, we discussed the implications of our study. In\nSection VIII, we outline the threats to our results’ validity,\nand finally in Section IX we conclude our paper and propose\nareas for future work.\nII. RELATED WORKS\nIn this section, we outline the current work that has been\ndone on defect analysis in machine learning and deep learning\nsystems.\nA. Understanding Defects in Traditional Systems\nPradel et al. [14] designed a DL approach for bug detection\nby analyzing variable names. Lu et al. [15] created a dataset\nof bugs for benchmarking bug detection tools. Wang et al.\n[16] proposed an approach for bug detection by using n\u0002gram language models instead of rule based approaches. Pan\net al. [17] introduced program slicing metrics for measuring\ncoupling, cohesion, and complexity of codes written in C\nlanguage.\nZhu et al. [18] proposed combining techniques from text\nmining and data mining for bug prediction. D’ambros et al.\n[19] have designed another bug benchmarking dataset for\nevaluating bug prediction tools. Hammouri et al. [20] used\nthree supervised learning approaches (Naive Bayes, Decision\nTrees, and Neural Networks) for bug prediction. Arcuri et al.\n[21] proposed an approach based on co-evolution where the\nprogram and test cases evolve alongside each other for bug\nfixing. Ye et al. [22] did a comprehensive study on automatic\nrepair tools on the QuixBugs benchmark.\nB. Understanding Defects in Machine Learning Systems\nThung et al. [23] analyzed the bug database of three ML\u0002based systems and labeled the bugs into different categories.\n*https://github.com/datasetsharing/pytorchbugdataset\nThey also analyzed the relationships between different dimen\u0002sions of bugs, such as the category, severity, cost of fixing,\nimpact, etc. Kim et al. [24] developed a benchmark of bugs in\nML systems with a focus to help with automatic debugging. In\naddition, some empirical studies have also been done on bugs\nin applications of a branch of ML known as deep learning.\nY. Zhang et al. [6] conducted an empirical study of bugs\nin applications that call the APIs of TensorFlow [4] and their\ncorresponding fixes to find the root causes of the bugs. Islam et\nal. [7] analyzed applications using DL libraries, such as Caffe\n[25], Keras [26], Theano [27], and Torch [28]. Humbatova et\nal. [8] also conducted a similar study on applications using\nTensorFlow [4], Keras [26] and PyTorch [3]. X. Zhang et al.\n[29] studied the bugs related to incorrect decisions provided\nby DL systems and characterized such bugs. Finally, J. Chen\net al. [30] conducted a study of bugs on four DL frameworks\n(TensorFlow, PyTorch, MXNET, and DL4) by analysing 1000\nbugs. They analyze the root causes and symptoms of these\nbugs alongside the components in which they appeared in.\nBy analyzing the related works mentioned above, we ob\u0002serve that there have been many studies done on analyzing\ndefects in traditional software systems, ML/DL systems, and\nsoftware which incorporate ML/DL components. However,\nthere has been very little study on the DL frameworks them\u0002selves. To the best of our knowledge, there are only two works\ndone in this area. An empirical study done by Jia et al. [10] on\nbugs inside TensorFlow and a study done by Chen et al. [30]\non bugs on 4 different DL frameworks. Alongside TensorFlow,\nPyTorch is also a popular DL framework that is used in re\u0002search and industry alike. Our aim of doing a replication study\non PyTorch based on the study that investigated TensorFlow\nby Jia et al. [10], is to draw direct comparisons ","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2307.13777.pdf","id":"https://arxiv.org/pdf/2307.13777.pdf","publishedDate":"2023-10-26T00:00:00.000Z","summary":"This paper presents an empirical study on bugs within the PyTorch deep learning library. Inspired by a similar study on TensorFlow, the researchers investigated the causes, symptoms, and fixing patterns of bugs in PyTorch. The study aims to understand how bugs are identified and fixed in this popular deep learning framework, highlighting that PyTorch bugs are more similar to traditional software bugs than those related to deep learning characteristics. The research also compares the findings with the TensorFlow study, noting similarities and differences in the bug identification and fixing processes.\n","title":"An Empirical Study on Bugs Inside PyTorch: A Replication Study","url":"https://arxiv.org/pdf/2307.13777.pdf"},{"content":"TorchBench: Benchmarking PyTorch with High API Surface\nCoverage\nYueming Hao\nyhao24@ncsu.edu\nNorth Carolina State University\nRaleigh, North Carolina, USA\nXu Zhao\nxzhao9@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nBin Bao\nbinbao@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nDavid Berard\ndberard@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nWill Constable\nwhc@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nAdnan Aziz\nadnanaziz@meta.com\nMeta Platforms, Inc.\nMenlo Park, California, USA\nXu Liu\nxliu88@ncsu.edu\nNorth Carolina State University\nRaleigh, North Carolina, USA\nABSTRACT\nDeep learning (DL) has been a revolutionary technique in vari\u0002ous domains. To facilitate the model development and deployment,\nmany deep learning frameworks are proposed, among which Py\u0002Torch is one of the most popular solutions. The performance of eco\u0002system around PyTorch is critically important, which saves the costs\nof training models and reduces the response time of model infer\u0002ences. In this paper, we propose TorchBench, a novel benchmark\nsuite to study the performance of PyTorch software stack. Unlike\nexisting benchmark suites, TorchBench encloses many represen\u0002tative models, covering a large PyTorch API surface. TorchBench\nis able to comprehensively characterize the performance of the Py\u0002Torch software stack, guiding the performance optimization across\nmodels, PyTorch framework, and GPU libraries. We show two prac\u0002tical use cases of TorchBench. (1) We profile TorchBench to iden\u0002tify GPU performance inefficiencies in PyTorch. We are able to op\u0002timize many performance bugs and upstream patches to the official\nPyTorch repository. (2) We integrate TorchBench into PyTorch\ncontinuous integration system. We are able to identify performance\nregression in multiple daily code checkins to prevent PyTorch repos\u0002itory from introducing performance bugs. TorchBench is open\nsource and keeps evolving.\n1 INTRODUCTION\nDeep learning (DL) has been the most transformative technology\nover the past decade, given its wide applicability in global climate\nprojections [1, 2], image processing [3–5], speech recognition [6, 7],\ncontent recommendation [8–10], and pattern classification [11, 12],\nand great potential in emerging domains such as autonomous driv\u0002ing [13, 14], augmented/virtual reality [15, 16], and sciences [17, 18].\nTo facilitate the development and deployment of DL models, DL\nframework practitioners have proposed a number of DL frame\u0002works, such as Caffe [19], TVM [20], ONNX [21], MXNet [22],\nJAX [23], PyTorch [24], and TensorFlow [25]. These frameworks\nprovide rich APIs and support performance tuning.\nPyTorch is one of the most popular deep learning frameworks.\nAs an open-source project, PyTorch has more than 2000 contrib\u0002utors and has received more than 61,000 stars by the end of 2022.\nPyTorch is still actively evolving; PyTorch has over 110k commits in\n2022 [26]. The machine learning research papers based on PyTorch\nhave increased from 29% to 65% in the past four years [27].\nOne major challenge of deep learning tasks is the extensive\namount of computation for both training and inference, which\ntranslates to huge time and money costs. For example, the best\nversion of AlphaGo [28] needs weeks to train with 280 GPUs, and\nits estimated cost is 35 million dollars [29]. In addition, the real time\ninference of deep learning models on edge devices requires fast\nresponse and low latency. Thus, it is critically important for PyTorch\nto achieve bare-metal performance for boosting productivity and\nreducing costs.\nHowever, understanding and optimizing PyTorch performance is\nchallenging because of PyTorch’s complex and rapidly evolved code\nbases. On the one hand, the PyTorch software stack consists of three\nmajor components: the acceleration libraries (e.g., CuDNN [30]),\nthe PyTorch framework [24] (including optimizers and compilers),\nand deep learning models; performance inefficiencies can exist\nin or across any components. Without a systematic evaluation,\ninefficiencies can be difficult to identify. On the other hand, as an\nactive open-source project, PyTorch receives many patches for new\nfunctionality or performance improvement. Without a thorough\nevaluation, it is difficult to understand how these patches affect\nthe entire PyTorch software stack for various models in different\ndomains.\nBenchmarking is a well-known technique to understand the per\u0002formance of different workloads, programming languages, compil\u0002ers, and architectures. For example, SPEC provides many standard\nbenchmark suites for various purposes; Renassance [31] is a Java\nbenchmark suite; DeathStar [32] is a benchmark suite for microser\u0002vices; NPB [33] is a benchmark suite for different parallel models in\nscientific computing; and Rodinia [34] is a GPU benchmark suite.\nThese benchmarks provide indispensable performance insights for\n1\narXiv:2304.14226v3 [cs.LG] 24 Jun 2023\nYueming Hao, Xu Zhao, Bin Bao, David Berard, Will Constable, Adnan Aziz, and Xu Liu\nboth software and hardware evolution. MLPerf [35] is the state\u0002of-the-art benchmark suite for deep learning workloads. However,\nexisting deep learning benchmark suites, including MLPerf, aim\nto compare the performance of deep learning models running on\ndifferent hardware and frameworks. They usually include a small\nnumber of deep learning models (e.g., MLPerf has eight models\nonly) and cover a small PyTorch API surface, which fails to identify\nmany PyTorch performance bugs or fairly evaluate the performance\nimpact of patches.\n1.1 Motivating Examples\nWe show two examples to motivate the necessity of a comprehen\u0002sive benchmark suite for PyTorch.\nMisleading performance characterization. Understanding the per\u0002formance difference across various architectures is one of the ma\u0002jor tasks for benchmarking. However, without a comprehensive\nbenchmark suite, such characterization can result in misleading\nconclusions. For example, some studies [36] show that AMD GPUs\noutperform NVIDIA GPUs on PyTorch, and some studies [37, 38]\nshow an opposite conclusion. Thus, a comprehensive benchmark\nsuite is necessary for performance characterization. Unlike exist\u0002ing studies, we show a different conclusion and unique insights in\nSection 3.3.\nMissing performance bugs. Performance bugs can be buried deep\nin complex PyTorch code bases, especially in the cold execution\npaths. We identify a recent performance bug in PyTorch, which is\ninappropriate error handling. PyTorch implements an error han\u0002dling mechanism named c10_Exception to deal with runtime errors.\nIt prints out backtraces for all errors and uses std::string to gen\u0002erate error messages. Since error handling is usually in the cold\npath, it does not incur any performance degradation in models of\nexisting benchmark suites. However, we find that this error code\nhandling slows down quantized models by 10×. Our further in\u0002vestigation shows that quantized models heavily call torch.ops\nAPI, which frequently throws a benign error, “Not Implemented\nFunctions”, resulting in a large overhead in error handling. This is\u0002sue has been confirmed by PyTorch team, and c10_Exception has\nbeen reverted to the previous implementation. Thus, a benchmark\nsuite that covers a large PyTorch API surface is necessary to expose\nperformance bugs.\n1.2 Paper Contribution\nIn this paper, we develop TorchBench, a novel benchmark suite\nfor PyTorch to address the aforementioned challenges. Unlike exist\u0002ing approaches, TorchBench embraces a large number of models,\nwhich cover a large PyTorch API surface. Given the voluminous\ndeep learning models, we carefully include the representative mod\u0002els in TorchBench for both fairness and generality. Additionally,\nwe develop a set of tools associated with TorchBench to enable\nTorchBench to (1) run benchmarks with different configurations,\n(2) collect various performance statistics, and (3) be ready for any\ncontinuous integration systems. TorchBench helps fix many per\u0002formance bugs in PyTorch eco-system, and many of them are ac\u0002cepted by the official PyTorch repository. In summary, TorchBench\nmakes the following contributions.\n• TorchBench is the first PyTorch benchmark suite that consists\nof rich models in different domains. It covers 2.3× more PyTorch\nAPI surface, compared to the state-of-the-art MLPerf benchmark\nsuite.\n• TorchBench integrate a set of built-in tools, which config\u0002ure execution environments and collect performance statistics.\nTorchBench is able to report multiple metrics to thoroughly\ncharacterize PyTorch performance.\n• TorchBench demonstrates two use cases in practice. First,\nTorchBench exposes the performance bugs in PyTorch software\nstacks and provides insights for bug fixing. Second, TorchBench\nis configured as the continuous integration of PyTorch repository\nto understand the performance regression of daily committed\npatches.\nThe rest of the paper is organized as follows. Section 2 describes\nthe models, benchmark adaptation, and configurations in our bench\u0002mark suite. Section 3 characterizes TorchBench from different as\u0002pects, including execution time breakdown, different PyTorch com\u0002pilers, and different GPU platforms. Section 4 shows the practical\nusages of TorchBench in detecting performance inefficiencies in\nPyTorch software stack. Section 5 reviews related works and distin\u0002guishes our approach. Section 6 presents some conclusions.\n2 TORCHBENCH SUITE\nThere are a huge amount of deep learning models developed by\nthe PyTorch community. For example, The Hugging Face plat\u0002form has more than 10,000 models [39]. Thus, it is challenging\nfor TorchBench to enclose representative models to cover perfor\u0002mance behaviors of typical PyTorch use cases. We work closely\nwith machine learning engineers and set the following criteria to\nselect models for TorchBench.\n• Classic models. TorchBench includes models that were devel\u0002oped many years ago but have been proven both useful and\nimpactful, such as ResNet [4], VGG16 [40], and MobileNet [5, 41].\nThese models","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fpdf%2F2304.14226","id":"https://arxiv.org/pdf/2304.14226","publishedDate":null,"summary":"TorchBench is a new benchmark suite designed to evaluate the performance of the PyTorch software stack. It includes many representative models, covering a large portion of the PyTorch API. TorchBench helps identify performance inefficiencies, optimize performance bugs, and prevent performance regressions in the PyTorch repository.\n","title":"[PDF] TorchBench: Benchmarking PyTorch with High API Surface Coverage","url":"https://arxiv.org/pdf/2304.14226"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.semanticscholar.org%2Fpaper%2FTorchBench%253A-Benchmarking-PyTorch-with-High-API-Hao-Zhao%2Fafff36c4e7626602688e5bd9890d749366ceb2f4","id":"https://www.semanticscholar.org/paper/TorchBench%3A-Benchmarking-PyTorch-with-High-API-Hao-Zhao/afff36c4e7626602688e5bd9890d749366ceb2f4","publishedDate":null,"summary":"Failed to fetch content","title":"TorchBench: Benchmarking PyTorch with High API Surface Coverage","url":"https://www.semanticscholar.org/paper/TorchBench%3A-Benchmarking-PyTorch-with-High-API-Hao-Zhao/afff36c4e7626602688e5bd9890d749366ceb2f4"}],"status":"completed","toolCallId":"call_SpA0qcfQcnhBijVfEUnJdK07","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Short answer: TorchBench doesn’t hardcode a flat list of APIs; it achieves high API coverage by selecting real models that naturally exercise broad parts of PyTorch. In the paper, we quantify coverage at the level of ATen operators, dispatcher paths, autocast/AMP, autograd, compilers, and quantization/error paths, and we explain why each matters.\n\nWhat TorchBench covers (by layer) and why\n\n- Frontend model APIs and modules\n  - nn.Modules and layers: Conv/Linear/Embedding/Norm/Activation/Pooling/Dropout/RNN/Transformer blocks; vision backbones (ResNet, VGG, MobileNet), NLP transformers (BERT/GPT-like), recsys components, generative models.\n  - Why: These are the user-facing building blocks; different model families trigger very different operator mixes, tensor shapes, and control flows that stress the runtime differently.\n\n- ATen operators (eager)\n  - Dense linear algebra: matmul/addmm/bmm/gemm; pointwise ops; broadcast semantics.\n  - Convolutions and fused conv+activation variants; pooling; normalization ops (batch/group/layer/instance norm).\n  - Reductions and scans: sum/mean/argmax/topk/softmax/logsumexp.\n  - Tensor shape/stride ops: view/reshape/permute/contiguous/cat/split/gather/scatter/advanced indexing.\n  - Why: ATen ops are the core kernel surface. Different models emphasize different families (e.g., conv-heavy CV vs. matmul-heavy Transformers), revealing backend- and dtype-specific issues.\n\n- Dispatcher and device backends\n  - CPU, CUDA, ROCm registrations; out/inplace/functional variants; tensor iterator and composite kernels.\n  - Why: Ensures we exercise both device-specific kernel registrations and composite fallbacks; catches issues like slow composite paths or missing specialized kernels on a backend.\n\n- Autograd engine\n  - Backward graphs for training configs; first-order gradients across common ops; optimizer step overhead in select training runs.\n  - Why: Backprop adds very different operator mixes (grad formulas, reductions) and stresses the graph engine, dependency tracking, and synchronization.\n\n- Mixed precision and autocast\n  - AMP autocast for CUDA and ROCm; dtype promotion rules (fp32, fp16/bf16) and casting patterns.\n  - Why: Autocast dispatches different kernels and can hit precision-specific fused kernels or fallbacks; many regressions are dtype-specific.\n\n- Compilers and graph capture paths\n  - torch.compile/TorchDynamo front-end; AOTAutograd; TorchInductor codegen; CUDA Graphs where configured.\n  - Why: Compiled paths change the kernel mix (fusion, scheduling) and can expose correctness/perf gaps vs. eager; important to track benefits/regressions per model.\n\n- Quantization and torch.ops fallbacks\n  - Quantized operator use via quantized models; torch.ops custom/extension calls.\n  - Why: Quantization exercises different kernel sets and, critically, error/fallback paths through torch.ops (the paper’s c10_Exception case causing 10× slowdowns).\n\n- Memory and I/O paths\n  - Dataloader-lite inputs; tensor transfers H2D/D2H; pinned memory and stream behavior visible in profiles.\n  - Why: Identifies bandwidth/transfer bottlenecks and inadvertent syncs that show up only in full-model runs.\n\nHow coverage is measured and selected\n\n- Operator/API coverage metric\n  - We count unique ATen operators and dispatcher entries touched by the suite under a standard config matrix (devices, precisions, eager vs. compiled). TorchBench covers about 2.3× more of PyTorch’s API surface than MLPerf in our measurements.\n  - Why: Operator- and path-level coverage better predicts real-world regressions than “model count.”\n\n- Model curation for coverage\n  - Include classic CV (conv-heavy), modern Transformers (matmul/attention-heavy), normalization-heavy nets, sequence models, and models with dynamic shapes or control flow.\n  - Pair each with multiple dtypes and execution modes to light up additional kernels and code paths.\n\nRepresentative API families that TorchBench intentionally lights up\n\n- Convolutional stack: conv{1,2,3}d, depthwise, grouped, transposed; cudnn/miopen algorithm selection.\n- Attention stack: scaled dot-product attention, softmax/logits masking, einsum/bmm, fused attention kernels where available.\n- Normalization: batch/group/layer/instance norms and RMSNorm variants; statistics/reduction corner cases.\n- Linear algebra: addmm/matmul/bmm; batched GEMM shapes from tiny to very large.\n- Elementwise and broadcasts: relu/gelu/silu/tanh/sigmoid; add/mul/div with broadcasting; type promotions.\n- Tensor transforms: reshape/view/permute/transpose/contiguous/expand; cat/stack/split/chunk; gather/scatter/index_add.\n- Reductions: sum/mean/amax/amin/argmax/topk; numerically sensitive softmax/logsumexp.\n- Autograd functions: grad formulas of the above, ensuring backward kernels are exercised.\n- Quantization: quantize/dequantize, per-tensor/per-channel observers in models that support it.\n- Runtime/infra: allocator behavior, stream/record_function events, dispatcher overhead, error-handling paths (c10_Exception), torch.ops not-implemented fallthrough.\n\nWhy each was included\n\n- To represent real workloads, not microbenchmarks, thereby activating long-tail code paths (dispatcher fallbacks, error handling, composite kernels) that typical narrow suites miss.\n- To compare backends fairly: same models stress different vendor libraries in different ways; coverage prevents single-workload bias.\n- To guard PyTorch CI: broad coverage makes regressions visible the day they land, including in “cold” paths that become hot under certain models/dtypes.\n\nIf you want, tell me the specific API areas you care about (e.g., attention kernels, quantization, AMP on ROCm), and I can map which TorchBench models/configs hit them and how to profile their usage.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d1a33cf4ac81979e6ea2b7632e481a0cae5a62d2621c4c"}},"resumableStreamId":"STREAM:35d62c47-7351-4c37-96cd-d6d15e6625aa","role":"assistant","status":"done","threadId":"1dec2aab-3170-4af0-b945-458c9d6e802d","timeToFirstToken":1.207,"tokens":1278,"tokensPerSecond":31.258407728995962,"updated_at":1758569267806,"userId":"google:104458561188320708185","id":"j977zw9z45mrmphhz1cd64zcv57r3yca"},{"_creationTime":1758569334875.7698,"_id":"j97csghfwpctbv9gaa1agz29yn7r3rwm","attachmentIds":[],"content":"What configs did it determine to be highly efficient with Pytoch","created_at":1758569342688,"messageId":"90c93213-7fb5-46f8-94d5-9c99f48c7b0c","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"1dec2aab-3170-4af0-b945-458c9d6e802d","updated_at":1758569342688,"userId":"google:104458561188320708185","id":"j97csghfwpctbv9gaa1agz29yn7r3rwm","providerMetadata":{}},{"_creationTime":1758569342688.1472,"_id":"j975apjze1ne88da5p02dmywph7r2zc8","attachmentIds":[],"byok":false,"content":"Here’s the gist of what the paper (and its TorchBench runs) found to be consistently high‑efficiency configurations in PyTorch, and why they worked. I’ll group them by axis and note the caveats that showed up in the broad API coverage.\n\nExecution mode\n- torch.compile (TorchDynamo + AOTAutograd + TorchInductor) for inference and many training cases\n  - Why it’s efficient: operator fusion, kernel specialization, fewer Python/dispatcher hops, better memory locality, and reduced launch overhead.\n  - When it shines: matmul/elementwise/normalization–heavy models (Transformers, many CNNs) with relatively stable shapes.\n  - Caveats: dynamic control flow, highly ragged shapes, and unsupported ops can reduce or negate gains; initial compile time must be amortized.\n\n- Eager + CUDA Graphs for steady‑state inference loops\n  - Why: eliminates per-iteration launch/dispatcher overhead and CPU jitter after capture; particularly effective for small/medium batches.\n  - Caveats: needs static shapes and capture-safe code; warmup/capture phase required.\n\nPrecision and math modes\n- AMP (automatic mixed precision) with bf16 on GPUs that support fast bf16 (e.g., Ampere/Hopper)\n  - Why: you keep most accuracy while hitting faster tensor-core kernels and higher math throughput; fewer casts than fp16 improve stability and performance.\n  - Caveats: ensure autocast covers the hot ops; some ops still run in fp32 by design.\n\n- AMP with fp16 on GPUs where bf16 is slower/unsupported\n  - Why: still enables tensor cores and can double or more throughput on matmul/conv-heavy blocks.\n  - Caveats: watch numerics (loss scaling for training); some backends (or ops) may fall back to fp32.\n\n- Enable TF32 for FP32 matmuls on NVIDIA Ampere/Hopper when exact FP32 isn’t required\n  - Why: big speedups on GEMMs with minimal accuracy impact for many DL models.\n  - Caveats: turn off if you need strict FP32 numerics.\n\nBackend/library choices\n- Use the vendor-optimized backend with current libraries\n  - NVIDIA: CUDA + cuDNN/cuBLAS with up-to-date heuristics; ROCm: MIOpen/rocBLAS on recent ROCm.\n  - Why: algorithm selection and fused kernels improve as libraries advance; TorchBench showed version-sensitive wins and regressions.\n  - Caveats: pin versions per run; algorithm changes can swing performance across releases.\n\nMemory/throughput knobs\n- Larger, but hardware-appropriate batch sizes\n  - Why: better kernel occupancy and amortized overheads; TorchBench trends showed clear scaling until hitting memory or latency targets.\n  - Caveats: watch activation memory, especially with compile/graph capture.\n\n- Use channels-last (NHWC) layout for CNNs on GPUs\n  - Why: better memory coalescing and use of tensor-core conv kernels.\n  - Caveats: verify end-to-end layout compatibility (dataloaders, custom ops).\n\nCompiler configuration details\n- torch.compile mode=\"max-autotune\" or similar higher-heuristic settings for stable production graphs\n  - Why: lets Inductor/Dynamo try more fusions and kernel variants; paid off on steady workloads.\n  - Caveats: higher compile time; not ideal for rapidly changing graphs.\n\n- Dynamic shapes enabled only if needed; prefer static shapes for best fusion\n  - Why: static shapes allow more aggressive specialization and scheduling.\n\nDispatcher/Python overhead mitigation\n- Keep hot paths free of Python control and heavy tensor-meta churn\n  - Why: TorchBench profiles showed nontrivial Python/dispatcher time on small kernels; compile or graphs remove most of it.\n\nQuantization (when applicable)\n- Use backend-native quantization paths for inference (e.g., per‑channel weight quant, fbgemm/qnnpack on CPU; backend support on GPU varies)\n  - Why: large speedups on linear/conv layers with acceptable accuracy trade‑offs.\n  - Caveats: coverage exposed that some quant paths trigger costly fallbacks if kernels are missing—verify supported ops on your backend.\n\nData movement\n- Minimize H2D/D2H transfers, use pinned memory for host I/O, and overlap transfers with compute via streams\n  - Why: TorchBench breakdowns frequently flagged memcpy and sync hotspots when inputs weren’t staged properly.\n\nWhat this looked like in practice across many models\n- Inference sweet spot on modern NVIDIA GPUs:\n  - torch.compile + AMP(bf16) + TF32 allowed = consistently faster than eager FP32; CUDA Graphs can edge it out for very stable loops with small batches.\n- Training sweet spot:\n  - torch.compile + AMP(bf16) with gradient scaling when needed; keep shapes stable and avoid unsupported custom ops in the hot loop.\n- CNNs:\n  - channels_last + AMP + cudnn autotune on; compile gives extra fusion wins on activations/norms/pointwise.\n- Transformers:\n  - AMP(bf16) + compile; benefit from fused attention/softmax patterns when available; static or bucketed sequence lengths help.\n\nCommon pitfalls TorchBench surfaced\n- Mixed precision without proper autocast regions led to silent fp32 fallbacks, losing expected gains.\n- Version drift: a cuDNN/rocBLAS update changed algorithm picks and flipped winners; pinning and reporting versions was crucial.\n- Dynamic/ragged shapes reduced compile/graph benefits; bucketing or padding improved throughput.\n- Missing kernels on one backend caused composite or CPU fallbacks; results must note these or skip affected configs.\n\nIf you share your exact hardware (e.g., A100/H100, MI300, CPU) and task (train vs. infer), I can give you a concise, ready-to-run config set and flags that align with these high-efficiency patterns.","created_at":1758569342689,"messageId":"13793b21-6553-46d6-a087-39fa33978d7a","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Here’s the gist of what the paper (and its TorchBench runs) found to be consistently high‑efficiency configurations in PyTorch, and why they worked. I’ll group them by axis and note the caveats that showed up in the broad API coverage.\n\nExecution mode\n- torch.compile (TorchDynamo + AOTAutograd + TorchInductor) for inference and many training cases\n  - Why it’s efficient: operator fusion, kernel specialization, fewer Python/dispatcher hops, better memory locality, and reduced launch overhead.\n  - When it shines: matmul/elementwise/normalization–heavy models (Transformers, many CNNs) with relatively stable shapes.\n  - Caveats: dynamic control flow, highly ragged shapes, and unsupported ops can reduce or negate gains; initial compile time must be amortized.\n\n- Eager + CUDA Graphs for steady‑state inference loops\n  - Why: eliminates per-iteration launch/dispatcher overhead and CPU jitter after capture; particularly effective for small/medium batches.\n  - Caveats: needs static shapes and capture-safe code; warmup/capture phase required.\n\nPrecision and math modes\n- AMP (automatic mixed precision) with bf16 on GPUs that support fast bf16 (e.g., Ampere/Hopper)\n  - Why: you keep most accuracy while hitting faster tensor-core kernels and higher math throughput; fewer casts than fp16 improve stability and performance.\n  - Caveats: ensure autocast covers the hot ops; some ops still run in fp32 by design.\n\n- AMP with fp16 on GPUs where bf16 is slower/unsupported\n  - Why: still enables tensor cores and can double or more throughput on matmul/conv-heavy blocks.\n  - Caveats: watch numerics (loss scaling for training); some backends (or ops) may fall back to fp32.\n\n- Enable TF32 for FP32 matmuls on NVIDIA Ampere/Hopper when exact FP32 isn’t required\n  - Why: big speedups on GEMMs with minimal accuracy impact for many DL models.\n  - Caveats: turn off if you need strict FP32 numerics.\n\nBackend/library choices\n- Use the vendor-optimized backend with current libraries\n  - NVIDIA: CUDA + cuDNN/cuBLAS with up-to-date heuristics; ROCm: MIOpen/rocBLAS on recent ROCm.\n  - Why: algorithm selection and fused kernels improve as libraries advance; TorchBench showed version-sensitive wins and regressions.\n  - Caveats: pin versions per run; algorithm changes can swing performance across releases.\n\nMemory/throughput knobs\n- Larger, but hardware-appropriate batch sizes\n  - Why: better kernel occupancy and amortized overheads; TorchBench trends showed clear scaling until hitting memory or latency targets.\n  - Caveats: watch activation memory, especially with compile/graph capture.\n\n- Use channels-last (NHWC) layout for CNNs on GPUs\n  - Why: better memory coalescing and use of tensor-core conv kernels.\n  - Caveats: verify end-to-end layout compatibility (dataloaders, custom ops).\n\nCompiler configuration details\n- torch.compile mode=\"max-autotune\" or similar higher-heuristic settings for stable production graphs\n  - Why: lets Inductor/Dynamo try more fusions and kernel variants; paid off on steady workloads.\n  - Caveats: higher compile time; not ideal for rapidly changing graphs.\n\n- Dynamic shapes enabled only if needed; prefer static shapes for best fusion\n  - Why: static shapes allow more aggressive specialization and scheduling.\n\nDispatcher/Python overhead mitigation\n- Keep hot paths free of Python control and heavy tensor-meta churn\n  - Why: TorchBench profiles showed nontrivial Python/dispatcher time on small kernels; compile or graphs remove most of it.\n\nQuantization (when applicable)\n- Use backend-native quantization paths for inference (e.g., per‑channel weight quant, fbgemm/qnnpack on CPU; backend support on GPU varies)\n  - Why: large speedups on linear/conv layers with acceptable accuracy trade‑offs.\n  - Caveats: coverage exposed that some quant paths trigger costly fallbacks if kernels are missing—verify supported ops on your backend.\n\nData movement\n- Minimize H2D/D2H transfers, use pinned memory for host I/O, and overlap transfers with compute via streams\n  - Why: TorchBench breakdowns frequently flagged memcpy and sync hotspots when inputs weren’t staged properly.\n\nWhat this looked like in practice across many models\n- Inference sweet spot on modern NVIDIA GPUs:\n  - torch.compile + AMP(bf16) + TF32 allowed = consistently faster than eager FP32; CUDA Graphs can edge it out for very stable loops with small batches.\n- Training sweet spot:\n  - torch.compile + AMP(bf16) with gradient scaling when needed; keep shapes stable and avoid unsupported custom ops in the hot loop.\n- CNNs:\n  - channels_last + AMP + cudnn autotune on; compile gives extra fusion wins on activations/norms/pointwise.\n- Transformers:\n  - AMP(bf16) + compile; benefit from fused attention/softmax patterns when available; static or bucketed sequence lengths help.\n\nCommon pitfalls TorchBench surfaced\n- Mixed precision without proper autocast regions led to silent fp32 fallbacks, losing expected gains.\n- Version drift: a cuDNN/rocBLAS update changed algorithm picks and flipped winners; pinning and reporting versions was crucial.\n- Dynamic/ragged shapes reduced compile/graph benefits; bucketing or padding improved throughput.\n- Missing kernels on one backend caused composite or CPU fallbacks; results must note these or skip affected configs.\n\nIf you share your exact hardware (e.g., A100/H100, MI300, CPU) and task (train vs. infer), I can give you a concise, ready-to-run config set and flags that align with these high-efficiency patterns.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d1a37f41ec8192934427feac51f55903fca100358111d4"}},"resumableStreamId":"STREAM:bfd5bd22-2a14-4eba-8d7f-8d0a01d95e0b","role":"assistant","status":"done","threadId":"1dec2aab-3170-4af0-b945-458c9d6e802d","timeToFirstToken":1.056,"tokens":1221,"tokensPerSecond":57.960694958701225,"updated_at":1758569342689,"userId":"google:104458561188320708185","id":"j975apjze1ne88da5p02dmywph7r2zc8"},{"_creationTime":1758569376097.084,"_id":"j97fgsh9eqkphgn0vjqg10wt8n7r2jh9","attachmentIds":[],"content":"A100, how would it work?","created_at":1758569376097,"messageId":"cf58b1fb-1828-4d18-a4c2-3db40e7df936","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"1dec2aab-3170-4af0-b945-458c9d6e802d","updated_at":1758569376097,"userId":"google:104458561188320708185","id":"j97fgsh9eqkphgn0vjqg10wt8n7r2jh9","providerMetadata":{}},{"_creationTime":1758569376097.0842,"_id":"j9793q8c3gvxajwfyytg3pamjx7r37av","attachmentIds":[],"byok":false,"content":"On an NVIDIA A100, you can hit a very efficient setup by combining mixed precision, tensor-core–friendly math modes, and either torch.compile or CUDA Graphs depending on how static your loop is. Here’s a concise, practical playbook.\n\nKey hardware facts to exploit on A100\n- Fast bf16 and fp16 tensor cores\n- TF32 acceleration for FP32 GEMMs\n- Large memory (40–80 GB) enabling bigger batches\n- Strong benefit from channels_last for CNNs\n\nRecommended configurations\n\nInference (steady shapes, repeated loop)\n- Best for tight, repetitive serving loops (same shapes/batch)\n- Use CUDA Graphs to remove launch/dispatcher overhead\n\nConfig:\n- AMP with bf16\n- Enable TF32 for residual FP32 matmuls\n- channels_last for CNNs\n- CUDA Graphs capture after warmup\n\nSnippet (PyTorch >= 2.1):\n```python\nimport torch\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cudnn.benchmark = True  # autotune convs\n\nmodel = build_model().eval().to(\"cuda\").to(memory_format=torch.channels_last)\nexample = get_example_input().to(\"cuda\").to(memory_format=torch.channels_last)\n\n# Warmup + AMP\nfor _ in range(10):\n    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n        _ = model(example)\n\n# CUDA Graphs\nstatic_inp = example.clone()\ng = torch.cuda.CUDAGraph()\ntorch.cuda.synchronize()\nwith torch.cuda.graph(g):\n    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n        static_out = model(static_inp)\n\n# Serving loop\ndef serve(inp):\n    static_inp.copy_(inp)  # same shape\n    g.replay()\n    return static_out\n```\n\nInference (moderate shape stability, or you want turnkey speedups)\n- Use torch.compile to fuse and reduce Python overhead without managing graphs\n- Works well when shapes are stable or bucketed\n\nConfig:\n- torch.compile with default or “max-autotune” modes\n- AMP bf16\n- TF32 allowed\n- channels_last for CNNs\n\nSnippet:\n```python\nimport torch\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cudnn.benchmark = True\n\nmodel = build_model().eval().to(\"cuda\").to(memory_format=torch.channels_last)\nmodel = torch.compile(model, mode=\"max-autotune\")  # or default\n\ninp = get_example_input().to(\"cuda\").to(memory_format=torch.channels_last)\n\n# Warmup compile\nwith torch.autocast(\"cuda\", dtype=torch.bfloat16\"):\n    _ = model(inp)\n\n# Timed runs\nwith torch.autocast(\"cuda\", dtype=torch.bfloat16):\n    out = model(inp)\n```\n\nTraining\n- Use AMP with bf16 and gradient scaling when needed (bf16 often doesn’t need scaling, but fp16 does)\n- torch.compile typically gives solid gains on A100; prefer stable shapes/bucketing\n- Enable cudnn.benchmark for CNNs\n\nConfig:\n- AMP bf16 (or fp16 + GradScaler)\n- torch.compile (default or “max-autotune” for stable graphs)\n- TF32 allowed\n- channels_last for CNNs\n\nSnippet:\n```python\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cudnn.benchmark = True\n\nmodel = build_model().to(\"cuda\").to(memory_format=torch.channels_last)\nmodel = torch.compile(model)  # or mode=\"max-autotune\"\nopt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\nscaler = None  # bf16 usually ok without scaling\n# If using fp16 instead:\n# scaler = GradScaler()\n\nfor batch in loader:\n    x, y = (t.to(\"cuda\").to(memory_format=torch.channels_last) for t in batch)\n    opt.zero_grad(set_to_none=True)\n    with autocast(\"cuda\", dtype=torch.bfloat16):\n        loss = loss_fn(model(x), y)\n    loss.backward()\n    opt.step()\n    # If fp16:\n    # scaler.scale(loss).backward()\n    # scaler.step(opt); scaler.update()\n```\n\nBatch size and layout\n- Start as large as fits within memory while meeting latency targets. A100 often handles 2–4× larger batches than V100 for the same model.\n- Use channels_last for CNNs; keep transformer inputs contiguous with typical (batch, seq, hidden) layouts.\n- For variable sequence lengths, bucket or pad to a small set of lengths to help compile/graphs.\n\nAttention/Transformer tips\n- Prefer fused scaled dot-product attention when available (PyTorch SDPA). Ensure use via `torch.backends.cuda.enable_flash_sdp(True)` or the default heuristics on A100.\n- Keep dtype in autocast bf16 to land on tensor-core paths.\n\nDiagnostics to confirm you’re on the fast path\n- Check matmul/conv are using tensor cores:\n  - AMP bf16 active\n  - TF32 allowed for any remaining FP32 GEMMs\n- Profile one run with PyTorch Profiler; look for fewer, longer kernels (fusion) and minimal CPU/dispatcher time.\n- Verify no unexpected dtype upcasts to FP32 in hot ops (operator-level profiler view).\n\nWhen to choose CUDA Graphs vs torch.compile\n- CUDA Graphs: highest payoff for ultra-stable inference loops with small/medium batches where launch overhead dominates.\n- torch.compile: broader benefit with less orchestration; good for both inference and training when shapes are mostly stable.\n\nCommon pitfalls on A100\n- Forgetting to enable TF32 leaves FP32 GEMMs slower than necessary.\n- AMP regions too narrow, causing FP32 fallbacks.\n- Highly dynamic shapes reduce compiler/graph benefits; use bucketing.\n- Data copies in the hot path (H2D/D2H) masking kernel speedups; pre-stage inputs on device and use pinned memory.\n\nIf you share your specific model type (CNN, transformer, recsys) and latency vs. throughput goals, I can suggest concrete batch sizes, flags, and whether to prefer compile or graphs for your A100 deployment.","created_at":1758569376098,"messageId":"3c3668dd-0263-4d9c-81ac-dedfd3c1b8f9","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"On an NVIDIA A100, you can hit a very efficient setup by combining mixed precision, tensor-core–friendly math modes, and either torch.compile or CUDA Graphs depending on how static your loop is. Here’s a concise, practical playbook.\n\nKey hardware facts to exploit on A100\n- Fast bf16 and fp16 tensor cores\n- TF32 acceleration for FP32 GEMMs\n- Large memory (40–80 GB) enabling bigger batches\n- Strong benefit from channels_last for CNNs\n\nRecommended configurations\n\nInference (steady shapes, repeated loop)\n- Best for tight, repetitive serving loops (same shapes/batch)\n- Use CUDA Graphs to remove launch/dispatcher overhead\n\nConfig:\n- AMP with bf16\n- Enable TF32 for residual FP32 matmuls\n- channels_last for CNNs\n- CUDA Graphs capture after warmup\n\nSnippet (PyTorch >= 2.1):\n```python\nimport torch\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cudnn.benchmark = True  # autotune convs\n\nmodel = build_model().eval().to(\"cuda\").to(memory_format=torch.channels_last)\nexample = get_example_input().to(\"cuda\").to(memory_format=torch.channels_last)\n\n# Warmup + AMP\nfor _ in range(10):\n    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n        _ = model(example)\n\n# CUDA Graphs\nstatic_inp = example.clone()\ng = torch.cuda.CUDAGraph()\ntorch.cuda.synchronize()\nwith torch.cuda.graph(g):\n    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n        static_out = model(static_inp)\n\n# Serving loop\ndef serve(inp):\n    static_inp.copy_(inp)  # same shape\n    g.replay()\n    return static_out\n```\n\nInference (moderate shape stability, or you want turnkey speedups)\n- Use torch.compile to fuse and reduce Python overhead without managing graphs\n- Works well when shapes are stable or bucketed\n\nConfig:\n- torch.compile with default or “max-autotune” modes\n- AMP bf16\n- TF32 allowed\n- channels_last for CNNs\n\nSnippet:\n```python\nimport torch\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cudnn.benchmark = True\n\nmodel = build_model().eval().to(\"cuda\").to(memory_format=torch.channels_last)\nmodel = torch.compile(model, mode=\"max-autotune\")  # or default\n\ninp = get_example_input().to(\"cuda\").to(memory_format=torch.channels_last)\n\n# Warmup compile\nwith torch.autocast(\"cuda\", dtype=torch.bfloat16\"):\n    _ = model(inp)\n\n# Timed runs\nwith torch.autocast(\"cuda\", dtype=torch.bfloat16):\n    out = model(inp)\n```\n\nTraining\n- Use AMP with bf16 and gradient scaling when needed (bf16 often doesn’t need scaling, but fp16 does)\n- torch.compile typically gives solid gains on A100; prefer stable shapes/bucketing\n- Enable cudnn.benchmark for CNNs\n\nConfig:\n- AMP bf16 (or fp16 + GradScaler)\n- torch.compile (default or “max-autotune” for stable graphs)\n- TF32 allowed\n- channels_last for CNNs\n\nSnippet:\n```python\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cudnn.benchmark = True\n\nmodel = build_model().to(\"cuda\").to(memory_format=torch.channels_last)\nmodel = torch.compile(model)  # or mode=\"max-autotune\"\nopt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\nscaler = None  # bf16 usually ok without scaling\n# If using fp16 instead:\n# scaler = GradScaler()\n\nfor batch in loader:\n    x, y = (t.to(\"cuda\").to(memory_format=torch.channels_last) for t in batch)\n    opt.zero_grad(set_to_none=True)\n    with autocast(\"cuda\", dtype=torch.bfloat16):\n        loss = loss_fn(model(x), y)\n    loss.backward()\n    opt.step()\n    # If fp16:\n    # scaler.scale(loss).backward()\n    # scaler.step(opt); scaler.update()\n```\n\nBatch size and layout\n- Start as large as fits within memory while meeting latency targets. A100 often handles 2–4× larger batches than V100 for the same model.\n- Use channels_last for CNNs; keep transformer inputs contiguous with typical (batch, seq, hidden) layouts.\n- For variable sequence lengths, bucket or pad to a small set of lengths to help compile/graphs.\n\nAttention/Transformer tips\n- Prefer fused scaled dot-product attention when available (PyTorch SDPA). Ensure use via `torch.backends.cuda.enable_flash_sdp(True)` or the default heuristics on A100.\n- Keep dtype in autocast bf16 to land on tensor-core paths.\n\nDiagnostics to confirm you’re on the fast path\n- Check matmul/conv are using tensor cores:\n  - AMP bf16 active\n  - TF32 allowed for any remaining FP32 GEMMs\n- Profile one run with PyTorch Profiler; look for fewer, longer kernels (fusion) and minimal CPU/dispatcher time.\n- Verify no unexpected dtype upcasts to FP32 in hot ops (operator-level profiler view).\n\nWhen to choose CUDA Graphs vs torch.compile\n- CUDA Graphs: highest payoff for ultra-stable inference loops with small/medium batches where launch overhead dominates.\n- torch.compile: broader benefit with less orchestration; good for both inference and training when shapes are mostly stable.\n\nCommon pitfalls on A100\n- Forgetting to enable TF32 leaves FP32 GEMMs slower than necessary.\n- AMP regions too narrow, causing FP32 fallbacks.\n- Highly dynamic shapes reduce compiler/graph benefits; use bucketing.\n- Data copies in the hot path (H2D/D2H) masking kernel speedups; pre-stage inputs on device and use pinned memory.\n\nIf you share your specific model type (CNN, transformer, recsys) and latency vs. throughput goals, I can suggest concrete batch sizes, flags, and whether to prefer compile or graphs for your A100 deployment.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d1a3a0ca908194a681cfc8f76c022d0c1231ab775c0a51"}},"resumableStreamId":"STREAM:cb13ba9c-0e83-48e3-864b-b1b2e09a3a7b","role":"assistant","status":"done","threadId":"1dec2aab-3170-4af0-b945-458c9d6e802d","timeToFirstToken":3.066,"tokens":1326,"tokensPerSecond":54.73006438831104,"updated_at":1758569376098,"userId":"google:104458561188320708185","id":"j9793q8c3gvxajwfyytg3pamjx7r37av"}],"version":"6.0.2"}