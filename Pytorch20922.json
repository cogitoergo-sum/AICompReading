{"threads":[{"_creationTime":1758565280976.374,"_id":"jd750wkfzsd63j8nwkzye1mbz97r3kqy","createdAt":1758565280977,"generationStatus":"completed","lastMessageAt":1758566146064,"model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"pinned":false,"threadId":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","title":"PyTorch 2 Explanation","updatedAt":1758566146064,"userId":"google:104458561188320708185","userSetTitle":false,"visibility":"visible","id":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","last_message_at":1758566146064,"created_at":1758565280977,"updated_at":1758566146064,"status":"completed","user_edited_title":false}],"messages":[{"_creationTime":1758565280976.3743,"_id":"j97ev3xaza21annkcx6wbm2z617r2jmb","attachmentIds":[],"content":"Explain this paper to me in depth using the Feynman technique, as if you were its author.\n\nPyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation\n\nhttps://dl.acm.org/doi/10.1145/3620665.3640366","created_at":1758565290038,"messageId":"ba62e535-ee3d-43b2-be25-e196311befeb","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","updated_at":1758565290038,"userId":"google:104458561188320708185","id":"j97ev3xaza21annkcx6wbm2z617r2jmb","providerMetadata":{}},{"_creationTime":1758565290038.757,"_id":"j9752x7jx13q74mw16b75zp4sx7r3pb7","attachmentIds":[],"byok":false,"content":"I’m going to teach you this paper as if I wrote it, using the Feynman technique: simple language, concrete examples, how the parts fit together, and why each design choice exists. By the end, you should be able to explain PyTorch 2’s compiler stack to someone else without hand-waving.\n\nWhat problem are we solving?\n- Users love eager PyTorch because it’s just Python: print, pdb, loops, ifs, dicts, NumPy, custom ops, etc.\n- Compilers love graphs because they can do cross-op optimization: fuse kernels, tile/schedule loops, vectorize, reduce memory traffic.\n- Tension: eager is flexible but hides the global structure; graph is optimizable but rigid. Prior PyTorch attempts either broke usability or correctness:\n  - torch.jit.trace recorded at the C++ dispatcher: misses Python control flow; can be wrong if inputs/paths change.\n  - torch.jit.script parsed Python to a restricted subset: fails on many real models.\n  - Lazy eval approaches: added overhead and latency, and still struggled with Python’s dynamism.\n\nGoal: Keep Python’s flexibility, but get graph-level performance. Make the “fast path” the default for normal code, and transparently fall back when things get too dynamic.\n\nThe core idea in one sentence\nIntercept Python execution at the frame level, rewrite bytecode on the fly to extract FX subgraphs of PyTorch ops, then compile those subgraphs to fast kernels with a backend that understands loops and fusion, while running the rest of the Python code as-is.\n\nThe two main components behind torch.compile\n1) TorchDynamo: Python-level graph capture\n- What it does:\n  - Hooks CPython’s Frame Evaluation API right before a frame runs.\n  - Analyzes and edits Python bytecode to identify stretches where we call PyTorch operators (ATen).\n  - Builds an FX graph for those stretches, leaving the rest of the code in Python.\n  - Replaces the original bytecode chunk with a call that executes the compiled graph, and then resumes Python.\n- Why this works:\n  - We get “define-by-run” behavior: we only capture what actually executes for the given inputs, including data-dependent control flow.\n  - We don’t need to understand all of Python statically. We capture local regions we can prove safe, and we bail out otherwise.\n- Guards and specialization:\n  - Captured graphs are specialized to assumptions (tensor dtypes/devices/shapes, control-flow choices, etc.) recorded as guards.\n  - If a guard fails on a later input (e.g., shape changes), we re-enter Dynamo, recapture with the new facts, and cache another compiled variant.\n  - This makes dynamic models feasible without penalizing stable ones.\n- Partial graphs are OK:\n  - If your code calls a non-PyTorch function, mutates Python lists, throws exceptions, does logging, etc., Dynamo will cut the subgraph before that point, execute Python, and then capture again after. You still get speedups where it counts.\n\n2) TorchInductor: default backend compiler\n- What it does:\n  - Takes the FX graph and lowers it into a compact, loop-level IR that looks like PyTorch eager in spirit (define-by-run), but is compiler-friendly (~50 core ops).\n  - Generates:\n    - GPU: Triton kernels (via OpenAI Triton), allowing highly tuned fused kernels.\n    - CPU: C++ with OpenMP for parallel loops.\n- Why loops matter:\n  - Many optimizations are loop transformations: fuse producer/consumer ops, tile for caches/SMs, vectorize, hoist invariants, reduce memory bandwidth.\n  - A loop-level IR makes expressing these easy and uniform.\n- Dynamic shapes:\n  - Inductor emits shape-polymorphic code guided by Dynamo’s guards. Where needed, it generates runtime shape checks and emits multiple kernels or uses indexing that tolerates variability.\n- Training support:\n  - We integrate with AOTAutograd: capture forward and a compiled backward ahead of time so that both directions benefit from fusion and scheduling.\n\nHow torch.compile stitches it together\n- When you call torch.compile(model):\n  - TorchDynamo intercepts Python frames and pulls out FX subgraphs of ATen ops.\n  - It hands those to a backend (by default TorchInductor; others can be plugged in).\n  - The backend compiles to fast kernels and returns a callable.\n  - The edited Python bytecode now calls that compiled segment directly, then continues normal Python execution.\n- Recompilation and caching:\n  - On new shapes/dtypes/branches, guards trigger re-capture and a new compiled artifact is cached.\n  - Common cases keep hitting the fast path without further overhead.\n\nWhy prior approaches failed and what’s novel here\n- Record/replay (trace) is unsound for Python control flow. We fix this by capturing at Python level, respecting the actual executed path per input and using guards for assumptions.\n- Static parsing can’t represent “all of Python.” We don’t try; we mix compiled pieces with vanilla Python, so we never force you into a restricted sublanguage.\n- Lazy evaluation added latency; Dynamo modifies bytecode and runs immediately, adding minimal overhead.\n- An “all-graph” backend is unrealistic for arbitrary Python. Our design’s unit is a subgraph, so we compile hot numeric parts and let Python do the rest.\n\nWhat exactly does the graph contain?\n- Dynamo builds FX graphs of ATen operations and tensor metadata logic it can safely model. It explicitly omits:\n  - Pure Python logic that can’t be proven tensor-only,\n  - External libraries not modeled (e.g., arbitrary NumPy calls, file I/O),\n  - Side-effectful constructs that would break replay.\n- Where we cut the graph:\n  - Any boundary where semantics can’t be preserved as a pure tensor function becomes a graph break. This gives correctness first, speed second.\n\nPerformance highlights (as measured in the paper)\n- On NVIDIA A100:\n  - 2.27× geometric mean speedup for inference.\n  - 1.41× geometric mean speedup for training.\n  - Benchmarked across 180+ real-world models; outperforms six other compiler stacks the paper compares against.\n- Overhead:\n  - Dynamo’s capture overhead is small relative to model runtime, and it amortizes via caching.\n- Why these wins happen:\n  - Fusing memory-bound ops reduces reads/writes and kernel launches.\n  - Loop tiling and vectorization map better to CPU/GPU memory hierarchies.\n  - For training, AOTAutograd allows cross-op fusion across backward steps that eager autograd couldn’t.\n\nKey subsystems we leaned on\n- FX: PyTorch’s graph IR used to represent the captured subgraph.\n- PrimTorch: canonicalization of thousands of ops into a smaller set of “ATen canonical” and “prim” ops, making backends tractable.\n- AOTAutograd: ahead-of-time capture of backward, enabling backend optimizations on gradients.\n- Triton: a DSL for writing high-performance GPU kernels that Inductor auto-generates, so users don’t have to handcraft kernels.\n\nConcrete example to make it tangible\n- Suppose your forward has: y = gelu(layernorm(x @ W + b))\n  - Eager: launches matmul (cuBLAS), then bias add, then layernorm (multiple tiny kernels), then GELU (tiny kernel). Lots of memory traffic.\n  - With torch.compile:\n    - Dynamo captures the whole region as a graph (plus guards for shapes/dtypes).\n    - Inductor lowers to a fused Triton kernel that computes the epilogue (bias + layernorm + GELU) in registers/shared memory, possibly keeping the matmul as a library call or fusing around it where profitable.\n    - Result: fewer launches, less DRAM traffic, better occupancy.\n\nWhat about dynamic control flow or Python lists?\n- If you have an if dependent on a tensor value, Dynamo specializes on the taken branch and guards it. If a future batch flips the condition, we compile a second variant.\n- If you manipulate Python lists or dicts in the middle, we make a graph break, run that logic in Python, then resume capture later. You still get fusion before and after.\n\nCorrectness model\n- We only compile what we can model purely from the inputs/guards.\n- Any ambiguous semantics trigger a graph break.\n- If your code has side effects, they happen in the Python parts in program order.\n- For deterministic numerical equivalence, we rely on ATen semantics and well-tested lowerings; you can turn on debug asserts and accuracy checks during development.\n\nWhen won’t you see big wins?\n- Single, very large ops already using vendor libraries at high efficiency (e.g., a giant matmul alone) may be near peak; fusion helps when there are adjacent memory-bound ops.\n- Extremely dynamic models that change shapes/branches constantly may trigger frequent recompiles; you can mitigate via batching, shape bucketing, or enabling dynamic-shape-friendly modes.\n- Heavy non-tensor Python work dominates runtime; we can’t speed up pure Python.\n\nHow to use it\n- In most cases:\n  - training/inference: compiled = torch.compile(model) and then run as usual.\n  - For control: tune mode/opts via torch.compile(..., backend=\"inductor\", dynamic=True/False, fullgraph=False, options=...).\n  - Fall back to eager by removing compile; you didn’t change your model code.\n\nDesign trade-offs we consciously made\n- Bytecode-level capture vs AST: more faithful to actual execution (including closures, exceptions, frames), less brittle across Python features.\n- Subgraph compilation vs whole-program IR: preserves Python’s generality; still hits hot numeric regions.\n- Loop-level IR vs giant op zoo: makes backends simpler, improvements composable, and dynamic shapes manageable.\n\nResults in context\n- The main novelty is not “a new compiler,” but a system boundary: bring the compiler to Python’s execution boundary (Frame Eval), then feed a loop-level backend that can generate high-quality kernels automatically. This pairing is what lets torch.compile be both low-friction and broadly effective.\n\nIf you remember only five things\n- torch.compile is on by wrapping your model; no model rewrite needed.\n- TorchDynamo captures chunks of your Python program at runtime by rewriting bytecode and building FX graphs guarded by runtime assumptions.\n- TorchInductor turns those graphs into fast loop code: Triton on GPUs, C++/OpenMP on CPUs, with fusion and tiling.\n- It handles dynamic shapes and control flow by specializing, guarding, caching multiple variants, and making safe graph breaks.\n- Across 180+ models, we saw strong speedups for both inference and training, outperforming other backends we evaluated.\n\nWould you like me to:\n- Walk through a specific snippet from your codebase and predict what gets fused?\n- Explain how guards are formed and when they trigger recompilation?\n- Compare Inductor’s codegen to XLA/TVM in more detail?","created_at":1758565290039,"messageId":"5599a6a5-9405-40e2-8164-2865b4019380","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":null,"queries":["PyTorch 2 Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation 3620665.3640366 PDF","TorchDynamo AOTAutograd PrimTorch TorchInductor paper ACM 2024 PyTorch 2"]},"result":[{"content":"PyTorch 2: Faster Machine Learning Through Dynamic\nPython Bytecode Transformation and Graph\nCompilation\nJason Ansel\nMeta\nEdward Yang\nMeta\nHorace He\nMeta\nNatalia Gimelshein\nOpenAI\nAnimesh Jain\nMeta\nMichael Voznesensky\nMeta\nBin Bao\nMeta\nPeter Bell\nQuansight\nDavid Berard\nMeta\nEvgeni Burovski\nQuansight\nGeeta Chauhan\nMeta\nAnjali Chourdia\nMeta\nWill Constable\nMeta\nAlban Desmaison\nMeta\nZachary DeVito\nMeta\nElias Ellison\nMeta\nWill Feng\nMeta\nJiong Gong\nIntel\nMichael Gschwind\nMeta\nBrian Hirsh\nMeta\nSherlock Huang\nMeta\nKshiteej Kalambarkar\nQuansight\nLaurent Kirsch\nMeta\nMichael Lazos\nMeta\nMario Lezcano\nQuansight\nYanbo Liang\nMeta\nJason Liang\nMeta\nYinghai Lu\nMeta\nCK Luk\nMeta\nBert Maher\nMeta\nYunjie Pan\nUniversity of Michigan\nChristian Puhrsch\nMeta\nMatthias Reso\nMeta\nMark Saroufim\nMeta\nMarcos Yukio Siraichi\nQuansight\nHelen Suk\nMeta\nMichael Suo\nMeta\nPhil Tillet\nOpenAI\nEikan Wang\nIntel\nXiaodong Wang\nMeta\nWilliam Wen\nMeta\nShunting Zhang\nMeta\nXu Zhao\nMeta\nKeren Zhou\nOpenAI\nGeorge Mason University\nRichard Zou\nMeta\nAjit Mathews\nMeta\nGregory Chanan\nMeta\nPeng Wu\nMeta\nSoumith Chintala\nMeta\nPermission to make digital or hard copies of part or all of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. Copyrights for third\u0002party components of this work must be honored. For all other uses, contact\nthe owner/author(s).\nASPLOS ’24, April 27-May 1, 2024, La Jolla, CA, USA\n© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0385-0/24/04\nhttps://doi.org/10.1145/3620665.3640366\nAbstract\nThis paper introduces two extensions to the popular PyTorch\nmachine learning framework, TorchDynamo and TorchIn\u0002ductor, which implement the torch.compile feature released\nin PyTorch 2. TorchDynamo is a Python-level just-in-time\n(JIT) compiler that enables graph compilation in PyTorch\nprograms without sacrificing the flexibility of Python. It\nachieves this by dynamically modifying Python bytecode\nbefore execution and extracting sequences of PyTorch oper\u0002ations into an FX graph, which is then JIT compiled using\none of many extensible backends. TorchInductor is the de\u0002fault compiler backend for TorchDynamo, which translates\nPyTorch programs into OpenAI’s Triton for GPUs and C++\nfor CPUs. Results show that TorchDynamo is able to capture\ngraphs more robustly than prior approaches while adding\nminimal overhead, and TorchInductor is able to provide a\n2.27× inference and 1.41× training geometric mean speedup\non an NVIDIA A100 GPU across 180+ real-world models,\nwhich outperforms six other compilers. These extensions\nprovide a new way to apply optimizations through compilers\nin eager mode frameworks like PyTorch.\nACM Reference Format:\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Ani\u0002mesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard,\nEvgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Consta\u0002ble, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng,\nJiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshi\u0002teej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano,\nYanbo Liang, Jason Liang, Yinghai Lu, CK Luk, Bert Maher, Yunjie\nPan, Christian Puhrsch, Matthias Reso, Mark Saroufim, Marcos\nYukio Siraichi, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang,\nXiaodong Wang, William Wen, Shunting Zhang, Xu Zhao, Keren\nZhou, Richard Zou, Ajit Mathews, Gregory Chanan, Peng Wu,\nand Soumith Chintala. 2024. PyTorch 2: Faster Machine Learning\nThrough Dynamic Python Bytecode Transformation and Graph\nCompilation. In 29th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems, Volume\n2 (ASPLOS ’24), April 27-May 1, 2024, La Jolla, CA, USA. ACM, New\nYork, NY, USA, 18 pages. https://doi.org/10.1145/3620665.3640366\n1 Introduction\nModern machine learning frameworks can be divided into ea\u0002ger mode frameworks, such as PyTorch [32] and JAX [8], and\ngraph mode frameworks, such as TensorFlow [1], Caffe [25],\nTheano [5], and CNTK [37]. Eager mode frameworks use\nan imperative define-by-run [47] approach where a machine\nlearning model is represented as code that is executed each\ntime one wants to run the model. Graph mode frameworks\ntake a more declarative define-and-run [47] approach, where\nthey expose a graph building API that requires users to first\nconstruct a graph and then later execute that graph.\nUsers of machine learning frameworks, and especially re\u0002searchers, have shown an overwhelming preference for the\neager programming model [22]. The eager mode model is\neasier to understand and can be debugged using standard\ntools such as print and pdb in Python [23]. This user pref\u0002erence towards eager mode has caused traditionally graph\nmode frameworks to switch to eager mode programming\nmodels [4].\nThe downside of eager mode frameworks is that they make\nit harder to apply graph-level optimizations through compil\u0002ers. The framework only has visibility of a single operator at a\ntime, and thus cannot automatically perform optimizations,\nlike fusion or scheduling, that cross operator boundaries.\nTo address this, there have been attempts to allow graph\ncapture in PyTorch through record/replay [17, 34], Python\nparsing [17], and lazy evaluation [39]. Unfortunately, these\napproaches have sacrificed much of the usability that draws\nusers to PyTorch. Record/replay is unsound and can produce\nincorrect behavior [17]. Python parsing works for simple\nprograms, but has not been able to replicate the complex\nsemantics of all of Python, so results will show it fails on\nover half of real-world models. Lazy evaluation incurs high\nrun-time overheads and adds latency to kernel launches. Ad\u0002ditionally, an exclusively graph mode backend for PyTorch is\nintractable for some models. Due to the flexibility provided\nby PyTorch, many model authors take advantage of features\nthat do not easily map to graphs, such as: dictionaries, lists,\ncustom classes, third party libraries (numpy, logging, etc),\ndisk/network, multiprocessing, exceptions, and handwritten\nkernels.\nThis paper presents two open source extensions to Py\u0002Torch: TorchDynamo and TorchInductor. These extensions\nare behind the torch.compile feature introduced in PyTorch\n2 and officially released in March 2023. TorchDynamo is a\nPython-level JIT compiler designed to allow graph compila\u0002tion in PyTorch programs while retaining the full flexibility\nof Python. TorchDynamo hooks into the Python frame eval\u0002uation API [9] in CPython to dynamically modify Python\nbytecode right before it is executed. It rewrites Python byte\u0002code in order to extract sequences of PyTorch operations into\nan FX graph [34] which is then just-in-time compiled with\nmany extensible backends. It creates this FX graph through\nbytecode analysis and is designed to generate smaller graph\nfragments that can be mixed with Python execution to get\nthe best of both worlds: usability and performance.\nTorchInductor is a new compiler backend for TorchDy\u0002namo. It translates PyTorch programs into OpenAI’s Tri\u0002ton [46] for GPUs and C++/OpenMP [15] for CPUs. TorchIn\u0002ductor is able to support the flexibility and dynamism of Py\u0002Torch by using similar abstractions to PyTorch eager mode.\nIt introduces a new define-by-run loop-level intermediate\nrepresentation (IR) to make it easy to add new operator low\u0002erings. Additionally, it is implemented in Python, so it is easy\nfor PyTorch users to extend and modify to meet their needs.\nExperimental results show that TorchDynamo is able to\ncapture graphs more robustly than prior approaches while\nadding minimal overhead. TorchDynamo is able to capture a\nsingle whole-program graph for most models and can grace\u0002fully fall back to partial graphs when needed. Measurements\nshow TorchInductor produces faster code on average than six\nother PyTorch compiler backends. Performance comparisons\ninclude both training and inference, CPU and GPU, float32\nand float16, and three large benchmark suites containing\n180+ full-sized models taken from real-world applications.\n2\n2 Prior Attempts at PyTorch Graph Capture\nGraph capture in PyTorch presents unique challenges when\ncompared to graph mode frameworks [1, 25, 5, 37], where\nthe user is restricted to only using constructs that are repre\u0002sentable in the graph. With PyTorch and other eager mode\nframeworks, the user is free to embed arbitrary code, includ\u0002ing non-PyTorch libraries, inside their models. This results\nin frequent conversion from PyTorch Tensors to Python\ntypes (via .item(), .tolist(), etc), usage of external libraries\n(numpy, logging, etc), and usage of Python constructs (classes,\nclosures, exceptions, control flow, etc) that do not map well\nto a fixed graph abstraction. Due to this mismatch between\nthe flexibility provided by Python/PyTorch, and the inflexibil\u0002ity of graph representations, prior attempts at graph capture\nin PyTorch have needed to place restrictions on the user\nexperience. While this tension between flexibility and repre\u0002sentation is solved by TorchDynamo, we examine prior art\nin the space to provide context and background.\n2.1 torch.jit.trace\ntorch.jit.trace uses record/replay with example inputs to\nproduce a TorchScript [17] graph. The recording is done at\nthe PyTorch dispatcher level, which is inside the C++ portion\nof PyTorch and used to dispatch operators to device-specific\nkernels and for autograd. Because the recording is done in\nC++, torch.jit.trace does not capture any control flow in\nPython. Consider this example:\ndef example1(x):\nif len(torch.nonzero(x)) > 1:\nreturn x + 1\nreturn x - 1\nWith example input torch.tensor([0, 0]), torch.jit.trace\nwould capture a graph equivalent to:\ndef example1_incorrect_capture(x):\ntorch.nonzero(x)\nreturn x - 1\nSince the path through the program is specialized on the\nexample input, a different input (such as torch.tensor([1,1]))\nwill give incorrect results. Additionally, any non-PyTorch\noperators (such as external librar","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdocs.pytorch.org%2Fassets%2Fpytorch2-2.pdf","id":"https://docs.pytorch.org/assets/pytorch2-2.pdf","publishedDate":null,"summary":"This paper introduces TorchDynamo and TorchInductor, extensions to PyTorch that implement the torch.compile feature in PyTorch 2. TorchDynamo is a JIT compiler that modifies Python bytecode to extract PyTorch operations into an FX graph, which is then compiled. TorchInductor is the default backend, translating PyTorch programs into OpenAI’s Triton for GPUs and C++ for CPUs. Results show significant speedups in inference and training on an NVIDIA A100 GPU across various models.\n","title":"[PDF] PyTorch 2: Faster Machine Learning Through Dynamic Python ...","url":"https://docs.pytorch.org/assets/pytorch2-2.pdf"},{"content":"To read the full-text of this research, you can request a copy directly from the authors.... Compilation-based operator fusion is adopted to reduce kernel launches and frequent I/O operations [31,70]. DL frameworks [4, 75] generally only fuse memory-intensive (MI) operators, while compute-intensive (CI) operators are handled separately using vendor libraries. Other studies [33,46,72] have further explored the fusion of CI operator and MI operator to complement resource utilization such as memory bandwidth and streaming processors. ...... Researchers further classify tensor operators into MI and CI categories for selective fusion. Early works [4, 75] stereotype CI operators as non-fusion boundaries, and only fuse MI operators to alleviate intensive offchip memory access. Other works [33,46] merge the CI operator with adjacent MI operators to make up for the imbalance in hardware resource usage. ...... However, both online and offline training require sufficient collection of runtime statistics. Aggressive techniques [4, 42] sequentially unfold all operators in the computation graph, reducing the search range from multiplication of operator spaces to their addition. But individual tuning for each operator will lead to global suboptimal decisions due to the lack of graph-level information. ... Wenhao Dai Haodong Deng Mengfei Rong Qingxiao Sun Large language models are popular around the world due to their powerful understanding capabilities. As the core component of LLMs, accelerating Transformer through parallelization has gradually become a hot research topic. Mask layers introduce sparsity into Transformer to reduce calculations. However, previous works rarely focus on the performance optimization of sparse Transformer. Moreover, rule-based mechanisms ignore the fusion opportunities of mixed-type operators and fail to adapt to various sequence lengths. To address the above problems, we propose STOF, a framework that incorporates optimizations for Sparse Transformer via flexible masking and operator fusion on GPU. We firstly unify the storage format and kernel implementation for the multi-head attention. Then, we map fusion schemes to compilation templates and determine the optimal parameter setting through a two-stage search engine. The experimental results show that compared to the state-of-the-art work, STOF achieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end inference.... • CoT Outputs from Standard LLMs: We evaluate Qwen2.5-14B-Instruct (Yang et al., 2024) by prompting it to \"Think step by step\", thereby generating CoT outputs and a final answer. ...... We utilize the Huggingface Transformers (Wolf et al., 2020), PyTorch (Ansel et al., 2024), and DeepSpeed (Rasley et al., 2020) libraries for training and inference. For the CoT Extraction module, the seed dataset D is 2WikiMultihopQA (Ho et al., 2020). ...... For dataset construction, we use DeepSeek-Distill-Qwen7B (DeepSeek-AI et al., 2025) as the LRM M R , and Qwen2.5-32B-Instruct (Yang et al., 2024) as the powerful summarizing model M p . The extraction model is trained from Llama-3.1-8B-Instruct (Grattafiori et al., 2024). ... Large Reasoning Models (LRMs) extend large language models with explicit, multi-step reasoning traces to enhance transparency and performance on complex tasks. However, these reasoning traces can be redundant or logically inconsistent, making them a new source of hallucination that is difficult to detect. Existing hallucination detection methods focus primarily on answer-level uncertainty and often fail to detect hallucinations or logical inconsistencies arising from the model's reasoning trace. This oversight is particularly problematic for LRMs, where the explicit thinking trace is not only an important support to the model's decision-making process but also a key source of potential hallucination. To this end, we propose RACE (Reasoning and Answer Consistency Evaluation), a novel framework specifically tailored for hallucination detection in LRMs. RACE operates by extracting essential reasoning steps and computing four diagnostic signals: inter-sample consistency of reasoning traces, entropy-based answer uncertainty, semantic alignment between reasoning and answers, and internal coherence of reasoning. This joint analysis enables fine-grained hallucination detection even when the final answer appears correct. Experiments across datasets and different LLMs demonstrate that RACE outperforms existing hallucination detection baselines, offering a robust and generalizable solution for evaluating LRMs. Our code is available at: https://github.com/bebr2/RACE.... The link is established by assuming that y ∼ Y . 2 In Bayesian inverse problems, one associates to (1) a prior distribution q 0 encoding beliefs on the possible values of X. One is interested in the a posteriori distribution of X given y, which by Bayes theorem is given by p (x|y) := ℓ(y|x)q 0 (x) /L(y) where L(y) = ℓ(y|x)q 0 (x) dx and ℓ(y|x) is the likelihood associated with (1). This is particularly useful in the cases of so-called ill-posed inverse problems, which arise when several maxima of x → ℓ(y|x) exist. ...... Therefore, we went from the standard EDM parameterization where channel multiplier and per layer multipliers are (192, [1, 2,3,4]) to (64, [1, 1, 2, 2, 4]). The self-attention layer is only active in the last Unet layer corresponding to a resolution of 16 pixels. ...... More information is available at https://github.com/gabrielvc/dgm_anisotropic_grf in the folder python/diff post gauss/configs/conditional sampler. Otherwise, the paper relies heavily on Pytorch [1] and in particular the Pytorch Lightning library [60]. The NUTS implementation from Pyro [5] ... Gabriel V Cardoso Mike Pereira Bayesian models based on Gaussian processes (GPs) offer a flexible framework to predict spatially distributed variables with uncertainty. But the use of nonstationary priors, often necessary for capturing complex spatial patterns, makes sampling from the predictive posterior distribution (PPD) computationally intractable. In this paper, we propose a two-step approach based on diffusion generative models (DGMs) to mimic PPDs associated with non-stationary GP priors: we replace the GP prior by a DGM surrogate, and leverage recent advances on training-free guidance algorithms for DGMs to sample from the desired posterior distribution. We apply our approach to a rich non-stationary GP prior from which exact posterior sampling is untractable and validate that the issuing distributions are close to their GP counterpart using several statistical metrics. We also demonstrate how one can fine-tune the trained DGMs to target specific parts of the GP prior. Finally we apply the proposed approach to solve inverse problems arising in environmental sciences, thus yielding state-of-the-art predictions.... In contemporary machine learning (e.g. foundation models), data streams typically consist of unlabeled data, augmented with some degree of supervised fine-tuning in the final training stages [1]. Such an approach is difficult to translate to the continual learning setting encountered in natural data streams, as the naive introduction of fine-tuning stages often leads to catastrophic forgetting [2][3][4]. ...... These modulations are then frozen and applied to the network during the feedforward weight learning, using only currently available unlabeled samples, and effectively consolidate the task-specific knowledge encoded in the modulations into the feedforward weights (Figure 1, middle). This departs from the conventional pretraining-finetuning paradigm, where naive reintegration of specialized models into the general one causes catastrophic forgetting [1, 4] (Figure 1, right). ...... We provide results with 10 sessions in appendix E. We additionally introduce a supervised crossentropy (CE) baseline with a projection head [103], which has been reported to outperform self-supervised methods. 1 For each session, we train the model for 100 orthogonalization epochs, where we train modulations for the new classes, and 200 consolidation epochs using a batch size of 256. We further start the first session with a pretraining phase of 250 epochs, where the feedforward weights are updated via VI, or analogously with SupCon and CE for the respective supervised methods. ... Biological brains learn continually from a stream of unlabeled data, while integrating specialized information from sparsely labeled examples without compromising their ability to generalize. Meanwhile, machine learning methods are susceptible to catastrophic forgetting in this natural learning setting, as supervised specialist fine-tuning degrades performance on the original task. We introduce task-modulated contrastive learning (TMCL), which takes inspiration from the biophysical machinery in the neocortex, using predictive coding principles to integrate top-down information continually and without supervision. We follow the idea that these principles build a view-invariant representation space, and that this can be implemented using a contrastive loss. Then, whenever labeled samples of a new class occur, new affine modulations are learned that improve separation of the new class from all others, without affecting feedforward weights. By co-opting the view-invariance learning mechanism, we then train feedforward weights to match the unmodulated representation of a data sample to its modulated counterparts. This introduces modulation invariance into the representation space, and, by also using past modulations, stabilizes it. Our experiments show improvements in both class-incremental and transfer learning over state-of-the-art unsupervised approaches, as well as over comparable supervised approaches, using as few as 1% of available labels. Taken together, our work suggests that top-down modulations play a crucial role in balancing stability and plasticity.... This poses a notable constraint in research ","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F380151019_PyTorch_2_Faster_Machine_Learning_Through_Dynamic_Python_Bytecode_Transformation_and_Graph_Compilation","id":"https://www.researchgate.net/publication/380151019_PyTorch_2_Faster_Machine_Learning_Through_Dynamic_Python_Bytecode_Transformation_and_Graph_Compilation","publishedDate":null,"summary":"This research explores techniques for accelerating machine learning, particularly focusing on operator fusion and optimization within the context of large language models (LLMs) and sparse Transformers. The study introduces STOF, a framework that optimizes Sparse Transformers through flexible masking and operator fusion on GPUs, achieving significant speedups in multi-head attention (MHA) computation and end-to-end inference compared to existing methods. The research also utilizes libraries like Huggingface Transformers, PyTorch, and DeepSpeed for training and inference, and evaluates LLMs such as Qwen2.5-14B-Instruct.\n","title":"PyTorch 2: Faster Machine Learning Through Dynamic Python ...","url":"https://www.researchgate.net/publication/380151019_PyTorch_2_Faster_Machine_Learning_Through_Dynamic_Python_Bytecode_Transformation_and_Graph_Compilation"},{"content":"PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch Computer Science 2025 PyGraph is introduced, a novel approach to automatically harness the power of CUDA Graphs within PyTorch2, and embodies three novel optimizations: it enables wider deployment of CUDA Graphs, reduces GPU kernel parameter copy overheads, and selectively deploys CUDA Graphs based on a cost-benefit analysis. [PDF] OMP4Py: a pure Python implementation of OpenMP César Piñeiro J. C. Pichel Computer Science 2024 OMP4Py is introduced, the first pure Python implementation of OpenMP, and it is demonstrated that it is possible to bring OpenMP's familiar directive-based parallelization paradigm to Python, allowing developers to write parallel code with the same level of control and flexibility as in C, C++, or Fortran. [PDF]","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.semanticscholar.org%2Fpaper%2FPyTorch-2%253A-Faster-Machine-Learning-Through-Dynamic-Ansel-Yang%2Fa509670659e8b054e2b7d1b6f8a0bc722398fa62","id":"https://www.semanticscholar.org/paper/PyTorch-2%3A-Faster-Machine-Learning-Through-Dynamic-Ansel-Yang/a509670659e8b054e2b7d1b6f8a0bc722398fa62","publishedDate":null,"summary":"The webpage discusses two research papers. The first paper, \"PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch,\" introduces a novel approach to automatically utilize CUDA Graphs within PyTorch2, with optimizations including wider deployment of CUDA Graphs, reduced GPU kernel parameter copy overheads, and selective deployment based on cost-benefit analysis. The second paper, \"OMP4Py: a pure Python implementation of OpenMP,\" presents the first pure Python implementation of OpenMP, enabling developers to write parallel code in Python with similar control and flexibility as in C, C++, or Fortran.\n","title":"[PDF] PyTorch 2: Faster Machine Learning Through Dynamic Python ...","url":"https://www.semanticscholar.org/paper/PyTorch-2%3A-Faster-Machine-Learning-Through-Dynamic-Ansel-Yang/a509670659e8b054e2b7d1b6f8a0bc722398fa62"},{"content":"PyTorch 2: Faster Machine Learning Through Dynamic\nPython Bytecode Transformation and Graph\nCompilation\nJason Ansel\nMeta\nEdward Yang\nMeta\nHorace He\nMeta\nNatalia Gimelshein\nOpenAI\nAnimesh Jain\nMeta\nMichael Voznesensky\nMeta\nBin Bao\nMeta\nDavid Berard\nMeta\nGeeta Chauhan\nMeta\nAnjali Chourdia\nMeta\nWill Constable\nMeta\nAlban Desmaison\nMeta\nZachary DeVito\nMeta\nElias Ellison\nMeta\nWill Feng\nMeta\nJiong Gong\nIntel\nMichael Gschwind\nMeta\nBrian Hirsh\nMeta\nSherlock Huang\nMeta\nLaurent Kirsch\nMeta\nMichael Lazos\nMeta\nYanbo Liang\nMeta\nJason Liang\nMeta\nYinghai Lu\nMeta\nCK Luk\nMeta\nBert Maher\nMeta\nYunjie Pan\nUniversity of Michigan\nChristian Puhrsch\nMeta\nMatthias Reso\nMeta\nMark Saroufim\nMeta\nHelen Suk\nMeta\nMichael Suo\nMeta\nPhil Tillet\nOpenAI\nEikan Wang\nIntel\nXiaodong Wang\nMeta\nWilliam Wen\nMeta\nShunting Zhang\nMeta\nXu Zhao\nMeta\nKeren Zhou\nOpenAI\nGeorge Mason University\nRichard Zou\nMeta\nAjit Mathews\nMeta\nGregory Chanan\nMeta\nPeng Wu\nMeta\nSoumith Chintala\nMeta\nAbstract\nThis paper introduces two extensions to the popular PyTorch\nmachine learning framework, TorchDynamo and TorchIn\u0002ductor, which implement the torch.compile feature released\nin PyTorch 2. TorchDynamo is a Python-level just-in-time\n(JIT) compiler that enables graph compilation in PyTorch\nPermission to make digital or hard copies of part or all of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. Copyrights for third\u0002party components of this work must be honored. For all other uses, contact\nthe owner/author(s).\nASPLOS ’24, April 27-May 1, 2024, La Jolla, CA, USA\n© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0385-0/24/04\nhttps://doi.org/10.1145/3620665.3640366\nprograms without sacrificing the flexibility of Python. It\nachieves this by dynamically modifying Python bytecode\nbefore execution and extracting sequences of PyTorch oper\u0002ations into an FX graph, which is then JIT compiled using\none of many extensible backends. TorchInductor is the de\u0002fault compiler backend for TorchDynamo, which translates\nPyTorch programs into OpenAI’s Triton for GPUs and C++\nfor CPUs. Results show that TorchDynamo is able to capture\ngraphs more robustly than prior approaches while adding\nminimal overhead, and TorchInductor is able to provide a\n2.27× inference and 1.41× training geometric mean speedup\non an NVIDIA A100 GPU across 180+ real-world models,\nwhich outperforms six other compilers. These extensions\nprovide a new way to apply optimizations through compilers\nin eager mode frameworks like PyTorch.\nACM Reference Format:\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh\nJain, Michael Voznesensky, Bin Bao, David Berard, Geeta Chauhan,\nAnjali Chourdia, Will Constable, Alban Desmaison, Zachary De\u0002Vito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian\nHirsh, Sherlock Huang, Laurent Kirsch, Michael Lazos, Yanbo Liang,\nJason Liang, Yinghai Lu, CK Luk, Bert Maher, Yunjie Pan, Christian\nPuhrsch, Matthias Reso, Mark Saroufim, Helen Suk, Michael Suo,\nPhil Tillet, Eikan Wang, Xiaodong Wang, William Wen, Shunt\u0002ing Zhang, Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews,\nGregory Chanan, Peng Wu, and Soumith Chintala. 2024. PyTorch\n2: Faster Machine Learning Through Dynamic Python Bytecode\nTransformation and Graph Compilation. In 29th ACM Interna\u0002tional Conference on Architectural Support for Programming Lan\u0002guages and Operating Systems, Volume 2 (ASPLOS ’24), April 27-May\n1, 2024, La Jolla, CA, USA. ACM, New York, NY, USA, 18 pages.\nhttps://doi.org/10.1145/3620665.3640366\n1 Introduction\nModern machine learning frameworks can be divided into ea\u0002ger mode frameworks, such as PyTorch [32] and JAX [8], and\ngraph mode frameworks, such as TensorFlow [1], Caffe [25],\nTheano [5], and CNTK [37]. Eager mode frameworks use\nan imperative define-by-run [47] approach where a machine\nlearning model is represented as code that is executed each\ntime one wants to run the model. Graph mode frameworks\ntake a more declarative define-and-run [47] approach, where\nthey expose a graph building API that requires users to first\nconstruct a graph and then later execute that graph.\nUsers of machine learning frameworks, and especially re\u0002searchers, have shown an overwhelming preference for the\neager programming model [22]. The eager mode model is\neasier to understand and can be debugged using standard\ntools such as print and pdb in Python [23]. This user pref\u0002erence towards eager mode has caused traditionally graph\nmode frameworks to switch to eager mode programming\nmodels [4].\nThe downside of eager mode frameworks is that they make\nit harder to apply graph-level optimizations through compil\u0002ers. The framework only has visibility of a single operator at a\ntime, and thus cannot automatically perform optimizations,\nlike fusion or scheduling, that cross operator boundaries.\nTo address this, there have been attempts to allow graph\ncapture in PyTorch through record/replay [17, 34], Python\nparsing [17], and lazy evaluation [39]. Unfortunately, these\napproaches have sacrificed much of the usability that draws\nusers to PyTorch. Record/replay is unsound and can produce\nincorrect behavior [17]. Python parsing works for simple\nprograms, but has not been able to replicate the complex\nsemantics of all of Python, so results will show it fails on\nover half of real-world models. Lazy evaluation incurs high\nrun-time overheads and adds latency to kernel launches. Ad\u0002ditionally, an exclusively graph mode backend for PyTorch is\nintractable for some models. Due to the flexibility provided\nby PyTorch, many model authors take advantage of features\nthat do not easily map to graphs, such as: dictionaries, lists,\ncustom classes, third party libraries (numpy, logging, etc),\ndisk/network, multiprocessing, exceptions, and handwritten\nkernels.\nThis paper presents two open source extensions to Py\u0002Torch: TorchDynamo and TorchInductor. These extensions\nare behind the torch.compile feature introduced in PyTorch\n2 and officially released in March 2023. TorchDynamo is a\nPython-level JIT compiler designed to allow graph compila\u0002tion in PyTorch programs while retaining the full flexibility\nof Python. TorchDynamo hooks into the Python frame eval\u0002uation API [9] in CPython to dynamically modify Python\nbytecode right before it is executed. It rewrites Python byte\u0002code in order to extract sequences of PyTorch operations into\nan FX graph [34] which is then just-in-time compiled with\nmany extensible backends. It creates this FX graph through\nbytecode analysis and is designed to generate smaller graph\nfragments that can be mixed with Python execution to get\nthe best of both worlds: usability and performance.\nTorchInductor is a new compiler backend for TorchDy\u0002namo. It translates PyTorch programs into OpenAI’s Tri\u0002ton [46] for GPUs and C++/OpenMP [15] for CPUs. TorchIn\u0002ductor is able to support the flexibility and dynamism of Py\u0002Torch by using similar abstractions to PyTorch eager mode.\nIt introduces a new define-by-run loop-level intermediate\nrepresentation (IR) to make it easy to add new operator low\u0002erings. Additionally, it is implemented in Python, so it is easy\nfor PyTorch users to extend and modify to meet their needs.\nExperimental results show that TorchDynamo is able to\ncapture graphs more robustly than prior approaches while\nadding minimal overhead. TorchDynamo is able to capture a\nsingle whole-program graph for most models and can grace\u0002fully fall back to partial graphs when needed. Measurements\nshow TorchInductor produces faster code on average than six\nother PyTorch compiler backends. Performance comparisons\ninclude both training and inference, CPU and GPU, float32\nand float16, and three large benchmark suites containing\n180+ full-sized models taken from real-world applications.\n2 Prior Attempts at PyTorch Graph Capture\nGraph capture in PyTorch presents unique challenges when\ncompared to graph mode frameworks [1, 25, 5, 37], where\nthe user is restricted to only using constructs that are repre\u0002sentable in the graph. With PyTorch and other eager mode\nframeworks, the user is free to embed arbitrary code, includ\u0002ing non-PyTorch libraries, inside their models. This results\nin frequent conversion from PyTorch Tensors to Python\ntypes (via .item(), .tolist(), etc), usage of external libraries\n(numpy, logging, etc), and usage of Python constructs (classes,\nclosures, exceptions, control flow, etc) that do not map well\nto a fixed graph abstraction. Due to this mismatch between\nthe flexibility provided by Python/PyTorch, and the inflexibil\u0002ity of graph representations, prior attempts at graph capture\n2\nin PyTorch have needed to place restrictions on the user\nexperience. While this tension between flexibility and repre\u0002sentation is solved by TorchDynamo, we examine prior art\nin the space to provide context and background.\n2.1 torch.jit.trace\ntorch.jit.trace uses record/replay with example inputs to\nproduce a TorchScript [17] graph. The recording is done at\nthe PyTorch dispatcher level, which is inside the C++ portion\nof PyTorch and used to dispatch operators to device-specific\nkernels and for autograd. Because the recording is done in\nC++, torch.jit.trace does not capture any control flow in\nPython. Consider this example:\ndef example1(x):\nif len(torch.nonzero(x)) > 1:\nreturn x + 1\nreturn x - 1\nWith example input torch.tensor([0, 0]), torch.jit.trace\nwould capture a graph equivalent to:\ndef example1_incorrect_capture(x):\ntorch.nonzero(x)\nreturn x - 1\nSince the path through the program is specialized on the\nexample input, a different input (such as torch.tensor([1,1]))\nwill give incorrect results. Additionally, any non-PyTorch\noperators (such as external libraries, prints, logging, side\neffects, etc.) will be omitted from the captured graph.\n2.2 torch.jit.script\ntorch.jit.script also constructs a TorchScript [17] graph,\nbut does so by parsing the Python AST and performing static","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fassets-global.website-files.com%2F5eda9dd611c901bc56b01a91%2F65cbd869aff1b575ff2bc95e_pytorch_2.pdf","id":"https://assets-global.website-files.com/5eda9dd611c901bc56b01a91/65cbd869aff1b575ff2bc95e_pytorch_2.pdf","publishedDate":null,"summary":"This paper introduces TorchDynamo and TorchInductor, extensions to the PyTorch machine learning framework, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch. TorchInductor is the default compiler backend for TorchDynamo, translating PyTorch programs into OpenAI’s Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor provides significant speedups on an NVIDIA A100 GPU across real-world models.\n","title":"PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation","url":"https://assets-global.website-files.com/5eda9dd611c901bc56b01a91/65cbd869aff1b575ff2bc95e_pytorch_2.pdf"},{"content":"[Skip to main content](https://pytorch.org/pytorch.org#ajax-content-wrap)\n\nHit enter to search or ESC to closeSearch\n\n[Close Search](https://pytorch.org/pytorch.org)\n\n[Announcements](https://pytorch.org/blog/category/announcements/)\n\n# PyTorch 2 paper and tutorial @ ASPLOS 2024\n\nBy [PyTorch Foundation](https://pytorch.org/pytorch.org)February 6, 2024May 9th, 2025[No Comments](https://pytorch.org/blog/pytorch-pytorch-2-paper-tutorial/#respond)\n\nThe PyTorch team is excited to share that our paper on PyTorch 2 has been accepted for presentation at the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), scheduled to take place from April 27 to May 1, 2024, in San Diego, CA, USA.\n\nThe paper delves into the implementation of torch.compile and highlights the key technologies driving it, including TorchDynamo (graph capture), TorchInductor (backend compiler), and Dynamic Shape support.\n\nDuring the ASPLOS conference, we’ll be conducting a tutorial on Saturday, April 27, focusing on the inner workings of PyTorch 2 and how systems researchers can leverage and build upon it. Stay tuned for more details as the event approaches – we look forward to your participation!\n\nA preview of the paper is attached below:\n\nTitle: **PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation.** [**Full Paper PDF**](https://docs.pytorch.org/assets/pytorch2-2.pdf)\n\n### Abstract\n\nThis paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI’s Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27x inference and 1.41x training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.\n\n### Authors\n\nJason Ansel (Meta); Edward Yang (Meta); Horace He (Meta); Natalia Gimelshein (OpenAI); Animesh Jain (Meta); Michael Voznesensky (Meta); Bin Bao (Meta); Peter Bell (Quansight); David Berard (Meta); Evgeni Burovski Quansight; Geeta Chauhan (Meta); Anjali Chourdia (Meta); Will Constable (Meta); Alban Desmaison (Meta); Zachary DeVito (Meta); Elias Ellison (Meta); Will Feng (Meta); Jiong Gong (Intel); Michael Gschwind (Meta); Brian Hirsh (Meta); Sherlock Huang (Meta); Kshiteej Kalambarkar (Quansight); Laurent Kirsch (Meta); Michael Lazos (Meta); Mario Lezcano (Quansight); Yanbo Liang (Meta); Jason Liang (Meta); Yinghai Lu (Meta); CK Luk (Meta); Bert Maher (Meta); Yunjie Pan (University of Michigan); Christian Puhrsch (Meta); Matthias Reso (Meta); Mark Saroufim (Meta); Marcos Yukio Siraichi (Quansight); Helen Suk (Meta); Michael Suo (Meta); Phil Tillet (OpenAI); Eikan Wang (Intel); Xiaodong Wang (Meta); William Wen (Meta); Shunting Zhang (Meta); Xu Zhao (Meta); Keren Zhou (OpenAI & George Mason University); Richard Zou (Meta); Ajit Mathews (Meta); Gregory Chanan (Meta); Peng Wu (Meta); Soumith Chintala (Meta)\n\n### ASPLOS’24 – Full Day Tutorial Schedule\n\nFull schedule for the ASPLOS’24 PyTorch 2 Tutoral on Saturday, April 27th is available [here](https://github.com/pytorch/workshops/tree/master/ASPLOS_2024)\n\n### Docs\n\nAccess comprehensive developer documentation for PyTorch\n\n[View Docs ›](https://pytorch.org/docs)\n\n### Tutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\n[View Tutorials ›](https://pytorch.org/tutorials)\n\n### Resources\n\nFind development resources and get your questions answered\n\n[View Resources ›](https://pytorch.org/resources)\n\n## **Stay in touch** for updates, event info, and the latest news\n\nBy submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. [Privacy Policy](https://www.linuxfoundation.org/privacy/).\n\n- [x-twitter](https://twitter.com/pytorch)\n- [facebook](https://www.facebook.com/pytorch)\n- [linkedin](https://www.linkedin.com/company/pytorch)\n- [youtube](https://www.youtube.com/pytorch)\n- [github](https://github.com/pytorch/pytorch)\n- [slack](https://join.slack.com/t/pytorch/shared_invite/zt-2j2la612p-miUinTTaxXczKOJw48poHA)\n- [wechat](https://pytorch.org/wechat/)\n\n© 2025 PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark usage, please see our [Policies](https://www.linuxfoundation.org/legal/policies) page. [Trademark Usage](https://www.linuxfoundation.org/trademark-usage). [Privacy Policy](http://www.linuxfoundation.org/privacy).\n\n[Close Menu](https://pytorch.org/pytorch.org)\n\n[Join us at PyTorch Conference in San Francisco, October 22-23. Register now!](https://events.linuxfoundation.org/pytorch-conference/)\n\n- [Learn](https://pytorch.org/pytorch.org)\n - [Get Started](https://pytorch.org/get-started/locally/)\n - [Tutorials](https://pytorch.org/tutorials/)\n - [Learn the Basics](https://pytorch.org/tutorials/beginner/basics/intro.html)\n - [PyTorch Recipes](https://pytorch.org/tutorials/recipes/recipes_index.html)\n - [Intro to PyTorch – YouTube Series](https://pytorch.org/tutorials/beginner/introyt.html)\n - [Webinars](https://pytorch.org/webinars/)\n- [Community](https://pytorch.org/pytorch.org)\n - [Landscape](https://landscape.pytorch.org/)\n - [Join the Ecosystem](https://pytorch.org/join-ecosystem)\n - [Community Hub](https://pytorch.org/community-hub/)\n - [Forums](https://discuss.pytorch.org/)\n - [Developer Resources](https://pytorch.org/resources/)\n - [Contributor Awards](https://pytorch.org/contributor-awards/)\n - [Community Events](https://pytorch.org/community-events/)\n - [PyTorch Ambassadors](https://pytorch.org/programs/ambassadors/)\n- [Projects](https://pytorch.org/pytorch.org)\n - [PyTorch](https://pytorch.org/projects/pytorch/)\n - [vLLM](https://pytorch.org/projects/vllm/)\n - [DeepSpeed](https://pytorch.org/projects/deepspeed/)\n - [Host Your Project](https://pytorch.org/projects/host-your-project/)\n- [Docs](https://pytorch.org/pytorch.org)\n - [PyTorch](https://pytorch.org/docs/stable/index.html)\n - [Domains](https://pytorch.org/domains/)\n- [Blog & News](https://pytorch.org/pytorch.org)\n - [Blog](https://pytorch.org/blog/)\n - [Announcements](https://pytorch.org/announcements)\n - [Case Studies](https://pytorch.org/case-studies/)\n - [Events](https://pytorch.org/events/)\n - [Newsletter](https://pytorch.org/newsletter/)\n- [About](https://pytorch.org/pytorch.org)\n - [PyTorch Foundation](https://pytorch.org/foundation/)\n - [Members](https://pytorch.org/members)\n - [Governing Board](https://pytorch.org/governing-board/)\n - [Technical Advisory Council](https://pytorch.org/tac/)\n - [Cloud Credit Program](https://pytorch.org/credits/)\n - [Staff](https://pytorch.org/staff/)\n - [Contact](https://pytorch.org/contact/)\n- [JOIN](https://pytorch.org/join/)\n\n- [github](https://github.com/pytorch/pytorch)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpytorch.org%2Fblog%2Fpytorch-pytorch-2-paper-tutorial%2F","id":"https://pytorch.org/blog/pytorch-pytorch-2-paper-tutorial/","publishedDate":"2024-02-06T00:00:00.000Z","summary":"The PyTorch team's paper on PyTorch 2 has been accepted for presentation at the ASPLOS conference (April 27 - May 1, 2024, in San Diego, CA, USA). The paper details the implementation of torch.compile, including TorchDynamo (graph capture), TorchInductor (backend compiler), and Dynamic Shape support. A tutorial on PyTorch 2 will be held on April 27, focusing on its inner workings. The paper, titled \"PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation,\" introduces TorchDynamo and TorchInductor, which implement the torch.compile feature. TorchInductor provides significant speedups on an NVIDIA A100 GPU across numerous models.\n","title":"PyTorch 2 paper and tutorial @ ASPLOS 2024","url":"https://pytorch.org/blog/pytorch-pytorch-2-paper-tutorial/"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdl.acm.org%2Fdoi%2Fpdf%2F10.1145%2F3620665.3640366","id":"https://dl.acm.org/doi/pdf/10.1145/3620665.3640366","publishedDate":"2024-04-27T00:00:00.000Z","summary":"Failed to fetch content","title":"PyTorch 2: Faster Machine Learning Through Dynamic Python ...","url":"https://dl.acm.org/doi/pdf/10.1145/3620665.3640366"},{"content":"[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffa18feda5f3a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40sgurwinderr%2Fpytorch-dynamo-and-aot-autograd-enhancing-performance-and-flexibility-fa18feda5f3a&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40sgurwinderr%2Fpytorch-dynamo-and-aot-autograd-enhancing-performance-and-flexibility-fa18feda5f3a&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# The Simple Path to PyTorch Graphs: Dynamo and AOT Autograd Explained\n\n[Gurwinder Singh](https://medium.com/@sgurwinderr?source=post_page---byline--fa18feda5f3a---------------------------------------)\n\n4 min read\n\n·\n\nApr 12, 2024\n\n--\n\n1\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nGraph acquisition in PyTorch refers to the process of creating and managing the computational graph that represents a neural network’s operations. This graph is central to PyTorch’s dynamic nature, allowing for flexible model architectures and efficient gradient computation. With the advent of tools like TorchScript, PyTorch Dynamo, and AOT Autograd, PyTorch continues to improve the performance and deployability of models without sacrificing its user-friendly interface. Understanding how PyTorch handles computational graphs is key to leveraging the full power of the framework.\n\n## PyTorch Dynamo\n\nTorchDynamo (torch.\\_dynamo) is an internal API that uses a CPython feature called the Frame Evaluation API to safely capture PyTorch graphs. Methods that are available externally for PyTorch users are surfaced through the `torch.compiler` namespace.\n\n## Benefits of PyTorch Dynamo\n\n- **Performance Gains**: By optimizing the execution graph on the fly, Dynamo can significantly speed up the performance of PyTorch models, particularly for models with a lot of small operations that can be fused together.\n- **Ease of Use**: Dynamo does not require any changes to the existing PyTorch code, making it very easy to adopt.\n- **Flexibility**: Since Dynamo operates dynamically, it can handle the dynamic computation graphs that are typical in PyTorch, including graphs with loops and conditionals.\n\n## Example Model (Simple 2 Layer NN)\n\n```\n# Define your PyTorch modelclass MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.fc1 = nn.Linear(10, 5) self.fc2 = nn.Linear(5, 2) self.relu= nn.ReLU() self.softmax=nn.Softmax(dim=1) def forward(self, x): result = self.softmax(self.fc2(self.relu(self.fc1(x)))) return result\n```\n\nThe model defined in the code snippet is a simple feedforward neural network implemented using PyTorch’s `nn.Module`. It consists of two fully connected layers ( `nn.Linear`) with a ReLU activation function ( `nn.ReLU`) applied after the first layer and a softmax activation function ( `nn.Softmax`) applied after the second layer.\n\n## **Create Forward Graph with Torch Dynamo**\n\nCode Snippet for Dynamo Backend for Saving Graph to SVG:\n\n```\ndef dynamo_backend(gm, sample_inputs): code = gm.print_readable() gm.graph.print_tabular() with open(\"model_graph.svg\", \"wb\") as file: file.write(FxGraphDrawer(gm,'f').get_dot_graph().create_svg()) return gm.forward\n```\n\nCompile the model and perform forward pass:\n\n```\nmodel = MyModel()input_tensor = torch.randn(1, 10)torch._dynamo.reset()# Compile the model with Dynamocompiled_f = torch.compile(model, backend=dynamo_backend)# Forward Passout = compiled_f(input_tensor)\n```\n\nThis is the generated graph for forward pass:\n\nPress enter or click to view image in full size\n\nForward graph generated with dynamo\\_backend()\n\n## AOTAutograd\n\n**AOTAutograd** captures not only the user-level code, but also backpropagation, which results in capturing the backwards pass “ahead-of-time”. This enables acceleration of both forwards and backwards pass using TorchInductor.\n\n## Benefits of AOTAutograd\n\nAOT Autograd works by analyzing the forward pass of your model and generating an optimized backward pass ahead of time. This is in contrast to the traditional approach where the backward pass is dynamically generated during runtime. By doing this ahead of time, AOT Autograd can apply optimizations that are not possible at runtime.\n\n- **Training Speed**: AOT Autograd can reduce the overhead of the backward pass, leading to faster training times.\n- **Customization**: It allows for customization of the backward pass, enabling more control over the training process.\n- **Integration**: AOT Autograd is designed to work seamlessly with existing PyTorch models, making it easy to integrate into current workflows.\n\n## Create Forward & Backward Graph with AOT Autograd\n\nCode Snippet for AOT Backend for Saving Graph to SVG:\n\n```\ndef aot_backend(gm, sample_inputs): # Forward compiler capture def fw(gm, sample_inputs): gm.print_readable() g = FxGraphDrawer(gm, 'fn') with open(\"forward_aot.svg\", \"wb\") as file: file.write(g.get_dot_graph().create_svg()) return make_boxed_func(gm.forward) # Backward compiler capture def bw(gm, sample_inputs): gm.print_readable() g = FxGraphDrawer(gm, 'fn') with open(\"backward_aot.svg\", \"wb\") as file: file.write(g.get_dot_graph().create_svg()) return make_boxed_func(gm.forward) # Call AOTAutograd gm_forward = aot_module_simplified(gm,sample_inputs, fw_compiler=fw, bw_compiler=bw) return gm_forward\n```\n\nCompile the model and perform forward and backward pass:\n\n```\nmodel = MyModel()input_tensor = torch.randn(1, 10)ouput = torch.randn(2)torch._dynamo.reset()# Compile the model with AOT Backendcompiled_f = torch.compile(model, backend=aot_backend)# Calculate Loss from Forward Passloss= torch.nn.functional.mse_loss(compiled_f(input_tensor), ouput)# Perform Backward Passout = loss.backward()\n```\n\nThese are the generated graphs for forward and backward pass:\n\nPress enter or click to view image in full size\n\nPress enter or click to view image in full size\n\nForward and Backward graph generated with aot\\_backend()\n\n## Conclusion\n\nPyTorch Dynamo and AOT Autograd are exciting developments for the PyTorch community. They offer the potential for significant performance improvements while maintaining the flexibility and ease of use that PyTorch users have come to appreciate. As these features continue to mature, they will likely become integral parts of the PyTorch ecosystem, helping to bridge the gap between rapid prototyping and efficient production deployment.\n\nBoth PyTorch Dynamo and AOT Autograd are tools aimed at improving the performance of PyTorch models by optimizing the execution of operations. While Dynamo focuses on runtime optimizations, AOT Autograd looks at optimizing the backward pass ahead of time. These tools can be particularly useful when you’re looking to scale up your models and need to squeeze out extra performance without significant refactoring of your existing codebase.\n\n[Pytorch](https://medium.com/tag/pytorch?source=post_page-----fa18feda5f3a---------------------------------------)\n\n[Pytorch Tutorial](https://medium.com/tag/pytorch-tutorial?source=post_page-----fa18feda5f3a---------------------------------------)\n\n[Deep Learning](https://medium.com/tag/deep-learning?source=post_page-----fa18feda5f3a---------------------------------------)\n\n[Optimization](https://medium.com/tag/optimization?source=post_page-----fa18feda5f3a---------------------------------------)\n\n[Graph](https://medium.com/tag/graph?source=post_page-----fa18feda5f3a---------------------------------------)\n\n[**Written by Gurwinder Singh**](https://medium.com/@sgurwinderr?source=post_page---post_author_info--fa18feda5f3a---------------------------------------)\n\n[6 followers](https://medium.com/@sgurwinderr/followers?source=post_page---post_author_info--fa18feda5f3a---------------------------------------)\n\n· [6 following](https://medium.com/@sgurwinderr/following?source=post_page---post_author_info--fa18feda5f3a---------------------------------------)\n\nGPU Software Development Engineer @ Intel, Loves to Work on Games & AR/VR\n\n## Responses (1)\n\nSee all responses\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----fa18feda5f3a---------------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----fa18feda5f3a---------------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----fa18feda5f3a---------------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----fa18feda5f3a---------------------------------------)\n\n[Press](mailto:pressinquiries@medium.com)\n\n[Blog](https://blog.medium.com/?source=post_page-----fa18feda5f3a---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fa18feda5f3a---------------------------------------)\n\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----fa18feda5f3a---------------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fa18feda5f3a---------------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----fa18feda5f3a---------------------------------------)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fmedium.com%2F%40sgurwinderr%2Fpytorch-dynamo-and-aot-autograd-enhancing-performance-and-flexibility-fa18feda5f3a","id":"https://medium.com/@sgurwinderr/pytorch-dynamo-and-aot-autograd-enhancing-performance-and-flexibility-fa18feda5f3a","publishedDate":"2024-04-11T00:00:00.000Z","summary":"This article explains how PyTorch handles computational graphs, focusing on tools like TorchScript, PyTorch Dynamo, and AOT Autograd to improve model performance and deployability. PyTorch Dynamo, an internal API, uses the Frame Evaluation API to capture PyTorch graphs and offers performance gains by optimizing execution graphs, ease of use without code changes, and flexibility in handling dynamic computations. The article includes a code example of a simple 2-layer neural network and a code snippet for a Dynamo backend that saves the graph to an SVG file.\n","title":"The Simple Path to PyTorch Graphs: Dynamo and AOT Autograd ...","url":"https://medium.com/@sgurwinderr/pytorch-dynamo-and-aot-autograd-enhancing-performance-and-flexibility-fa18feda5f3a"},{"content":"- [Skip to primary navigation](https://pyimagesearch.com/pyimagesearch.com#genesis-nav-primary)\n- [Skip to main content](https://pyimagesearch.com/pyimagesearch.com#genesis-content)\n- [Skip to primary sidebar](https://pyimagesearch.com/pyimagesearch.com#genesis-sidebar-primary)\n- [Skip to footer](https://pyimagesearch.com/pyimagesearch.com#genesis-footer-widgets)\n\n[Accelerating Neural Networks](https://pyimagesearch.com/category/accelerating-neural-networks/) [Deep Learning](https://pyimagesearch.com/category/deep-learning/) [PyTorch 2.0](https://pyimagesearch.com/category/pytorch-2-0/) [TorchDynamo](https://pyimagesearch.com/category/torchdynamo/) [TorchInductor](https://pyimagesearch.com/category/torchinductor/)\n\nby [Puneet Mangla](https://pyimagesearch.com/author/puneetm/) on April 24, 2023\n\n[Click here to download the source code to this post](https://pyimagesearch.com/pyimagesearch.com#download-the-code)\n\n[Home](https://pyimagesearch.com/) » [Blog](https://pyimagesearch.com/blog/) » What’s Behind PyTorch 2.0? TorchDynamo and TorchInductor (primarily for developers)\n\n## **Table of Contents**\n\n- [What’s Behind PyTorch 2.0? TorchDynamo and TorchInductor (primarily for developers)](https://pyimagesearch.com/pyimagesearch.com#h2BPTitle)\n - [Technology Behind PyTorch 2.0](https://pyimagesearch.com/pyimagesearch.com#h3Technology)\n - [TorchDynamo](https://pyimagesearch.com/pyimagesearch.com#h3TorchDynamo)\n - [TorchInductor](https://pyimagesearch.com/pyimagesearch.com#h3TorchInductor)\n - [AOT Autograd](https://pyimagesearch.com/pyimagesearch.com#h3Autograd)\n - [PrimTorch](https://pyimagesearch.com/pyimagesearch.com#h3PrimTorch)\n - [Configuring Your Development Environment](https://pyimagesearch.com/pyimagesearch.com#h3Configuring)\n - [Installation](https://pyimagesearch.com/pyimagesearch.com#h3Installation)\n - [Verification](https://pyimagesearch.com/pyimagesearch.com#h3Verification)\n - [Need Help Configuring Your Development Environment?](https://pyimagesearch.com/pyimagesearch.com#h3Help)\n - [Project Structure](https://pyimagesearch.com/pyimagesearch.com#h3Structure)\n - [Accelerating DNNs Using TorchDynamo](https://pyimagesearch.com/pyimagesearch.com#h3Accelerating)\n - [TorchDynamo vs. TorchScript vs. FX Tracing](https://pyimagesearch.com/pyimagesearch.com#h3vs)\n - [Data-Dependent Control Flow](https://pyimagesearch.com/pyimagesearch.com#h3Flow)\n - [Non-PyTorch Libraries](https://pyimagesearch.com/pyimagesearch.com#h3Libraries)\n - [TorchDynamo FX Graphs](https://pyimagesearch.com/pyimagesearch.com#h3FX)\n- [Summary](https://pyimagesearch.com/pyimagesearch.com#h2Summary)\n - [Citation Information](https://pyimagesearch.com/pyimagesearch.com#h3Citation)\n\n## [**What’s Behind PyTorch 2.0? TorchDynamo and TorchInductor (primarily for developers)**](https://pyimagesearch.com/pyimagesearch.com\\#TOC)\n\nIn our [previous lesson](https://pyimg.co/fh15d) of this series, we saw how PyTorch 2.0’s new feature, `torch.compile`, can provide significant speedups of 30-200% in eager mode for most models we run daily. PyTorch 2.0 introduces the following new technologies:\n\n- TorchDynamo\n- TorchInductor\n- AOT Autograd\n- PrimTorch\n\nThese technologies make the PyTorch 2.0 code run faster (with less memory) by JIT-compiling the PyTorch 2.0 code into optimized kernels, all while requiring minimal code changes.\n\nThis lesson covers these technologies in more detail and digs deeper into the usage and behavior of both TorchDynamo and TorchInductor. We will see how we can accelerate our models just like `torch.compile` but using TorchDynamo/TorchInductor and demonstrate some of the advantages of both TorchDynamo and TorchInductor over previous compiler solutions like TorchScript and FX (Function eXtraction) Tracing. This tutorial is primarily for developers who want to accelerate their deep learning models with PyTorch 2.0.\n\nIn this series, you will learn about **Accelerating Deep Learning Models with PyTorch 2.0**.\n\nThis lesson is the 2nd of a 2-part series on **Accelerating Deep Learning Models with PyTorch 2.0**:\n\n1. [_What’s New in PyTorch 2.0? `torch.compile`_](https://pyimg.co/fh15d)\n2. [**_What’s Behind PyTorch 2.0? TorchDynamo and TorchInductor (primarily for developers)_**](https://pyimg.co/u6xmk) **(this tutorial)**\n\n**To learn what’s behind PyTorch 2.0,** **_just keep reading._**\n\n#### Looking for the source code to this post?\n\n[Jump Right To The Downloads Section](https://pyimagesearch.com/pyimagesearch.com#download-the-code)\n\n## [**What’s Behind PyTorch 2.0? TorchDynamo and TorchInductor (primarily for developers)**](https://pyimagesearch.com/pyimagesearch.com\\#TOC)\n\nWe will start with understanding the new technologies behind PyTorch 2.0.\n\n### [**Technology Behind PyTorch 2.0**](https://pyimagesearch.com/pyimagesearch.com\\#TOC)\n\n### [**TorchDynamo**](https://pyimagesearch.com/pyimagesearch.com\\#TOC)\n\nTorchDynamo (shown in **Figure 1**) is PyTorch’s latest compiler solution that leverages [JIT (Just In Time)](https://pyimagesearch.com/2023/02/27/learning-jax-in-2023-part-2-jaxs-power-tools-grad-jit-vmap-and-pmap/) compilation to transform a general Python program into an FX Graph. The FX Graph is an intermediate representation of your code that can be further compiled and optimized. TorchDynamo extracts FX Graphs by inspecting [Python bytecode](https://opensource.com/article/18/4/introduction-python-bytecode) at runtime and detecting calls to PyTorch operations.\n\n**Figure 1:** The Default Python vs. TorchDynamo behavior (source: [PyTorch 2.0](https://pytorch.org/docs/2.0/dynamo/index.html)).\n\nAdditionally, TorchDynamo is designed to mix Python execution with compiled backends to get the best of both worlds: usability and performance. Finally, it can support the arbitrary program by breaking the FX Graph whenever it encounters any unsupported feature (e.g., data-dependent flows or non-PyTorch libraries).\n\nThe approach is based on a CPython feature known as [Frame Evaluation API](https://peps.python.org/pep-0523/) that can safely and correctly [capture graphs 99%](https://dev-discuss.pytorch.org/t/torchdynamo-update-8-torchdynamo-passed-correctness-check-on-7k-github-models/663) of the time without incurring additional overhead. Other previous compiler solutions like TorchScript and FX Tracing need help to capture graphs even 50% of the time, and that too with additional overhead.\n\nHere’s how TorchDynamo’s behavior differs from the default Python behavior.\n\n### [**TorchInductor**](https://pyimagesearch.com/pyimagesearch.com\\#TOC)\n\nTorchDynamo should be paired with a backend that can efficiently convert captured FX Graphs into fast machine code to make PyTorch 2.0 programs faster. However, a lot is lost while exporting the FX Graphs in different existing backends. Some have fundamentally different execution models than PyTorch, and others are only optimized for inference and not training.\n\nTorchInductor is the new compiler backend that compiles the FX Graphs generated by TorchDynamo into optimized C++/Triton kernels. TorchInductor uses a Pythonic define-by-run loop level intermediate representation to automatically map PyTorch models into generated [Triton](https://github.com/openai/triton) code on GPUs and C++/OpenMP on CPUs. TorchInductor’s core loop level intermediate representation contains only `~50` operators, and it is implemented in Python, making it easily hackable and extensible.\n\n### [**AOT Autograd**](https://pyimagesearch.com/pyimagesearch.com\\#TOC)\n\n[AOT Autograd](https://pytorch.org/functorch/stable/notebooks/aot_autograd_optimizations.html) is PyTorch’s new [automatic differentiation](https://pyimagesearch.com/2022/12/05/automatic-differentiation-part-1-understanding-the-math/) engine that leverages PyTorch’s `torch_dispatch` mechanism to trace through the autograd engine and create ahead-of-time backward passes. This accelerates both forward and backward passes.\n\n### [**PrimTorch**](https://pyimagesearch.com/pyimagesearch.com\\#TOC)\n\nWith more than 2000+ operators, writing a backend for PyTorch is challenging and draining. Hence, with PrimTorch ( **Figure 2**), PyTorch 2.0 canonicalizes these operations into smaller primitive operations. As a result, there are mainly two operation sets in PrimTorch:\n\n- **Prim ops:** contain `~250` low-level operations are well suited for compilers and can be fused for good performance.\n- **ATen ops:** contain `~750` canonical operators suited for backends that won’t have a compilation to recover performance from a lower-level operator set like Prim ops.\n\n**Figure 2:** Overview of PrimTorch (source: [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/#developervendor-experience)).\n\n### [**Configuring Your Development Environment**](https://pyimagesearch.com/pyimagesearch.com\\#TOC)\n\n### [**Installation**](https://pyimagesearch.com/pyimagesearch.com\\#TOC)\n\nAs in previous versions, PyTorch 2.0 is available as a Python `pip` package. However, to successfully install PyTorch 2.0, your system should have installed the latest CUDA (Compute Unified Device Architecture) versions (11.6 and 11.7). Here’s how you can install PyTorch 2.0 nightly version via pip:\n\nFor CUDA version 11.7:\n\n```\n$ pip3 install numpy --pre torch --force-reinstall --extra-index-url https://download.pytorch.org/whl/nightly/cu117\n```\n\nFor CUDA version 11.6:\n\n```\n$ pip3 install numpy --pre torch --force-reinstall --extra-index-url https://download.pytorch.org/whl/nightly/cu116\n```\n\nHowever, if you don’t have CUDA 11.6 or 11.7 installed on your system, you can download all the required dependencies in the PyTorch nightly binaries with docker.\n\n```\n$ sudo apt install -y nvidia-docker2\n$ sudo systemctl restart docker\n$ docker pull ghcr.io/pytorch/pytorch-nightly\n$ docker run --gpus all -it ghcr.io/pytorch/pytorch-nightly:latest /bin/bash\n```\n\nBe sure to specify `--gpus all` so your container can access all your GPUs.\n\n### [**Verification**](https://pyimagesearch.com/pyimagesearch.com\\#TOC)\n\nOptionally, you can verify your","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpyimagesearch.com%2F2023%2F04%2F24%2Fwhats-behind-pytorch-2-0-torchdynamo-and-torchinductor-primarily-for-developers%2F","id":"https://pyimagesearch.com/2023/04/24/whats-behind-pytorch-2-0-torchdynamo-and-torchinductor-primarily-for-developers/","publishedDate":"2023-04-24T00:00:00.000Z","summary":"This article delves into the technologies behind PyTorch 2.0, focusing on TorchDynamo and TorchInductor. These technologies, along with AOT Autograd and PrimTorch, are key to PyTorch 2.0's ability to speed up code execution and reduce memory usage through JIT-compilation. The tutorial is aimed at developers looking to accelerate their deep learning models using PyTorch 2.0, and it explores the usage and behavior of TorchDynamo and TorchInductor, comparing them to previous compiler solutions like TorchScript and FX Tracing.\n","title":"What's Behind PyTorch 2.0? TorchDynamo and TorchInductor ...","url":"https://pyimagesearch.com/2023/04/24/whats-behind-pytorch-2-0-torchdynamo-and-torchinductor-primarily-for-developers/"}],"status":"completed","toolCallId":"call_9n9i0i427oGNlV0XxF3ZoMsW","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"I’m going to teach you this paper as if I wrote it, using the Feynman technique: simple language, concrete examples, how the parts fit together, and why each design choice exists. By the end, you should be able to explain PyTorch 2’s compiler stack to someone else without hand-waving.\n\nWhat problem are we solving?\n- Users love eager PyTorch because it’s just Python: print, pdb, loops, ifs, dicts, NumPy, custom ops, etc.\n- Compilers love graphs because they can do cross-op optimization: fuse kernels, tile/schedule loops, vectorize, reduce memory traffic.\n- Tension: eager is flexible but hides the global structure; graph is optimizable but rigid. Prior PyTorch attempts either broke usability or correctness:\n  - torch.jit.trace recorded at the C++ dispatcher: misses Python control flow; can be wrong if inputs/paths change.\n  - torch.jit.script parsed Python to a restricted subset: fails on many real models.\n  - Lazy eval approaches: added overhead and latency, and still struggled with Python’s dynamism.\n\nGoal: Keep Python’s flexibility, but get graph-level performance. Make the “fast path” the default for normal code, and transparently fall back when things get too dynamic.\n\nThe core idea in one sentence\nIntercept Python execution at the frame level, rewrite bytecode on the fly to extract FX subgraphs of PyTorch ops, then compile those subgraphs to fast kernels with a backend that understands loops and fusion, while running the rest of the Python code as-is.\n\nThe two main components behind torch.compile\n1) TorchDynamo: Python-level graph capture\n- What it does:\n  - Hooks CPython’s Frame Evaluation API right before a frame runs.\n  - Analyzes and edits Python bytecode to identify stretches where we call PyTorch operators (ATen).\n  - Builds an FX graph for those stretches, leaving the rest of the code in Python.\n  - Replaces the original bytecode chunk with a call that executes the compiled graph, and then resumes Python.\n- Why this works:\n  - We get “define-by-run” behavior: we only capture what actually executes for the given inputs, including data-dependent control flow.\n  - We don’t need to understand all of Python statically. We capture local regions we can prove safe, and we bail out otherwise.\n- Guards and specialization:\n  - Captured graphs are specialized to assumptions (tensor dtypes/devices/shapes, control-flow choices, etc.) recorded as guards.\n  - If a guard fails on a later input (e.g., shape changes), we re-enter Dynamo, recapture with the new facts, and cache another compiled variant.\n  - This makes dynamic models feasible without penalizing stable ones.\n- Partial graphs are OK:\n  - If your code calls a non-PyTorch function, mutates Python lists, throws exceptions, does logging, etc., Dynamo will cut the subgraph before that point, execute Python, and then capture again after. You still get speedups where it counts.\n\n2) TorchInductor: default backend compiler\n- What it does:\n  - Takes the FX graph and lowers it into a compact, loop-level IR that looks like PyTorch eager in spirit (define-by-run), but is compiler-friendly (~50 core ops).\n  - Generates:\n    - GPU: Triton kernels (via OpenAI Triton), allowing highly tuned fused kernels.\n    - CPU: C++ with OpenMP for parallel loops.\n- Why loops matter:\n  - Many optimizations are loop transformations: fuse producer/consumer ops, tile for caches/SMs, vectorize, hoist invariants, reduce memory bandwidth.\n  - A loop-level IR makes expressing these easy and uniform.\n- Dynamic shapes:\n  - Inductor emits shape-polymorphic code guided by Dynamo’s guards. Where needed, it generates runtime shape checks and emits multiple kernels or uses indexing that tolerates variability.\n- Training support:\n  - We integrate with AOTAutograd: capture forward and a compiled backward ahead of time so that both directions benefit from fusion and scheduling.\n\nHow torch.compile stitches it together\n- When you call torch.compile(model):\n  - TorchDynamo intercepts Python frames and pulls out FX subgraphs of ATen ops.\n  - It hands those to a backend (by default TorchInductor; others can be plugged in).\n  - The backend compiles to fast kernels and returns a callable.\n  - The edited Python bytecode now calls that compiled segment directly, then continues normal Python execution.\n- Recompilation and caching:\n  - On new shapes/dtypes/branches, guards trigger re-capture and a new compiled artifact is cached.\n  - Common cases keep hitting the fast path without further overhead.\n\nWhy prior approaches failed and what’s novel here\n- Record/replay (trace) is unsound for Python control flow. We fix this by capturing at Python level, respecting the actual executed path per input and using guards for assumptions.\n- Static parsing can’t represent “all of Python.” We don’t try; we mix compiled pieces with vanilla Python, so we never force you into a restricted sublanguage.\n- Lazy evaluation added latency; Dynamo modifies bytecode and runs immediately, adding minimal overhead.\n- An “all-graph” backend is unrealistic for arbitrary Python. Our design’s unit is a subgraph, so we compile hot numeric parts and let Python do the rest.\n\nWhat exactly does the graph contain?\n- Dynamo builds FX graphs of ATen operations and tensor metadata logic it can safely model. It explicitly omits:\n  - Pure Python logic that can’t be proven tensor-only,\n  - External libraries not modeled (e.g., arbitrary NumPy calls, file I/O),\n  - Side-effectful constructs that would break replay.\n- Where we cut the graph:\n  - Any boundary where semantics can’t be preserved as a pure tensor function becomes a graph break. This gives correctness first, speed second.\n\nPerformance highlights (as measured in the paper)\n- On NVIDIA A100:\n  - 2.27× geometric mean speedup for inference.\n  - 1.41× geometric mean speedup for training.\n  - Benchmarked across 180+ real-world models; outperforms six other compiler stacks the paper compares against.\n- Overhead:\n  - Dynamo’s capture overhead is small relative to model runtime, and it amortizes via caching.\n- Why these wins happen:\n  - Fusing memory-bound ops reduces reads/writes and kernel launches.\n  - Loop tiling and vectorization map better to CPU/GPU memory hierarchies.\n  - For training, AOTAutograd allows cross-op fusion across backward steps that eager autograd couldn’t.\n\nKey subsystems we leaned on\n- FX: PyTorch’s graph IR used to represent the captured subgraph.\n- PrimTorch: canonicalization of thousands of ops into a smaller set of “ATen canonical” and “prim” ops, making backends tractable.\n- AOTAutograd: ahead-of-time capture of backward, enabling backend optimizations on gradients.\n- Triton: a DSL for writing high-performance GPU kernels that Inductor auto-generates, so users don’t have to handcraft kernels.\n\nConcrete example to make it tangible\n- Suppose your forward has: y = gelu(layernorm(x @ W + b))\n  - Eager: launches matmul (cuBLAS), then bias add, then layernorm (multiple tiny kernels), then GELU (tiny kernel). Lots of memory traffic.\n  - With torch.compile:\n    - Dynamo captures the whole region as a graph (plus guards for shapes/dtypes).\n    - Inductor lowers to a fused Triton kernel that computes the epilogue (bias + layernorm + GELU) in registers/shared memory, possibly keeping the matmul as a library call or fusing around it where profitable.\n    - Result: fewer launches, less DRAM traffic, better occupancy.\n\nWhat about dynamic control flow or Python lists?\n- If you have an if dependent on a tensor value, Dynamo specializes on the taken branch and guards it. If a future batch flips the condition, we compile a second variant.\n- If you manipulate Python lists or dicts in the middle, we make a graph break, run that logic in Python, then resume capture later. You still get fusion before and after.\n\nCorrectness model\n- We only compile what we can model purely from the inputs/guards.\n- Any ambiguous semantics trigger a graph break.\n- If your code has side effects, they happen in the Python parts in program order.\n- For deterministic numerical equivalence, we rely on ATen semantics and well-tested lowerings; you can turn on debug asserts and accuracy checks during development.\n\nWhen won’t you see big wins?\n- Single, very large ops already using vendor libraries at high efficiency (e.g., a giant matmul alone) may be near peak; fusion helps when there are adjacent memory-bound ops.\n- Extremely dynamic models that change shapes/branches constantly may trigger frequent recompiles; you can mitigate via batching, shape bucketing, or enabling dynamic-shape-friendly modes.\n- Heavy non-tensor Python work dominates runtime; we can’t speed up pure Python.\n\nHow to use it\n- In most cases:\n  - training/inference: compiled = torch.compile(model) and then run as usual.\n  - For control: tune mode/opts via torch.compile(..., backend=\"inductor\", dynamic=True/False, fullgraph=False, options=...).\n  - Fall back to eager by removing compile; you didn’t change your model code.\n\nDesign trade-offs we consciously made\n- Bytecode-level capture vs AST: more faithful to actual execution (including closures, exceptions, frames), less brittle across Python features.\n- Subgraph compilation vs whole-program IR: preserves Python’s generality; still hits hot numeric regions.\n- Loop-level IR vs giant op zoo: makes backends simpler, improvements composable, and dynamic shapes manageable.\n\nResults in context\n- The main novelty is not “a new compiler,” but a system boundary: bring the compiler to Python’s execution boundary (Frame Eval), then feed a loop-level backend that can generate high-quality kernels automatically. This pairing is what lets torch.compile be both low-friction and broadly effective.\n\nIf you remember only five things\n- torch.compile is on by wrapping your model; no model rewrite needed.\n- TorchDynamo captures chunks of your Python program at runtime by rewriting bytecode and building FX graphs guarded by runtime assumptions.\n- TorchInductor turns those graphs into fast loop code: Triton on GPUs, C++/OpenMP on CPUs, with fusion and tiling.\n- It handles dynamic shapes and control flow by specializing, guarding, caching multiple variants, and making safe graph breaks.\n- Across 180+ models, we saw strong speedups for both inference and training, outperforming other backends we evaluated.\n\nWould you like me to:\n- Walk through a specific snippet from your codebase and predict what gets fused?\n- Explain how guards are formed and when they trigger recompilation?\n- Compare Inductor’s codegen to XLA/TVM in more detail?","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d193b1daec81a08a30949b2bf9ba4503bf0abe7399583a"}},"resumableStreamId":"STREAM:39f04ba8-0cf9-4966-b5f3-82676997306a","role":"assistant","status":"done","threadId":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","timeToFirstToken":0.642,"tokens":2289,"tokensPerSecond":32.057476576614434,"updated_at":1758565290039,"userId":"google:104458561188320708185","id":"j9752x7jx13q74mw16b75zp4sx7r3pb7"},{"_creationTime":1758565702539.2993,"_id":"j97dka56qj61dpfsk3x51vhvtn7r3syx","attachmentIds":[],"content":"how does pytorch 2 improve from pytorch 1","created_at":1758565702539,"messageId":"c2e02b2c-2a75-4d47-8749-7466b46717b9","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","updated_at":1758565702539,"userId":"google:104458561188320708185","id":"j97dka56qj61dpfsk3x51vhvtn7r3syx","providerMetadata":{}},{"_creationTime":1758565702539.2996,"_id":"j972s1gjy63x7e32aba52xh6r17r3b6k","attachmentIds":[],"byok":false,"content":"Here’s the practical, high‑signal answer.\n\nTL;DR\n- PyTorch 2 adds torch.compile, which transparently compiles eager PyTorch code into fused kernels for speed, without changing your model code.\n- It keeps Python’s flexibility but extracts optimizable graphs at runtime, then lowers them to efficient CPU/GPU code.\n- Typical speedups: roughly 1.5–2.5× on real models (varies by workload), plus fewer kernel launches and lower memory traffic.\n\nWhat’s new in PyTorch 2 vs PyTorch 1\n\n1) One-line compilation: torch.compile\n- PyTorch 1: Eager by default. JIT Script/Trace existed but required a restricted sublanguage or were fragile with Python control flow.\n- PyTorch 2: torch.compile(model) becomes the default fast path. No model rewrites. It compiles hot regions and falls back to Python elsewhere, preserving correctness and debuggability.\n\n2) TorchDynamo: robust graph capture at Python bytecode level\n- PyTorch 1: Graph capture was either tracing at the C++ dispatcher (missing Python control flow) or scripting a subset of Python.\n- PyTorch 2: TorchDynamo hooks CPython’s frame evaluation, rewrites bytecode on the fly, and builds FX graphs for the tensor parts you actually execute. It:\n  - Specializes with guards on shapes/dtypes/devices/branches.\n  - Recaptures and caches variants when guards fail (dynamic-shape friendly).\n  - Makes safe graph breaks around “too-Pythonic” code but still compiles before/after.\n\n3) TorchInductor: new default backend compiler\n- PyTorch 1: Limited fusion and kernel codegen; relied mostly on vendor libraries and manual kernel paths.\n- PyTorch 2: TorchInductor lowers FX graphs to a loop-level IR, then generates:\n  - GPU: Triton kernels with aggressive fusion, tiling, vectorization.\n  - CPU: C++/OpenMP with similar loop optimizations.\n- Result: fewer kernel launches, less DRAM traffic, better cache/SM utilization.\n\n4) AOTAutograd + PrimTorch integration\n- PyTorch 1: Autograd ran op-by-op; fusion opportunities across backward were limited.\n- PyTorch 2: AOTAutograd captures/compiles forward and backward together, enabling cross-op fusion in gradients. PrimTorch/ATen canonicalization reduces thousands of ops to a manageable core set that backends can optimize well.\n\n5) Dynamic shapes and control flow that actually work\n- PyTorch 1: Static-ish assumptions often required; dynamic control flow made JIT tricky or brittle.\n- PyTorch 2: Specialization with guards makes dynamic control flow and varying shapes practical. You get multiple cached compiled variants when inputs vary; otherwise you stay on a fast, compiled path.\n\n6) Performance, out of the box\n- PyTorch 1: Good performance for big library ops (e.g., cuBLAS matmul) but overhead between ops, especially for memory-bound pointwise/reductions.\n- PyTorch 2: Inductor fuses epilogues and surrounding pointwise/reduction ops into larger kernels, often delivering 1.5–2.5× end-to-end gains on real models (exact numbers depend on model, hardware, and training vs inference).\n\n7) Better developer experience\n- Minimal code changes: usually just torch.compile.\n- Debuggability: You can disable/enable compile, inspect generated graphs/kernels, and let graph breaks preserve Python semantics.\n- Backend pluggability: Inductor by default, but other compilers can be integrated.\n\nWhen you’ll notice the difference most\n- Models with lots of small/medium tensor ops where fusion reduces kernel launches and memory bandwidth.\n- Training workloads where fused backward (AOTAutograd + Inductor) removes overhead.\n- Dynamic models (varying sequence lengths, branches) that previously defeated static tracing.\n\nWhen improvements are smaller\n- Workloads dominated by a few very large vendor-library ops (e.g., huge matmuls) may already be near peak; benefits come mainly from fusing surrounding epilogues.\n- Code dominated by non-tensor Python won’t speed up—PyTorch 2 compiles only tensor graphs.\n\nHow to adopt\n- Start with: compiled = torch.compile(model)\n- Optional tuning:\n  - torch.compile(model, backend=\"inductor\", mode=\"max-autotune\"/\"default\")\n  - dynamic=True to be more flexible with shapes\n- Remove torch.compile to fall back to eager without code changes.\n\nIf you share your model type (e.g., CNN, Transformer, diffusion) and hardware, I can outline where PyTorch 2 is likely to fuse and how much speedup to expect.","created_at":1758565702540,"messageId":"d9ef9f3c-ae1e-469f-ab82-df03af269cf1","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Here’s the practical, high‑signal answer.\n\nTL;DR\n- PyTorch 2 adds torch.compile, which transparently compiles eager PyTorch code into fused kernels for speed, without changing your model code.\n- It keeps Python’s flexibility but extracts optimizable graphs at runtime, then lowers them to efficient CPU/GPU code.\n- Typical speedups: roughly 1.5–2.5× on real models (varies by workload), plus fewer kernel launches and lower memory traffic.\n\nWhat’s new in PyTorch 2 vs PyTorch 1\n\n1) One-line compilation: torch.compile\n- PyTorch 1: Eager by default. JIT Script/Trace existed but required a restricted sublanguage or were fragile with Python control flow.\n- PyTorch 2: torch.compile(model) becomes the default fast path. No model rewrites. It compiles hot regions and falls back to Python elsewhere, preserving correctness and debuggability.\n\n2) TorchDynamo: robust graph capture at Python bytecode level\n- PyTorch 1: Graph capture was either tracing at the C++ dispatcher (missing Python control flow) or scripting a subset of Python.\n- PyTorch 2: TorchDynamo hooks CPython’s frame evaluation, rewrites bytecode on the fly, and builds FX graphs for the tensor parts you actually execute. It:\n  - Specializes with guards on shapes/dtypes/devices/branches.\n  - Recaptures and caches variants when guards fail (dynamic-shape friendly).\n  - Makes safe graph breaks around “too-Pythonic” code but still compiles before/after.\n\n3) TorchInductor: new default backend compiler\n- PyTorch 1: Limited fusion and kernel codegen; relied mostly on vendor libraries and manual kernel paths.\n- PyTorch 2: TorchInductor lowers FX graphs to a loop-level IR, then generates:\n  - GPU: Triton kernels with aggressive fusion, tiling, vectorization.\n  - CPU: C++/OpenMP with similar loop optimizations.\n- Result: fewer kernel launches, less DRAM traffic, better cache/SM utilization.\n\n4) AOTAutograd + PrimTorch integration\n- PyTorch 1: Autograd ran op-by-op; fusion opportunities across backward were limited.\n- PyTorch 2: AOTAutograd captures/compiles forward and backward together, enabling cross-op fusion in gradients. PrimTorch/ATen canonicalization reduces thousands of ops to a manageable core set that backends can optimize well.\n\n5) Dynamic shapes and control flow that actually work\n- PyTorch 1: Static-ish assumptions often required; dynamic control flow made JIT tricky or brittle.\n- PyTorch 2: Specialization with guards makes dynamic control flow and varying shapes practical. You get multiple cached compiled variants when inputs vary; otherwise you stay on a fast, compiled path.\n\n6) Performance, out of the box\n- PyTorch 1: Good performance for big library ops (e.g., cuBLAS matmul) but overhead between ops, especially for memory-bound pointwise/reductions.\n- PyTorch 2: Inductor fuses epilogues and surrounding pointwise/reduction ops into larger kernels, often delivering 1.5–2.5× end-to-end gains on real models (exact numbers depend on model, hardware, and training vs inference).\n\n7) Better developer experience\n- Minimal code changes: usually just torch.compile.\n- Debuggability: You can disable/enable compile, inspect generated graphs/kernels, and let graph breaks preserve Python semantics.\n- Backend pluggability: Inductor by default, but other compilers can be integrated.\n\nWhen you’ll notice the difference most\n- Models with lots of small/medium tensor ops where fusion reduces kernel launches and memory bandwidth.\n- Training workloads where fused backward (AOTAutograd + Inductor) removes overhead.\n- Dynamic models (varying sequence lengths, branches) that previously defeated static tracing.\n\nWhen improvements are smaller\n- Workloads dominated by a few very large vendor-library ops (e.g., huge matmuls) may already be near peak; benefits come mainly from fusing surrounding epilogues.\n- Code dominated by non-tensor Python won’t speed up—PyTorch 2 compiles only tensor graphs.\n\nHow to adopt\n- Start with: compiled = torch.compile(model)\n- Optional tuning:\n  - torch.compile(model, backend=\"inductor\", mode=\"max-autotune\"/\"default\")\n  - dynamic=True to be more flexible with shapes\n- Remove torch.compile to fall back to eager without code changes.\n\nIf you share your model type (e.g., CNN, Transformer, diffusion) and hardware, I can outline where PyTorch 2 is likely to fuse and how much speedup to expect.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d19547256c81a1a2d127a2f5bb05750ecb6d00edb21f1c"}},"resumableStreamId":"STREAM:cc5aa517-d37e-4599-b3c8-5159a437c49d","role":"assistant","status":"done","threadId":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","timeToFirstToken":1.228,"tokens":987,"tokensPerSecond":41.12157320223315,"updated_at":1758565702540,"userId":"google:104458561188320708185","id":"j972s1gjy63x7e32aba52xh6r17r3b6k"},{"_creationTime":1758565748437.3335,"_id":"j9770094qv7pr4kt33wc1xjqv97r3gam","attachmentIds":[],"content":"Given a naive transformer model, how will it fuse or speedup","created_at":1758565748437,"messageId":"4aa95631-d447-4f18-8c38-5117a6bd75d8","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","updated_at":1758565748437,"userId":"google:104458561188320708185","id":"j9770094qv7pr4kt33wc1xjqv97r3gam","providerMetadata":{}},{"_creationTime":1758565748437.3337,"_id":"j97emh2fsfcj9n5jjejk9w367s7r2yv5","attachmentIds":[],"byok":false,"content":"Short answer: PyTorch 2 will keep big library ops (GEMMs/attention matmuls) as-is, and aggressively fuse the “glue” around them (bias, activations, layernorms, residual adds, dropout masks, small reductions). That cuts kernel launches and DRAM traffic, so end-to-end you often see 1.3–2× speedups for a naive Transformer, sometimes more on inference.\n\nConcretely, for a standard encoder/decoder block:\n\nWhat stays as vendor libraries\n- QKV projections: three linear layers -> GEMMs via cuBLAS (GPU) or MKL/OpenBLAS (CPU).\n- Attention score matmul: Q @ K^T.\n- Value aggregation: softmax(P) @ V.\n- MLP inner matmul(s).\nThese are already near peak; Inductor usually doesn’t replace them, but it optimizes what happens before/after.\n\nWhat gets fused around them\n- Linear epilogues: bias add + activation fusions on the MLP path:\n  - matmul + bias + gelu/silu + dropout + residual add often becomes 1–2 Triton kernels instead of 4–6.\n- LayerNorm: reductions (mean/var) + scale/shift + optional activation can be fused into one kernel. Pre-/post-norm blocks benefit a lot here.\n- Attention epilogue/prologue:\n  - Scaling of QK^T by 1/sqrt(d_k).\n  - Masking (causal or padding) + bias add.\n  - Softmax (with max-subtract + exp + sum + divide) fused as a single kernel.\n  - Optional dropout on attention probabilities fused into the same or subsequent kernel.\n- Residual add chains: add + dropout + cast/clamp patterns coalesce.\n- Type/cast/layout fixups: many view/reshape/contiguous/cast ops disappear or are sunk/hoisted.\n\nDynamic-shape handling\n- Sequence length, batch size, and head count variations are handled with guards. Inductor emits shape-polymorphic kernels or caches a few specialized variants. For common L choices (e.g., 128/256/512), you’ll hit cached compiled paths.\n\nTraining-specific wins\n- AOTAutograd lets the backward pass of LayerNorm, GELU, residual adds, and many pointwise chains fuse similarly to forward.\n- Gradient epilogues (e.g., dY scaling, mask application) fuse.\n- Parameter gradient reductions may be scheduled more cache-friendly on CPU.\n\nTypical fusion per sub-block\n\nSelf-attention forward:\n- Q = xWq + bq; K = xWk + bk; V = xWv + bv\n  - Each matmul stays as cuBLAS; bias adds may be fused into a single epilogue kernel or folded where possible.\n- Scores = (Q @ K^T) * scale\n  - Matmul via cuBLAS; the scale multiply can be fused into the softmax kernel rather than a separate pass.\n- Scores = apply_mask(scores) [+ bias]\n  - Masking and bias add fused into the softmax kernel or an immediately adjacent kernel.\n- P = softmax(scores)\n  - Max-subtract, exp, sum, divide fused as one kernel.\n- O = P @ V\n  - cuBLAS matmul.\n- O = dropout(O) + x (residual) -> layernorm\n  - Dropout + residual add fused; LayerNorm fused into one kernel, including affine.\n\nMLP forward (typical GELU block):\n- z1 = xW1 + b1\n- z2 = gelu(z1)\n- z3 = dropout(z2)\n- y = z3W2 + b2\n- out = dropout(y) + x (residual) -> layernorm\nFusions:\n- z1 bias + gelu + dropout often one kernel.\n- Post-MLP residual add + dropout fused.\n- LayerNorm fused.\n\nBackward highlights\n- GELU backward fused with upstream/downstream elementwise grads.\n- LayerNorm backward (mean/var reductions + affine grads) in one or two kernels.\n- Mask/scale/softmax backward fused to minimize reads/writes.\n\nCPU vs GPU\n- GPU: Triton kernels do the heavy fused pointwise/reduction work; matmuls stick to cuBLAS/cutlass. Biggest wins from reducing global memory traffic and kernel launches.\n- CPU: C++/OpenMP loops fuse the same patterns; wins from better cache locality and vectorization.\n\nExpected speedups (rough guidance; depends on sizes/hardware)\n- Inference, fp16/bf16, mid-size models (e.g., 12–24 layers, L=128–1024): 1.3–2.0× end-to-end.\n- Training: 1.2–1.6× common; larger if your naive model had many small ops and Python overhead.\n- Small batch, short sequence: launch overhead dominates, so fusion helps more.\n- Very large matmul-dominated setups: improvement mainly from fused epilogues and norms, so smaller gains.\n\nThings that can inhibit fusion\n- Frequent graph breaks: Python-side control flow or custom ops inside the block. Keep tensor math contiguous.\n- Tensor aliasing/mutation patterns that force materialization.\n- Unusual dtypes/layouts that lack kernel templates.\n- Excessively dynamic shapes each step, causing many recompiles; try bucketing sequence lengths.\n\nHow to maximize speedups in a naive Transformer\n- Use standard ops: torch.nn.functional or nn modules for softmax, layernorm, dropout, gelu/silu.\n- Keep mask computation in tensors (no Python loops); precompute attention masks as tensors.\n- Avoid unnecessary .contiguous() or .detach() calls.\n- Prefer fused-friendly patterns: combine bias+activation instead of separating by intermediate views.\n- Wrap the module with torch.compile and run a few warmup steps to populate caches.\n\nIf you share your model sizes (d_model, n_heads, L, batch) and device, I can sketch the exact fusion boundaries you’ll likely see and where the biggest savings come from.","created_at":1758565748438,"messageId":"513e8101-f2c3-40c4-b749-85a9c7867278","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Short answer: PyTorch 2 will keep big library ops (GEMMs/attention matmuls) as-is, and aggressively fuse the “glue” around them (bias, activations, layernorms, residual adds, dropout masks, small reductions). That cuts kernel launches and DRAM traffic, so end-to-end you often see 1.3–2× speedups for a naive Transformer, sometimes more on inference.\n\nConcretely, for a standard encoder/decoder block:\n\nWhat stays as vendor libraries\n- QKV projections: three linear layers -> GEMMs via cuBLAS (GPU) or MKL/OpenBLAS (CPU).\n- Attention score matmul: Q @ K^T.\n- Value aggregation: softmax(P) @ V.\n- MLP inner matmul(s).\nThese are already near peak; Inductor usually doesn’t replace them, but it optimizes what happens before/after.\n\nWhat gets fused around them\n- Linear epilogues: bias add + activation fusions on the MLP path:\n  - matmul + bias + gelu/silu + dropout + residual add often becomes 1–2 Triton kernels instead of 4–6.\n- LayerNorm: reductions (mean/var) + scale/shift + optional activation can be fused into one kernel. Pre-/post-norm blocks benefit a lot here.\n- Attention epilogue/prologue:\n  - Scaling of QK^T by 1/sqrt(d_k).\n  - Masking (causal or padding) + bias add.\n  - Softmax (with max-subtract + exp + sum + divide) fused as a single kernel.\n  - Optional dropout on attention probabilities fused into the same or subsequent kernel.\n- Residual add chains: add + dropout + cast/clamp patterns coalesce.\n- Type/cast/layout fixups: many view/reshape/contiguous/cast ops disappear or are sunk/hoisted.\n\nDynamic-shape handling\n- Sequence length, batch size, and head count variations are handled with guards. Inductor emits shape-polymorphic kernels or caches a few specialized variants. For common L choices (e.g., 128/256/512), you’ll hit cached compiled paths.\n\nTraining-specific wins\n- AOTAutograd lets the backward pass of LayerNorm, GELU, residual adds, and many pointwise chains fuse similarly to forward.\n- Gradient epilogues (e.g., dY scaling, mask application) fuse.\n- Parameter gradient reductions may be scheduled more cache-friendly on CPU.\n\nTypical fusion per sub-block\n\nSelf-attention forward:\n- Q = xWq + bq; K = xWk + bk; V = xWv + bv\n  - Each matmul stays as cuBLAS; bias adds may be fused into a single epilogue kernel or folded where possible.\n- Scores = (Q @ K^T) * scale\n  - Matmul via cuBLAS; the scale multiply can be fused into the softmax kernel rather than a separate pass.\n- Scores = apply_mask(scores) [+ bias]\n  - Masking and bias add fused into the softmax kernel or an immediately adjacent kernel.\n- P = softmax(scores)\n  - Max-subtract, exp, sum, divide fused as one kernel.\n- O = P @ V\n  - cuBLAS matmul.\n- O = dropout(O) + x (residual) -> layernorm\n  - Dropout + residual add fused; LayerNorm fused into one kernel, including affine.\n\nMLP forward (typical GELU block):\n- z1 = xW1 + b1\n- z2 = gelu(z1)\n- z3 = dropout(z2)\n- y = z3W2 + b2\n- out = dropout(y) + x (residual) -> layernorm\nFusions:\n- z1 bias + gelu + dropout often one kernel.\n- Post-MLP residual add + dropout fused.\n- LayerNorm fused.\n\nBackward highlights\n- GELU backward fused with upstream/downstream elementwise grads.\n- LayerNorm backward (mean/var reductions + affine grads) in one or two kernels.\n- Mask/scale/softmax backward fused to minimize reads/writes.\n\nCPU vs GPU\n- GPU: Triton kernels do the heavy fused pointwise/reduction work; matmuls stick to cuBLAS/cutlass. Biggest wins from reducing global memory traffic and kernel launches.\n- CPU: C++/OpenMP loops fuse the same patterns; wins from better cache locality and vectorization.\n\nExpected speedups (rough guidance; depends on sizes/hardware)\n- Inference, fp16/bf16, mid-size models (e.g., 12–24 layers, L=128–1024): 1.3–2.0× end-to-end.\n- Training: 1.2–1.6× common; larger if your naive model had many small ops and Python overhead.\n- Small batch, short sequence: launch overhead dominates, so fusion helps more.\n- Very large matmul-dominated setups: improvement mainly from fused epilogues and norms, so smaller gains.\n\nThings that can inhibit fusion\n- Frequent graph breaks: Python-side control flow or custom ops inside the block. Keep tensor math contiguous.\n- Tensor aliasing/mutation patterns that force materialization.\n- Unusual dtypes/layouts that lack kernel templates.\n- Excessively dynamic shapes each step, causing many recompiles; try bucketing sequence lengths.\n\nHow to maximize speedups in a naive Transformer\n- Use standard ops: torch.nn.functional or nn modules for softmax, layernorm, dropout, gelu/silu.\n- Keep mask computation in tensors (no Python loops); precompute attention masks as tensors.\n- Avoid unnecessary .contiguous() or .detach() calls.\n- Prefer fused-friendly patterns: combine bias+activation instead of separating by intermediate views.\n- Wrap the module with torch.compile and run a few warmup steps to populate caches.\n\nIf you share your model sizes (d_model, n_heads, L, batch) and device, I can sketch the exact fusion boundaries you’ll likely see and where the biggest savings come from.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d19575f1c88190a0781dfd1b3e62f40b87d77e149b27cc"}},"resumableStreamId":"STREAM:bd46f83a-d5c7-4281-b6a2-1fbf935c1c88","role":"assistant","status":"done","threadId":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","timeToFirstToken":1.937,"tokens":1277,"tokensPerSecond":62.40836672857003,"updated_at":1758565748438,"userId":"google:104458561188320708185","id":"j97emh2fsfcj9n5jjejk9w367s7r2yv5"},{"_creationTime":1758565893429.771,"_id":"j970ezyt33rs9hbw7pbtq6ccws7r2y1p","attachmentIds":[],"content":"how about diffusion","created_at":1758565893429,"messageId":"5bc2c213-b077-4339-90e8-39f2afcd4ba2","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","updated_at":1758565893429,"userId":"google:104458561188320708185","id":"j970ezyt33rs9hbw7pbtq6ccws7r2y1p","providerMetadata":{}},{"_creationTime":1758565893429.7712,"_id":"j977z0s8v3kndvrg5w91hd197s7r2rha","attachmentIds":[],"byok":false,"content":"Short answer: PyTorch 2 speeds up diffusion pipelines mainly by fusing the “glue” around the big conv/GEMM ops in the UNet and text encoder, optimizing attention/LayerNorm, and reducing kernel launches and DRAM traffic during the denoising loop. Expect roughly 1.2–2× end‑to‑end gains depending on model size, scheduler, precision, and hardware, with larger wins at small batch/short sequences where launch overhead dominates.\n\nWhere the speedups come from in diffusion\n\nUNet (2D/Latent UNet as in Stable Diffusion/SDXL)\n- Convolutions:\n  - Large convs usually stay in vendor libraries (cuDNN). PyTorch 2 won’t replace a well-optimized conv, but it fuses surrounding pointwise ops.\n- Epilogues around convs:\n  - conv + bias + groupnorm/instancenorm + activation (SiLU/GELU) often becomes 1 fused Triton kernel for the norm+activation; sometimes bias can be folded.\n  - Residual add + activation fuses.\n- Attention blocks (cross/self attention in mid/skip paths):\n  - QKV linears: GEMMs via cuBLAS; epilogue bias/cast fused.\n  - Scale, mask/bias add, softmax fused into a single kernel.\n  - Dropout and residual adds coalesce.\n  - LayerNorm fused forward/backward.\n- Upsample/downsample:\n  - Interpolate + post-ops (casts, clamp, add) can fuse; view/permute chains get sunk/removed when safe.\n\nText encoder (Transformer, e.g., CLIP/SDXL text)\n- Same benefits as a naive Transformer:\n  - Softmax, LayerNorm, residual chains fuse.\n  - Linear epilogues (bias+activation) and dropout fuse.\n  - Matmuls stay in cuBLAS; surrounding pointwise ops fuse.\n\nVAE encoder/decoder\n- Conv blocks with GroupNorm/SiLU: similar fusion as in UNet.\n- Sampling steps (mu/logvar, reparameterization) are cheap; pointwise ops fuse away.\n\nSchedulers and denoising loop\n- The outer loop runs T steps (e.g., 20–50 with modern samplers).\n- Per-step overhead drops because:\n  - Many small tensor ops in the scheduler update (scale, add, exp/log, clamp) fuse into few kernels.\n  - Type/cast/layout fixups and reshapes are eliminated or sunk.\n  - Fewer kernel launches per step reduces latency significantly in low‑batch inference.\n- If your scheduler uses dynamic control flow, Dynamo guards specialize on the taken paths; variants are cached.\n\nTraining vs inference\n- Inference: Biggest wins from fused norms/activations and reduced launch overhead across T steps. fp16/bf16 autocast plus Inductor fusion helps.\n- Training:\n  - AOTAutograd enables fusion in backward for GroupNorm/LayerNorm, SiLU/GELU, residual chains, and attention’s softmax/mask/scale.\n  - Gradient epilogues (scales, masks) fuse; reductions are scheduled for cache/SM efficiency.\n  - Expect 1.2–1.6× commonly, more if your model had lots of small ops.\n\nDynamic shapes common in diffusion\n- Variable image sizes (e.g., 512→1024) and text sequence lengths are handled with guards. Inductor either emits shape‑polymorphic kernels or caches specialized variants. If you sweep many resolutions in one run, consider bucketing to reduce recompiles.\n\nTypical fused regions inside a UNet block\n- Before: conv -> bias -> groupnorm -> silu -> add(residual)\n- After: conv (cuDNN) + one fused kernel for groupnorm+affine+silu, then a fused add(+dropout if used).\n- Attention sub-block:\n  - After QK matmul: scale + mask + softmax fused.\n  - After PV matmul: dropout + residual add + layernorm fused.\n\nPractical tips to maximize PyTorch 2 benefits for diffusion\n- Use standard nn layers and F.* ops (GroupNorm/LayerNorm, SiLU, softmax); avoid custom Python loops in hot paths.\n- Keep masks and scheduler math as tensor ops, not Python scalars/loops.\n- Minimize gratuitous .contiguous(), .detach(), and in‑place ops that can force graph breaks.\n- Compile the whole pipeline:\n  - For inference: wrap UNet, VAE decoder, and text encoder with torch.compile individually, or compile a pipeline function that invokes them per step.\n  - Warm up a few steps to populate compiled caches.\n- Enable fp16/bf16 with autocast on GPUs that support it; fusion + reduced bandwidth often gives the best gains.\n- If you see many recompiles (shape guard churn), bucket resolutions and max text lengths.\n\nRough speedup expectations (guidance; varies by hardware/model)\n- SD 1.5/2.1, 512×512, batch=1, fp16 on A100/4090:\n  - UNet step latency often drops 20–40%; end‑to‑end 1.3–1.8× for 20–50 steps.\n- SDXL (heavier attention and norms):\n  - Similar or slightly higher gains due to more fusable glue around larger blocks.\n- CPU runs benefit too (OpenMP loops) but absolute latency may still be GPU‑dominated.\n\nTell me your exact setup (model variant, resolution, batch, precision, GPU/CPU), and I’ll outline the likely fusion map per block and we can estimate the step and total time savings.","created_at":1758565893430,"messageId":"47628e08-79e4-449c-9237-7bf13ba9d4cf","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Short answer: PyTorch 2 speeds up diffusion pipelines mainly by fusing the “glue” around the big conv/GEMM ops in the UNet and text encoder, optimizing attention/LayerNorm, and reducing kernel launches and DRAM traffic during the denoising loop. Expect roughly 1.2–2× end‑to‑end gains depending on model size, scheduler, precision, and hardware, with larger wins at small batch/short sequences where launch overhead dominates.\n\nWhere the speedups come from in diffusion\n\nUNet (2D/Latent UNet as in Stable Diffusion/SDXL)\n- Convolutions:\n  - Large convs usually stay in vendor libraries (cuDNN). PyTorch 2 won’t replace a well-optimized conv, but it fuses surrounding pointwise ops.\n- Epilogues around convs:\n  - conv + bias + groupnorm/instancenorm + activation (SiLU/GELU) often becomes 1 fused Triton kernel for the norm+activation; sometimes bias can be folded.\n  - Residual add + activation fuses.\n- Attention blocks (cross/self attention in mid/skip paths):\n  - QKV linears: GEMMs via cuBLAS; epilogue bias/cast fused.\n  - Scale, mask/bias add, softmax fused into a single kernel.\n  - Dropout and residual adds coalesce.\n  - LayerNorm fused forward/backward.\n- Upsample/downsample:\n  - Interpolate + post-ops (casts, clamp, add) can fuse; view/permute chains get sunk/removed when safe.\n\nText encoder (Transformer, e.g., CLIP/SDXL text)\n- Same benefits as a naive Transformer:\n  - Softmax, LayerNorm, residual chains fuse.\n  - Linear epilogues (bias+activation) and dropout fuse.\n  - Matmuls stay in cuBLAS; surrounding pointwise ops fuse.\n\nVAE encoder/decoder\n- Conv blocks with GroupNorm/SiLU: similar fusion as in UNet.\n- Sampling steps (mu/logvar, reparameterization) are cheap; pointwise ops fuse away.\n\nSchedulers and denoising loop\n- The outer loop runs T steps (e.g., 20–50 with modern samplers).\n- Per-step overhead drops because:\n  - Many small tensor ops in the scheduler update (scale, add, exp/log, clamp) fuse into few kernels.\n  - Type/cast/layout fixups and reshapes are eliminated or sunk.\n  - Fewer kernel launches per step reduces latency significantly in low‑batch inference.\n- If your scheduler uses dynamic control flow, Dynamo guards specialize on the taken paths; variants are cached.\n\nTraining vs inference\n- Inference: Biggest wins from fused norms/activations and reduced launch overhead across T steps. fp16/bf16 autocast plus Inductor fusion helps.\n- Training:\n  - AOTAutograd enables fusion in backward for GroupNorm/LayerNorm, SiLU/GELU, residual chains, and attention’s softmax/mask/scale.\n  - Gradient epilogues (scales, masks) fuse; reductions are scheduled for cache/SM efficiency.\n  - Expect 1.2–1.6× commonly, more if your model had lots of small ops.\n\nDynamic shapes common in diffusion\n- Variable image sizes (e.g., 512→1024) and text sequence lengths are handled with guards. Inductor either emits shape‑polymorphic kernels or caches specialized variants. If you sweep many resolutions in one run, consider bucketing to reduce recompiles.\n\nTypical fused regions inside a UNet block\n- Before: conv -> bias -> groupnorm -> silu -> add(residual)\n- After: conv (cuDNN) + one fused kernel for groupnorm+affine+silu, then a fused add(+dropout if used).\n- Attention sub-block:\n  - After QK matmul: scale + mask + softmax fused.\n  - After PV matmul: dropout + residual add + layernorm fused.\n\nPractical tips to maximize PyTorch 2 benefits for diffusion\n- Use standard nn layers and F.* ops (GroupNorm/LayerNorm, SiLU, softmax); avoid custom Python loops in hot paths.\n- Keep masks and scheduler math as tensor ops, not Python scalars/loops.\n- Minimize gratuitous .contiguous(), .detach(), and in‑place ops that can force graph breaks.\n- Compile the whole pipeline:\n  - For inference: wrap UNet, VAE decoder, and text encoder with torch.compile individually, or compile a pipeline function that invokes them per step.\n  - Warm up a few steps to populate compiled caches.\n- Enable fp16/bf16 with autocast on GPUs that support it; fusion + reduced bandwidth often gives the best gains.\n- If you see many recompiles (shape guard churn), bucket resolutions and max text lengths.\n\nRough speedup expectations (guidance; varies by hardware/model)\n- SD 1.5/2.1, 512×512, batch=1, fp16 on A100/4090:\n  - UNet step latency often drops 20–40%; end‑to‑end 1.3–1.8× for 20–50 steps.\n- SDXL (heavier attention and norms):\n  - Similar or slightly higher gains due to more fusable glue around larger blocks.\n- CPU runs benefit too (OpenMP loops) but absolute latency may still be GPU‑dominated.\n\nTell me your exact setup (model variant, resolution, batch, precision, GPU/CPU), and I’ll outline the likely fusion map per block and we can estimate the step and total time savings.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d1960638f8819cba7b1bbfcabe69c8098d1727bd3375bf"}},"resumableStreamId":"STREAM:7214e2bb-51c7-49c6-b7fb-7e437792e5c3","role":"assistant","status":"done","threadId":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","timeToFirstToken":1.461,"tokens":1186,"tokensPerSecond":71.70062269512121,"updated_at":1758565893430,"userId":"google:104458561188320708185","id":"j977z0s8v3kndvrg5w91hd197s7r2rha"},{"_creationTime":1758565923709.4534,"_id":"j972npv9rn56s843f227j329a57r3nt1","attachmentIds":[],"content":"how does it compare against tensorflow in efficiency?","created_at":1758565923709,"messageId":"7a1dcbd6-b837-4d45-89b3-54f957d3ac3d","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","updated_at":1758565923709,"userId":"google:104458561188320708185","id":"j972npv9rn56s843f227j329a57r3nt1","providerMetadata":{}},{"_creationTime":1758565923709.4536,"_id":"j977jgw45hgnewrzk2abap3vts7r3w4g","attachmentIds":[],"byok":false,"content":"Short answer: On GPUs, PyTorch 2 with torch.compile (Dynamo + Inductor) is generally on par with, and often faster than, TensorFlow 2/XLA for many research-style models (Transformers, diffusion UNets) because it fuses the “glue” ops aggressively and keeps Python usability. TensorFlow/XLA can still shine in very static, well‑tuned graphs (especially on TPUs) and in production deployments via TFLite/TF Serving. Which is “more efficient” depends on your model, hardware, and how static your shapes/graphs are.\n\nHow they get efficiency\n- PyTorch 2 (torch.compile):\n  - Captures executed Python into FX graphs with guards, then Inductor generates loop-level code (Triton GPU kernels; C++/OpenMP on CPU). This yields strong fusion of epilogues (bias/activation), norms, softmax, masking, residual chains, etc., cutting kernel launches and DRAM traffic.\n  - Handles dynamic shapes/branches via specialization+guards; caches variants, so real workloads with varying seq lengths or image sizes still hit compiled paths.\n  - Plays very well with modern PyTorch kernels (FlashAttention v3, scaled dot-product attention, fused LayerNorm) and CUDA Graphs in max-autotune modes.\n- TensorFlow 2 (tf.function + XLA):\n  - Traces functions to HLO and compiles with XLA. Very strong at whole-graph optimization when shapes are static and control flow is functionalized.\n  - TPU support is first-class; XLA+TPU often yields excellent throughput/cost for large, static training/inference.\n  - Mature mobile/edge stack (TFLite/XNNPACK) is highly optimized for CPU/embedded inference efficiency.\n\nPractical comparisons by workload\n- Transformers (GPU):\n  - Inference/training: PyTorch 2 often achieves 1.3–2× end-to-end over eager PyTorch by fusing softmax/mask/scale, norms, residuals, GELU/SwiGLU epilogues, etc. Against TF/XLA, results vary by model and tuning; in many open benchmarks and user reports, PyTorch 2 matches or edges out TF on NVIDIA GPUs when both are “on” and reasonably tuned.\n  - Highly static, productionized graphs with XLA can be competitive; on TPUs, TF/JAX+XLA typically wins on cost/perf.\n- Diffusion (UNet/DiT on GPU):\n  - PyTorch 2 tends to do very well: Inductor fuses GroupNorm/Silu chains, attention softmax/mask/scale, and scheduler math across T denoising steps. Official PyTorch/HF posts show ~1.3–2.5× speedups vs eager PyTorch on H100; TF/XLA can be strong if the pipeline is refactored for static shapes, but in common Diffusers-style Python code, torch.compile usually achieves bigger wins with less code churn.\n- Large matmul‑dominated segments:\n  - Both frameworks lean on cuBLAS/cuDNN/CUTLASS; differences come from surrounding fusion and launch overhead. Edge goes to the stack that better fuses epilogues and reduces syncs; PyTorch 2 often does.\n- TPU:\n  - TensorFlow (and JAX) + XLA are the native path; they typically deliver better efficiency on TPUs than PyTorch’s experimental TPU paths.\n\nDynamic shapes and control flow\n- PyTorch 2: Specialization with guards plus multiple cached variants makes dynamic seq lengths, image sizes, or conditional branches practical without hand‑rewriting code.\n- TensorFlow: Best results with static shapes; AutoGraph can stage some Python control flow, but frequent shape changes can trigger retracing/compiles or require tf.shape-based coding and careful constraints.\n\nCompilation overhead and developer UX\n- PyTorch 2: One‑liner torch.compile, mixes compiled chunks with Python safely; you can aim for fullgraph=True to catch breaks. Initial compile cost exists; regional compilation and CUDA Graphs help. Good debugging story (dynamo.explain, inspect kernels).\n- TensorFlow: tf.function/XLA prefers graph‑friendly code; you may need to restructure code to avoid retraces and hit fusion patterns. Once stabilized, compile overhead amortizes well.\n\nCPU/edge inference\n- TensorFlow/TFLite with XNNPACK is very efficient on ARM/embedded. For server CPUs, both TF/XLA and PyTorch 2 (Inductor -> C++/OpenMP, MKL/oneDNN, and quantization) can be competitive; leadership depends on kernels and quantization paths for your ops.\n\nEcosystem effects\n- PyTorch: Strong research velocity, many kernels/features land first; recent posts show 1.5–2.5× on diffusion/Flux using torch.compile, CUDA Graphs, FA3, fp8 quantization.\n- TensorFlow: Production tooling (TFX, Serving, TFLite) is mature; on enterprise pipelines and mobile, TF may be more efficient operationally even if raw kernel time is similar.\n\nRules of thumb\n- Use PyTorch 2 if:\n  - You run on NVIDIA GPUs, your model has lots of “glue” ops around big matmuls/convs, and you value minimal code changes with strong fusion.\n  - You have dynamic shapes or Pythonic control flow.\n- Use TensorFlow/XLA if:\n  - You run primarily on TPUs.\n  - You deploy to mobile/edge with TFLite, or have static, productionized graphs benefiting from XLA’s whole-graph passes.\n\nIf you share your exact model (e.g., ViT/Transformer/DiT, batch/seq length or resolution), hardware (GPU/TPU/CPU), and whether shapes vary at runtime, I can recommend settings (torch.compile modes, TF/XLA flags) and estimate which stack will be more efficient for your case.","created_at":1758565923710,"messageId":"eac46e5e-44bc-4c2d-84ee-3dfbd1e9b80d","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"news","domains":null,"queries":["PyTorch 2 vs TensorFlow performance 2025 benchmarks torch.compile Inductor vs XLA 2024 2025","ACM paper PyTorch 2 performance numbers compared to TensorFlow XLA 2024 2025 diffusion transformer benchmarks","TensorFlow XLA vs PyTorch Inductor benchmark stable diffusion transformer 2024-2025 blog"]},"result":[{"content":"- [Articles & Reviews](https://www.phoronix.com/reviews)\n- [News Archive](https://www.phoronix.com/news)\n- [Forums](https://www.phoronix.com/forums/)\n- [Premium Ad-Free](https://www.phoronix.com/phoronix-premium)\n- [Contact](https://www.phoronix.com/contact)\n- Popular Categories\n- Close\n\n- [Articles & Reviews](https://www.phoronix.com/reviews)\n- [News Archive](https://www.phoronix.com/news)\n- [Forums](https://www.phoronix.com/forums/)\n- [Premium](https://www.phoronix.com/phoronix-premium)\n- [Contact](https://www.phoronix.com/contact)\n- Categories\n[Computers](https://www.phoronix.com/reviews/Computers) [Display Drivers](https://www.phoronix.com/reviews/Display+Drivers) [Graphics Cards](https://www.phoronix.com/reviews/Graphics+Cards) [Linux Gaming](https://www.phoronix.com/reviews/Linux+Gaming) [Memory](https://www.phoronix.com/reviews/Memory) [Motherboards](https://www.phoronix.com/reviews/Motherboards) [Processors](https://www.phoronix.com/reviews/Processors) [Software](https://www.phoronix.com/reviews/Software) [Storage](https://www.phoronix.com/reviews/Storage) [Operating Systems](https://www.phoronix.com/reviews/Operating+Systems) [Peripherals](https://www.phoronix.com/reviews/Peripherals)\n\n\n# PyTorch 2.8 Released With Better Intel CPU Performance For LLM Inference\n\nWritten by [Michael Larabel](https://www.michaellarabel.com/) in [Programming](https://www.phoronix.com/linux/Programming) on 6 August 2025 at 08:50 PM EDT. [3 Comments](https://www.phoronix.com/forums/node/1567296)\n\nPyTorch 2.8 released today as the newest feature update to this widely-used machine learning library that has become a crucial piece for deep learning and other AI usage. There are a few interesting changes worth highlighting with the new PyTorch 2.8 release.\nPiquing my interest with PyTorch 2.8 is improved Intel CPU performance. In particular, a focus on high performance quantized large language model (LLM) inference for Intel CPUs using the native PyTorch version. The [change](https://github.com/pytorch/pytorch/issues/155435) outlines the LLM quantization work done by Intel engineers to enhance their x86\\_64 CPU performance with native PyTorch. A16W8, DA8W8 and A16W4 are among the supported modes. That issue ticket noted:\n\n> \"With this feature, the performance with PyTorch native stack can reach the same level or even better in some cases as comparing with popular LLM serving frameworks like vLLM when running offline mode on a single x86\\_64 CPU device, which enables PyTorch users to run LLM quantization with native experience and good performance.\"\n\nThere have been a lot of Intel CPU commits this cycle such as for [FP8 QCONV](https://github.com/pytorch/pytorch/commit/e1a20988f3724317a7ee79c1777d574a8282a122), [FP8 QLINEAR](https://github.com/pytorch/pytorch/commit/c2185dc4a5626848df37cad214b73d5ae7dd4f17), and [using AMX-based micro-kernels in more instances](https://github.com/pytorch/pytorch/commit/b6f84b3b0fef781653911420253dcff6767197dc). The AMX micro-kernel improvement can be quite beneficial:\n\n> \"GEMM templates for INT4 weights are used for lowering \\`aten.\\_weight\\_int4pack\\_mm\\_for\\_cpu\\` with Inductor when max-autotune is on. Currently, AMX-based microkernels are used only when M >= 16 if input tensor has shape \\[M, K\\]. However, we find that AMX kernel brings performance benefit when 4 < M < 16. For example, on a 6th gen of Intel(R) Xeon(R) platform, E2E latency can be improved by up to > 20% when running Llama-3.1-8B on 32 cores for M = 8. So, this PR changes the threshold so that AMX is used when M > 4.\"\n\nToo bad though that my AvenueCity reference server remains non-operational and thus unable to test the newest PyTorch release (and other Intel open-source improvements in recent months) on the flagship Xeon 6980P Granite Rapids processors... So, unfortunately, no new Xeon 6900P benchmarks at this time on Phoronix.\nAlso on the Intel side for PyTorch 2.8 is experimental support for the Intel XCCL GPU distributed back-end. XCCL is a distributed back-end for Intel discrete GPUs for various distributed training paradigms.\nPyTorch 2.8 also brings SYCL support to the PyTorch CPP Extension API, A16W4 support for XPU devices, experimental wheel variant support, and other enhancements.\nDownloads and more details on the PyTorch 2.8 release via [the PyTorch.org blog](https://pytorch.org/blog/pytorch-2-8/) and [GitHub](https://github.com/pytorch/pytorch/releases/tag/v2.8.0).\n\n[3 Comments](https://www.phoronix.com/forums/node/1567296)\n\n[Tweet](https://twitter.com/share)\n\n[Rust Innovation Lab Announced By The Rust Foundation](https://www.phoronix.com/news/Rust-Innovation-Lab)\n\n[ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization](https://www.phoronix.com/news/ollama-0.11.9-More-Performance)\n\n[Rust 1.90 Switching To LLD Linker On Linux For Faster Linking Times](https://www.phoronix.com/news/Rust-1.90-LLD-Linking)\n\n[Meson 1.9 Released With New Rust Features, Adds Swift/C++ Interoperability](https://www.phoronix.com/news/Meson-1.9-Released)\n\n[Tinygrad 0.11 Released With AMD MI350 Support, NVIDIA Blackwell](https://www.phoronix.com/news/Tinygrad-0.11-Released)\n\n[Git 2.51 Released With More Changes Preparing For Git 3.0](https://www.phoronix.com/news/Git-2.51-Released)\n\nMichael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via [Twitter](https://twitter.com/MichaelLarabel), [LinkedIn](https://www.linkedin.com/in/michaellarabel/), or contacted via [MichaelLarabel.com](https://www.michaellarabel.com/).\n\n[Linus Torvalds Marks Bcachefs As Now \"Externally Maintained\"](https://www.phoronix.com/news/Bcachefs-Externally-Maintained)\n\n[Btrfs Developer Josef Bacik Leaving Meta & Stepping Back From Kernel Development](https://www.phoronix.com/news/Josef-Bacik-Leaves-Meta)\n\n[sudo-rs Is Now The Default sudo Of Ubuntu 25.10](https://www.phoronix.com/news/Ubuntu--Now-Default-sudo-rs)\n\n[Open-Source NVIDIA Linux Driver Usage About To Become Much More Reliable](https://www.phoronix.com/news/Nouveau-More-Reliable-Fix)\n\n[ollama 0.11.9 Introducing A Nice CPU/GPU Performance Optimization](https://www.phoronix.com/news/ollama-0.11.9-More-Performance)\n\n[bsd-user-4-linux Lets FreeBSD Binaries Run Unmodified On Linux](https://www.phoronix.com/news/FreeBSD-Q2-2025-Status-Report)\n\n[GIMP 3.1.4 Released With A Project From 2006 Resurrected And Another Big Feature](https://www.phoronix.com/news/GIMP-3.1.4-Released)\n\n[GNOME Executive Director Steps Down After Four Months](https://www.phoronix.com/news/Steven-Deobald-Steps-Down)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.phoronix.com%2Fnews%2FPyTorch-2.8-Released","id":"https://www.phoronix.com/news/PyTorch-2.8-Released","publishedDate":"2025-08-06T00:00:00.000Z","summary":"PyTorch 2.8 has been released, bringing improved Intel CPU performance, especially for large language model (LLM) inference. The update includes optimizations for Intel x86_64 CPUs, with support for A16W8, DA8W8, and A16W4 quantization modes. These improvements, developed by Intel engineers, aim to match or exceed the performance of popular LLM serving frameworks like vLLM when running offline on a single CPU. Key enhancements include FP8 QCONV and QLINEAR support, and the use of AMX-based micro-kernels, which can improve latency by over 20% on some Intel Xeon platforms when running Llama-3.1-8B.\n","title":"PyTorch 2.8 Released With Better Intel CPU Performance For LLM Inference","url":"https://www.phoronix.com/news/PyTorch-2.8-Released"},{"content":"[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3d7fd94ce701&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40lambdafluxofficial%2Feverything-you-need-to-know-about-pytorch-compile-3d7fd94ce701&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40lambdafluxofficial%2Feverything-you-need-to-know-about-pytorch-compile-3d7fd94ce701&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n# Everything You Need to Know About PyTorch Compile\n\n[LambdaFlux](https://medium.com/@lambdafluxofficial?source=post_page---byline--3d7fd94ce701---------------------------------------)\n\n4 min read\n\n·\n\nMar 23, 2025\n\n--\n\nListen\n\nShare\n\n`torch.compile()` is a feature introduced in PyTorch 2.0 that compiles PyTorch models into more efficient machine code, improving the performance.\n\n## How to Use `torch.compile`\n\nUsing PyTorch’s compile functionality is straightforward — you simply pass your model, function or piece of code to `torch.compile()`, which returns an optimized version.\n\nIn the following example, Microsoft’s `Phi-4-mini-instruct model` is optimized using `torch.compile()`. Similarly, you can compile any other HuggingFace model, any `nn.Module ` or any other compatible PyTorch code. You can also use the `@torch.compile` decorator instead of calling `torch.compile()` directly.\n\n## Example: Compiling a Hugging Face Model\n\n```\nimport torchimport timefrom transformers import AutoModelForCausalLM, AutoTokenizer# Load a transformer model and tokenizermodel_name = \"microsoft/Phi-4-mini-instruct\"tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to('cuda')def timed(func): start = torch.cuda.Event(enable_timing=True) end = torch.cuda.Event(enable_timing=True) start.record() result = func() end.record() torch.cuda.synchronize() return result, start.elapsed_time(end) / 1000text = \"Hugging Face Transformers are amazing!\"inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")def benchmark(model, inputs, n=2000): times = [] with torch.no_grad(): for _ in range(n): _, elapsed = timed(lambda: model(**inputs)) times.append(elapsed) return sum(times) / n# Baseline: uncompiled modelavg_time_uncompiled = benchmark(model, inputs)print(f\"Average inference time (uncompiled): {avg_time_uncompiled:.6f} seconds\")# Compile the model using torch.compilecompiled_model = torch.compile(model)# Benchmark compiled modelavg_time_compiled = benchmark(compiled_model, inputs)print(f\"Average inference time (compiled): {avg_time_compiled:.6f} seconds\")# Speedup ratiospeedup = avg_time_uncompiled / avg_time_compiledprint(f\"Speedup: {speedup:.2f}x\")\n```\n\nExample Output:\n\n```\nAverage inference time (uncompiled): 0.005221 secondsAverage inference time (compiled): 0.002328 secondsSpeedup: 2.24x\n```\n\n## Compilation Modes in `torch.compile`\n\n`torch.compile` provides four compilation modes:\n\n- **“default”**: The default mode, used when no mode parameter is specified. It provides a good balance between performance and overhead.\n- **“reduce-overhead”**: Minimizes Python-related overhead using CUDA graphs. However, it may increase memory usage.\n- **“max-autotune”**: Uses Triton or template-based matrix multiplications on supported devices. It takes longer to compile but optimizes for the fastest possible execution. On GPUs it enables CUDA graphs by default.\n- **“max-autotune-no-cudagraphs”**: Similar to **“max-autotune”**, but without CUDA graphs.\n\nBy choosing the right mode based on your use case, you can maximize performance while managing memory and compilation time effectively.\n\n## Understanding Compilation and Graph Breaks\n\nA **graph break** occurs in `torch.compile` when torchdynamo working underneath PyTorch compiler cannot capture a portion of the code into an **FX graph** for optimization. This separates the execution into an **optimized (FX graph) part** and a **non-optimized (eager mode) part**. This can significantly reduce the performance benefits of compilation.\n\nThis behavior is documented in the PyTorch Compiler FAQ, it explains that excessive graph breaks hinder performance by preventing full graph capture.\n\n`torch._dynamo.explain` is a tool for identifying all graph breaks in a PyTorch program. It also provides reasons as well for each graph break, helping developers diagnose and fix the underlying issues.\n\nThe following example demonstrates how to analyze a compiled Hugging Face model using `torch._dynamo.explain`. Line `dynamo.explain(cmodel)(**inputs)` runs the compiled model and provides insights into how `torch.compile` interacts with it.\n\nYou can see the output of the below code contains:\n\n- **Graph Count** — Number of compiled graphs.\n- **Graph Break Count** — Number of times graph breaks occurred.\n- **Break Reasons** — Detailed reasons why each break occurred.\n\n```\nimport torch._dynamo as dynamomodel_name = \"microsoft/Phi-4-mini-instruct\"tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to('cuda')cmodel = torch.compile(model)text = \"Hugging Face Transformers are amazing!\"inputs = tokenizer(text, return_tensors=\"pt\").to('cuda')explanation = dynamo.explain(cmodel)(**inputs)print(explanation)\n```\n\nOutput:\n\n```\nGraph Count: 6Graph Break Count: 5Op Count: 1542Break Reasons: Break Reason 1: Reason: Dynamic control flow is not supported at the moment. Please use functorch.experimental.control_flow.cond to explicitly capture the control flow. For more information about this error, see: https://pytorch.org/docs/main/generated/exportdb/index.html#cond-operands User Stack: Break Reason 2: Reason: generic_jump TensorVariable() User Stack: Ops per Graph: Ops 1:.....\n```\n\n## Using `fullgraph=True` to Prevent Graph Breaks\n\nBy default, `torch.compile` allows some graph breaks to ensure correctness. However, if you want to enforce full graph compilation without any breaks, you can use `fullgraph=True ` as parameter in `torch.compile` method. This forces PyTorch to raise an error if a graph break occurs.\n\n## Few Additional Things\n\n**Backend Selection (** `backend` **parameter in** `torch.compile` **)**:\n\n- `backend` can be either a string or a callable function which determines the compilation backend. `\"inductor\"` is the default backend, offering a good balance between performance and overhead. You can also register your custom backend using [this guide](https://pytorch.org/docs/main/torch.compiler_custom_backends.html#registering-custom-backends).\n\n**Optimize Memory Usage**\n\n- Consider using `torch.cuda.empty_cache()` to free up some memory. This will release empty cached memory held by the caching allocator.\n\n**Profiling Model’s Execution**\n\n- Use [PyTorch’s profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html) to identify bottlenecks.\n\n## Conclusion\n\n`torch.compile` can significantly speed up PyTorch code, when used correctly you can maximize performance for both inference and training. Additionally, you can reduce graph breaks to ensure more efficient execution.\n\n[Pytorch](https://medium.com/tag/pytorch?source=post_page-----3d7fd94ce701---------------------------------------)\n\n[Python](https://medium.com/tag/python?source=post_page-----3d7fd94ce701---------------------------------------)\n\n[Optimization](https://medium.com/tag/optimization?source=post_page-----3d7fd94ce701---------------------------------------)\n\n[Llm](https://medium.com/tag/llm?source=post_page-----3d7fd94ce701---------------------------------------)\n\n[Compile](https://medium.com/tag/compile?source=post_page-----3d7fd94ce701---------------------------------------)\n\n[**Written by LambdaFlux**](https://medium.com/@lambdafluxofficial?source=post_page---post_author_info--3d7fd94ce701---------------------------------------)\n\n[3 followers](https://medium.com/@lambdafluxofficial/followers?source=post_page---post_author_info--3d7fd94ce701---------------------------------------)\n\n· [3 following](https://medium.com/@lambdafluxofficial/following?source=post_page---post_author_info--3d7fd94ce701---------------------------------------)\n\n## No responses yet\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----3d7fd94ce701---------------------------------------)\n\n[Status](https://status.medium.com/?source=post_page-----3d7fd94ce701---------------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----3d7fd94ce701---------------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----3d7fd94ce701---------------------------------------)\n\n[Press](mailto:pressinquiries@medium.com)\n\n[Blog](https://blog.medium.com/?source=post_page-----3d7fd94ce701---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3d7fd94ce701---------------------------------------)\n\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----3d7fd94ce701---------------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3d7fd94ce701---------------------------------------)\n\n","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fmedium.com%2F%40lambdafluxofficial%2Feverything-you-need-to-know-about-pytorch-compile-3d7fd94ce701","id":"https://medium.com/@lambdafluxofficial/everything-you-need-to-know-about-pytorch-compile-3d7fd94ce701","publishedDate":"2025-03-23T00:00:00.000Z","summary":"This article discusses `torch.compile()`, a feature in PyTorch 2.0 that optimizes models into more efficient machine code. It explains how to use `torch.compile()` by passing your model or code to it, which returns an optimized version. The article provides an example of compiling a Hugging Face model and includes example output demonstrating a speedup in inference time. It also mentions the four compilation modes available in `torch.compile`: \"default\", \"reduce-overhead\", \"max-autotune\", and \"max-optimizations\".\n","title":"Everything You Need to Know About PyTorch Compile - LambdaFlux - Medium","url":"https://medium.com/@lambdafluxofficial/everything-you-need-to-know-about-pytorch-compile-3d7fd94ce701"},{"content":"[Skip to main content](https://pytorch.org/pytorch.org#ajax-content-wrap)\n\nHit enter to search or ESC to closeSearch\n\n[Close Search](https://pytorch.org/pytorch.org)\n\n[Blog](https://pytorch.org/blog/category/blog/)\n\n# torch.compile and Diffusers: A Hands-On Guide to Peak Performance\n\nBy [Sayak Paul (Hugging Face), Animesh Jain (Meta), Benjamin Bossan (Hugging Face)](https://pytorch.org/pytorch.org)July 17, 2025[No Comments](https://pytorch.org/blog/torch-compile-and-diffusers-a-hands-on-guide-to-peak-performance/#respond)\n\nDiffusers is the [go-to library](https://github.com/huggingface/diffusers/) that provides a unified interface to cutting-edge and open diffusion models for image, video, and audio. Over the past few months, we have deepened its integration with [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html). By tailoring the compilation workflow to the diffusion model architecture, [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) delivers significant speed-ups with minimal impact on user experience. In this post, we will show how to unlock these gains. The target audience for this post is\n\n- **Diffusion-model authors** – Learn the small code changes that make your models compiler-friendly so that end-users can benefit from performance boosts.\n- **Diffusion-model users** – Understand compile time vs. run time trade-offs, how to avoid unnecessary recompilations, and other aspects that can help you choose the right setting. We’ll walk through two community-favorite pipelines from _Diffusers_ to illustrate the payoff.\n\nWhile the examples live in the _Diffusers_ repo, most of the principles apply to other deep learning workloads as well.\n\n**Table of contents**\n\n- Background\n- Use `torch.compile` Effectively For Diffusion Models\n - Vanilla Compilation\n - For Model Authors: Use `fullgraph=True`\n - For Model Users: Use Regional Compilation\n - For Model Users: Reduce Recompilations\n- Extend [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) to Popular Diffusers Features\n - Memory-constrained GPUs\n - LoRA adapters\n- Operational Hardening\n- Conclusion\n- Links to important resources\n\n## Background\n\n[`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) delivers its best gains when you know the model well enough to compile the right sub-modules. In this section, we’ll first outline the factors that shape the [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)user experience, then dissect diffusion architectures to pinpoint which components benefit most from compilation.\n\n### Core Factors for [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) Performance and Usability\n\n[`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) turns a Python program into an optimized graph and then emits machine code, but the speedup and ease of use depends on the following factors:\n\n**Compile latency –** As a JIT compiler, [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) springs into action on the first run, so users experience the compile cost up front. This startup cost could be high, especially with large graphs.\n\n_Mitigation_ _:_ try **regional compilation** to target small, repeated regions. While this may limit the maximum possible speedup compared to compiling the full model, it often strikes a better balance between performance and compile time, so evaluate the trade-off before deciding.\n\n**Graph breaks** — Dynamic Python or unsupported ops slice the Python program into many small graphs, slashing potential speed-ups. Model developers should strive to keep the compute-heavy part of the model free of any graph breaks.\n\n_Mitigation_ _:_ turn on `fullgraph=True`, identify the breaks, and eliminate them while preparing the model.\n\n**Recompilations** – [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) specializes its code to the exact input shapes, so changing the resolution from 512 × 512 to 1024 × 1024 triggers a recompile and the accompanying latency.\n\n_Mitigation_ _:_ enable `dynamic=True` to relax shape constraints. Note that `dynamic=True` works well for diffusion models, but the [recommended](https://docs.pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html#abridged-public-api) way is to use `mark_dynamic` to selectively apply dynamism to your model.\n\n**Device-to-host (DtoH) syncs** can also get in the way of optimal performance **,** but these are non-trivial and have to be treated on a case-by-case basis. Most widely used diffusion pipelines in Diffusers are free of those syncs. Interested readers can check out [this document](https://pytorch.org/blog/presenting-flux-fast-making-flux-go-brrr-on-h100s) to learn more. Since these syncs contribute to little latency increase compared to other mentioned factors, we will not be focusing on them for the rest of this post.\n\n### Diffusion Model Architecture\n\nWe’ll use [**Flux‑1‑Dev**](https://huggingface.co/black-forest-labs/FLUX.1-dev/), an open‑source text‑to‑image model from Black Forest Labs, as our running example. A diffusion pipeline is _not_ a single network; it is a collection of models:\n\n- **Text encoders** – CLIP‑Text and T5 convert the user prompt into embeddings.\n- **Denoiser** – a Diffusion Transformer (DiT) progressively refines a noisy latent, conditioned on those embeddings.\n- **Decoder (VAE)** – transforms the final latent into RGB pixels.\n\nAmong these components, the **DiT** dominates the compute budget. You _could_ compile every component in the pipeline, but that only adds compile latency, recompilations, and potential graph breaks, overheads that barely matter, since these parts already account for a tiny slice of the total runtime. For these reasons, we restrict [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) to the DiT component instead of the entire pipeline.\n\n## Use [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) Effectively For Diffusion Models\n\n### Vanilla Compilation\n\nLet’s establish a baseline that we can incrementally improve, while maintaining a smooth [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) user experience. Load the Flux‑1‑Dev checkpoint and generate an image the usual way:\n\n```\nimport torch\nfrom diffusers import FluxPipeline\n\npipe = FluxPipeline.from_pretrained(\n\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\n\nprompt = \"A cat holding a sign that says hello world\"\nout = pipe(\nprompt=prompt,\nguidance_scale=3.5,\nnum_inference_steps=28,\nmax_sequence_length=512,\n\n).images[0]\n\nout.save(\"image.png\")\n```\n\nNow compile the compute‑heavy **Diffusion Transformer** sub‑module:\n\n```\npipe.transformer.compile(fullgraph=True)\n\n```\n\nThat single line cuts latency on an H100 from 6.7 seconds to 4.5 seconds, achieving roughly **1.5x** speedup, _without_ sacrificing image quality. Under the hood, [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) fuses kernels and removes Python overhead, driving both memory and compute efficiency.\n\n### For Model Authors: Use `fullgraph=True`\n\nThe DiT’s forward pass is structurally simple, so we expect it to form one contiguous graph. This flag instructs [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) to raise an error if any graph break occurs, letting you catch unsupported ops early rather than silently leaving potential performance gains on the table. We recommend that the diffusion model authors set `fullgraph=True` early in the model preparation phase and fix graph breaks. Please refer to the [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) [troubleshooting](https://docs.pytorch.org/docs/stable/torch.compiler_troubleshooting.html) doc and the [manual](https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0) to fix graph breaks.\n\n### For Model Users: Use Regional Compilation\n\nIf you’re following along, you’ll notice the first inference call is very slow, taking 67.4 seconds on an H100 machine. This is the compile overhead. Compilation latency grows with the size of the graph handed to the compiler. One practical way to reduce this cost is to **compile smaller, repeated blocks**, a strategy we call [_regional compilation_](https://docs.pytorch.org/tutorials/recipes/regional_compilation.html).\n\nA DiT is essentially a stack of identical Transformer layers. If we compile one layer once and reuse its kernels for every subsequent layer, we can slash compile time while keeping nearly all of the runtime gains we saw with full‑graph compilation.\n\nDiffusers exposes this via a single-line helper:\n\n```\npipe.transformer.compile_repeated_blocks(fullgraph=True)\n```\n\nOn an H100, this cuts compile latency from **67.4 seconds to 9.6 seconds**, reducing the cold start by **7x** while still delivering the 1.5x runtime speedup achieved by the full model compilation. If you’d like to dive deeper or enable your new model with regional compilation, the implementation discussion lives in [PR](https://github.com/huggingface/diffusers/pull/XXXX).\n\nNote that the compile time numbers above are cold-start measurements: we wiped off the compilation cache with the torch.\\_inductor.utils.fresh\\_inductor\\_cache API, so [`torch.compile`](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) had to start from scratch. Alternatively, in a _warm start_, cached compiler artifacts (stored either on the local disk or on a remote cache) let the compiler skip parts of the compilation process, reducing compile latency. For o","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpytorch.org%2Fblog%2Ftorch-compile-and-diffusers-a-hands-on-guide-to-peak-performance%2F","id":"https://pytorch.org/blog/torch-compile-and-diffusers-a-hands-on-guide-to-peak-performance/","publishedDate":"2025-07-17T00:00:00.000Z","summary":"This blog post provides a hands-on guide to using `torch.compile` with the Diffusers library for diffusion models to achieve peak performance. It targets both diffusion model authors and users, offering insights into optimizing compilation workflows for speed-ups. The post covers topics like effective use of `torch.compile`, regional compilation, reducing recompilations, extending `torch.compile` to popular Diffusers features, and operational hardening.\n","title":"A Hands-On Guide to Peak Performance – PyTorch","url":"https://pytorch.org/blog/torch-compile-and-diffusers-a-hands-on-guide-to-peak-performance/"},{"content":"torch.compile(): A Game-Changer for PyTorch Model Performance torch.compile() is an incredible innovation from the PyTorch team. Think of it as a compiler for neural networks—similar to how GCC works for C and C++ code, but specifically designed for optimizing deep learning models. This breakthrough makes running PyTorch models faster and more efficient than ever before. The best part? Using torch.compile() is remarkably simple. You can apply it to any PyTorch model or Hugging Face model with just a single line of code: import torch model =toch.compile(model) While this one-liner introduces a slight overhead during the compilation process, the benefits far outweigh the cost. The compiled model runs significantly faster, making it ideal for high-performance inference and training scenarios. How Does torch.compile() Speed Up Your Model? We now know that running a model with torch.compile() results in significant speedups. This improvement primarily comes from two key factors: Reducing Python Overhead Optimizing GPU Read/Writes Let’s dive into each of these in detail. Reducing Python Overhead Without torch.compile(), PyTorch operates in eager mode, where the Python interpreter executes code line by line. In this mode, the interpreter doesn’t know what operations will be executed in subsequent lines. This lack of foresight creates inefficiencies, as the code is executed layer by layer, with no opportunity for global optimization. With torch.compile(), however, the entire model is analyzed at once. The compiler inspects all the operations in the model, determining what will happen and when. This global view allows it to optimize the execution process. Unlike eager mode, where execution happens step-by-step with limited foresight, torch.compile() sees the entire model as a whole and eliminates unnecessary steps and that’s how it optimises that process . The first thing torch.compile() does is completely remove the reliance on the Python interpreter during execution. Instead, it compiles the entire neural network into a single, optimized object. By doing so, it avoids the inefficiencies of interpreting Python code and executes the operations in a highly efficient, streamlined manner. This optimization ensures faster execution by reducing the computational overhead associated with Python’s dynamic interpretation, allowing the model to run much more efficiently. Optimizing GPU read/writes To understand how torch.compile() optimizes GPU read/writes, we first need to delve into how GPUs operate and handle data for computations. How GPUs Work Let’s break this down with a simple explanation: Every computer has a CPU, which typically consists of a few powerful cores and is connected to RAM (Random Access Memory). The CPU communicates with RAM to perform operations, a process we are all familiar with. GPUs, however, differ from CPUs in architecture and memory design: Cores: GPUs have many simpler cores compared to the few powerful cores in a CPU. Memory: GPUs use High Bandwidth Memory (HBM), which is faster than traditional DRAM but limited in capacity. Here’s how computations generally occur on a GPU: All the model parameters, activations, and other variables are stored in the GPU’s HBM memory. For any operation to be performed, the required input data must travel from HBM memory to the GPU cores. Inside the GPU, the data moves through caches and registers on the GPU chip for computations. After the operation is completed, the result is written back to HBM memory. This constant data transfer between the HBM and GPU cores is time-consuming and significantly impacts performance. Even though operations within the GPU cores are extremely fast, the memory bandwidth between the HBM and GPU cores often becomes a bottleneck. CPU and GPU Architecture (Image sourced from https://developer.nvidia.com/blog/improving-gpu-memory-oversubscription-performance/) The Challenge: GPU Memory Bandwidth Each GPU has a fixed Memory Bandwidth, which defines the rate at which data can move between its memory (HBM) and its compute cores. This becomes a critical bottleneck for deep learning tasks, which often require frequent memory access for: Loading inputs into the GPU cores Performing calculations on the GPU Writing the outputs back to HBM memory Without torch.compile(), PyTorch processes operations sequentially in eager mode, and this constant back-and-forth creates inefficiencies, significantly increasing the time it takes to complete operations. How torch.compile() Solves This torch.compile() optimizes GPU read/write operations in two key ways: Reducing Memory Transfers: The compiler analyzes the entire computation graph upfront and minimizes unnecessary data transfers between HBM and GPU cores. Kernel Fusion: Multiple operations are fused into a single GPU kernel, reducing the number of trips between memory and compute cores. Kernel Fusion: A Key Optimization Kernel fusion combines multiple operations into a single kernel. Instead of performing multiple back-and-forth memory operations, fused kernels execute all operations in one go. This reduces overhead and improves execution efficiency. Why Is This Important? To better understand the impact, consider the GPU memory hierarchy: HBM: Fast but limited in capacity (~40–80GB). On-Chip Memory: Extremely fast but small (~tens of MBs). Frequent data movement between these memory tiers introduces inefficiencies. torch.compile() addresses this by streamlining data transfers and reducing memory bandwidth bottlenecks. Conclusion torch.compile() is a game-changing feature that allows PyTorch models to run faster by: Reducing Python Overhead: Compiling the entire model into an optimized object eliminates inefficiencies in dynamic interpretation. Optimizing GPU Read/Writes: Techniques like kernel fusion minimize memory transfers and maximize GPU performance. With just one line of code, you can achieve impressive speedups and unlock the full potential of your hardware for deep learning workloads. Whether you’re training massive models or running inference at scale, torch.compile() is a powerful tool you should add to your deep learning arsenal.","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fai.gopubby.com%2Ftorch-compile-how-it-makes-pytorch-models-so-fast-d8362488911f%3Fgi%3D8887959da087","id":"https://ai.gopubby.com/torch-compile-how-it-makes-pytorch-models-so-fast-d8362488911f?gi=8887959da087","publishedDate":"2025-01-15T00:00:00.000Z","summary":"`torch.compile()` is a PyTorch innovation that acts as a compiler for neural networks, optimizing deep learning models for faster and more efficient execution. Applying it to a PyTorch or Hugging Face model is simple: `model = torch.compile(model)`. It speeds up models by reducing Python overhead and optimizing GPU read/writes.  It analyzes the entire model to optimize execution, eliminating unnecessary steps, and compiles the neural network into a single, optimized object, avoiding Python interpretation inefficiencies. GPUs use High Bandwidth Memory (HBM) for faster data access compared to CPU's DRAM.\n","title":"torch.compile(): How It Makes PyTorch Models So Fast?","url":"https://ai.gopubby.com/torch-compile-how-it-makes-pytorch-models-so-fast-d8362488911f?gi=8887959da087"},{"content":"[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe862a963b9e9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40ethanbrooks42%2Fjax-vs-torch-compile-has-pytorch-just-killed-jax-e862a963b9e9&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40ethanbrooks42%2Fjax-vs-torch-compile-has-pytorch-just-killed-jax-e862a963b9e9&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\nPytorch leaving JAX behind\n\n# **JAX vs. torch.compile: Has PyTorch just killed JAX?**\n\n[![Ethan Brooks](https://miro.medium.com/v2/resize:fill:88:88/1*3jPRAbG8ZmVSmIBuRTClKA.png)](https://medium.com/@ethanbrooks42?source=post_page---byline--e862a963b9e9---------------------------------------)\n\n[Ethan Brooks](https://medium.com/@ethanbrooks42?source=post_page---byline--e862a963b9e9---------------------------------------)\n\n·\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fee188fb6cdce%2Fe862a963b9e9&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40ethanbrooks42%2Fjax-vs-torch-compile-has-pytorch-just-killed-jax-e862a963b9e9&user=Ethan+Brooks&userId=ee188fb6cdce&source=post_page-ee188fb6cdce--byline--e862a963b9e9---------------------post_header------------------)\n\n3 min read\n\n·\n\nNov 9, 2024\n\n--\n\nListen\n\nShare\n\nIn the ever-evolving landscape of machine learning frameworks, JAX and PyTorch have long been leading choices for researchers and practitioners. With PyTorch’s latest addition of `torch.compile`, many have started to wonder: Is JAX's performance advantage still relevant, or has PyTorch finally caught up?\n\nLet’s break down what `torch.compile` brings to the table and whether it challenges JAX's dominance.\n\n# PyTorch Strikes Back with `torch.compile`\n\nPyTorch’s introduction of `torch.compile` has certainly generated buzz, as it aims to optimize models for faster execution by combining eager execution with just-in-time (JIT) compilation. This promises significant speedups for PyTorch users, especially those who value ease of use alongside performance gains.\n\nHowever, while `torch.compile` looks promising, it isn't without its limitations. Many users have reported inconsistencies, buggy behaviors, and performance issues in more complex use cases. `torch.compile` can be effective for simpler models, but it often falls short in edge cases like computing higher-order derivatives, which are crucial in fields like reinforcement learning and GANs. For many, these challenges mean that the reliability of `torch.compile` isn't yet at the level needed to fully replace JAX.\n\n# JAX’s Strengths: Not Just About Speed\n\nWhile performance is often the headline, JAX offers more than just raw computational speed. The functional programming approach, together with features like `jit`, `vmap`, and `pmap`, helps users write code that is not only faster but also more modular and clean. These composable transformations make JAX particularly well-suited for scaling experiments, especially in research-heavy fields where reproducibility and scalability are key.\n\nAdditionally, JAX has a natural affinity for TPUs, making it the go-to for large-scale distributed training on Google’s hardware. TPU clusters offer high-speed interconnects that are challenging to replicate elsewhere, and JAX’s design takes full advantage of this hardware. In contrast, while `torch.compile` provides GPU optimization, compatibility issues still arise, particularly when dealing with diverse hardware environments.\n\n# The Road Ahead\n\nThe ongoing rivalry between JAX and PyTorch is a fascinating one, reflecting different philosophies of machine learning development. PyTorch has always prioritized developer friendliness, and `torch.compile` is another step in that direction. It's trying to combine ease of use with improved performance, but as of now, it's not yet the full JAX replacement some might hope for.\n\nJAX continues to shine in domains where complex transformations and distributed training are required. It’s also more lightweight compared to PyTorch’s bulkier infrastructure, which matters for deployment in resource-constrained environments.\n\nSo, has `torch.compile` killed the case for JAX? Not quite yet. While it's an exciting step forward for PyTorch users, JAX remains indispensable for researchers needing cutting-edge performance and flexibility. Both frameworks will likely coexist, each catering to its unique audience, and that's good news for the diversity and progress of machine learning research.\n\n## Conclusion\n\nThe competition between PyTorch and JAX is far from over. As both frameworks continue to evolve, it’s clear that there is room — and need — for both in the machine learning community. PyTorch’s torch.compile is a significant move towards closing the gap, but JAX’s focus on scalability, flexibility, and raw performance keeps it firmly in the game. Rather than one replacing the other, the real advantage lies in having multiple tools that can be leveraged for different tasks, pushing the boundaries of what’s possible in machine learning.\n\nWhich framework do you lean towards for your projects, and why? Share your thoughts below!\n\n[Jax](https://medium.com/tag/jax?source=post_page-----e862a963b9e9---------------------------------------)\n\n[Pytorch](https://medium.com/tag/pytorch?source=post_page-----e862a963b9e9---------------------------------------)\n\n[Compile](https://medium.com/tag/compile?source=post_page-----e862a963b9e9---------------------------------------)\n\n[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----e862a963b9e9---------------------------------------)\n\n[Framework](https://medium.com/tag/framework?source=post_page-----e862a963b9e9---------------------------------------)\n\n[![Ethan Brooks](https://miro.medium.com/v2/resize:fill:96:96/1*3jPRAbG8ZmVSmIBuRTClKA.png)](https://medium.com/@ethanbrooks42?source=post_page---post_author_info--e862a963b9e9---------------------------------------)\n\n[![Ethan Brooks](https://miro.medium.com/v2/resize:fill:128:128/1*3jPRAbG8ZmVSmIBuRTClKA.png)](https://medium.com/@ethanbrooks42?source=post_page---post_author_info--e862a963b9e9---------------------------------------)\n\nFollow\n\n[**Written by Ethan Brooks**](https://medium.com/@ethanbrooks42?source=post_page---post_author_info--e862a963b9e9---------------------------------------)\n\n[4 Followers](https://medium.com/@ethanbrooks42/followers?source=post_page---post_author_info--e862a963b9e9---------------------------------------)\n\n· [2 Following](https://medium.com/@ethanbrooks42/following?source=post_page---post_author_info--e862a963b9e9---------------------------------------)\n\nWorking in the field of AI for over 5 years in different roles, individual contributor, product manager & strategist.\n\nFollow\n\n## No responses yet\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----e862a963b9e9---------------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----e862a963b9e9---------------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----e862a963b9e9---------------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----e862a963b9e9---------------------------------------)\n\n[Press](mailto:pressinquiries@medium.com)\n\n[Blog](https://blog.medium.com/?source=post_page-----e862a963b9e9---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e862a963b9e9---------------------------------------)\n\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----e862a963b9e9---------------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e862a963b9e9---------------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----e862a963b9e9---------------------------------------)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fmedium.com%2F%40ethanbrooks42%2Fjax-vs-torch-compile-has-pytorch-just-killed-jax-e862a963b9e9","id":"https://medium.com/@ethanbrooks42/jax-vs-torch-compile-has-pytorch-just-killed-jax-e862a963b9e9","publishedDate":"2024-11-09T00:00:00.000Z","summary":"This article discusses the competition between JAX and PyTorch, specifically focusing on PyTorch's new `torch.compile` feature. While `torch.compile` aims to optimize models for faster execution, it has limitations such as inconsistencies and performance issues in complex scenarios. JAX, on the other hand, offers advantages beyond speed, including a functional programming approach that promotes modularity and scalability, and strong support for TPUs, making it ideal for large-scale distributed training.\n","title":"JAX vs. torch.compile: Has PyTorch just killed JAX?","url":"https://medium.com/@ethanbrooks42/jax-vs-torch-compile-has-pytorch-just-killed-jax-e862a963b9e9"},{"content":"# Computer Science > Machine Learning\n\n**arXiv:2508.04035** (cs)\n\n\\[Submitted on 6 Aug 2025\\]\n\n# Title:A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs\n\nAuthors: [Zakariya Ba Alawi](https://arxiv.org/search/cs?searchtype=author&query=Alawi,+Z+B)\n\nView a PDF of the paper titled A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs, by Zakariya Ba Alawi\n\n[View PDF](https://arxiv.org/pdf/2508.04035) [HTML (experimental)](https://arxiv.org/html/2508.04035v1)\n\n> Abstract:This paper presents a comprehensive comparative survey of TensorFlow and PyTorch, the two leading deep learning frameworks, focusing on their usability, performance, and deployment trade-offs. We review each framework's programming paradigm and developer experience, contrasting TensorFlow's graph-based (now optionally eager) approach with PyTorch's dynamic, Pythonic style. We then compare model training speeds and inference performance across multiple tasks and data regimes, drawing on recent benchmarks and studies. Deployment flexibility is examined in depth - from TensorFlow's mature ecosystem (TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript support) to PyTorch's newer production tools (TorchScript compilation, ONNX export, and TorchServe). We also survey ecosystem and community support, including library integrations, industry adoption, and research trends (e.g., PyTorch's dominance in recent research publications versus TensorFlow's broader tooling in enterprise). Applications in computer vision, natural language processing, and other domains are discussed to illustrate how each framework is used in practice. Finally, we outline future directions and open challenges in deep learning framework design, such as unifying eager and graph execution, improving cross-framework interoperability, and integrating compiler optimizations (XLA, JIT) for improved speed. Our findings indicate that while both frameworks are highly capable for state-of-the-art deep learning, they exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored in research, whereas TensorFlow provides a fuller production-ready ecosystem - understanding these trade-offs is key for practitioners selecting the appropriate tool. We include charts, code snippets, and more than 20 references to academic papers and official documentation to support this comparative analysis\n\n| | |\n| --- | --- |\n| Comments: | 14 pages, 15 figures, 43 references |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| MSC classes: | 68T05 |\n| ACM classes: | I.2.6; I.2.10 |\n| Cite as: | [arXiv:2508.04035](https://arxiv.org/abs/2508.04035) \\[cs.LG\\] |\n| (or [arXiv:2508.04035v1](https://arxiv.org/abs/2508.04035v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2508.04035](https://doi.org/10.48550/arXiv.2508.04035) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Zakariya Ba Alawi \\[ [view email](https://arxiv.org/show-email/b0dfae6a/2508.04035)\\] **\\[v1\\]**\nWed, 6 Aug 2025 02:55:57 UTC (1,255 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs, by Zakariya Ba Alawi\n\n- [View PDF](https://arxiv.org/pdf/2508.04035)\n- [HTML (experimental)](https://arxiv.org/html/2508.04035v1)\n- [TeX Source](https://arxiv.org/src/2508.04035)\n- [Other Formats](https://arxiv.org/format/2508.04035)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[< prev](https://arxiv.org/prevnext?id=2508.04035&function=prev&context=cs.LG)  \\|  [next >](https://arxiv.org/prevnext?id=2508.04035&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-08](https://arxiv.org/list/cs.LG/2025-08)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2508.04035?context=cs) [cs.AI](https://arxiv.org/abs/2508.04035?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2508.04035)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2508.04035)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2508.04035)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2508.04035) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2508.04035","id":"https://arxiv.org/abs/2508.04035","publishedDate":"2025-08-06T00:00:00.000Z","summary":"This paper is a comparative survey of TensorFlow and PyTorch, the two leading deep learning frameworks. It focuses on their usability, performance, and deployment trade-offs. The paper contrasts TensorFlow's graph-based approach with PyTorch's dynamic style, compares model training speeds and inference performance, and examines deployment flexibility. It also surveys ecosystem and community support, and discusses applications in computer vision and natural language processing. The findings indicate that PyTorch offers simplicity and flexibility, while TensorFlow provides a fuller production-ready ecosystem.\n","title":"A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs","url":"https://arxiv.org/abs/2508.04035"},{"content":"[All Posts](https://apxml.com/posts)\n\n→\n\n[TensorFlow vs PyTorch vs JAX: Performance Benchmark](https://apxml.com/posts/tensorflow-vs-pytorch-vs-jax-performance-benchmark)\n\n# TensorFlow vs PyTorch vs JAX: Performance Benchmark\n\nBy Wei Ming T. on Mar 24, 2025\n\nNeed to cite this page? View citation guide\n\nTo cite this post, please use the following information:\n\n- Author: Thor, Wei Ming\n\n- Page Title: TensorFlow vs PyTorch vs JAX: Performance Benchmark\n\n- Website Name: ApX Machine Learning\n\n- Publication Date: March 24, 2025\n\n- Last Updated: May 25, 2025\n\n- URL: https://apxml.com/posts/tensorflow-vs-pytorch-vs-jax-performance-benchmark\n\n\nMLA (9th edition) Example:\n\n```\nThor, Wei Ming. \"TensorFlow vs PyTorch vs JAX: Performance Benchmark.\" ApX Machine Learning, 24 March 2025, https://apxml.com/posts/tensorflow-vs-pytorch-vs-jax-performance-benchmark.\n```\n\nRecommended Posts\n\n- [10 Reasons to Learn JAX for Machine Learning in 2025](https://apxml.com/posts/reasons-to-learn-jax)\n- [Is TensorFlow Still Relevant in 2025?](https://apxml.com/posts/is-tensorflow-still-relevant)\n- [PyTorch vs. TensorFlow: Comparison for Machine Learning Engineers](https://apxml.com/posts/pytorch-vs-tensorflow-critical-differences)\n- [TensorFlow vs PyTorch: Which Framework to Learn in 2025?](https://apxml.com/posts/tensorflow-vs-pytorch)\n- [How To Select the Correct TensorFlow Version for Your NVIDIA GPU](https://apxml.com/posts/select-tensorflow-version-nvidia-gpu)\n\nRecommended Courses\n\nRelated to this post\n\n[PyTorch for TensorFlow Developers](https://apxml.com/courses/pytorch-for-tensorflow-developers) [Getting Started with TensorFlow](https://apxml.com/courses/getting-started-with-tensorflow) [Getting Started with JAX](https://apxml.com/courses/getting-started-with-jax) [Advanced JAX: Performance, Optimization, and Scale](https://apxml.com/courses/advanced-jax) [Advanced TensorFlow Techniques](https://apxml.com/courses/advanced-tensorflow) [View all courses](https://apxml.com/courses/)\n\nConnect With Us\n\nFollow for updates on AI/ML research and practical tips.\n\n[LinkedIn](https://www.linkedin.com/company/apxml/) [Sponsor Content](https://apxml.com/advertise)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fapxml.com%2Fposts%2Ftensorflow-vs-pytorch-vs-jax-performance-benchmark","id":"https://apxml.com/posts/tensorflow-vs-pytorch-vs-jax-performance-benchmark","publishedDate":"2025-03-24T00:00:00.000Z","summary":"This webpage, \"TensorFlow vs PyTorch vs JAX: Performance Benchmark,\" by Wei Ming T. (March 24, 2025, updated May 25, 2025), provides a performance benchmark of the three machine learning frameworks. It also offers citation information, links to related articles (e.g., \"10 Reasons to Learn JAX for Machine Learning in 2025\") and courses (e.g., \"PyTorch for TensorFlow Developers\"), and links to the author's LinkedIn and sponsor content.\n","title":"TensorFlow vs PyTorch vs JAX: Performance Benchmark","url":"https://apxml.com/posts/tensorflow-vs-pytorch-vs-jax-performance-benchmark"},{"content":"A Comparative Survey of PyTorch vs TensorFlow\nfor Deep Learning: Usability, Performance, and\nDeployment Trade-offs\nZakariya Ba Alawi\nDepartment of Software Engineering\nAlfaisal University, Riyadh, Saudi Arabia\nEmail: zbaalawi@alfaisal.edu\nAbstract—This paper presents a comprehensive comparative\nsurvey of TensorFlow and PyTorch, the two leading deep learning\nframeworks, focusing on their usability, performance, and de\u0002ployment trade-offs. We review each framework’s programming\nparadigm and developer experience, contrasting TensorFlow’s\ngraph-based (now optionally eager) approach with PyTorch’s\ndynamic, Pythonic style [1], [2]. We then compare model training\nspeeds and inference performance across multiple tasks and\ndata regimes, drawing on recent benchmarks and studies [3],\n[4]. Deployment flexibility is examined in depth – from Tensor\u0002Flow’s mature ecosystem (TensorFlow Lite for mobile/embedded,\nTensorFlow Serving, and JavaScript support) [5] to PyTorch’s\nnewer production tools (TorchScript compilation, ONNX export,\nand TorchServe) [6]. We also survey ecosystem and community\nsupport, including library integrations, industry adoption, and\nresearch trends (e.g., PyTorch’s dominance in recent research\npublications [7] versus TensorFlow’s broader tooling in en\u0002terprise). Applications in computer vision, natural language\nprocessing, and other domains are discussed to illustrate how\neach framework is used in practice. Finally, we outline future\ndirections and open challenges in deep learning framework\ndesign, such as unifying eager and graph execution, improving\ncross-framework interoperability, and integrating compiler opti\u0002mizations (XLA, JIT) for improved speed. Our findings indicate\nthat while both frameworks are highly capable for state-of-the\u0002art deep learning, they exhibit distinct trade-offs: PyTorch offers\nsimplicity and flexibility favored in research, whereas TensorFlow\nprovides a fuller production-ready ecosystem – understanding\nthese trade-offs is key for practitioners selecting the appropriate\ntool. We include charts, code snippets, and 20+ references to\nacademic papers and official documentation to support this\ncomparative analysis.\nI. INTRODUCTION\nDeep learning has revolutionized numerous fields by en\u0002abling neural networks to learn from large data. As the field\nmatured, several deep learning frameworks emerged to sim\u0002plify the development of deep models. Among these, Tensor\u0002Flow (released by Google in 2015) [1] and PyTorch (released\nby Facebook in 2016) [2] have become the most popular and\nwidely adopted frameworks in both industry and academia.\nEach framework provides an extensive library of operations\nfor tensor computation and automatic differentiation on CPUs,\nGPUs, and other accelerators, but they differ significantly in\ndesign philosophy and user interface.\nTensorFlow (TF) pioneered a static computational graph\nparadigm in its early versions [8], [9]. Developers would\nFig. 1. Illustration related to Introduction.\nconstruct a graph of tensor operations which could then be\noptimized and executed efficiently. This approach offered\nadvantages in performance and deployment – the entire graph\ncould be serialized, optimized, and run on diverse platforms\n– but it came at the cost of ease of use and flexibil\u0002ity [10]. Writing TensorFlow 1.x code often meant defining\nplaceholders and sessions, which could be less intuitive and\nharder to debug for newcomers. In response to developer\nfeedback and competition, TensorFlow 2.x (released 2019)\nswitched to eager execution by default, essentially adopting\na dynamic graph mode similar to PyTorch. It also integrated\nKeras (a high-level API) to improve usability [11]. Today,\nTensorFlow provides both low-level control (with tf.Graph\nfor constructing static graphs when needed) and high-level\nconvenience through the Keras interface. It remains known for\nits production-grade features and scalability, retaining strong\nsupport for distributed training and optimized deployment on\nvarious hardware (CPUs, GPUs, TPUs) [12].\nPyTorch, on the other hand, was built from the ground\nup with an imperative, define-by-run ethos [2]. Influenced by\nearlier dynamic frameworks like Chainer and Torch7, PyTorch\nintroduced an intuitive Pythonic style where the model’s for\u0002ward pass is executed step-by-step like standard Python code.\nThis design makes debugging natural (using Python’s tools)\narXiv:2508.04035v1 [cs.LG] 6 Aug 2025\nand allows for dynamic model architectures (e.g., models with\ninput-dependent control flow). Initially, some feared that dy\u0002namic execution might sacrifice performance relative to static\ngraph frameworks, but PyTorch demonstrated that with careful\nimplementation and optimized libraries, it could achieve speed\ncomparable to or exceeding static graph frameworks. Indeed,\nPyTorch’s success in combining ease-of-use with high perfor\u0002mance influenced TensorFlow’s evolution – the TensorFlow\nteam incorporated many of PyTorch’s ideas (eager execution,\ntighter integration with Python) in TF 2.0 [13]. Over time,\nPyTorch also expanded its focus from research to production:\nadding tools like TorchScript (to save models in a graph form\nfor C++ deployment) and TorchServe (for serving models at\nscale) [14].\nFig. 2. Example comparative adoption trends for TensorFlow vs PyTorch\n(placeholder chart).\nDespite these convergences, notable differences remain be\u0002tween TensorFlow and PyTorch. Developer preferences of\u0002ten stem from the frameworks’ distinct usability and design\nphilosophies. Furthermore, each framework has continued to\ninnovate: TensorFlow extended into areas like federated learn\u0002ing, graph computation for big data, and TinyML (TensorFlow\nLite on microcontrollers) [15], while PyTorch introduced com\u0002piled mode in PyTorch 2.0 (with ahead-of-time optimization)\nand embraces Python’s ecosystem for things like data loading\nand visualization. As a result, the question “TensorFlow vs\nPyTorch: which to use?” has no simple answer – it depends on\nthe context of use (research prototyping vs production deploy\u0002ment), the specific application domain, and even organizational\nconsiderations (existing codebases, team expertise, etc.).\nThe goal of this paper is to provide a comparative survey\nof TensorFlow and PyTorch across multiple dimensions:\n• Code style and developer experience: We compare how\nmodels are defined and trained in each framework, and\nhow the frameworks differ in terms of learning curve,\nflexibility, and debugging.\n• Performance: Using evidence from literature and bench\u0002marks, we examine training throughput and inference\nlatency for each framework on comparable tasks, as well\nas memory efficiency and scalability to multiple devices.\n• Deployment and production: We survey the tools and\nformats each framework offers for deploying models\n(e.g., TensorFlow Lite, TensorFlow Serving, Tensor\u0002Flow.js for TF; TorchScript, ONNX, and C++ runtime\nfor PyTorch) and discuss interoperability.\n• Ecosystem and community: We compare the ecosys\u0002tems, including third-party libraries (for vision, NLP,\netc.), community support, and the popularity of each\nframework in research and industry.\n• Applications in vision, NLP, etc.: We highlight represen\u0002tative use cases and success stories in different domains,\nillustrating any preferences for one framework over the\nother in those areas.\n• Future directions: We discuss emerging trends (such as\nJAX and novel compiler frameworks) and open chal\u0002lenges that could influence the future development of\nPyTorch and TensorFlow.\nPrior comparative studies: We build upon and update find\u0002ings from earlier works. Several studies (e.g., Bahrampour\net al., 2015; Shi et al., 2016) compared older frameworks\nlike Theano, Torch7, and Caffe [16], [17]. More recently,\nYapıcı and Topaloglu (2021) compared multiple frameworks ˘\n(TensorFlow, Keras, Theano, Torch) on vision tasks, find\u0002ing TensorFlow had the fastest training on small images\nwhile PyTorch excelled on larger images [3]. Novac et al.\n(2022) performed an in-depth TensorFlow vs PyTorch analysis\non convolutional networks, evaluating criteria such as user\u0002friendliness, documentation, training time, and accuracy [4].\nOur survey differs by focusing specifically on the current\nversions of TensorFlow 2.x vs PyTorch 1.x/2.0 (up to 2024),\nand by incorporating the latest performance data and tools\n(e.g., TF’s XLA compiler, PyTorch’s JIT). We also include\nreferences to industry perspectives and recent usage surveys.\nBy aggregating insights from literature and recent experiments,\nthis work aims to serve as a guide for practitioners and\nresearchers to make informed decisions on which framework\nto use and how to leverage their capabilities best.\nThe rest of this paper is organized as follows. In Section II,\nwe summarize key background and comparative studies on\ndeep learning frameworks. Section III compares the program\u0002ming interface and developer experience of TensorFlow vs\nPyTorch. Section IV presents quantitative comparisons of\ntraining and inference performance, including benchmark re\u0002sults and memory/runtime considerations. Section V discusses\ndeployment frameworks and portability. Section VI compares\nthe ecosystem, tooling, and community support. Section VII\nhighlights usage in various application domains. Section VIII\noutlines future directions and challenges. Finally, Section IX\nconcludes the paper.\nII. RELATED WORK\nEarly comparisons of deep learning frameworks date back to\nthe mid-2010s, when neural network libraries such as Theano,\nTorch7, Caffe, and MXNet were prevalent. For example,\nBahrampour et al. (2015) compared Caffe, Theano, and Torch\non vision tasks, and Shi et al. (2016) benchmarked state-of-the\u0002art tools available at that time [16], [17]. These studies gener\u0002ally found that low-level differences (e.g., how efficiently each\nframework utilized CUDA kernels) could lead to performance\nvariations, but many of those frameworks required significant\nboilerplate code and had steep learning curves (Theano’s graph\ndefinition","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.arxiv.org%2Fpdf%2F2508.04035","id":"https://www.arxiv.org/pdf/2508.04035","publishedDate":"2025-08-06T00:00:00.000Z","summary":"This paper provides a comparative survey of TensorFlow and PyTorch, the two leading deep learning frameworks. It focuses on their usability, performance, and deployment trade-offs. The paper contrasts TensorFlow's graph-based approach with PyTorch's dynamic style, compares model training speeds and inference performance, and examines deployment flexibility. It also surveys ecosystem and community support, including library integrations and industry adoption. The findings indicate that PyTorch offers simplicity and flexibility favored in research, while TensorFlow provides a fuller production-ready ecosystem.\n","title":"[PDF] A Comparative Survey of PyTorch vs TensorFlow for Deep Learning","url":"https://www.arxiv.org/pdf/2508.04035"},{"content":"# A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs\n\nZakariya Ba Alawi\nDepartment of Software Engineering\nAlfaisal University, Riyadh, Saudi Arabia\nEmail: zbaalawi@alfaisal.edu\n\n###### Abstract\n\nThis paper presents a comprehensive comparative survey of TensorFlow and PyTorch, the two leading deep learning frameworks, focusing on their usability, performance, and deployment trade-offs. We review each framework’s programming paradigm and developer experience, contrasting TensorFlow’s graph-based (now optionally eager) approach with PyTorch’s dynamic, Pythonic style \\[ [1](https://arxiv.org/html/2508.04035v1#bib.bib1), [2](https://arxiv.org/html/2508.04035v1#bib.bib2)\\]. We then compare model training speeds and inference performance across multiple tasks and data regimes, drawing on recent benchmarks and studies \\[ [3](https://arxiv.org/html/2508.04035v1#bib.bib3), [4](https://arxiv.org/html/2508.04035v1#bib.bib4)\\]. Deployment flexibility is examined in depth – from TensorFlow’s mature ecosystem (TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript support) \\[ [5](https://arxiv.org/html/2508.04035v1#bib.bib5)\\] to PyTorch’s newer production tools (TorchScript compilation, ONNX export, and TorchServe) \\[ [6](https://arxiv.org/html/2508.04035v1#bib.bib6)\\]. We also survey ecosystem and community support, including library integrations, industry adoption, and research trends (e.g., PyTorch’s dominance in recent research publications \\[ [7](https://arxiv.org/html/2508.04035v1#bib.bib7)\\] versus TensorFlow’s broader tooling in enterprise). Applications in computer vision, natural language processing, and other domains are discussed to illustrate how each framework is used in practice. Finally, we outline future directions and open challenges in deep learning framework design, such as unifying eager and graph execution, improving cross-framework interoperability, and integrating compiler optimizations (XLA, JIT) for improved speed. Our findings indicate that while both frameworks are highly capable for state-of-the-art deep learning, they exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored in research, whereas TensorFlow provides a fuller production-ready ecosystem – understanding these trade-offs is key for practitioners selecting the appropriate tool. We include charts, code snippets, and 20+ references to academic papers and official documentation to support this comparative analysis.\n\n## I Introduction\n\nFigure 1: Illustration related to Introduction.\n\nDeep learning has revolutionized numerous fields by enabling neural networks to learn from large data. As the field matured, several deep learning frameworks emerged to simplify the development of deep models. Among these, TensorFlow (released by Google in 2015) \\[ [1](https://arxiv.org/html/2508.04035v1#bib.bib1)\\] and PyTorch (released by Facebook in 2016) \\[ [2](https://arxiv.org/html/2508.04035v1#bib.bib2)\\] have become the most popular and widely adopted frameworks in both industry and academia. Each framework provides an extensive library of operations for tensor computation and automatic differentiation on CPUs, GPUs, and other accelerators, but they differ significantly in design philosophy and user interface.\n\nTensorFlow (TF) pioneered a static computational graph paradigm in its early versions \\[ [8](https://arxiv.org/html/2508.04035v1#bib.bib8), [9](https://arxiv.org/html/2508.04035v1#bib.bib9)\\]. Developers would construct a graph of tensor operations which could then be optimized and executed efficiently. This approach offered advantages in performance and deployment – the entire graph could be serialized, optimized, and run on diverse platforms – but it came at the cost of ease of use and flexibility \\[ [10](https://arxiv.org/html/2508.04035v1#bib.bib10)\\]. Writing TensorFlow 1.x code often meant defining placeholders and sessions, which could be less intuitive and harder to debug for newcomers. In response to developer feedback and competition, TensorFlow 2.x (released 2019) switched to eager execution by default, essentially adopting a dynamic graph mode similar to PyTorch. It also integrated Keras (a high-level API) to improve usability \\[ [11](https://arxiv.org/html/2508.04035v1#bib.bib11)\\]. Today, TensorFlow provides both low-level control (with tf.Graph for constructing static graphs when needed) and high-level convenience through the Keras interface. It remains known for its production-grade features and scalability, retaining strong support for distributed training and optimized deployment on various hardware (CPUs, GPUs, TPUs) \\[ [12](https://arxiv.org/html/2508.04035v1#bib.bib12)\\].\n\nPyTorch, on the other hand, was built from the ground up with an imperative, define-by-run ethos \\[ [2](https://arxiv.org/html/2508.04035v1#bib.bib2)\\]. Influenced by earlier dynamic frameworks like Chainer and Torch7, PyTorch introduced an intuitive Pythonic style where the model’s forward pass is executed step-by-step like standard Python code. This design makes debugging natural (using Python’s tools) and allows for dynamic model architectures (e.g., models with input-dependent control flow). Initially, some feared that dynamic execution might sacrifice performance relative to static graph frameworks, but PyTorch demonstrated that with careful implementation and optimized libraries, it could achieve speed comparable to or exceeding static graph frameworks. Indeed, PyTorch’s success in combining ease-of-use with high performance influenced TensorFlow’s evolution – the TensorFlow team incorporated many of PyTorch’s ideas (eager execution, tighter integration with Python) in TF 2.0 \\[ [13](https://arxiv.org/html/2508.04035v1#bib.bib13)\\]. Over time, PyTorch also expanded its focus from research to production: adding tools like TorchScript (to save models in a graph form for C++ deployment) and TorchServe (for serving models at scale) \\[ [14](https://arxiv.org/html/2508.04035v1#bib.bib14)\\].\n\nFigure 2: Example comparative adoption trends for TensorFlow vs PyTorch (placeholder chart).\n\nDespite these convergences, notable differences remain between TensorFlow and PyTorch. Developer preferences often stem from the frameworks’ distinct usability and design philosophies. Furthermore, each framework has continued to innovate: TensorFlow extended into areas like federated learning, graph computation for big data, and TinyML (TensorFlow Lite on microcontrollers) \\[ [15](https://arxiv.org/html/2508.04035v1#bib.bib15)\\], while PyTorch introduced compiled mode in PyTorch 2.0 (with ahead-of-time optimization) and embraces Python’s ecosystem for things like data loading and visualization. As a result, the question “TensorFlow vs PyTorch: which to use?” has no simple answer – it depends on the context of use (research prototyping vs production deployment), the specific application domain, and even organizational considerations (existing codebases, team expertise, etc.).\n\nThe goal of this paper is to provide a comparative survey of TensorFlow and PyTorch across multiple dimensions:\n\n- •\n\n\nCode style and developer experience: We compare how models are defined and trained in each framework, and how the frameworks differ in terms of learning curve, flexibility, and debugging.\n\n- •\n\n\nPerformance: Using evidence from literature and benchmarks, we examine training throughput and inference latency for each framework on comparable tasks, as well as memory efficiency and scalability to multiple devices.\n\n- •\n\n\nDeployment and production: We survey the tools and formats each framework offers for deploying models (e.g., TensorFlow Lite, TensorFlow Serving, TensorFlow.js for TF; TorchScript, ONNX, and C++ runtime for PyTorch) and discuss interoperability.\n\n- •\n\n\nEcosystem and community: We compare the ecosystems, including third-party libraries (for vision, NLP, etc.), community support, and the popularity of each framework in research and industry.\n\n- •\n\n\nApplications in vision, NLP, etc.: We highlight representative use cases and success stories in different domains, illustrating any preferences for one framework over the other in those areas.\n\n- •\n\n\nFuture directions: We discuss emerging trends (such as JAX and novel compiler frameworks) and open challenges that could influence the future development of PyTorch and TensorFlow.\n\n\nPrior comparative studies: We build upon and update findings from earlier works. Several studies (e.g., Bahrampour et al., 2015; Shi et al., 2016) compared older frameworks like Theano, Torch7, and Caffe \\[ [16](https://arxiv.org/html/2508.04035v1#bib.bib16), [17](https://arxiv.org/html/2508.04035v1#bib.bib17)\\]. More recently, Yapıcı and Topaloğlu (2021) compared multiple frameworks (TensorFlow, Keras, Theano, Torch) on vision tasks, finding TensorFlow had the fastest training on small images while PyTorch excelled on larger images \\[ [3](https://arxiv.org/html/2508.04035v1#bib.bib3)\\]. Novac et al. (2022) performed an in-depth TensorFlow vs PyTorch analysis on convolutional networks, evaluating criteria such as user-friendliness, documentation, training time, and accuracy \\[ [4](https://arxiv.org/html/2508.04035v1#bib.bib4)\\]. Our survey differs by focusing specifically on the current versions of TensorFlow 2.x vs PyTorch 1.x/2.0 (up to 2024), and by incorporating the latest performance data and tools (e.g., TF’s XLA compiler, PyTorch’s JIT). We also include references to industry perspectives and recent usage surveys. By aggregating insights from literature and recent experiments, this work aims to serve as a guide for practitioners and researchers to make informed decisions on which framework to use and how to leverage their capabilities best.\n\nThe rest of this paper is organized as follows. In Section [II](https://arxiv.org/html/2508.04035v1#S2), we summarize key background and comparative studies on deep learning frameworks. Section [III](https://","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fhtml%2F2508.04035v1","id":"https://arxiv.org/html/2508.04035v1","publishedDate":"2025-08-06T00:00:00.000Z","summary":"This paper provides a comparative survey of TensorFlow and PyTorch, the two leading deep learning frameworks. It focuses on their usability, performance, and deployment trade-offs. The paper contrasts TensorFlow's graph-based approach with PyTorch's dynamic style, compares model training speeds and inference performance, and examines deployment flexibility. It also surveys ecosystem and community support, including library integrations and industry adoption. The findings indicate that PyTorch offers simplicity and flexibility favored in research, while TensorFlow provides a fuller production-ready ecosystem.\n","title":"A Comparative Survey of PyTorch vs TensorFlow for Deep Learning","url":"https://arxiv.org/html/2508.04035v1"},{"content":"[Leapcell0.3](https://leapcell.io/)\n\n[Docs](https://docs.leapcell.io) [Pricing](https://leapcell.io/pricing) [Blog](https://leapcell.io/blog) [Contact Us](https://discord.gg/qF7efny8x2)\n\nEN\n\n[Log in](https://leapcell.io/login) [Sign up for FREE](https://leapcell.io/signup)\n\n[Back to Tech Articles](https://leapcell.io/articles)\n\nThis post is also available in [日本語](https://jp.leapcell.io/blog/tensorflow-vs-pytorch-2025-nen-no-hikaku-bunseki), [한국어](https://kr.leapcell.io/blog/tensorflow-dae-pytorch-2025nyeon-bigyo-bunseok), and [Deutsch](https://de.leapcell.io/blog/tensorflow-vs-pytorch-eine-vergleichende-analyse-fuer-2025)\n\n## Key Takeaways\n\n- PyTorch is favored for research and rapid prototyping.\n- TensorFlow excels in large-scale and production deployments.\n- Learning both frameworks is beneficial for AI practitioners.\n\n## Introduction\n\nTensorFlow (by Google) and PyTorch (by Meta AI) remain the two most dominant deep‑learning frameworks in 2025. Both offer powerful tools for designing, training, and deploying neural networks, yet they differ significantly in philosophy, performance, and ecosystem .\n\n## 1\\. Programming Model & Ease of Use\n\n- **PyTorch** uses a dynamic (\"define-by-run\") graph model that feels natural in Python, making it more intuitive and easier to debug. Many users describe it as “Pythonic” and prefer it for rapid prototyping .\n- **TensorFlow**, since version 2.x, supports eager execution too, narrowing the usability gap . However, it still emphasizes static graphs for deployment needs.\n\n## 2\\. Research vs Production\n\n- **PyTorch** dominates in research and academia—around 85% of deep‑learning papers use it . Its flexibility suits experimental architectures and cutting-edge models .\n- **TensorFlow** retains a strong presence in production environments. Its mature tools—TensorFlow Serving, TFLite, TFX—and optimized graph execution give it an edge in deployment scenarios .\n\n## 3\\. Performance & Resource Efficiency\n\n- **Training Speed**: For smaller models, PyTorch often trains faster thanks to less overhead . For larger models and longer runtimes, TensorFlow’s static graph can yield better GPU utilization and memory efficiency .\n- **Resource Consumption**: Studies show TensorFlow (with its XNNPACK back-end) uses less energy than PyTorch in inference on CPU-based platforms .\n\n## 4\\. Ecosystem & Tooling\n\n- **TensorFlow**: Offers production‑focused tools—TensorBoard for visualization, TensorFlow Serving, TensorFlow Lite for mobile, and a rich TFX pipeline .\n- **PyTorch**: Benefits from ONNX for inter-framework compatibility and TorchScript for production deployment . It integrates well with extensions like DeepSpeed for large‑scale training .\n\n## 5\\. Community & Adoption Trends\n\n- **PyTorch** currently leads in research community adoption with 55% of production share in Q3 2025, expected to grow further . It's especially popular in North America and Europe .\n- **TensorFlow** holds steady in enterprise use, especially in large-scale production environments and mobile/edge applications.\n\n## 6\\. Learning Curve & Onboarding\n\n- **PyTorch** is generally easier to pick up for those familiar with Python and scientific computing .\n- **TensorFlow**, despite evolving, still requires understanding its broader ecosystem. Yet, Keras (now framework‑agnostic) offers an easier entry point regardless of which backend is used .\n\n## 7\\. When to Use Which?\n\n| Use Case | Recommended Framework |\n| --- | --- |\n| Research & prototyping | **PyTorch** |\n| Production at scale, especially mobile or enterprise pipelines | **TensorFlow** |\n| Cross-framework compatibility | Either with **ONNX** or **Keras** |\n| Learning AI fundamentals | Start with PyTorch or Keras |\n\nLearning both is usually a smart strategy—TensorFlow equips you with deployment tools, while PyTorch readies you for experimentation and cutting-edge work .\n\n## Conclusion\n\nThere is no definitive “winner”—each framework excels in different domains:\n\n- **Choose PyTorch** for research, agility, and Pythonic clarity.\n- **Choose TensorFlow** for production robustness, deployment tools, and mobile/edge integration.\n\nFor a well‑rounded skill set, start with PyTorch and layer in TensorFlow (via Keras or TFLite) as needed. This flexible foundation prepares you for diverse real‑world AI roles and projects.\n\n## FAQs\n\n### Which framework is better for research?\n\nPyTorch is generally preferred for research due to its flexibility.\n\n### Which is more suitable for production environments?\n\nTensorFlow offers robust tools for production and deployment.\n\n### Should I learn both TensorFlow and PyTorch?\n\nLearning both provides a versatile foundation for diverse AI projects.\n\n### [We are Leapcell, your top choice for hosting backend projects.](https://leapcell.io/?lc_t=blog_p)\n\n[Leapcell](https://leapcell.io/?lc_t=blog_p) is the Next-Gen Serverless Platform for Web Hosting, Async Tasks, and Redis:\n\n**Multi-Language Support**\n\n- Develop with Node.js, Python, Go, or Rust.\n\n**Deploy unlimited projects for free**\n\n- pay only for usage — no requests, no charges.\n\n**Unbeatable Cost Efficiency**\n\n- Pay-as-you-go with no idle charges.\n- Example: $25 supports 6.94M requests at a 60ms average response time.\n\n**Streamlined Developer Experience**\n\n- Intuitive UI for effortless setup.\n- Fully automated CI/CD pipelines and GitOps integration.\n- Real-time metrics and logging for actionable insights.\n\n**Effortless Scalability and High Performance**\n\n- Auto-scaling to handle high concurrency with ease.\n- Zero operational overhead — just focus on building.\n\nExplore more in the [Documentation](https://docs.leapcell.io/)!\n\nFollow us on X: [@LeapcellHQ](https://x.com/LeapcellHQ)\n\n### Share this article\n\n## More Posts from Leapcell\n\n[**Comparing MVC and DDD Layered Architectures in Go: A Detailed Guide**](https://leapcell.io/blog/comparing-mvc-and-ddd-layered-architectures-in-go) [**Understanding PyTorch DataLoader: Efficient Data Loading for Deep Learning**](https://leapcell.io/blog/understanding-pytorch-dataloader-efficient-data-loading-for-deep-learning)\n\n## Popular Posts\n\n- [Clean Architecture in Go Using go-clean-arch](https://leapcell.io/blog/clean-architecture-in-go-using-go-clean-arch)\n- [Why Use Underscore Fields in Go Structs](https://leapcell.io/blog/why-use-underscore-fields-in-go-structs)\n- [any vs interface{} in Go: What’s the Real Difference?](https://leapcell.io/blog/go-any-vs-interface-difference)\n- [16 Bad TypeScript Habits You Need to Break Now](https://leapcell.io/blog/16-bad-typescript-habits-you-need-to-break)\n- [More Posts](https://leapcell.io/articles)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fleapcell.io%2Fblog%2Ftensorflow-vs-pytorch-a-comparative-analysis-for-2025","id":"https://leapcell.io/blog/tensorflow-vs-pytorch-a-comparative-analysis-for-2025","publishedDate":"2025-06-16T00:00:00.000Z","summary":"This webpage provides a comparative analysis of TensorFlow and PyTorch for 2025. Key takeaways include: PyTorch is favored for research and rapid prototyping, while TensorFlow excels in large-scale and production deployments. Both frameworks are dominant in deep learning, but differ in programming model, performance, ecosystem, and community adoption. PyTorch is generally easier to learn and is popular in research, while TensorFlow is strong in production environments. The article recommends learning both frameworks, with PyTorch for research and TensorFlow for production, especially in mobile or enterprise pipelines.\n","title":"TensorFlow vs PyTorch: A Comparative Analysis for 2025 | Leapcell","url":"https://leapcell.io/blog/tensorflow-vs-pytorch-a-comparative-analysis-for-2025"},{"content":"[Back to Articles](https://huggingface.co/blog)\n\nGenerative AI models, such as Stable Diffusion XL (SDXL), enable the creation of high-quality, realistic content with wide-ranging applications. However, harnessing the power of such models presents significant challenges and computational costs. SDXL is a large image generation model whose UNet component is about three times as large as the one in the previous version of the model. Deploying a model like this in production is challenging due to the increased memory requirements, as well as increased inference times. Today, we are thrilled to announce that Hugging Face Diffusers now supports serving SDXL using JAX on Cloud TPUs, enabling high-performance, cost-efficient inference.\n\n[Google Cloud TPUs](https://cloud.google.com/tpu) are custom-designed AI accelerators, which are optimized for training and inference of large AI models, including state-of-the-art LLMs and generative AI models such as SDXL. The new [Cloud TPU v5e](https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-and-a3-gpus-in-ga) is purpose-built to bring the cost-efficiency and performance required for large-scale AI [training](https://cloud.google.com/blog/products/compute/using-cloud-tpu-multislice-to-scale-ai-workloads) and [inference](https://cloud.google.com/blog/products/compute/how-cloud-tpu-v5e-accelerates-large-scale-ai-inference). At less than half the cost of TPU v4, TPU v5e makes it possible for more organizations to train and deploy AI models.\n\n🧨 Diffusers JAX integration offers a convenient way to run SDXL on TPU via [XLA](https://github.com/openxla/xla), and we built a demo to showcase it. You can try it out in [this Space](https://huggingface.co/spaces/google/sdxl) or in the playground embedded below:\n\nUnder the hood, this demo runs on several TPU v5e-4 instances (each instance has 4 TPU chips) and takes advantage of parallelization to serve four large 1024×1024 images in about 4 seconds. This time includes format conversions, communications time, and frontend processing; the actual generation time is about 2.3s, as we'll see below!\n\nIn this blog post,\n\n1. [We describe why JAX + TPU + Diffusers is a powerful framework to run SDXL](http://huggingface.co/huggingface.co#why-jax--tpu-v5e-for-sdxl)\n2. [Explain how you can write a simple image generation pipeline with Diffusers and JAX](http://huggingface.co/huggingface.co#how-to-write-an-image-generation-pipeline-in-jax)\n3. [Show benchmarks comparing different TPU settings](http://huggingface.co/huggingface.co#benchmark)\n\n## Why JAX + TPU v5e for SDXL?\n\nServing SDXL with JAX on Cloud TPU v5e with high performance and cost-efficiency is possible thanks to the combination of purpose-built TPU hardware and a software stack optimized for performance. Below we highlight two key factors: JAX just-in-time (jit) compilation and XLA compiler-driven parallelism with JAX pmap.\n\n#### JIT compilation\n\nA notable feature of JAX is its [just-in-time (jit) compilation](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html). The JIT compiler traces code during the first run and generates highly optimized TPU binaries that are re-used in subsequent calls.\nThe catch of this process is that it requires all input, intermediate, and output shapes to be **static**, meaning that they must be known in advance. Every time we change the shapes\na new and costly compilation process will be triggered again. JIT compilation is ideal for services that can be designed around static shapes: compilation runs once, and then we take advantage of super-fast inference times.\n\nImage generation is well-suited for JIT compilation. If we always generate the same number of images and they have the same size, then the output shapes are constant and known in advance. The text inputs are also constant: by design, Stable Diffusion and SDXL use fixed-shape embedding vectors (with padding) to represent the prompts typed by the user. Therefore, we can write JAX code that relies on fixed shapes, and that can be greatly optimized!\n\n#### High-performance throughput for high batch sizes\n\nWorkloads can be scaled across multiple devices using JAX's [pmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html), which expresses single-program multiple-data (SPMD) programs. Applying pmap to a function will compile a function with XLA, then execute it in parallel on various XLA devices.\nFor text-to-image generation workloads this means that increasing the number of images rendered simultaneously is straightforward to implement and doesn't compromise performance. For example, running SDXL on a TPU with 8 chips will generate 8 images in the same time it takes for 1 chip to create a single image.\n\nTPU v5e instances come in multiple shapes, including 1, 4 and 8-chip shapes, all the way up to 256 chips (a full TPU v5e pod), with ultra-fast ICI links between chips. This allows you to choose the TPU shape that best suits your use case and easily take advantage of the parallelism that JAX and TPUs provide.\n\n## How to write an image generation pipeline in JAX\n\nWe'll go step by step over the code you need to write to run inference super-fast using JAX! First, let's import the dependencies.\n\n```\n# Show best practices for SDXL JAX\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom diffusers import FlaxStableDiffusionXLPipeline\nimport time\n\n```\n\nWe'll now load the base SDXL model and the rest of the components required for inference. The diffusers pipeline takes care of downloading and caching everything for us. Adhering to JAX's functional approach, the model's parameters are returned separately and will have to be passed to the pipeline during inference:\n\n```\npipeline, params = FlaxStableDiffusionXLPipeline.from_pretrained(\n \"stabilityai/stable-diffusion-xl-base-1.0\", split_head_dim=True\n)\n\n```\n\nModel parameters are downloaded in 32-bit precision by default. To save memory and run computation faster we'll convert them to `bfloat16`, an efficient 16-bit representation. However, there's a caveat: for best results, we have to keep the _scheduler state_ in `float32`, otherwise precision errors accumulate and result in low-quality or even black images.\n\n```\nscheduler_state = params.pop(\"scheduler\")\nparams = jax.tree_util.tree_map(lambda x: x.astype(jnp.bfloat16), params)\nparams[\"scheduler\"] = scheduler_state\n\n```\n\nWe are now ready to set up our prompt and the rest of the pipeline inputs.\n\n```\ndefault_prompt = \"high-quality photo of a baby dolphin ​​playing in a pool and wearing a party hat\"\ndefault_neg_prompt = \"illustration, low-quality\"\ndefault_seed = 33\ndefault_guidance_scale = 5.0\ndefault_num_steps = 25\n\n```\n\nThe prompts have to be supplied as tensors to the pipeline, and they always have to have the same dimensions across invocations. This allows the inference call to be compiled. The pipeline `prepare_inputs` method performs all the necessary steps for us, so we'll create a helper function to prepare both our prompt and negative prompt as tensors. We'll use it later from our `generate` function:\n\n```\ndef tokenize_prompt(prompt, neg_prompt):\n prompt_ids = pipeline.prepare_inputs(prompt)\n neg_prompt_ids = pipeline.prepare_inputs(neg_prompt)\n return prompt_ids, neg_prompt_ids\n\n```\n\nTo take advantage of parallelization, we'll replicate the inputs across devices. A Cloud TPU v5e-4 has 4 chips, so by replicating the inputs we get each chip to generate a different image, in parallel. We need to be careful to supply a different random seed to each chip so the 4 images are different:\n\n```\nNUM_DEVICES = jax.device_count()\n# Model parameters don't change during inference,\n# so we only need to replicate them once.\np_params = replicate(params)\ndef replicate_all(prompt_ids, neg_prompt_ids, seed):\n p_prompt_ids = replicate(prompt_ids)\n p_neg_prompt_ids = replicate(neg_prompt_ids)\n rng = jax.random.PRNGKey(seed)\n rng = jax.random.split(rng, NUM_DEVICES)\n return p_prompt_ids, p_neg_prompt_ids, rng\n\n```\n\nWe are now ready to put everything together in a generate function:\n\n```\ndef generate(\n prompt,\n negative_prompt,\n seed=default_seed,\n guidance_scale=default_guidance_scale,\n num_inference_steps=default_num_steps,\n):\n prompt_ids, neg_prompt_ids = tokenize_prompt(prompt, negative_prompt)\n prompt_ids, neg_prompt_ids, rng = replicate_all(prompt_ids, neg_prompt_ids, seed)\n images = pipeline(\n prompt_ids,\n p_params,\n rng,\n num_inference_steps=num_inference_steps,\n neg_prompt_ids=neg_prompt_ids,\n guidance_scale=guidance_scale,\n jit=True,\n ).images\n # convert the images to PIL\n images = images.reshape((images.shape[0] * images.shape[1], ) + images.shape[-3:])\n return pipeline.numpy_to_pil(np.array(images))\n\n```\n\n`jit=True` indicates that we want the pipeline call to be compiled. This will happen the first time we call `generate`, and it will be very slow – JAX needs to trace the operations, optimize them, and convert them to low-level primitives. We'll run a first generation to complete this process and warm things up:\n\n```\nstart = time.time()\nprint(f\"Compiling ...\")\ngenerate(default_prompt, default_neg_prompt)\nprint(f\"Compiled in {time.time() - start}\")\n\n```\n\nThis took about three minutes the first time we ran it.\nBut once the code has been compiled, inference will be super fast. Let's try again!\n\n```\nstart = time.time()\nprompt = \"llama in ancient Greece, oil on canvas\"\nneg_prompt = \"cartoon, illustration, animation\"\nimages = generate(prompt, neg_prompt)\nprint(f\"Inference in {time.time() - start}\")\n\n```\n\nIt now took about 2s to generate the 4 images!\n\n## Benchmark\n\nThe following measures were obtained running SDXL 1.0 base for 20 steps, with the default Euler Discrete scheduler. We compare Cloud TPU v5e with TPUv4 for the same batch sizes. Do note that, due to parallelism, a TPU v5e-4 like the ones we use in our demo will generate **4 images** when using a batch size of 1 (or 8 images with a batch size of 2). Similarly, a TPU v5e-8 will generate ","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fhuggingface.co%2Fblog%2Fsdxl_jax","id":"https://huggingface.co/blog/sdxl_jax","publishedDate":"2024-01-02T00:00:00.000Z","summary":"This blog post announces that Hugging Face Diffusers now supports serving Stable Diffusion XL (SDXL) using JAX on Cloud TPUs, enabling high-performance, cost-efficient inference. The demo runs on TPU v5e-4 instances and generates four 1024x1024 images in about 4 seconds. The post explains why JAX + TPU v5e is a powerful framework for SDXL, highlighting JAX's just-in-time (jit) compilation and XLA compiler-driven parallelism.\n","title":"🧨 Accelerating Stable Diffusion XL Inference with JAX on Cloud TPU v5e","url":"https://huggingface.co/blog/sdxl_jax"},{"content":"[Skip to main content](https://pytorch.org/pytorch.org#ajax-content-wrap)\n\nHit enter to search or ESC to closeSearch\n\n[Close Search](https://pytorch.org/pytorch.org)\n\n[Blog](https://pytorch.org/blog/category/blog/)\n\n# Presenting Flux Fast: Making Flux go brrr on H100s\n\nBy [Joel Schlosser (Meta), Christian Puhrsch (Meta), and Sayak Paul (Hugging Face)](https://pytorch.org/pytorch.org)June 25, 2025July 9th, 2025[No Comments](https://pytorch.org/blog/presenting-flux-fast-making-flux-go-brrr-on-h100s/#respond)\n\nIn our earlier post, [diffusion-fast](https://pytorch.org/blog/accelerating-generative-ai-3/), we showed how the Stable Diffusion XL (SDXL) pipeline can be optimized up to 3x using native PyTorch code. Back then, SDXL was an open SoTA pipeline for image generation. Quite unsurprisingly, a lot has changed since then, and it’s safe to say that [Flux](https://blog.fal.ai/flux-the-largest-open-sourced-text2img-model-now-available-on-fal/) is now one of the most capable open-weight models in the space.\n\nIn this post, we’re excited to show how we enabled ~2.5x speedup on Flux.1-Schnell and Flux.1-Dev using (mainly) pure PyTorch code and a beefy GPU like H100.\n\nIf you cannot wait to get started with the code, you can find the repository [here](https://github.com/huggingface/flux-fast/).\n\n## Overview of the optimizations\n\nThe pipelines shipped in the Diffusers library try to be as `torch.compile`-friendly as possible. This means:\n\n- No graph breaks wherever possible\n- No recompilations wherever possible\n- None-to-minimal CPU<->GPU syncs to reduce inductor cache lookup overhead\n\nTherefore, it already gives us a reasonable starting point. For this project, we took the same underlying principles used in the diffusion-fast project and applied them to the [FluxPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux). Below, we share an overview of the optimizations we applied (details in the [repository](https://github.com/huggingface/flux-fast/)):\n\n- `torch.compile ` with “fullgraph=True” and “max-autotune” mode, ensuring the use of CUDAgraphs\n- Combining q,k,v projections for attention computation. This is particularly helpful during quantization as it thickens the dimensionality, improving compute density\n- `torch.channels_last` memory format for the decoder output\n- [Flash Attention v3 (FA3)](https://pytorch.org/blog/flashattention-3/) with (unscaled) conversion of inputs to `torch.float8_e4m3fn`\n- Dynamic float8 activation quantization and quantization of Linear layer weights via [torchao’s](https://github.com/pytorch/ao) [`float8_dynamic_activation_float8_weight`](https://docs.pytorch.org/ao/main/generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig.html#torchao.quantization.Float8DynamicActivationFloat8WeightConfig)\n- Some flags for tuning Inductor performance on this model:\n - conv\\_1x1\\_as\\_mm = True\n - epilogue\\_fusion = False\n - coordinate\\_descent\\_tuning = True\n - coordinate\\_descent\\_check\\_all\\_directions = True\n- torch.export + Ahead-of-time Inductor (AOTI) + CUDAGraphs\n\nMost of these optimizations are self-explanatory, barring these two:\n\n- Inductor flags. Interested readers can check out [this blog post](https://pytorch.org/blog/accelerating-generative-ai-3) for more details.\n- With AoT compilation, we aim to eliminate the framework overhead and obtain a compiled binary that can be exported through `torch.export` With CUDAGraphs, we want to enable optimization of kernel launches. More details are available in [this post](https://pytorch.org/blog/accelerating-generative-ai-4/).\n\nUnlike LLMs, diffusion models are heavily compute-bound, so optimizations from [gpt-fast](https://pytorch.org/blog/accelerating-generative-ai-2/) don’t exactly carry over here. The figure below shows the impact of each of the optimizations (applied incrementally from left-right) to Flux.1-Schnell on an H100 700W GPU:\n\nFor Flux.1-Dev on H100, we have the following\n\nBelow is a visual comparison of the images obtained with different optimizations applied to Flux.1-Dev:\n\nIt should be noted that only FP8 quantization is lossy in nature so, for most of these optimizations, the image quality should stay identical. However, in this case, we see very negligible differences in the case of FP8.\n\n### Note on CUDA syncs\n\nDuring our investigations, we found out that at the first step of the [denoising loop](https://github.com/huggingface/diffusers/blob/b0f7036d9af75c5df0f39d2d6353964e4c520534/src/diffusers/pipelines/flux/pipeline_flux.py#L900), there’s a CPU<->GPU sync point caused by [this step](https://github.com/huggingface/diffusers/blob/b0f7036d9af75c5df0f39d2d6353964e4c520534/src/diffusers/schedulers/scheduling_flow_match_euler_discrete.py#L355) in the scheduler. We could get rid of it by adding `self.scheduler.set_begin_index(0)` at the beginning of the denoising loop ( [PR](https://github.com/huggingface/diffusers/pull/11696)).\n\nIt actually makes a bigger deal when torch.compile is used, since the CPU has to wait for the sync before it can do a Dynamo cache lookup and then launch instructions on the GPU, and this cache lookup is a bit slow. Hence, the takeaway message is that it’s always wise to [profile](https://docs.pytorch.org/tutorials/recipes/recipes/profiler_recipe.html) your pipeline implementation and to try to eliminate these syncs as much as possible to benefit compilation.\n\n## Conclusion\n\nThe post went over a recipe to optimize Flux for Hopper architectures using native PyTorch code. The recipe tries to balance between simplicity and performance. Other kinds of optimizations are also likely possible (such as using fused MLP kernels and fused adaptive LayerNorm kernels), but for the purpose of simplicity, we didn’t go over them.\n\nAnother crucial point is that GPUs with the Hopper architecture are generally costly. So, to provide reasonable speed-memory trade-offs on consumer GPUs, there are other (often `torch.compile`-compatible) options available in the Diffusers library, too. We invite you to check them [here](https://huggingface.co/docs/diffusers/main/en/optimization/memory) and [here](https://huggingface.co/docs/diffusers/main/en/optimization/fp16).\n\nWe invite you to try these techniques out on other models and share the results. Happy optimizing!\n\n### Docs\n\nAccess comprehensive developer documentation for PyTorch\n\n[View Docs ›](https://pytorch.org/docs)\n\n### Tutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\n[View Tutorials ›](https://pytorch.org/tutorials)\n\n### Resources\n\nFind development resources and get your questions answered\n\n[View Resources ›](https://pytorch.org/resources)\n\n## **Stay in touch** for updates, event info, and the latest news\n\nBy submitting this form, I consent to receive marketing emails from the LF and its projects regarding their events, training, research, developments, and related announcements. I understand that I can unsubscribe at any time using the links in the footers of the emails I receive. [Privacy Policy](https://www.linuxfoundation.org/privacy/).\n\n- [x-twitter](https://twitter.com/pytorch)\n- [facebook](https://www.facebook.com/pytorch)\n- [linkedin](https://www.linkedin.com/company/pytorch)\n- [youtube](https://www.youtube.com/pytorch)\n- [github](https://github.com/pytorch/pytorch)\n- [slack](https://join.slack.com/t/pytorch/shared_invite/zt-2j2la612p-miUinTTaxXczKOJw48poHA)\n- [discord](https://discord.com/invite/eNSRmh92XT)\n- [wechat](https://pytorch.org/wechat/)\n\n© 2025 PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark usage, please see our [Policies](https://www.linuxfoundation.org/legal/policies) page. [Trademark Usage](https://www.linuxfoundation.org/trademark-usage). [Privacy Policy](http://www.linuxfoundation.org/privacy).\n\n[Close Menu](https://pytorch.org/pytorch.org)\n\n[Join us at PyTorch Conference in San Francisco, October 22-23. Register now!](https://events.linuxfoundation.org/pytorch-conference/)\n\n- [Learn](https://pytorch.org/pytorch.org)\n - [Get Started](https://pytorch.org/get-started/locally/)\n - [Tutorials](https://pytorch.org/tutorials/)\n - [Learn the Basics](https://pytorch.org/tutorials/beginner/basics/intro.html)\n - [PyTorch Recipes](https://docs.pytorch.org/tutorials/recipes_index.html)\n - [Intro to PyTorch – YouTube Series](https://pytorch.org/tutorials/beginner/introyt.html)\n - [Webinars](https://pytorch.org/webinars/)\n- [Community](https://pytorch.org/pytorch.org)\n - [Landscape](https://landscape.pytorch.org/)\n - [Join the Ecosystem](https://pytorch.org/join-ecosystem)\n - [Community Hub](https://pytorch.org/community-hub/)\n - [Forums](https://discuss.pytorch.org/)\n - [Developer Resources](https://pytorch.org/resources/)\n - [Contributor Awards](https://pytorch.org/contributor-awards/)\n - [Community Events](https://pytorch.org/community-events/)\n - [PyTorch Ambassadors](https://pytorch.org/programs/ambassadors/)\n- [Projects](https://pytorch.org/pytorch.org)\n - [PyTorch](https://pytorch.org/projects/pytorch/)\n - [vLLM](https://pytorch.org/projects/vllm/)\n - [DeepSpeed](https://pytorch.org/projects/deepspeed/)\n - [Host Your Project](https://pytorch.org/projects/host-your-project/)\n- [Docs](https://pytorch.org/pytorch.org)\n - [PyTorch](https://pytorch.org/docs/stable/index.html)\n - [Domains](https://pytorch.org/domains/)\n- [Blog & News](https://pytorch.org/pytorch.org)\n - [Blog](https://pytorch.org/blog/)\n - [Announcements](https://pytorch.org/announcements)\n - [Case Studies](https://pytorch.org/case-studies/)\n - [Events](https://pytorch.org/events/)\n - [Newsletter](https://pytorch.org/newsletter/)\n- [About](https://pytorch.org/pytorch.org)\n - [PyTorch Foundation](https://pytorch.org/foundation/)\n - [Members](https://pytorch.org/members)\n - [Governing Board](https://pytorch.org/governing-board/)\n - [Technical Advisory Council](https://pytorch.org/tac","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fpytorch.org%2Fblog%2Fpresenting-flux-fast-making-flux-go-brrr-on-h100s%2F","id":"https://pytorch.org/blog/presenting-flux-fast-making-flux-go-brrr-on-h100s/","publishedDate":"2025-06-25T00:00:00.000Z","summary":"This blog post details optimizations that enabled a ~2.5x speedup on Flux.1-Schnell and Flux.1-Dev using PyTorch code and an H100 GPU. The optimizations include torch.compile, combining q,k,v projections, torch.channels_last memory format, Flash Attention v3, dynamic float8 activation quantization, and torch.export + Ahead-of-time Inductor (AOTI) + CUDAGraphs. The post also provides a link to the code repository.\n","title":"Making Flux go brrr on H100s – PyTorch","url":"https://pytorch.org/blog/presenting-flux-fast-making-flux-go-brrr-on-h100s/"},{"content":"B200 SXM6 180GB - available for $3.99/h [Deploy now](https://cloud.datacrunch.io/signin/?utm_source=landing_page&utm_medium=top_banner)\n\n# FLUX on B200 vs H100: Real-Time Image Inference with WaveSpeedAI\n\nApr 8, 2025•Antonio Dominguez•7 min read\n\nShare\n\n[WaveSpeedAI](https://wavespeed.ai/) has teamed up with European GPU cloud provider [DataCrunch](https://datacrunch.io/) to achieve a breakthrough in generative image and video model deployment. By optimizing the open-weight [**FLUX-dev**](https://github.com/black-forest-labs/flux) model on DataCrunch’s cutting-edge NVIDIA [**B200**](https://www.nvidia.com/en-us/data-center/dgx-b200/) GPU, our collaboration delivers up to **6× faster** image inference [compared to industry-standard baselines](https://github.com/sayakpaul/diffusers-torchao).\n\nIn this post, we provide a technical overview of the FLUX-dev model and the B200 GPU, discuss the challenges of scaling FLUX-dev with standard inference stacks, and share benchmark results demonstrating how WaveSpeedAI’s proprietary framework significantly improves latency and cost-efficiency. Enterprise ML teams will learn how this **WaveSpeedAI + DataCrunch** solution translates to faster API responses and significantly reduced cost per image – empowering real-world AI applications. _(WaveSpeedAI was founded by Zeyi Cheng, who leads our mission to accelerate generative AI inference.)_\n\n_This blog is cross-posted to_ [_WaveSpeedAI blog_](https://wavespeed.ai/blog) _._\n\n## FLUX-Dev: SOTA image generation model\n\n[FLUX-dev](https://blackforestlabs.ai/announcing-black-forest-labs/) is a state-of-the-art (SOTA) open-source image generation model capable of text-to-image and image-to-image generation. Its capabilities include good world understanding and prompt adherence (thanks to the [T5](https://arxiv.org/abs/1910.10683) text encoder), style diversity, complex scene semantics and composition understanding. The model output quality is comparable to or can surpass popular close-source models like [Midjourney v6.0](https://updates.midjourney.com/version-6-1/), [DALL·E 3](https://cdn.openai.com/papers/dall-e-3.pdf) (HD), and [SD3-Ultra](https://arxiv.org/pdf/2403.03206). FLUX-dev has rapidly become the most popular image generation model in the open-source community, setting a new benchmark for quality, versatility, and prompt alignment.\n\nFLUX-dev uses [flow matching](https://arxiv.org/abs/2210.02747), and its model architecture is based on a hybrid architecture of [multimodal](https://arxiv.org/abs/2403.03206) and [parallel](https://arxiv.org/abs/2302.05442) [diffusion transformer](https://arxiv.org/abs/2212.09748) blocks. The architecture has **12B parameters**, approximately 33 GB fp16/bf16. Therefore, FLUX-dev is computationally demanding with this large parameter count and iterative diffusion process. Efficient inference is essential for large-scale inference scenarios where user experience is crucial.\n\n## NVIDIA’s Blackwell GPU architecture: B200\n\nBlackwell architecture includes new features such as 5th-generation tensor cores ( **fp8, fp4**), Tensor Memory ( **TMEM**) and CTA pairs ( **2 CTA**).\n\n- **TMEM**: Tensor Memory is a new on-chip memory level, augmenting the traditional hierarchy of registers, shared memory (L1/SMEM), and global memory. In Hopper (e.g. H100), on-chip data was managed via registers (per thread) and shared memory (per thread block or CTA), with high-speed transfers via the Tensor Memory Accelerator ( **TMA**) into shared memory. Blackwell retains those but adds **TMEM as an extra 256 KB of SRAM per SM** dedicated to tensor-core operations. TMEM does not fundamentally change _how_ you write CUDA kernels (the logical algorithm is the same) but adds new tools to **optimize data flow** (see [ThunderKittens Now Optimized for NVIDIA Blackwell GPUs](https://www.together.ai/blog/thunderkittens-nvidia-blackwell-gpus)).\n- **2CTA (CTA Pairs) and Cluster Cooperation**: Blackwell also introduces **CTA pairs** as a way to tightly [couple two CTAs on the same SM](https://www.together.ai/blog/thunderkittens-nvidia-blackwell-gpus). A CTA pair is essentially a cluster of size 2 (two thread blocks scheduled concurrently on one SM with special sync abilities). While Hopper allows up to 8 or 16 CTAs in a cluster to share data via DSM, Blackwell’s CTA pair enables them to **use the tensor cores on common data collectively**. In fact, the Blackwell PTX model allows [**two CTAs to execute tensor core instructions that access each other’s TMEM**](https://www.together.ai/blog/thunderkittens-nvidia-blackwell-gpus).\n- **5th-generation tensor cores (fp8, fp4):** The tensor cores in the B200 are notably larger and **~2–2.5x faster than the tensor cores in the H100**. High tensor core utilization is critical for achieving major new-generation hardware speedups (see [Benchmarking and Dissecting the Nvidia Hopper GPU Architecture](https://arxiv.org/abs/2402.13499)).\n\n**Performance numbers without sparsity**\n\n| **Technical Specifications** |\n| --- |\n| **H100 SXM** | **HGX B200** |\n| **FP16/BF16** | 0.989 PFLOPS | **2.25 PFLOPS** |\n| **INT8** | 1.979 PFLOPS | **4.5 PFLOPS** |\n| **FP8** | 1.979 PFLOPS | **4.5 PFLOPS** |\n| **FP4** | NaN | **9 PFLOPS** |\n| **GPU Memory** | 80 GB HBM3 | **180GB HBM3E** |\n| **GPU Memory Bandwidth** | 3.35 TB/s | **7.7TB/s** |\n| **NVLink bandwidth per GPU** | 900GB/s | **1,800GB/s** |\n\n- [NVIDIA Blackwell Datasheet](https://resources.nvidia.com/en-us-blackwell-architecture/datasheet)\n- [NVIDIA H100 Datasheet](https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet)\n\nOperator level micro-benchmarking of GEMM and attention shows the following:\n\n- BF16 and FP8 cuBLAS, CUTLASS **GEMM** kernels: up to **2x faster** than cuBLAS GEMMs on H100;\n- **Attention:** cuDNN speed is **2x faster** than FA3 on H100.\n\nThe benchmarking results suggest that the B200 is exceptionally well-suited for large-scale AI workloads, especially generative models requiring high memory throughput and dense computing.\n\n## Challenges with Standard Inference Stacks\n\nRunning FLUX-dev on typical inference pipelines (e.g., PyTorch + Hugging Face Diffusers), even on high-end GPUs like H100, presents several challenges:\n\n- **High latency** per image due to CPU-GPU overhead and lack of kernel fusion;\n- **Suboptimal GPU utilization** and idle tensor cores;\n- **Memory and bandwidth bottlenecks** during iterative diffusion steps.\n\nThe optimization objectives of serving large-scale and cheap inference are **higher throughput and lower latency**, reducing the image generation cost.\n\n## WaveSpeedAI’s Proprietary Inference Framework\n\nWaveSpeedAI addresses these bottlenecks with a proprietary framework purpose-built for generative inference. Developed by founder Zeyi Cheng, this framework is our in-house high-performance inference engine optimized specifically for state-of-the-art diffusion transformer models such as **FLUX-dev** and [**Wan 2.1**](https://arxiv.org/abs/2503.20314). Key innovations in the inference engine include:\n\n- **End-to-end GPU execution** eliminating CPU bottlenecks;\n- **Custom CUDA kernels** and **kernel fusion** for optimized execution;\n- **Advanced quantization and mixed precision (BF16/FP8)** using the Blackwell Transformer Engine while maintaining the highest precision;\n- **Optimized memory planning and preallocation**;\n- **Latency-first scheduling mechanisms** that prioritize speed over batching depth.\n\nOur inference engine follows an HW-SW co-design, fully utilizing the B200's compute and memory capacity. It represents a significant leap forward in AI model serving, allowing us to deliver ultra-low latency and high-efficiency inference at the production scale.\n\nWe decompose the problem of efficient AI inference into multiple dimensions, like quantization, kernel optimizations, distributed inference (see [ParaAttention](https://github.com/chengzeyi/ParaAttention)) and algorithmic efficiency using DiT caching — some images require fewer denoising steps to attain a reasonable quality than others (e.g. [AdaCache](https://arxiv.org/html/2411.02397v1)).\n\nWe decompose the problem of efficient AI inference into multiple dimensions, like quantization, compiler level with kernel optimizations, distributed inference (see [ParaAttention](https://github.com/chengzeyi/ParaAttention)) and algorithmic efficiency using DiT caching — some images require fewer denoising steps to attain a reasonable quality than others (e.g. [AdaCache](https://arxiv.org/html/2411.02397v1)).\n\nWe evaluate how these optimizations impact output quality, prioritizing lossless vs. loosely optimizations. **That is, we do not apply optimization that could significantly reduce model capabilities or completely collapse visible output quality, like text rendering and scene semantics.**\n\n## Benchmark: WaveSpeedAI on B200 vs. H100 Baseline\n\n**Model outputs using different optimizations settings:**\n\n_Prompt: photograph of an alternative female with an orange bandana, light brown long hair, clear frame glasses, septum piecing \\[sic\\], beige overalls hanging off one shoulder, a white tube top underneath, she is sitting in her apartment on a bohemian rug, in the style of a vogue magazine shoot_\n\n## Implications\n\nThe performance improvements translate into:\n\n- **AI algorithm design** (e.g. DiT activation caching) and **system optimization**, using GPU architecture-tuned kernels, for better HW utilization;\n- **Reduced inference latency** leading to new possibilities (e.g. [Test-Time Compute in diffusion models](https://www.google.com/search?client=safari&rls=en&q=test+time+scaling+diffusion+models&ie=UTF-8&oe=UTF-8));\n- **Lower cost per image** due to improved efficiency and reduced hardware utilisation.\n\nWe have achieved **B200 equal H100 cost-performance ratio but half the generation latency**. Thus, the cost per generation doesn’t increase while now enabling new real-time possibilities without sacrificing model capabilities. Sometimes more isn’t more b","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdatacrunch.io%2Fblog%2Fflux-on-b200-vs-h100-real-time-image-inference-with-wavespeedai","id":"https://datacrunch.io/blog/flux-on-b200-vs-h100-real-time-image-inference-with-wavespeedai","publishedDate":"2025-04-08T00:00:00.000Z","summary":"WaveSpeedAI and DataCrunch have collaborated to optimize the FLUX-dev image generation model on NVIDIA's B200 GPU, achieving up to 6x faster image inference compared to standard baselines. FLUX-dev is a state-of-the-art open-source model known for its high-quality image generation capabilities. The B200 GPU, with its Blackwell architecture, offers features like 5th-generation tensor cores and Tensor Memory (TMEM) to optimize data flow. This solution aims to provide faster API responses and reduce the cost per image for real-world AI applications.\n","title":"FLUX on B200 vs H100: Real-Time Image Inference with WaveSpeedAI","url":"https://datacrunch.io/blog/flux-on-b200-vs-h100-real-time-image-inference-with-wavespeedai"}],"status":"completed","toolCallId":"call_r0dnfh2K6Afcv1O9RD2Z3BzL","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Short answer: On GPUs, PyTorch 2 with torch.compile (Dynamo + Inductor) is generally on par with, and often faster than, TensorFlow 2/XLA for many research-style models (Transformers, diffusion UNets) because it fuses the “glue” ops aggressively and keeps Python usability. TensorFlow/XLA can still shine in very static, well‑tuned graphs (especially on TPUs) and in production deployments via TFLite/TF Serving. Which is “more efficient” depends on your model, hardware, and how static your shapes/graphs are.\n\nHow they get efficiency\n- PyTorch 2 (torch.compile):\n  - Captures executed Python into FX graphs with guards, then Inductor generates loop-level code (Triton GPU kernels; C++/OpenMP on CPU). This yields strong fusion of epilogues (bias/activation), norms, softmax, masking, residual chains, etc., cutting kernel launches and DRAM traffic.\n  - Handles dynamic shapes/branches via specialization+guards; caches variants, so real workloads with varying seq lengths or image sizes still hit compiled paths.\n  - Plays very well with modern PyTorch kernels (FlashAttention v3, scaled dot-product attention, fused LayerNorm) and CUDA Graphs in max-autotune modes.\n- TensorFlow 2 (tf.function + XLA):\n  - Traces functions to HLO and compiles with XLA. Very strong at whole-graph optimization when shapes are static and control flow is functionalized.\n  - TPU support is first-class; XLA+TPU often yields excellent throughput/cost for large, static training/inference.\n  - Mature mobile/edge stack (TFLite/XNNPACK) is highly optimized for CPU/embedded inference efficiency.\n\nPractical comparisons by workload\n- Transformers (GPU):\n  - Inference/training: PyTorch 2 often achieves 1.3–2× end-to-end over eager PyTorch by fusing softmax/mask/scale, norms, residuals, GELU/SwiGLU epilogues, etc. Against TF/XLA, results vary by model and tuning; in many open benchmarks and user reports, PyTorch 2 matches or edges out TF on NVIDIA GPUs when both are “on” and reasonably tuned.\n  - Highly static, productionized graphs with XLA can be competitive; on TPUs, TF/JAX+XLA typically wins on cost/perf.\n- Diffusion (UNet/DiT on GPU):\n  - PyTorch 2 tends to do very well: Inductor fuses GroupNorm/Silu chains, attention softmax/mask/scale, and scheduler math across T denoising steps. Official PyTorch/HF posts show ~1.3–2.5× speedups vs eager PyTorch on H100; TF/XLA can be strong if the pipeline is refactored for static shapes, but in common Diffusers-style Python code, torch.compile usually achieves bigger wins with less code churn.\n- Large matmul‑dominated segments:\n  - Both frameworks lean on cuBLAS/cuDNN/CUTLASS; differences come from surrounding fusion and launch overhead. Edge goes to the stack that better fuses epilogues and reduces syncs; PyTorch 2 often does.\n- TPU:\n  - TensorFlow (and JAX) + XLA are the native path; they typically deliver better efficiency on TPUs than PyTorch’s experimental TPU paths.\n\nDynamic shapes and control flow\n- PyTorch 2: Specialization with guards plus multiple cached variants makes dynamic seq lengths, image sizes, or conditional branches practical without hand‑rewriting code.\n- TensorFlow: Best results with static shapes; AutoGraph can stage some Python control flow, but frequent shape changes can trigger retracing/compiles or require tf.shape-based coding and careful constraints.\n\nCompilation overhead and developer UX\n- PyTorch 2: One‑liner torch.compile, mixes compiled chunks with Python safely; you can aim for fullgraph=True to catch breaks. Initial compile cost exists; regional compilation and CUDA Graphs help. Good debugging story (dynamo.explain, inspect kernels).\n- TensorFlow: tf.function/XLA prefers graph‑friendly code; you may need to restructure code to avoid retraces and hit fusion patterns. Once stabilized, compile overhead amortizes well.\n\nCPU/edge inference\n- TensorFlow/TFLite with XNNPACK is very efficient on ARM/embedded. For server CPUs, both TF/XLA and PyTorch 2 (Inductor -> C++/OpenMP, MKL/oneDNN, and quantization) can be competitive; leadership depends on kernels and quantization paths for your ops.\n\nEcosystem effects\n- PyTorch: Strong research velocity, many kernels/features land first; recent posts show 1.5–2.5× on diffusion/Flux using torch.compile, CUDA Graphs, FA3, fp8 quantization.\n- TensorFlow: Production tooling (TFX, Serving, TFLite) is mature; on enterprise pipelines and mobile, TF may be more efficient operationally even if raw kernel time is similar.\n\nRules of thumb\n- Use PyTorch 2 if:\n  - You run on NVIDIA GPUs, your model has lots of “glue” ops around big matmuls/convs, and you value minimal code changes with strong fusion.\n  - You have dynamic shapes or Pythonic control flow.\n- Use TensorFlow/XLA if:\n  - You run primarily on TPUs.\n  - You deploy to mobile/edge with TFLite, or have static, productionized graphs benefiting from XLA’s whole-graph passes.\n\nIf you share your exact model (e.g., ViT/Transformer/DiT, batch/seq length or resolution), hardware (GPU/TPU/CPU), and whether shapes vary at runtime, I can recommend settings (torch.compile modes, TF/XLA flags) and estimate which stack will be more efficient for your case.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d196344ec881a29864b134e6ff11dd0dab0213987f3d24"}},"resumableStreamId":"STREAM:f7b0d4a3-8af1-4a8e-8251-06d742e7ad37","role":"assistant","status":"done","threadId":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","timeToFirstToken":2.823,"tokens":1237,"tokensPerSecond":27.708711332123737,"updated_at":1758565923710,"userId":"google:104458561188320708185","id":"j977jgw45hgnewrzk2abap3vts7r3w4g"},{"_creationTime":1758566146063.7205,"_id":"j972839nbe0v14qanfd3kktnfh7r2z0t","attachmentIds":[],"content":"how does pytorch works with existing distributive computing methods? or does it have it's own?","created_at":1758566146064,"messageId":"84a06246-107d-4ae5-bbc6-732f19748b04","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"role":"user","status":"done","threadId":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","updated_at":1758566146064,"userId":"google:104458561188320708185","id":"j972839nbe0v14qanfd3kktnfh7r2z0t","providerMetadata":{}},{"_creationTime":1758566146063.7207,"_id":"j976103t3mhjmax9t3c0fkjrcx7r313m","attachmentIds":[],"byok":false,"content":"PyTorch has its own distributed stack, but it’s designed to interoperate with common cluster/orchestration systems. You can use PyTorch-native APIs for data/model/optimizer parallelism, and run them under Slurm, Kubernetes, Ray, MPI, or cloud services. Here’s the landscape and how they fit together.\n\nCore PyTorch distributed building blocks\n- torch.distributed (ProcessGroup + backends): The foundation. Spawns N processes (typically 1 per GPU) and provides collectives (all_reduce, broadcast, all_gather, etc.).\n  - Backends: NCCL (GPUs), Gloo (CPU), MPI (via system MPI).\n  - Launchers: torchrun (recommended), torch.distributed.run, or you can integrate with Slurm srun, mpirun, etc.\n- DistributedDataParallel (DDP): Data-parallel training across GPUs/nodes. Each process holds a replica of the model; gradients are synchronized via all-reduce. This is the default for most workloads.\n- Fully Sharded Data Parallel (FSDP): Shards model parameters, gradients, and optimizer states across processes to fit larger models and reduce memory. Reconstructs weights just-in-time for compute; overlaps comms with compute.\n- Tensor parallelism: Split layers’ tensors (weights/activations) across processes for very large models. PyTorch provides primitives (RPC, DTensor, ProcessGroup), and higher-level libs like TorchDistX, Megatron-LM, and vLLM implement patterns.\n- Pipeline parallelism: Split the model by layers across stages and pipeline microbatches. Implemented in torch.distributed.pipeline (experimental in some releases) and in libraries like Megatron-LM, FairScale, DeepSpeed.\n- Distributed autograd/RPC: For model-parallel patterns spanning machines with non-collective comms. torch.distributed.rpc enables parameter-server-like or actor patterns.\n- DTensor/DeviceMesh: A newer abstraction to express sharding/replication layouts once and compile to efficient collectives, enabling 2D/3D hybrid parallelism (data + tensor + pipeline).\n- Checkpointing and elastic training: torch.distributed.elastic (torchelastic) handles node failures, autoscaling, re-rendezvous.\n\nHow PyTorch 2’s compiler fits (torch.compile)\n- Works with DDP/FSDP. Each rank compiles its local model segments. Graph-capture respects collective boundaries; communication ops are graph breaks (run as usual), while compute-heavy regions get compiled/fused.\n- With FSDP, compilation happens on the wrapped module segments; you can compile before or after wrapping depending on requirements (common practice: wrap model, then call torch.compile on the wrapper or on submodules).\n- CUDA Graphs and communication overlap: Inductor-generated kernels play fine with CUDA Graphs; overlapping NCCL comms with compute is still driven by your parallelism strategy (DDP bucketing, FSDP overlapping, gradient accumulation).\n\nInteroperability with external distributed systems\n- Slurm/LSF/PBS: Use torchrun within job scripts; environment variables (MASTER_ADDR, MASTER_PORT, RANK, WORLD_SIZE) are picked up automatically. You can also use srun to launch 1 process per GPU.\n- Kubernetes: Use Kubernetes Jobs/Pods or Kubeflow; torchrun inside containers. Operator frameworks (KubeFlow’s PyTorchJob) handle rendezvous and scaling.\n- MPI: Use the MPI backend or just use mpirun to launch torchrun processes; NCCL usually outperforms MPI for GPU collectives, so many stick with NCCL for performance and use MPI only for launch.\n- Ray: Ray Train integrates with PyTorch DDP/FSDP; Ray handles cluster management, fault tolerance, and data loading sharding.\n- DeepSpeed: A popular library layering on PyTorch for ZeRO sharding (similar goals to FSDP), pipeline/tensor parallelism, activation checkpointing, optimizer/communication optimizations. You can mix torch.compile with DeepSpeed in many cases; check for version compatibility.\n- Hugging Face Accelerate: Simplifies launching DDP/FSDP/DeepSpeed across machines/GPUs; still uses PyTorch under the hood.\n- Triton Inference Server, TorchServe: For distributed inference, you can deploy compiled PyTorch models and scale horizontally; batching/scheduling handled by the serving layer.\n\nChoosing a parallelism strategy\n- Small to medium models that fit per GPU: Use DDP (fast, simple). Tune gradient bucketing (bucket_cap_mb), use fp16/bf16, and enable overlap (ddp_static_graph=True when applicable).\n- Very large models (OOM with DDP): Use FSDP or DeepSpeed ZeRO. Start with FSDP full-shard or hybrid sharding; use CPU offload/NVMe offload only if needed.\n- Extremely large models (LLMs): Combine FSDP/ZeRO with tensor parallel (Megatron-style) and possibly pipeline parallel. DTensor/DeviceMesh aims to make these hybrid layouts more straightforward.\n- Sequence-parallel variants (specific to some LLM stacks): Partition sequence dimension to reduce activation memory; available in Megatron/DeepSpeed integrations.\n- Inference at scale: Use torch.compile for per-rank kernel fusion; combine with tensor/replica sharding for memory, and a serving layer for request batching. For multi-node attention-heavy models, consider paged KV cache libraries (e.g., vLLM) that integrate with PyTorch.\n\nPerformance tips\n- Use NCCL for GPU collectives; set NCCL env vars appropriately (NCCL_P2P_DISABLE, NCCL_IB_HCA, NCCL_SOCKET_IFNAME) for your fabric.\n- Pin one process per GPU; set num_workers and prefetching in DataLoader to avoid IO bottlenecks.\n- Overlap comms and compute:\n  - DDP gradient bucketing and find_unused_parameters=False when possible.\n  - FSDP with forward/backward prefetch and use_orig_params (newer APIs may vary).\n- Mixed precision (AMP) with GradScaler for training; bf16 on Ampere+ if supported.\n- Checkpoint sharding: Use PyTorch’s distributed checkpointing APIs to save/load sharded states efficiently.\n- Profiling: torch.profiler with distributed view, NCCL debug tools, and tensorboard plugin to pinpoint communication hot spots.\n\nFault tolerance and elasticity\n- Use torch.distributed.elastic (torchelastic) or Ray for elastic job management. Supports worker restarts, membership changes, and checkpoint-based progress recovery.\n- For preemption-prone environments, checkpoint frequently and ensure your data loader and sampler handle rank changes (DistributedSampler with drop_last, seed setting).\n\nBottom line\n- Yes, PyTorch has its own distributed computing methods (DDP, FSDP, RPC/DTensor, etc.), and they are first-class.\n- It also plays nicely with existing distributed ecosystems: you can launch under Slurm/K8s/MPI/Ray and mix in libraries like DeepSpeed or Megatron for specialized parallelism.\n- PyTorch 2’s compiler accelerates the per-process compute without changing how you orchestrate distributed training/inference.","created_at":1758566146065,"messageId":"0ea112c2-bd6a-49c5-b969-ea53fb5c4d0f","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"high"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"PyTorch has its own distributed stack, but it’s designed to interoperate with common cluster/orchestration systems. You can use PyTorch-native APIs for data/model/optimizer parallelism, and run them under Slurm, Kubernetes, Ray, MPI, or cloud services. Here’s the landscape and how they fit together.\n\nCore PyTorch distributed building blocks\n- torch.distributed (ProcessGroup + backends): The foundation. Spawns N processes (typically 1 per GPU) and provides collectives (all_reduce, broadcast, all_gather, etc.).\n  - Backends: NCCL (GPUs), Gloo (CPU), MPI (via system MPI).\n  - Launchers: torchrun (recommended), torch.distributed.run, or you can integrate with Slurm srun, mpirun, etc.\n- DistributedDataParallel (DDP): Data-parallel training across GPUs/nodes. Each process holds a replica of the model; gradients are synchronized via all-reduce. This is the default for most workloads.\n- Fully Sharded Data Parallel (FSDP): Shards model parameters, gradients, and optimizer states across processes to fit larger models and reduce memory. Reconstructs weights just-in-time for compute; overlaps comms with compute.\n- Tensor parallelism: Split layers’ tensors (weights/activations) across processes for very large models. PyTorch provides primitives (RPC, DTensor, ProcessGroup), and higher-level libs like TorchDistX, Megatron-LM, and vLLM implement patterns.\n- Pipeline parallelism: Split the model by layers across stages and pipeline microbatches. Implemented in torch.distributed.pipeline (experimental in some releases) and in libraries like Megatron-LM, FairScale, DeepSpeed.\n- Distributed autograd/RPC: For model-parallel patterns spanning machines with non-collective comms. torch.distributed.rpc enables parameter-server-like or actor patterns.\n- DTensor/DeviceMesh: A newer abstraction to express sharding/replication layouts once and compile to efficient collectives, enabling 2D/3D hybrid parallelism (data + tensor + pipeline).\n- Checkpointing and elastic training: torch.distributed.elastic (torchelastic) handles node failures, autoscaling, re-rendezvous.\n\nHow PyTorch 2’s compiler fits (torch.compile)\n- Works with DDP/FSDP. Each rank compiles its local model segments. Graph-capture respects collective boundaries; communication ops are graph breaks (run as usual), while compute-heavy regions get compiled/fused.\n- With FSDP, compilation happens on the wrapped module segments; you can compile before or after wrapping depending on requirements (common practice: wrap model, then call torch.compile on the wrapper or on submodules).\n- CUDA Graphs and communication overlap: Inductor-generated kernels play fine with CUDA Graphs; overlapping NCCL comms with compute is still driven by your parallelism strategy (DDP bucketing, FSDP overlapping, gradient accumulation).\n\nInteroperability with external distributed systems\n- Slurm/LSF/PBS: Use torchrun within job scripts; environment variables (MASTER_ADDR, MASTER_PORT, RANK, WORLD_SIZE) are picked up automatically. You can also use srun to launch 1 process per GPU.\n- Kubernetes: Use Kubernetes Jobs/Pods or Kubeflow; torchrun inside containers. Operator frameworks (KubeFlow’s PyTorchJob) handle rendezvous and scaling.\n- MPI: Use the MPI backend or just use mpirun to launch torchrun processes; NCCL usually outperforms MPI for GPU collectives, so many stick with NCCL for performance and use MPI only for launch.\n- Ray: Ray Train integrates with PyTorch DDP/FSDP; Ray handles cluster management, fault tolerance, and data loading sharding.\n- DeepSpeed: A popular library layering on PyTorch for ZeRO sharding (similar goals to FSDP), pipeline/tensor parallelism, activation checkpointing, optimizer/communication optimizations. You can mix torch.compile with DeepSpeed in many cases; check for version compatibility.\n- Hugging Face Accelerate: Simplifies launching DDP/FSDP/DeepSpeed across machines/GPUs; still uses PyTorch under the hood.\n- Triton Inference Server, TorchServe: For distributed inference, you can deploy compiled PyTorch models and scale horizontally; batching/scheduling handled by the serving layer.\n\nChoosing a parallelism strategy\n- Small to medium models that fit per GPU: Use DDP (fast, simple). Tune gradient bucketing (bucket_cap_mb), use fp16/bf16, and enable overlap (ddp_static_graph=True when applicable).\n- Very large models (OOM with DDP): Use FSDP or DeepSpeed ZeRO. Start with FSDP full-shard or hybrid sharding; use CPU offload/NVMe offload only if needed.\n- Extremely large models (LLMs): Combine FSDP/ZeRO with tensor parallel (Megatron-style) and possibly pipeline parallel. DTensor/DeviceMesh aims to make these hybrid layouts more straightforward.\n- Sequence-parallel variants (specific to some LLM stacks): Partition sequence dimension to reduce activation memory; available in Megatron/DeepSpeed integrations.\n- Inference at scale: Use torch.compile for per-rank kernel fusion; combine with tensor/replica sharding for memory, and a serving layer for request batching. For multi-node attention-heavy models, consider paged KV cache libraries (e.g., vLLM) that integrate with PyTorch.\n\nPerformance tips\n- Use NCCL for GPU collectives; set NCCL env vars appropriately (NCCL_P2P_DISABLE, NCCL_IB_HCA, NCCL_SOCKET_IFNAME) for your fabric.\n- Pin one process per GPU; set num_workers and prefetching in DataLoader to avoid IO bottlenecks.\n- Overlap comms and compute:\n  - DDP gradient bucketing and find_unused_parameters=False when possible.\n  - FSDP with forward/backward prefetch and use_orig_params (newer APIs may vary).\n- Mixed precision (AMP) with GradScaler for training; bf16 on Ampere+ if supported.\n- Checkpoint sharding: Use PyTorch’s distributed checkpointing APIs to save/load sharded states efficiently.\n- Profiling: torch.profiler with distributed view, NCCL debug tools, and tensorboard plugin to pinpoint communication hot spots.\n\nFault tolerance and elasticity\n- Use torch.distributed.elastic (torchelastic) or Ray for elastic job management. Supports worker restarts, membership changes, and checkpoint-based progress recovery.\n- For preemption-prone environments, checkpoint frequently and ensure your data loader and sampler handle rank changes (DistributedSampler with drop_last, seed setting).\n\nBottom line\n- Yes, PyTorch has its own distributed computing methods (DDP, FSDP, RPC/DTensor, etc.), and they are first-class.\n- It also plays nicely with existing distributed ecosystems: you can launch under Slurm/K8s/MPI/Ray and mix in libraries like DeepSpeed or Megatron for specialized parallelism.\n- PyTorch 2’s compiler accelerates the per-process compute without changing how you orchestrate distributed training/inference.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_68d19702dd4c819eb9dd652877131d7c070dd72a102bbf39"}},"resumableStreamId":"STREAM:67ed84b3-3323-4b49-9629-85e003ed375a","role":"assistant","status":"done","threadId":"df1bd45e-cfae-43ad-9664-b81b6e7ef1c5","timeToFirstToken":0.766,"tokens":1484,"tokensPerSecond":58.952051801533386,"updated_at":1758566146065,"userId":"google:104458561188320708185","id":"j976103t3mhjmax9t3c0fkjrcx7r313m"}],"version":"6.0.2"}