{"threads":[{"_creationTime":1757968805078.5771,"_id":"jd77vkyksd54jdqfq0sxfvph8n7qm4kc","createdAt":1757968805078,"generationStatus":"completed","lastMessageAt":1757969303181,"model":"gpt-5-chat","pinned":false,"threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","title":"Relay Compiler Summary","updatedAt":1757969303181,"userId":"google:104458561188320708185","userSetTitle":false,"visibility":"visible","id":"366d04f1-b26e-4f64-b89c-62575b2136c0","last_message_at":1757969303181,"created_at":1757968805078,"updated_at":1757969303181,"status":"completed","user_edited_title":false}],"messages":[{"_creationTime":1757968805078.5774,"_id":"j978tacjwfy7b215xqr1r0ybys7qnbvn","attachmentIds":[],"content":"https://arxiv.org/abs/1904.08368\nGenerate a summary for this paper:\nRelay: A High-Level Compiler for Deep Learning","created_at":1757968805078,"messageId":"edd26d81-7cb1-4444-9d9e-a20ff5be82a2","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","updated_at":1757968805078,"userId":"google:104458561188320708185","id":"j978tacjwfy7b215xqr1r0ybys7qnbvn","providerMetadata":{}},{"_creationTime":1757968805078.5776,"_id":"j972h1rmv1r03wedw1d6wqq9x97qmk1f","attachmentIds":[],"byok":false,"content":"Here’s a concise summary of “Relay: A High-Level Compiler for Deep Learning” (arXiv:1904.08368):\n\nProblem\n- Existing DL systems split between dynamic, eager frameworks (flexible but harder to optimize) and static graph compilers (optimizable but rigid).\n- Operator-centric IRs limit whole-program optimization, new op extensibility, and cross-hardware portability.\n\nCore Idea\n- Relay is a high-level, functional intermediate representation (IR) and compiler for deep learning that unifies flexibility and optimization.\n- It treats models as first-class programs (not just operator DAGs), enabling whole-program analysis and transformations while preserving the ability to express dynamic control flow.\n\nKey Design Features\n- Functional, statically typed IR:\n  - First-class functions, higher-order programs, closures.\n  - Shape-dependent types and type inference to reason about tensor shapes statically where possible.\n- Expressivity for dynamic models:\n  - Supports data-dependent control flow, recursion, and dynamic shapes while remaining optimizable.\n- Extensibility:\n  - New operators and user-defined functions can be added in the same IR, avoiding hard-coded operator catalogs.\n- Separation of concerns:\n  - High-level Relay IR lowered into a tensor-level IR (TVM/TIR) for scheduling and codegen.\n  - Clean layering enables both graph-level and kernel-level optimizations.\n\nCompiler Pipeline and Optimizations\n- Frontends import from mainstream frameworks.\n- High-level passes: inlining, dead-code elimination, common subexpression elimination, algebraic simplifications, constant folding, automatic differentiation (source-to-source AD).\n- Shape and type inference, effect/alias analysis to enable safe transformations.\n- Operator fusion and layout transformations at the IR level.\n- Lowering to TVM’s tensor IR for auto-scheduling, hardware-specific optimizations, and code generation across CPUs, GPUs, and specialized accelerators.\n\nAutomatic Differentiation\n- Built-in, first-class AD via IR transformations (not baked into ops), which:\n  - Works with higher-order functions and control flow.\n  - Enables whole-program differentiation and optimization across training and inference.\n\nPortability and Performance\n- Couples Relay’s program-level optimizations with TVM’s backend to target diverse hardware.\n- Demonstrates that a high-level, programmatic IR can retain the flexibility of dynamic frameworks while unlocking static-compiler optimizations and cross-hardware performance.\n\nContributions\n- A principled, functional, shape-aware IR for deep learning programs.\n- A unified compiler stack integrating program-level and tensor-level optimizations.\n- First-class AD and dynamic control flow support within an optimizable IR.\n- Practical integration with TVM for end-to-end deployment.\n\nImpact\n- Influenced subsequent IR design across DL compilers (e.g., richer type/shape systems, program-level AD, and multi-level IR stacks).\n- Helped move the ecosystem beyond fixed operator graphs toward more general, optimizable programming models for DL.","created_at":1757968805079,"messageId":"b39da28f-ef02-4479-9fcf-2af5c903c285","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"research paper","domains":["arxiv.org","tvm.apache.org","proceedings.mlr.press"],"queries":["arXiv 1904.08368 Relay: A High-Level Compiler for Deep Learning paper summary 2019","Relay high-level IR compiler deep learning TVM arXiv:1904.08368 authors date key contributions"]},"result":[{"content":"# Computer Science\n\n## Authors and titles for April 2019\n\nTotal of 4942 entries : 1911-3910 [2001-4000](https://arxiv.org/list/cs/2019-04?skip=2000&show=2000) [4001-4942](https://arxiv.org/list/cs/2019-04?skip=4000&show=2000)\n\nShowing up to 2000 entries per page:\n[fewer](https://arxiv.org/list/cs/2019-04?skip=1910&show=1000)\n\\|\nmore\n\\|\nall\n\n\\[1911\\] [arXiv:1904.05658](https://arxiv.org/abs/1904.05658)\\[ [pdf](https://arxiv.org/pdf/1904.05658), [other](https://arxiv.org/format/1904.05658)\\]\n\nTitle:\nMxML: Mixture of Meta-Learners for Few-Shot Classification\n\n[Minseop Park](https://arxiv.org/search/cs?searchtype=author&query=Park,+M), [Jungtaek Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim,+J), [Saehoon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim,+S), [Yanbin Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+Y), [Seungjin Choi](https://arxiv.org/search/cs?searchtype=author&query=Choi,+S)\n\nComments:\n12 pages\n\nSubjects:Machine Learning (cs.LG); Machine Learning (stat.ML)\n\n\\[1912\\] [arXiv:1904.05664](https://arxiv.org/abs/1904.05664)\\[ [pdf](https://arxiv.org/pdf/1904.05664), [other](https://arxiv.org/format/1904.05664)\\]\n\nTitle:\nRDNA Balance: Load Balancing by Isolation of Elephant Flows using Strict Source Routing\n\n[Rodolfo V. Valentim](https://arxiv.org/search/cs?searchtype=author&query=Valentim,+R+V), [Rodolfo S. Villaca](https://arxiv.org/search/cs?searchtype=author&query=Villaca,+R+S), [Moises R. N. Ribeiro](https://arxiv.org/search/cs?searchtype=author&query=Ribeiro,+M+R+N), [Magnos Martinello](https://arxiv.org/search/cs?searchtype=author&query=Martinello,+M), [Cristina K. Dominicini](https://arxiv.org/search/cs?searchtype=author&query=Dominicini,+C+K), [Diego R. Mafioletti](https://arxiv.org/search/cs?searchtype=author&query=Mafioletti,+D+R)\n\nComments:\n3 pages, 4 figures\n\nSubjects:Networking and Internet Architecture (cs.NI)\n\n\\[1913\\] [arXiv:1904.05667](https://arxiv.org/abs/1904.05667)\\[ [pdf](https://arxiv.org/pdf/1904.05667), [other](https://arxiv.org/format/1904.05667)\\]\n\nTitle:\nViTa-SLAM: Biologically-Inspired Visuo-Tactile SLAM\n\n[Oliver Struckmeier](https://arxiv.org/search/cs?searchtype=author&query=Struckmeier,+O), [Kshitij Tiwari](https://arxiv.org/search/cs?searchtype=author&query=Tiwari,+K), [Martin J. Pearson](https://arxiv.org/search/cs?searchtype=author&query=Pearson,+M+J), [Ville Kyrki](https://arxiv.org/search/cs?searchtype=author&query=Kyrki,+V)\n\nComments:\n2 pages, 5 figures, ICRA 2019 workshop\n\nSubjects:Robotics (cs.RO)\n\n\\[1914\\] [arXiv:1904.05673](https://arxiv.org/abs/1904.05673)\\[ [pdf](https://arxiv.org/pdf/1904.05673), [other](https://arxiv.org/format/1904.05673)\\]\n\nTitle:\nYUVMultiNet: Real-time YUV multi-task CNN for autonomous driving\n\n[Thomas Boulay](https://arxiv.org/search/cs?searchtype=author&query=Boulay,+T), [Said El-Hachimi](https://arxiv.org/search/cs?searchtype=author&query=El-Hachimi,+S), [Mani Kumar Surisetti](https://arxiv.org/search/cs?searchtype=author&query=Surisetti,+M+K), [Pullarao Maddu](https://arxiv.org/search/cs?searchtype=author&query=Maddu,+P), [Saranya Kandan](https://arxiv.org/search/cs?searchtype=author&query=Kandan,+S)\n\nComments:\nThis paper is accepted for CVPR workshop demo\n\nSubjects:Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)\n\n\\[1915\\] [arXiv:1904.05674](https://arxiv.org/abs/1904.05674)\\[ [pdf](https://arxiv.org/pdf/1904.05674), [other](https://arxiv.org/format/1904.05674)\\]\n\nTitle:\nCross-topic distributional semantic representations via unsupervised mappings\n\n[Eleftheria Briakou](https://arxiv.org/search/cs?searchtype=author&query=Briakou,+E), [Nikos Athanasiou](https://arxiv.org/search/cs?searchtype=author&query=Athanasiou,+N), [Alexandros Potamianos](https://arxiv.org/search/cs?searchtype=author&query=Potamianos,+A)\n\nComments:\nNAACL-HLT 2019\n\nSubjects:Computation and Language (cs.CL); Machine Learning (cs.LG)\n\n\\[1916\\] [arXiv:1904.05677](https://arxiv.org/abs/1904.05677)\\[ [pdf](https://arxiv.org/pdf/1904.05677), [other](https://arxiv.org/format/1904.05677)\\]\n\nTitle:\nDeep Back-Projection Networks for Single Image Super-resolution\n\n[Muhammad Haris](https://arxiv.org/search/cs?searchtype=author&query=Haris,+M), [Greg Shakhnarovich](https://arxiv.org/search/cs?searchtype=author&query=Shakhnarovich,+G), [Norimichi Ukita](https://arxiv.org/search/cs?searchtype=author&query=Ukita,+N)\n\nComments:\nTo appear in TPAMI 2020. The code is available at [this https URL](https://github.com/alterzero/DBPN-Pytorch) arXiv admin note: substantial text overlap with [arXiv:1803.02735](https://arxiv.org/abs/1803.02735)\n\nSubjects:Computer Vision and Pattern Recognition (cs.CV)\n\n\\[1917\\] [arXiv:1904.05681](https://arxiv.org/abs/1904.05681)\\[ [pdf](https://arxiv.org/pdf/1904.05681), [other](https://arxiv.org/format/1904.05681)\\]\n\nTitle:\nKnitting Skeletons: A Computer-Aided Design Tool for Shaping and Patterning of Knitted Garments\n\n[Alexandre Kaspar](https://arxiv.org/search/cs?searchtype=author&query=Kaspar,+A), [Liane Makatura](https://arxiv.org/search/cs?searchtype=author&query=Makatura,+L), [Wojciech Matusik](https://arxiv.org/search/cs?searchtype=author&query=Matusik,+W)\n\nComments:\nProject page: [this http URL](http://knitskel.csail.mit.edu)\n\nSubjects:Human-Computer Interaction (cs.HC)\n\n\\[1918\\] [arXiv:1904.05682](https://arxiv.org/abs/1904.05682)\\[ [pdf](https://arxiv.org/pdf/1904.05682), [other](https://arxiv.org/format/1904.05682)\\]\n\nTitle:\nMultiplicative Up-Drift\n\n[Benjamin Doerr](https://arxiv.org/search/cs?searchtype=author&query=Doerr,+B) (École Polytechnique, CNRS, LIX), [Timo Kötzing](https://arxiv.org/search/cs?searchtype=author&query=K%C3%B6tzing,+T) (Hasso Plattner Institute)\n\nComments:\nSignificantly extended version of: Benjamin Doerr and Timo Kötzing. Multiplicative up-drift. In Genetic and Evolutionary Computation Conference, GECCO 2019, pages 1470-1478. ACM, 2019\n\nJournal-ref:\nAlgorithmica 83(10): 3017-3058 (2021)\n\nSubjects:Neural and Evolutionary Computing (cs.NE); Data Structures and Algorithms (cs.DS)\n\n\\[1919\\] [arXiv:1904.05688](https://arxiv.org/abs/1904.05688)\\[ [pdf](https://arxiv.org/pdf/1904.05688), [other](https://arxiv.org/format/1904.05688)\\]\n\nTitle:\nLearning to Take Good Pictures of People with a Robot Photographer\n\n[Rhys Newbury](https://arxiv.org/search/cs?searchtype=author&query=Newbury,+R), [Akansel Cosgun](https://arxiv.org/search/cs?searchtype=author&query=Cosgun,+A), [Mehmet Koseoglu](https://arxiv.org/search/cs?searchtype=author&query=Koseoglu,+M), [Tom Drummond](https://arxiv.org/search/cs?searchtype=author&query=Drummond,+T)\n\nSubjects:Robotics (cs.RO)\n\n\\[1920\\] [arXiv:1904.05709](https://arxiv.org/abs/1904.05709)\\[ [pdf](https://arxiv.org/pdf/1904.05709), [other](https://arxiv.org/format/1904.05709)\\]\n\nTitle:\nElucidating image-to-set prediction: An analysis of models, losses and datasets\n\n[Luis Pineda](https://arxiv.org/search/cs?searchtype=author&query=Pineda,+L), [Amaia Salvador](https://arxiv.org/search/cs?searchtype=author&query=Salvador,+A), [Michal Drozdzal](https://arxiv.org/search/cs?searchtype=author&query=Drozdzal,+M), [Adriana Romero](https://arxiv.org/search/cs?searchtype=author&query=Romero,+A)\n\nSubjects:Computer Vision and Pattern Recognition (cs.CV)\n\n\\[1921\\] [arXiv:1904.05710](https://arxiv.org/abs/1904.05710)\\[ [pdf](https://arxiv.org/pdf/1904.05710), [other](https://arxiv.org/format/1904.05710)\\]\n\nTitle:\nA Case Study of Execution of Untrusted Business Process on Permissioned Blockchain\n\n[Vahid Pourheidari](https://arxiv.org/search/cs?searchtype=author&query=Pourheidari,+V), [Sara Rouhani](https://arxiv.org/search/cs?searchtype=author&query=Rouhani,+S), [Ralph deters](https://arxiv.org/search/cs?searchtype=author&query=deters,+R)\n\nJournal-ref:\nIEEE Blockchain 2018\n\nSubjects:Cryptography and Security (cs.CR)\n\n\\[1922\\] [arXiv:1904.05712](https://arxiv.org/abs/1904.05712)\\[ [pdf](https://arxiv.org/pdf/1904.05712), [other](https://arxiv.org/format/1904.05712)\\]\n\nTitle:\nReconstructing Network Inputs with Additive Perturbation Signatures\n\n[Nick Moran](https://arxiv.org/search/cs?searchtype=author&query=Moran,+N), [Chiraag Juvekar](https://arxiv.org/search/cs?searchtype=author&query=Juvekar,+C)\n\nSubjects:Computer Vision and Pattern Recognition (cs.CV); Cryptography and Security (cs.CR)\n\n\\[1923\\] [arXiv:1904.05717](https://arxiv.org/abs/1904.05717)\\[ [pdf](https://arxiv.org/pdf/1904.05717), [other](https://arxiv.org/format/1904.05717)\\]\n\nTitle:\nThe MOMMS Family of Matrix Multiplication Algorithms\n\n[Tyler M. Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith,+T+M), [Robert A. van de Geijn](https://arxiv.org/search/cs?searchtype=author&query=van+de+Geijn,+R+A)\n\nSubjects:Mathematical Software (cs.MS)\n\n\\[1924\\] [arXiv:1904.05724](https://arxiv.org/abs/1904.05724)\\[ [pdf](https://arxiv.org/pdf/1904.05724), [other](https://arxiv.org/format/1904.05724)\\]\n\nTitle:\nImproving SIEM for Critical SCADA Water Infrastructures Using Machine Learning\n\n[Hanan Hindy](https://arxiv.org/search/cs?searchtype=author&query=Hindy,+H), [David Brosset](https://arxiv.org/search/cs?searchtype=author&query=Brosset,+D), [Ethan Bayne](https://arxiv.org/search/cs?searchtype=author&query=Bayne,+E), [Amar Seeam](https://arxiv.org/search/cs?searchtype=author&query=Seeam,+A), [Xavier Bellekens](https://arxiv.org/search/cs?searchtype=author&query=Bellekens,+X)\n\nComments:\n17 pages, 8 figures, 4 tables. In the proceeding of International Workshop on the Security of Industrial Control Systems and Cyber-Physical Systems CyberICPS, In Conjunction With ESORICS 2018\n\nJournal-ref:\nComputer Security. SECPRE 2018, CyberICPS 2018. Lecture Notes in Computer Science, vol 11387. Springer, Cham\n\nSubjects:Cryptography and Security (cs.CR); Machine Learning (cs.LG)\n\n\\[1925\\] [arXiv:1904.05728](https://arxiv.org/abs/1904.05728)\\[ [pdf](https://arxiv.org/pdf/1904.05728), [other](https://arxiv.org/format/1904.05728)\\]\n\nTitle:\nTechnical Report: Safe, Aggressive Quadrotor Flight via Reachab","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Flist%2Fcs%2F2019-04%3Fskip%3D1910%26show%3D2000","id":"https://arxiv.org/list/cs/2019-04?skip=1910&show=2000","publishedDate":"2019-05-28T00:00:00.000Z","summary":"The webpage lists recent computer science research papers from April 2019, totaling 4,942 entries. Highlights include:\n\n- **MxML: Mixture of Meta-Learners for Few-Shot Classification** — Proposes a novel meta-learning approach combining multiple learners to improve few-shot image classification.\n- **RDNA Balance: Load Balancing by Isolation of Elephant Flows** — Addresses network load balancing by isolating large data flows using strict source routing techniques.\n- **ViTa-SLAM: Biologically-Inspired Visuo-Tactile SLAM** — Introduces a visuo-tactile SLAM system inspired by biological mechanisms for robotics applications.\n- **YUVMultiNet: Real-time YUV multi-task CNN for Autonomous Driving** — Presents a multi-task CNN for real-time processing in autonomous vehicle perception.\n- **Cross-topic Distributional Semantic Representations** — Develops unsupervised methods for cross-topic semantic mapping, relevant to language understanding.\n\nThis list showcases advances in machine learning, networking, robotics, and computer vision, with several papers accepted to notable conferences like CVPR and NAACL-HLT.","title":"Computer Science Apr 2019","url":"https://arxiv.org/list/cs/2019-04?skip=1910&show=2000"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Flist%2Fcs%2F2019-04%3Fskip%3D2300%26show%3D10","id":"https://arxiv.org/list/cs/2019-04?skip=2300&show=10","publishedDate":"2024-12-06T00:00:00.000Z","summary":"Failed to fetch content","title":"Computer Science Apr 2019","url":"https://arxiv.org/list/cs/2019-04?skip=2300&show=10"},{"content":"[View PDF](https://arxiv.org/pdf/2002.03794)\n\n> Abstract:The difficulty of deploying various deep learning (DL) models on diverse DL hardware has boosted the research and development of DL compilers in the community. Several DL compilers have been proposed from both industry and academia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output. However, none of the existing survey has analyzed the unique design architecture of the DL compilers comprehensively. In this paper, we perform a comprehensive survey of existing DL compilers by dissecting the commonly adopted design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend optimizations. Specifically, we provide a comprehensive comparison among existing DL compilers from various aspects. In addition, we present detailed analysis on the design of multi-level IRs and illustrate the commonly adopted optimization techniques. Finally, several insights are highlighted as the potential research directions of DL compiler. This is the first survey paper focusing on the design architecture of DL compilers, which we hope can pave the road for future research towards DL compiler.\n\n## Submission history\n\nFrom: Mingzhen Li \\[ [view email](https://arxiv.org/show-email/acd77279/2002.03794)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2002.03794v1)**\nThu, 6 Feb 2020 07:29:08 UTC (680 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2002.03794v2)**\nThu, 27 Feb 2020 05:08:44 UTC (672 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2002.03794v3)**\nSun, 5 Apr 2020 10:34:01 UTC (481 KB)\n\n**\\[v4\\]**\nFri, 28 Aug 2020 09:19:43 UTC (568 KB)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2002.03794","id":"https://arxiv.org/abs/2002.03794","publishedDate":"2020-02-06T00:00:00.000Z","summary":"This survey provides a comprehensive analysis of deep learning (DL) compilers, focusing on their architectural design, optimization techniques, and key components. It discusses prominent DL compilers like TensorFlow XLA and TVM, highlighting their input processing, multi-level intermediate representations (IRs), and backend optimizations. The paper emphasizes the importance of multi-level IRs in enabling hardware-specific optimizations and compares various compiler architectures across multiple aspects. Additionally, it explores common optimization strategies used to improve performance and efficiency. The survey aims to guide future research directions by identifying challenges and potential improvements in DL compiler design, serving as a foundational resource for advancing DL deployment across diverse hardware platforms.","title":"The Deep Learning Compiler: A Comprehensive Survey","url":"https://arxiv.org/abs/2002.03794"},{"content":"# Computer Science > Programming Languages\n\n**arXiv:2002.11054** (cs)\n\n\\[Submitted on 25 Feb 2020 ( [v1](https://arxiv.org/abs/2002.11054v1)), last revised 1 Mar 2020 (this version, v2)\\]\n\n# Title:MLIR: A Compiler Infrastructure for the End of Moore's Law\n\nAuthors: [Chris Lattner](https://arxiv.org/search/cs?searchtype=author&query=Lattner,+C), [Mehdi Amini](https://arxiv.org/search/cs?searchtype=author&query=Amini,+M), [Uday Bondhugula](https://arxiv.org/search/cs?searchtype=author&query=Bondhugula,+U), [Albert Cohen](https://arxiv.org/search/cs?searchtype=author&query=Cohen,+A), [Andy Davis](https://arxiv.org/search/cs?searchtype=author&query=Davis,+A), [Jacques Pienaar](https://arxiv.org/search/cs?searchtype=author&query=Pienaar,+J), [River Riddle](https://arxiv.org/search/cs?searchtype=author&query=Riddle,+R), [Tatiana Shpeisman](https://arxiv.org/search/cs?searchtype=author&query=Shpeisman,+T), [Nicolas Vasilache](https://arxiv.org/search/cs?searchtype=author&query=Vasilache,+N), [Oleksandr Zinenko](https://arxiv.org/search/cs?searchtype=author&query=Zinenko,+O)\n\nView a PDF of the paper titled MLIR: A Compiler Infrastructure for the End of Moore's Law, by Chris Lattner and 9 other authors\n\n[View PDF](https://arxiv.org/pdf/2002.11054)\n\n> Abstract:This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR aims to address software fragmentation, improve compilation for heterogeneous hardware, significantly reduce the cost of building domain specific compilers, and aid in connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and also across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, and identifying the challenges and opportunities posed by this novel design point in design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.\n\n| | |\n| --- | --- |\n| Subjects: | Programming Languages (cs.PL); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2002.11054](https://arxiv.org/abs/2002.11054) \\[cs.PL\\] |\n| (or [arXiv:2002.11054v2](https://arxiv.org/abs/2002.11054v2) \\[cs.PL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2002.11054](https://doi.org/10.48550/arXiv.2002.11054) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Uday Bondhugula \\[ [view email](https://arxiv.org/show-email/287d8686/2002.11054)\\] **[\\[v1\\]](https://arxiv.org/abs/2002.11054v1)**\nTue, 25 Feb 2020 17:24:50 UTC (526 KB)\n**\\[v2\\]**\nSun, 1 Mar 2020 00:38:46 UTC (526 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MLIR: A Compiler Infrastructure for the End of Moore's Law, by Chris Lattner and 9 other authors\n\n- [View PDF](https://arxiv.org/pdf/2002.11054)\n- [TeX Source](https://arxiv.org/src/2002.11054)\n- [Other Formats](https://arxiv.org/format/2002.11054)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.PL\n\n[< prev](https://arxiv.org/prevnext?id=2002.11054&function=prev&context=cs.PL)  \\|  [next >](https://arxiv.org/prevnext?id=2002.11054&function=next&context=cs.PL)\n\n[new](https://arxiv.org/list/cs.PL/new) \\| [recent](https://arxiv.org/list/cs.PL/recent) \\| [2020-02](https://arxiv.org/list/cs.PL/2020-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2002.11054?context=cs) [cs.LG](https://arxiv.org/abs/2002.11054?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2002.11054)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2002.11054)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2002.11054)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2002.html#abs-2002-11054) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2002-11054)\n\n[Uday Bondhugula](https://dblp.uni-trier.de/search/author?author=Uday%20Bondhugula) [Albert Cohen](https://dblp.uni-trier.de/search/author?author=Albert%20Cohen) [Tatiana Shpeisman](https://dblp.uni-trier.de/search/author?author=Tatiana%20Shpeisman) [Andy Davis](https://dblp.uni-trier.de/search/author?author=Andy%20Davis) [Nicolas Vasilache](https://dblp.uni-trier.de/search/author?author=Nicolas%20Vasilache)\n\n…\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2002.11054) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2002.11054","id":"https://arxiv.org/abs/2002.11054","publishedDate":"2020-02-25T00:00:00.000Z","summary":"The paper introduces MLIR (Multi-Level Intermediate Representation), a flexible and extensible compiler infrastructure designed to address challenges posed by the end of Moore's Law. MLIR aims to reduce software fragmentation, enhance compilation for diverse and heterogeneous hardware, and lower the costs of developing domain-specific compilers. It supports designing and implementing code generators, translators, and optimizers across multiple abstraction levels, application domains, and hardware targets. The authors discuss MLIR's architecture, design principles, and semantics, emphasizing its role as a research artifact that fosters innovation in compiler development, programming languages, and computer architecture. Overall, MLIR provides a unified framework to facilitate efficient compilation workflows in an evolving hardware landscape.","title":"MLIR: A Compiler Infrastructure for the End of Moore's Law","url":"https://arxiv.org/abs/2002.11054"},{"content":"# Computer Science > Machine Learning\n\n**arXiv:2006.06762** (cs)\n\n\\[Submitted on 11 Jun 2020 ( [v1](https://arxiv.org/abs/2006.06762v1)), last revised 15 Oct 2023 (this version, v5)\\]\n\n# Title:Ansor: Generating High-Performance Tensor Programs for Deep Learning\n\nAuthors: [Lianmin Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng,+L), [Chengfan Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia,+C), [Minmin Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun,+M), [Zhao Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+Z), [Cody Hao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+C+H), [Ameer Haj-Ali](https://arxiv.org/search/cs?searchtype=author&query=Haj-Ali,+A), [Yida Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+Y), [Jun Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+J), [Danyang Zhuo](https://arxiv.org/search/cs?searchtype=author&query=Zhuo,+D), [Koushik Sen](https://arxiv.org/search/cs?searchtype=author&query=Sen,+K), [Joseph E. Gonzalez](https://arxiv.org/search/cs?searchtype=author&query=Gonzalez,+J+E), [Ion Stoica](https://arxiv.org/search/cs?searchtype=author&query=Stoica,+I)\n\nView a PDF of the paper titled Ansor: Generating High-Performance Tensor Programs for Deep Learning, by Lianmin Zheng and 11 other authors\n\n[View PDF](https://arxiv.org/pdf/2006.06762)\n\n> Abstract:High-performance tensor programs are crucial to guarantee efficient execution of deep neural networks. However, obtaining performant tensor programs for different operators on various hardware platforms is notoriously challenging. Currently, deep learning systems rely on vendor-provided kernel libraries or various search strategies to get performant tensor programs. These approaches either require significant engineering effort to develop platform-specific optimization code or fall short of finding high-performance programs due to restricted search space and ineffective exploration strategy.\n> We present Ansor, a tensor program generation framework for deep learning applications. Compared with existing search strategies, Ansor explores many more optimization combinations by sampling programs from a hierarchical representation of the search space. Ansor then fine-tunes the sampled programs with evolutionary search and a learned cost model to identify the best programs. Ansor can find high-performance programs that are outside the search space of existing state-of-the-art approaches. In addition, Ansor utilizes a task scheduler to simultaneously optimize multiple subgraphs in deep neural networks. We show that Ansor improves the execution performance of deep neural networks relative to the state-of-the-art on the Intel CPU, ARM CPU, and NVIDIA GPU by up to $3.8\\\\times$, $2.6\\\\times$, and $1.7\\\\times$, respectively.\n\n| | |\n| --- | --- |\n| Comments: | OSDI 2020 |\n| Subjects: | Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Performance (cs.PF); Programming Languages (cs.PL); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2006.06762](https://arxiv.org/abs/2006.06762) \\[cs.LG\\] |\n| (or [arXiv:2006.06762v5](https://arxiv.org/abs/2006.06762v5) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2006.06762](https://doi.org/10.48550/arXiv.2006.06762) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Lianmin Zheng \\[ [view email](https://arxiv.org/show-email/6c34870d/2006.06762)\\] **[\\[v1\\]](https://arxiv.org/abs/2006.06762v1)**\nThu, 11 Jun 2020 19:40:09 UTC (955 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2006.06762v2)**\nMon, 15 Jun 2020 16:21:22 UTC (954 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2006.06762v3)**\nFri, 30 Oct 2020 18:32:44 UTC (2,666 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2006.06762v4)**\nWed, 4 Nov 2020 07:29:29 UTC (2,721 KB)\n**\\[v5\\]**\nSun, 15 Oct 2023 07:00:36 UTC (748 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Ansor: Generating High-Performance Tensor Programs for Deep Learning, by Lianmin Zheng and 11 other authors\n\n- [View PDF](https://arxiv.org/pdf/2006.06762)\n- [TeX Source](https://arxiv.org/src/2006.06762)\n- [Other Formats](https://arxiv.org/format/2006.06762)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[< prev](https://arxiv.org/prevnext?id=2006.06762&function=prev&context=cs.LG)  \\|  [next >](https://arxiv.org/prevnext?id=2006.06762&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2020-06](https://arxiv.org/list/cs.LG/2020-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2006.06762?context=cs) [cs.NE](https://arxiv.org/abs/2006.06762?context=cs.NE) [cs.PF](https://arxiv.org/abs/2006.06762?context=cs.PF) [cs.PL](https://arxiv.org/abs/2006.06762?context=cs.PL) [stat](https://arxiv.org/abs/2006.06762?context=stat) [stat.ML](https://arxiv.org/abs/2006.06762?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2006.06762)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2006.06762)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2006.06762)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2006.html#abs-2006-06762) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2006-06762)\n\n[Lianmin Zheng](https://dblp.uni-trier.de/search/author?author=Lianmin%20Zheng) [Zhao Wu](https://dblp.uni-trier.de/search/author?author=Zhao%20Wu) [Cody Hao Yu](https://dblp.uni-trier.de/search/author?author=Cody%20Hao%20Yu) [Yida Wang](https://dblp.uni-trier.de/search/author?author=Yida%20Wang) [Jun Yang](https://dblp.uni-trier.de/search/author?author=Jun%20Yang)\n\n…\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2006.06762) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2006.06762","id":"https://arxiv.org/abs/2006.06762","publishedDate":"2020-06-11T00:00:00.000Z","summary":"The paper introduces Ansor, a framework designed to generate high-performance tensor programs critical for efficient deep learning computations. Traditional methods rely on vendor libraries or limited search strategies, which often require extensive engineering effort or fail to discover optimal programs. Ansor addresses these challenges by exploring a vast search space through hierarchical program sampling, combined with evolutionary search and a learned cost model to identify the best-performing tensor programs. It can generate programs that outperform existing solutions, achieving up to 3.8× speedups on Intel CPUs, 2.6× on ARM CPUs, and 1.7× on NVIDIA GPUs. Additionally, Ansor features a task scheduler that optimizes multiple subgraphs in neural networks simultaneously, significantly enhancing overall execution efficiency across diverse hardware platforms.","title":"Ansor: Generating High-Performance Tensor Programs for Deep Learning","url":"https://arxiv.org/abs/2006.06762"},{"content":"[View PDF](https://arxiv.org/pdf/2307.04995)\n\n> Abstract:Deep neural networks (DNNs) are of critical use in different domains. To accelerate DNN computation, tensor compilers are proposed to generate efficient code on different domain-specific accelerators. Existing tensor compilers mainly focus on optimizing computation efficiency. However, memory access is becoming a key performance bottleneck because the computational performance of accelerators is increasing much faster than memory performance. The lack of direct description of memory access and data dependence in current tensor compilers' intermediate representation (IR) brings significant challenges to generate memory-efficient code.\n>\n> In this paper, we propose IntelliGen, a tensor compiler that can generate high-performance code for memory-intensive operators by considering both computation and data movement optimizations. IntelliGen represent a DNN program using GIR, which includes primitives indicating its computation, data movement, and parallel strategies. This information will be further composed as an instruction-level dataflow graph to perform holistic optimizations by searching different memory access patterns and computation operations, and generating memory-efficient code on different hardware. We evaluate IntelliGen on NVIDIA GPU, AMD GPU, and Cambricon MLU, showing speedup up to 1.97x, 2.93x, and 16.91x(1.28x, 1.23x, and 2.31x on average), respectively, compared to current most performant frameworks.\n\n## Submission history\n\nFrom: Zixuan Ma \\[ [view email](https://arxiv.org/show-email/30f7cdc7/2307.04995)\\]\n\n**\\[v1\\]**\nTue, 11 Jul 2023 03:17:40 UTC (1,126 KB)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2307.04995","id":"https://arxiv.org/abs/2307.04995","publishedDate":"2023-07-11T00:00:00.000Z","summary":"The paper introduces PowerFusion, a tensor compiler designed to optimize deep neural network (DNN) computations by explicitly modeling data movement and memory access alongside computation. Unlike existing compilers that mainly focus on computational efficiency, PowerFusion (also referred to as IntelliGen) employs a novel intermediate representation (GIR) capturing primitives for computation, data movement, and parallel strategies. This enables holistic, instruction-level optimization of memory-intensive operators across diverse hardware such as NVIDIA GPUs, AMD GPUs, and Cambricon MLUs. Experimental results demonstrate significant speedups—up to nearly 17x on some hardware—by generating memory-efficient code that addresses the memory access bottleneck in modern accelerators.","title":"PowerFusion: A Tensor Compiler with Explicit Data Movement Description and Instruction-level Graph IR","url":"https://arxiv.org/abs/2307.04995"},{"content":"# Computer Science > Programming Languages\n\n**arXiv:1802.04730** (cs)\n\n\\[Submitted on 13 Feb 2018 ( [v1](https://arxiv.org/abs/1802.04730v1)), last revised 29 Jun 2018 (this version, v3)\\]\n\n# Title:Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions\n\nAuthors: [Nicolas Vasilache](https://arxiv.org/search/cs?searchtype=author&query=Vasilache,+N), [Oleksandr Zinenko](https://arxiv.org/search/cs?searchtype=author&query=Zinenko,+O), [Theodoros Theodoridis](https://arxiv.org/search/cs?searchtype=author&query=Theodoridis,+T), [Priya Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal,+P), [Zachary DeVito](https://arxiv.org/search/cs?searchtype=author&query=DeVito,+Z), [William S. Moses](https://arxiv.org/search/cs?searchtype=author&query=Moses,+W+S), [Sven Verdoolaege](https://arxiv.org/search/cs?searchtype=author&query=Verdoolaege,+S), [Andrew Adams](https://arxiv.org/search/cs?searchtype=author&query=Adams,+A), [Albert Cohen](https://arxiv.org/search/cs?searchtype=author&query=Cohen,+A)\n\nView a PDF of the paper titled Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions, by Nicolas Vasilache and 8 other authors\n\n[View PDF](https://arxiv.org/pdf/1802.04730)\n\n> Abstract:Deep learning models with convolutional and recurrent networks are now ubiquitous and analyze massive amounts of audio, image, video, text and graph data, with applications in automatic translation, speech-to-text, scene understanding, ranking user preferences, ad placement, etc. Competing frameworks for building these networks such as TensorFlow, Chainer, CNTK, Torch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between usability and expressiveness, research or production orientation and supported hardware. They operate on a DAG of computational operators, wrapping high-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for various CPUs), and automate memory allocation, synchronization, distribution. Custom operators are needed where the computation does not fit existing high-performance library calls, usually at a high engineering cost. This is frequently required when new operators are invented by researchers: such operators suffer a severe performance penalty, which limits the pace of innovation. Furthermore, even if there is an existing runtime call these frameworks can use, it often doesn't offer optimal performance for a user's particular network architecture and dataset, missing optimizations between operators as well as optimizations that can be done knowing the size and shape of data. Our contributions include (1) a language close to the mathematics of deep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time compiler to convert a mathematical description of a deep learning DAG into a CUDA kernel with delegated memory management and synchronization, also providing optimizations such as operator fusion and specialization for specific sizes, (3) a compilation cache populated by an autotuner. \\[Abstract cutoff\\]\n\n| | |\n| --- | --- |\n| Subjects: | Programming Languages (cs.PL); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1802.04730](https://arxiv.org/abs/1802.04730) \\[cs.PL\\] |\n| (or [arXiv:1802.04730v3](https://arxiv.org/abs/1802.04730v3) \\[cs.PL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1802.04730](https://doi.org/10.48550/arXiv.1802.04730) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Albert Cohen \\[ [view email](https://arxiv.org/show-email/56f6b42b/1802.04730)\\] **[\\[v1\\]](https://arxiv.org/abs/1802.04730v1)**\nTue, 13 Feb 2018 16:53:01 UTC (160 KB)\n**[\\[v2\\]](https://arxiv.org/abs/1802.04730v2)**\nTue, 6 Mar 2018 08:57:44 UTC (156 KB)\n**\\[v3\\]**\nFri, 29 Jun 2018 00:16:36 UTC (156 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions, by Nicolas Vasilache and 8 other authors\n\n- [View PDF](https://arxiv.org/pdf/1802.04730)\n- [TeX Source](https://arxiv.org/src/1802.04730)\n- [Other Formats](https://arxiv.org/format/1802.04730)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.PL\n\n[< prev](https://arxiv.org/prevnext?id=1802.04730&function=prev&context=cs.PL)  \\|  [next >](https://arxiv.org/prevnext?id=1802.04730&function=next&context=cs.PL)\n\n[new](https://arxiv.org/list/cs.PL/new) \\| [recent](https://arxiv.org/list/cs.PL/recent) \\| [2018-02](https://arxiv.org/list/cs.PL/2018-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1802.04730?context=cs) [cs.LG](https://arxiv.org/abs/1802.04730?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1802.04730)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1802.04730)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1802.04730)\n\n### [1 blog link](https://arxiv.org/tb/1802.04730)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1802.html#abs-1802-04730) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1802-04730)\n\n[Nicolas Vasilache](https://dblp.uni-trier.de/search/author?author=Nicolas%20Vasilache) [Oleksandr Zinenko](https://dblp.uni-trier.de/search/author?author=Oleksandr%20Zinenko) [Theodoros Theodoridis](https://dblp.uni-trier.de/search/author?author=Theodoros%20Theodoridis) [Priya Goyal](https://dblp.uni-trier.de/search/author?author=Priya%20Goyal) [Zachary DeVito](https://dblp.uni-trier.de/search/author?author=Zachary%20DeVito)\n\n…\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1802.04730) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F1802.04730","id":"https://arxiv.org/abs/1802.04730","publishedDate":"2018-02-13T00:00:00.000Z","summary":"The paper introduces Tensor Comprehensions, a framework-agnostic language designed for high-performance machine learning computations, particularly for deep learning models involving convolutional and recurrent networks. It addresses limitations in existing frameworks like TensorFlow and PyTorch, which rely on high-performance libraries but often require costly custom operators and lack optimal performance tuning. The authors propose a polyhedral Just-In-Time (JIT) compiler that translates mathematical descriptions of deep learning DAGs into optimized CUDA kernels, incorporating features like operator fusion, size-specific specialization, and memory management. An autotuning cache further enhances performance by selecting optimal configurations. Overall, the work aims to simplify the development of efficient, custom operators and improve performance portability across hardware platforms.","title":"Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions","url":"https://arxiv.org/abs/1802.04730"},{"content":"# Computer Science > Machine Learning\n\n**arXiv:2207.04606** (cs)\n\n\\[Submitted on 11 Jul 2022 ( [v1](https://arxiv.org/abs/2207.04606v1)), last revised 21 Feb 2023 (this version, v4)\\]\n\n# Title:SparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning\n\nAuthors: [Zihao Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye,+Z), [Ruihang Lai](https://arxiv.org/search/cs?searchtype=author&query=Lai,+R), [Junru Shao](https://arxiv.org/search/cs?searchtype=author&query=Shao,+J), [Tianqi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+T), [Luis Ceze](https://arxiv.org/search/cs?searchtype=author&query=Ceze,+L)\n\nView a PDF of the paper titled SparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning, by Zihao Ye and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2207.04606)\n\n> Abstract:Sparse tensors are rapidly becoming critical components of modern deep learning workloads. However, developing high-performance sparse operators can be difficult and tedious, and existing vendor libraries cannot satisfy the escalating demands from new operators. Sparse tensor compilers simplify the development of operators, but efficient sparse compilation for deep learning remains challenging because a single sparse format cannot maximize hardware efficiency, and single-shot compilers cannot keep up with latest hardware and system advances. In this paper, we observe that the key to addressing both these challenges is to leverage composable formats and composable transformations. We propose SparseTIR, a sparse tensor compilation abstraction that offers composable formats and composable transformations for deep learning workloads. SparseTIR constructs a search space over these composable components for performance tuning. With these improvements, SparseTIR obtains consistent performance speedups vs vendor libraries on GPUs for single operators: 1.20-2.34x for GNN operators, 1.05-2.98x for sparse attention operators, and 0.56-7.45x for sparse convolution operators. SparseTIR also accelerates end-to-end GNNs by 1.08-1.52x for GraphSAGE training, and 4.20-40.18x for RGCN inference.\n\n| | |\n| --- | --- |\n| Comments: | To appear at ASPLOS 2023 (19 pages, 23 figures), source code available at [this https URL](https://github.com/uwsampl/sparsetir), artifact available at [this https URL](https://github.com/uwsampl/sparsetir-artifact) |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Programming Languages (cs.PL) |\n| Cite as: | [arXiv:2207.04606](https://arxiv.org/abs/2207.04606) \\[cs.LG\\] |\n| (or [arXiv:2207.04606v4](https://arxiv.org/abs/2207.04606v4) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2207.04606](https://doi.org/10.48550/arXiv.2207.04606) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Zihao Ye \\[ [view email](https://arxiv.org/show-email/83f9ee76/2207.04606)\\] **[\\[v1\\]](https://arxiv.org/abs/2207.04606v1)**\nMon, 11 Jul 2022 03:49:53 UTC (2,798 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2207.04606v2)**\nFri, 26 Aug 2022 03:57:10 UTC (2,840 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2207.04606v3)**\nFri, 11 Nov 2022 01:48:13 UTC (3,092 KB)\n**\\[v4\\]**\nTue, 21 Feb 2023 16:51:55 UTC (3,740 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled SparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning, by Zihao Ye and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2207.04606)\n- [TeX Source](https://arxiv.org/src/2207.04606)\n- [Other Formats](https://arxiv.org/format/2207.04606)\n\n[view license](http://creativecommons.org/licenses/by-nc-sa/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[< prev](https://arxiv.org/prevnext?id=2207.04606&function=prev&context=cs.LG)  \\|  [next >](https://arxiv.org/prevnext?id=2207.04606&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2022-07](https://arxiv.org/list/cs.LG/2022-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2207.04606?context=cs) [cs.AI](https://arxiv.org/abs/2207.04606?context=cs.AI) [cs.PL](https://arxiv.org/abs/2207.04606?context=cs.PL)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2207.04606)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2207.04606)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2207.04606)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n×\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2207.04606) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Farxiv.org%2Fabs%2F2207.04606","id":"https://arxiv.org/abs/2207.04606","publishedDate":"2022-07-11T00:00:00.000Z","summary":"The paper \"SparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning\" introduces SparseTIR, a new compiler abstraction designed to improve the efficiency of sparse tensor operations critical to modern deep learning tasks. Sparse tensors are vital for workloads like graph neural networks (GNNs), sparse attention, and sparse convolution, but existing vendor libraries and single-format compilers struggle to optimize performance across diverse hardware and operators. SparseTIR addresses this by enabling composable formats and transformations, creating a flexible search space for performance tuning. Experimental results show significant speedups over vendor libraries—up to 2.98x on GPUs for sparse operators—and notable acceleration in end-to-end GNN training and inference, demonstrating its effectiveness for optimizing sparse deep learning workloads. The approach is set to be presented at ASPLOS 2023, with source code and artifacts publicly available.","title":"SparseTIR: Composable Abstractions for Sparse Compilation in Deep Learning","url":"https://arxiv.org/abs/2207.04606"}],"status":"completed","toolCallId":"call_ubTw57CF1kOMYlDJQizjcR74","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Here’s a concise summary of “Relay: A High-Level Compiler for Deep Learning” (arXiv:1904.08368):\n\nProblem\n- Existing DL systems split between dynamic, eager frameworks (flexible but harder to optimize) and static graph compilers (optimizable but rigid).\n- Operator-centric IRs limit whole-program optimization, new op extensibility, and cross-hardware portability.\n\nCore Idea\n- Relay is a high-level, functional intermediate representation (IR) and compiler for deep learning that unifies flexibility and optimization.\n- It treats models as first-class programs (not just operator DAGs), enabling whole-program analysis and transformations while preserving the ability to express dynamic control flow.\n\nKey Design Features\n- Functional, statically typed IR:\n  - First-class functions, higher-order programs, closures.\n  - Shape-dependent types and type inference to reason about tensor shapes statically where possible.\n- Expressivity for dynamic models:\n  - Supports data-dependent control flow, recursion, and dynamic shapes while remaining optimizable.\n- Extensibility:\n  - New operators and user-defined functions can be added in the same IR, avoiding hard-coded operator catalogs.\n- Separation of concerns:\n  - High-level Relay IR lowered into a tensor-level IR (TVM/TIR) for scheduling and codegen.\n  - Clean layering enables both graph-level and kernel-level optimizations.\n\nCompiler Pipeline and Optimizations\n- Frontends import from mainstream frameworks.\n- High-level passes: inlining, dead-code elimination, common subexpression elimination, algebraic simplifications, constant folding, automatic differentiation (source-to-source AD).\n- Shape and type inference, effect/alias analysis to enable safe transformations.\n- Operator fusion and layout transformations at the IR level.\n- Lowering to TVM’s tensor IR for auto-scheduling, hardware-specific optimizations, and code generation across CPUs, GPUs, and specialized accelerators.\n\nAutomatic Differentiation\n- Built-in, first-class AD via IR transformations (not baked into ops), which:\n  - Works with higher-order functions and control flow.\n  - Enables whole-program differentiation and optimization across training and inference.\n\nPortability and Performance\n- Couples Relay’s program-level optimizations with TVM’s backend to target diverse hardware.\n- Demonstrates that a high-level, programmatic IR can retain the flexibility of dynamic frameworks while unlocking static-compiler optimizations and cross-hardware performance.\n\nContributions\n- A principled, functional, shape-aware IR for deep learning programs.\n- A unified compiler stack integrating program-level and tensor-level optimizations.\n- First-class AD and dynamic control flow support within an optimizable IR.\n- Practical integration with TVM for end-to-end deployment.\n\nImpact\n- Influenced subsequent IR design across DL compilers (e.g., richer type/shape systems, program-level AD, and multi-level IR stacks).\n- Helped move the ecosystem beyond fixed operator graphs toward more general, optimizable programming models for DL.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_04bc183947d06fe30068c879aed3e081a288474b561f4427ae"}},"resumableStreamId":"STREAM:6bb9c52a-465e-4498-9b5d-84c56391eb16","role":"assistant","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","timeToFirstToken":1.006,"tokens":600,"tokensPerSecond":27.749514383498287,"updated_at":1757968805079,"userId":"google:104458561188320708185","id":"j972h1rmv1r03wedw1d6wqq9x97qmk1f"},{"_creationTime":1757968875421.1123,"_id":"j973b1bp9sj8yxz8995y39zydn7qm0eq","attachmentIds":[],"content":"how did Relay fix the dynamic and static issue?","created_at":1757968875421,"messageId":"5534c0be-a87a-477b-869c-0a6bed807bd8","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","updated_at":1757968875421,"userId":"google:104458561188320708185","id":"j973b1bp9sj8yxz8995y39zydn7qm0eq","providerMetadata":{}},{"_creationTime":1757968875421.1125,"_id":"j97bjwx14mxf3b0g2x7m1fhpqs7qnpga","attachmentIds":[],"byok":false,"content":"Relay bridges the “dynamic vs. static” divide by combining a flexible, program-like IR with static reasoning and compiler optimizations. Concretely:\n\n- Program-first (not op-graph-only): Models are expressed as a functional program with first-class functions, higher-order constructs, recursion, and data-dependent control flow. This gives the dynamic expressivity of eager frameworks.\n\n- Static typing with shape-aware types: Relay uses a statically typed system that tracks tensor shapes (and some dependencies on sizes) in types. Where shapes are known, the compiler can do static checks and optimizations; where they aren’t, it still compiles valid programs and defers checks.\n\n- Whole-program analysis and transformations: Because models are real programs, Relay can run standard compiler passes (inlining, DCE, CSE, constant folding), effect/alias analysis, and operator fusion across control-flow boundaries—not just within a fixed DAG. This recovers many of the performance benefits of static graphs even for dynamic models.\n\n- First-class automatic differentiation as an IR transform: AD is implemented via source-to-source transformations on Relay programs, so differentiation works through higher-order functions and control flow. That avoids baking gradients into a fixed operator set and keeps dynamic expressivity.\n\n- Multi-level lowering: Relay keeps high-level structure for analysis and fusion, then lowers to TVM’s tensor IR (TIR) for schedule-level, hardware-specific optimizations. This preserves dynamic modeling at the top while achieving static-like performance at the bottom.\n\n- Extensible operator and function definitions: New ops and user-defined functions live in the same typed IR, so you’re not constrained by a static catalog yet the compiler can still analyze and optimize them.\n\nIn short: Relay lets you write dynamic models as real programs, but equips them with a static, shape-aware type system and a compiler pipeline that performs whole-program reasoning and then lowers to an aggressively optimized tensor IR. You get dynamic expressivity with static-graph-level performance.","created_at":1757968875422,"messageId":"e3ded97c-aebc-4224-b14a-536f55d813c5","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Relay bridges the “dynamic vs. static” divide by combining a flexible, program-like IR with static reasoning and compiler optimizations. Concretely:\n\n- Program-first (not op-graph-only): Models are expressed as a functional program with first-class functions, higher-order constructs, recursion, and data-dependent control flow. This gives the dynamic expressivity of eager frameworks.\n\n- Static typing with shape-aware types: Relay uses a statically typed system that tracks tensor shapes (and some dependencies on sizes) in types. Where shapes are known, the compiler can do static checks and optimizations; where they aren’t, it still compiles valid programs and defers checks.\n\n- Whole-program analysis and transformations: Because models are real programs, Relay can run standard compiler passes (inlining, DCE, CSE, constant folding), effect/alias analysis, and operator fusion across control-flow boundaries—not just within a fixed DAG. This recovers many of the performance benefits of static graphs even for dynamic models.\n\n- First-class automatic differentiation as an IR transform: AD is implemented via source-to-source transformations on Relay programs, so differentiation works through higher-order functions and control flow. That avoids baking gradients into a fixed operator set and keeps dynamic expressivity.\n\n- Multi-level lowering: Relay keeps high-level structure for analysis and fusion, then lowers to TVM’s tensor IR (TIR) for schedule-level, hardware-specific optimizations. This preserves dynamic modeling at the top while achieving static-like performance at the bottom.\n\n- Extensible operator and function definitions: New ops and user-defined functions live in the same typed IR, so you’re not constrained by a static catalog yet the compiler can still analyze and optimize them.\n\nIn short: Relay lets you write dynamic models as real programs, but equips them with a static, shape-aware type system and a compiler pipeline that performs whole-program reasoning and then lowers to an aggressively optimized tensor IR. You get dynamic expressivity with static-graph-level performance.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0cbb057be234a30e0068c879ec10108194a26c0e0088e247cb"}},"resumableStreamId":"STREAM:7ac3b442-3c56-4466-918c-fe7cacbc2679","role":"assistant","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","timeToFirstToken":0.919,"tokens":402,"tokensPerSecond":60.44203879115923,"updated_at":1757968875422,"userId":"google:104458561188320708185","id":"j97bjwx14mxf3b0g2x7m1fhpqs7qnpga"},{"_creationTime":1757968920071.9536,"_id":"j973yff7yyth8j3fcrefkcw3qn7qnwen","attachmentIds":[],"content":"how does relay connect with GPU and where does it reside in the entire ecosystem","created_at":1757968920073,"messageId":"8c2c5055-b230-4d14-940e-8279d2b0218c","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","updated_at":1757968920073,"userId":"google:104458561188320708185","id":"j973yff7yyth8j3fcrefkcw3qn7qnwen","providerMetadata":{}},{"_creationTime":1757968920071.9539,"_id":"j974hc3n7jx2sxagd7ththyywh7qm0ft","attachmentIds":[],"byok":false,"content":"Short answer:\n- Relay is the high-level IR and graph/program compiler in the TVM stack.\n- It targets GPUs by lowering Relay programs to TVM’s tensor-level IR (TIR), which is then scheduled and code-generated into CUDA/ROCm kernels and runtime calls.\n- In the ecosystem, Relay sits above TIR and codegen backends, and below frontends that import models from PyTorch/ONNX/etc.\n\nMore detail:\n\nWhere Relay sits in the stack\n- Frontends: Import models from frameworks (e.g., PyTorch via TorchScript/FX, TensorFlow, ONNX). These are translated into Relay.\n- Relay (program-level IR): Functional, typed IR for whole-model optimization and automatic differentiation. Performs high-level passes: type/shape inference, inlining, DCE, CSE, constant folding, operator fusion, layout transforms, quantization transformations, etc.\n- Lowering boundary: After high-level optimizations, each Relay op or fused region is lowered into TIR (Tensor IR).\n- TIR (tensor-level IR): Describes loop nests, memory layouts, and thread/block bindings abstractly.\n- Scheduling/auto-tuning: TVM’s schedulers (incl. AutoTVM/Ansor/MetaSchedule) produce optimized schedules for the target hardware.\n- Codegen backends: Generate target-specific code:\n  - GPUs: CUDA (NVIDIA), ROCm/HIP (AMD), OpenCL, Metal, Vulkan, or via vendor libraries like cuDNN/cuBLAS when available.\n  - CPUs and accelerators: LLVM, VTA, etc.\n- Runtime: TVM runtime loads compiled modules, manages device contexts, memory, and executes kernels. Relay itself does not execute; the runtime executes compiled artifacts.\n\nHow Relay connects to GPUs\n- Target specification: You compile with a GPU target (e.g., target=\"cuda\" for NVIDIA). Relay passes are target-aware and may pick fusion/layout strategies suited for GPUs.\n- Operator fusion and partitioning: Relay groups graph regions into fused functions. Each fused function becomes a TIR “primitive” to be scheduled for the GPU, minimizing kernel launches and intermediate writes.\n- Lowering to TIR with GPU semantics: In TIR, loops are mapped to GPU threads/warps/blocks via thread axes (`blockIdx.x`, `threadIdx.x`, etc.). Memory scopes like global, shared, local are used to control placement and caching.\n- Scheduling and autotuning: Schedules tile loops, bind axes to threads/blocks, use shared memory, vectorize, unroll, and pipeline to match GPU characteristics. Auto-tuners search schedule spaces for best performance on the specific GPU.\n- Code generation and libraries:\n  - TVM can emit CUDA C/PTX and link it into a loadable module.\n  - For certain ops, TVM may call into vendor libraries (e.g., cuBLAS for GEMM, cuDNN for convolutions) via external codegen paths or composite patterns. Relay can “partition” subgraphs to those external codegens.\n- Runtime execution:\n  - The TVM runtime manages a cuda/rocm device, allocates device memory, transfers inputs/outputs, and launches kernels.\n  - Packed function interface allows invoking compiled functions from Python/C++/Rust, etc.\n\nEcosystem relationship\n- Relay vs ONNX/MLIR/XLA: Relay is TVM’s high-level IR, analogous in layer to XLA HLO or MLIR’s high-level dialects. Unlike fixed op-graphs, it’s a functional language with shape-dependent typing and whole-program AD.\n- Extensibility: New ops or fused patterns can be expressed in Relay and lowered through the same pipeline to GPUs or other devices without needing framework changes.\n- Deployment: Relay is typically used offline in a “compile then run” flow to produce a deployable module that the TVM runtime loads on the target GPU device.\n\nTypical workflow example\n1) Import model: from ONNX/PyTorch into Relay.\n2) Apply Relay passes: constant fold, fuse, quantize, etc., configured for target=\"cuda\".\n3) Lower to TIR: generate primitive functions for fused regions.\n4) Schedule/auto-tune: run MetaSchedule to find fast kernels on your specific GPU.\n5) Build: emit CUDA modules and link with TVM runtime.\n6) Run: load module, set inputs, execute on GPU via TVM runtime.\n\nKey takeaway\nRelay doesn’t “talk to the GPU” directly. It expresses and optimizes the high-level program, then hands off to TVM’s TIR, scheduler, and codegen to produce GPU kernels and runtime calls. In the overall ecosystem, Relay is the top-level compiler IR that bridges framework models to efficient, device-specific implementations across GPUs and other hardware.","created_at":1757968920074,"messageId":"881383c9-dca9-4b62-bda0-204943d9a603","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Short answer:\n- Relay is the high-level IR and graph/program compiler in the TVM stack.\n- It targets GPUs by lowering Relay programs to TVM’s tensor-level IR (TIR), which is then scheduled and code-generated into CUDA/ROCm kernels and runtime calls.\n- In the ecosystem, Relay sits above TIR and codegen backends, and below frontends that import models from PyTorch/ONNX/etc.\n\nMore detail:\n\nWhere Relay sits in the stack\n- Frontends: Import models from frameworks (e.g., PyTorch via TorchScript/FX, TensorFlow, ONNX). These are translated into Relay.\n- Relay (program-level IR): Functional, typed IR for whole-model optimization and automatic differentiation. Performs high-level passes: type/shape inference, inlining, DCE, CSE, constant folding, operator fusion, layout transforms, quantization transformations, etc.\n- Lowering boundary: After high-level optimizations, each Relay op or fused region is lowered into TIR (Tensor IR).\n- TIR (tensor-level IR): Describes loop nests, memory layouts, and thread/block bindings abstractly.\n- Scheduling/auto-tuning: TVM’s schedulers (incl. AutoTVM/Ansor/MetaSchedule) produce optimized schedules for the target hardware.\n- Codegen backends: Generate target-specific code:\n  - GPUs: CUDA (NVIDIA), ROCm/HIP (AMD), OpenCL, Metal, Vulkan, or via vendor libraries like cuDNN/cuBLAS when available.\n  - CPUs and accelerators: LLVM, VTA, etc.\n- Runtime: TVM runtime loads compiled modules, manages device contexts, memory, and executes kernels. Relay itself does not execute; the runtime executes compiled artifacts.\n\nHow Relay connects to GPUs\n- Target specification: You compile with a GPU target (e.g., target=\"cuda\" for NVIDIA). Relay passes are target-aware and may pick fusion/layout strategies suited for GPUs.\n- Operator fusion and partitioning: Relay groups graph regions into fused functions. Each fused function becomes a TIR “primitive” to be scheduled for the GPU, minimizing kernel launches and intermediate writes.\n- Lowering to TIR with GPU semantics: In TIR, loops are mapped to GPU threads/warps/blocks via thread axes (`blockIdx.x`, `threadIdx.x`, etc.). Memory scopes like global, shared, local are used to control placement and caching.\n- Scheduling and autotuning: Schedules tile loops, bind axes to threads/blocks, use shared memory, vectorize, unroll, and pipeline to match GPU characteristics. Auto-tuners search schedule spaces for best performance on the specific GPU.\n- Code generation and libraries:\n  - TVM can emit CUDA C/PTX and link it into a loadable module.\n  - For certain ops, TVM may call into vendor libraries (e.g., cuBLAS for GEMM, cuDNN for convolutions) via external codegen paths or composite patterns. Relay can “partition” subgraphs to those external codegens.\n- Runtime execution:\n  - The TVM runtime manages a cuda/rocm device, allocates device memory, transfers inputs/outputs, and launches kernels.\n  - Packed function interface allows invoking compiled functions from Python/C++/Rust, etc.\n\nEcosystem relationship\n- Relay vs ONNX/MLIR/XLA: Relay is TVM’s high-level IR, analogous in layer to XLA HLO or MLIR’s high-level dialects. Unlike fixed op-graphs, it’s a functional language with shape-dependent typing and whole-program AD.\n- Extensibility: New ops or fused patterns can be expressed in Relay and lowered through the same pipeline to GPUs or other devices without needing framework changes.\n- Deployment: Relay is typically used offline in a “compile then run” flow to produce a deployable module that the TVM runtime loads on the target GPU device.\n\nTypical workflow example\n1) Import model: from ONNX/PyTorch into Relay.\n2) Apply Relay passes: constant fold, fuse, quantize, etc., configured for target=\"cuda\".\n3) Lower to TIR: generate primitive functions for fused regions.\n4) Schedule/auto-tune: run MetaSchedule to find fast kernels on your specific GPU.\n5) Build: emit CUDA modules and link with TVM runtime.\n6) Run: load module, set inputs, execute on GPU via TVM runtime.\n\nKey takeaway\nRelay doesn’t “talk to the GPU” directly. It expresses and optimizes the high-level program, then hands off to TVM’s TIR, scheduler, and codegen to produce GPU kernels and runtime calls. In the overall ecosystem, Relay is the top-level compiler IR that bridges framework models to efficient, device-specific implementations across GPUs and other hardware.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_023e2843ee999dba0068c87a18b79081a1baa0cdfc18f90f79"}},"resumableStreamId":"STREAM:2b31946a-25c4-458b-b1d0-1c3f8566a862","role":"assistant","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","timeToFirstToken":0.754,"tokens":987,"tokensPerSecond":58.75,"updated_at":1757968920074,"userId":"google:104458561188320708185","id":"j974hc3n7jx2sxagd7ththyywh7qm0ft"},{"_creationTime":1757968996398.0107,"_id":"j97c279tz3a5sgfgg7x9gykbhd7qm78c","attachmentIds":[],"content":"how much is relay used in current day AI infrasutrcture?","created_at":1757968996398,"messageId":"1dccd8b5-8a51-42f0-9210-6eb200095816","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","updated_at":1757968996398,"userId":"google:104458561188320708185","id":"j97c279tz3a5sgfgg7x9gykbhd7qm78c","providerMetadata":{}},{"_creationTime":1757968996398.011,"_id":"j971ze5he8125wd1pyzz5m7ehs7qm3ba","attachmentIds":[],"byok":false,"content":"Short answer: Relay is still present in Apache TVM, but its role in day-to-day AI infrastructure has diminished as the TVM project shifted focus to the newer Relax IR and TensorIR in the “TVM Unity” direction. In production, most large orgs standardize on framework-native stacks (PyTorch 2.x Inductor/Triton, TensorRT, OpenXLA/XLA, ONNX Runtime) and only a minority use TVM/Relay at scale today.\n\nMore detail:\n- Within TVM itself:\n  - Relay remains the high-level, functional IR described in the 2019 paper and is still in the codebase and docs. It’s used by existing frontends and legacy models.\n  - TVM’s current roadmap emphasizes Relax (a newer graph/program IR with first-class symbolic shapes and progressive lowering) plus TensorIR for kernel-level optimization. Many new features, LLM-centric flows, and Python-first transformations target Relax/TIR rather than Relay.\n  - Community discussions and docs describe phasing out legacy components and pushing toward Relax/TIR for end-to-end pipelines; Relay isn’t “removed,” but it’s no longer the primary innovation focus.\n\n- In broader industry:\n  - Production inference stacks commonly rely on:\n    - NVIDIA: TensorRT/Torch-TensorRT, cuDNN/cuBLAS, Triton Inference Server.\n    - PyTorch 2.x: Dynamo/Inductor with Triton kernels, which captured a lot of “compiler” mindshare.\n    - Google/Alphabet ecosystem: OpenXLA/XLA, StableHLO/MLIR.\n    - ONNX Runtime: widely used for portability and hardware provider EPs.\n  - TVM (and thus Relay in older flows) sees targeted use at companies needing custom hardware support, embedded/edge deployments, or cross-vendor portability—often in research, startups, or specific product lines. Public adopters have included various chip vendors and embedded players, but this is a smaller slice compared to the dominant vendor stacks above.\n\n- Practical takeaway if you’re choosing today:\n  - If you’re on mainstream NVIDIA/AMD/CPU and primarily PyTorch: PyTorch Inductor/Triton or TensorRT usually yields better engineering fit and support.\n  - If you need portable compilation across diverse or custom accelerators, or want heavy program-level control: use TVM’s modern path (Relax + TensorIR). Relay can still work for legacy models, but new projects should plan around Relax.\n\nIn short: Relay influenced many later designs and is still available in TVM, but current “AI infra” use is limited; the TVM community’s momentum and most new development are centered on Relax/TensorIR rather than Relay.","created_at":1757968996399,"messageId":"1341905e-535a-4155-a311-1296b6dab1d6","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"args":{"category":"news","domains":null,"queries":["Relay TVM adoption 2025 usage who uses TVM Relay production 2025-09","Apache TVM Relay IR usage current 2024 2025 companies production GPU inference","Relay vs MLIR adoption 2025 TVM stack Relay deprecation?"]},"result":[{"content":"[Home](https://www.intelligent-power-today.com/) [Protection](https://www.intelligent-power-today.com/channel/protection) [Electrical Protection](https://www.intelligent-power-today.com/category/electrical-protection) [Transformer Protection](https://www.intelligent-power-today.com/category/electrical-protection/transformer-protection)\n\n# The Rise of Digital Relays\n\nFor decades, transformer protection relied on analog electromechanical relays. While these served their purpose, the advent of digital relays has revolutionized this crucial aspect of power system operation. This article explores the key advantages of digital relays over their analog counterparts, highlighting their impact on transformer protection performance. This article explores the emergence of digital relays in transformer protection, highlighting their benefits, performance, and technical considerations.\n\n[Visit Our Electrical Protection Study Course](https://electricityforum.com/electrical-training/advanced-protective-relay-training)\n\n## Limitations of Traditional Analog Relays\n\nAnalog relays operate on the principle of comparing electrical quantities like current or voltage to predetermined thresholds. These relays typically utilize physical components like coils, levers, and contacts to perform their functions. While reliable, analog relays have several limitations:\n\n- **Limited Functionality:** Analog relays are designed for specific protection functions, such as overcurrent or overload protection. Implementing complex protection algorithms can be challenging with analog technology.\n- **Slow Response Times:** Analog relays rely on mechanical components for operation, which can introduce delays in response to fault conditions. This can lead to increased fault clearing times and potentially more extensive equipment damage.\n- **Limited Diagnostics:** Fault analysis with analog relays often requires manual interpretation of relay flags and physical inspection. This can be time-consuming and prone to human error.\n\n## Advantages of Digital Relays\n\n### Enhanced Accuracy and Sensitivity\n\nDigital relays provide superior accuracy and sensitivity compared to analog relays. They employ sophisticated algorithms to analyze electrical signals, allowing for more precise detection of faults. This accuracy is crucial for identifying and isolating issues before they can cause damage to the transformer or affect the broader power grid.\n\n### Faster Response Times\n\nOne of the most significant benefits of digital relays is their rapid response to fault conditions. These devices can process and react to complex electrical data much faster than analog relays, enabling quicker isolation of faults and minimizing the potential for extensive system damage.\n\n### Improved Diagnostics and Monitoring\n\nDigital relays come equipped with advanced diagnostic and monitoring capabilities. They continuously record and analyze electrical parameters, providing valuable data that can be used for preventive maintenance and fault analysis. This feature helps in identifying potential problems before they escalate into major failures, thus enhancing the overall reliability of the transformer protection system.\n\n## Integration with Smart Grid Technologies\n\nDigital relays are inherently compatible with smart grid technologies, allowing for seamless integration into modern power systems. They can communicate with other smart devices, enabling coordinated actions and sharing vital information for grid management. This integration facilitates a more dynamic and responsive protection system, capable of adapting to changing conditions in real time.\n\n### Remote Configuration and Control\n\nThe ability to configure and control digital relays remotely is a significant advantage. This feature allows for adjustments to be made without the need for physical interaction with the relay, reducing maintenance costs and improving safety for personnel.\n\n## Technical Considerations\n\n### System Compatibility\n\nIntegrating digital relays into existing power systems can present challenges, particularly in terms of compatibility with older equipment. Careful planning and testing are required to ensure that the new digital relays can effectively communicate and operate within the established infrastructure.\n\n### Cybersecurity\n\nWith the increased use of digital and networked technologies, cybersecurity becomes a crucial consideration. Digital relays must be equipped with robust security measures to protect against cyber threats, ensuring the integrity and reliability of the transformer protection system.\n\n## The Future of Digital Relays\n\nThe trend towards digitalization in transformer protection is clear, with digital relays set to play a pivotal role in the future of power systems. Their advanced features and capabilities represent a significant leap forward in terms of efficiency, reliability, and flexibility.\n\nDigital relays have transformed the landscape of transformer protection, offering numerous benefits over traditional analog relays, including improved accuracy, faster response times, and enhanced diagnostic capabilities. As power systems continue to evolve, the adoption of digital relays is set to increase, driven by their ability to meet the demanding requirements of modern electrical infrastructure. Ensuring system compatibility and addressing cybersecurity are essential to maximizing the potential of digital relay technology, paving the way for more resilient and efficient power systems.\n\n## Related Articles\n\n[Electrical Protection](https://www.intelligent-power-today.com/category/electrical-protection)\n\n### [Optimizing Protection for Industrial Applications](https://www.intelligent-power-today.com/electrical-protection/industrial-applications)\n\nTransformers play a critical role in industrial facilities, stepping up or down voltage levels to power motors, machinery, and lighting systems....\n\n[Electrical Protection](https://www.intelligent-power-today.com/category/electrical-protection)\n\n### [Solid-State Transformer Protection: A Game Changer?](https://www.intelligent-power-today.com/electrical-protection/transformer-protection/solid-state-protection)\n\nThe advent of solid-state transformer (SST) technology marks a significant evolution in the field of electrical transformer protection. Moving beyond...\n\n[Electrical Protection](https://www.intelligent-power-today.com/category/electrical-protection)\n\n### [Transformer Protection And High Impedance Faults](https://www.intelligent-power-today.com/electrical-protection/transformer-protection/high-impedance-faults)\n\nElectrical transformers are vital components of the power distribution network, ensuring the transfer of electricity across various voltage levels....\n\n[Electrical Protection](https://www.intelligent-power-today.com/category/electrical-protection)\n\n### [Improving Grid Stability](https://www.intelligent-power-today.com/electrical-protection/transformer-protection/improving-grid-stability)\n\nElectrical transformers are vital components in power transmission and distribution systems, stepping up or down voltage levels to efficiently...\n\n[Electrical Protection](https://www.intelligent-power-today.com/category/electrical-protection)\n\n### [Conventional Methods to AI-Based Solutions](https://www.intelligent-power-today.com/electrical-protection/transformer-protection/ai-based-solutions)\n\nEvolution of Transformer Protection\nThe protection of electrical transformers is crucial for maintaining the stability and efficiency of power...\n\n[Electrical Protection](https://www.intelligent-power-today.com/category/electrical-protection)\n\n### [Protection for Smart Grid Integration](https://www.intelligent-power-today.com/electrical-protection/smart-grid-integration)\n\nThe concept of smart grids is revolutionizing the power delivery landscape. Characterized by two-way power flow, distributed generation, and...\n\n[Electrical Protection](https://www.intelligent-power-today.com/category/electrical-protection)\n\n### [Cybersecurity Threats and Transformer Protection](https://www.intelligent-power-today.com/electrical-protection/transformer-protection/cybersecurity-threats)\n\nThe increasing reliance on digital technologies for power system operation has introduced new vulnerabilities. Transformer protection systems,...\n\n[Electrical Protection](https://www.intelligent-power-today.com/category/electrical-protection)\n\n### [Overcoming Relay Coordination Challenges](https://www.intelligent-power-today.com/electrical-protection/transformer-protection/relay-coordination-challenges)\n\nRelay coordination in electrical transformer protection is a critical process that ensures the safety and reliability of the power system. Effective...\n\n[Protective Relays](https://www.intelligent-power-today.com/category/protective-relays)\n\n### [Differential Protection for Power Transformers](https://www.intelligent-power-today.com/protective-relays/differential-protection-relays/differential-protection-for-power-transformers)\n\nTransformers are vital components in electrical power systems, facilitating the transfer of electrical energy between different circuit voltages....","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.intelligent-power-today.com%2Felectrical-protection%2Ftransformer-protection%2Fdigital-relays","id":"https://www.intelligent-power-today.com/electrical-protection/transformer-protection/digital-relays","publishedDate":"2025-09-24T00:00:00.000Z","summary":"The webpage discusses the transition from traditional analog electromechanical relays to advanced digital relays in transformer protection. It highlights the limitations of analog relays—such as limited functionality, slower response times, and manual diagnostics—and emphasizes the advantages of digital relays, including higher accuracy, faster fault detection, enhanced diagnostics, and better integration with smart grid technologies. Digital relays also support remote configuration, improving maintenance efficiency and system reliability. Overall, the shift to digital relays significantly enhances transformer protection performance and aligns with modern, intelligent power system management.","title":"The Rise of Digital Relays in Transformer Protection","url":"https://www.intelligent-power-today.com/electrical-protection/transformer-protection/digital-relays"},{"content":"![red logo](https://cdn.asp.events/CLIENT_IBC_4ED0594D_5056_B739_542FB395BDA17423/sites/ibc-2024/media/sml-logo.png)\n\n[REGISTER YOUR INTEREST FOR IBC2025](https://show.ibc.org/register-interest-ibc2025) [BOOK A STAND](https://show.ibc.org/exhibitor-enquiry-ibc2025)\n\n# Accelerator Project\n\n### **RESPONSIVE NARRATIVE FACTORY**\n\n### FINAL SHOWCASE SESSION\n\n[iframe](https://players.brightcove.net/3737230800001/rJ6KcZ7aQ_default/index.html?videoId=6337360071112)\n\n**Watch the Responsive Narrative Factory project's Final Showcase Session in the hero video above, and also via IBC365.**\n\n**[View on IBC365](https://www.ibc.org/video/ibc-accelerators-responsive-narrative-factory/10163.article)**\n\n**Delivering customised video experiences at scale.**\n\nThe success of future streaming services will depend on how effectively that video content is tailored to individual preferences and needs.\n\nIf a viewer feels that a program was personally made for them, then engagement improves and the chances of repeat viewing and personal recommendation increase. The Responsive Narrative Factory Accelerator project demonstrated at IBC 2023 that dynamically aligning rich media metadata with viewing preference metadata creates highly customised and extremely engaging viewing experiences. The Responsive Narrative Factory approach exceeded other demonstrations of personalised video that have had limited industry traction to date.\n\nTo fully realise the potential for customisation at scale requires rich datasets, more efficient ways of generating and transporting metadata, and to adopt a modular approach to content production and delivery.\n\nThis project brought together subject experts across the supply chain who collaborated to identify and overcome the many challenges of broadcasting personalised video.\n\n**This proof of concept was delivered using existing CDN technology by a multi-disciplinary team of over 40, including:**\n\n**Champions**: BBC, IET\n\n**Participants**: Infuse Video, Metarex, Cuvo.io, JPB Media Solutions, EZDRM\n\n* * *\n\n**Kickstart Day Pitch:**\n\n[iframe](https://www.youtube.com/embed/sxdlu-CC2nc)\n\n### The Objectives\n\nDevelop a scalable solution for broadcasters to deliver user-defined storytelling using modular video components. This will improve engagement, value, and accessibility by tailoring content to each viewer. By combining emerging technologies, to innovate a way to transform custom video delivery from experimental one-off projects to a commercially viable and mainstream approach. **The key goals of the project are to achieve the following:**\n\n**Increase engagement and value**\n\nAudiences receive greater value by choosing their own viewing experience, which has been proven to increase their demand and engagement time.\n\n**Improve inclusivity**\n\nDynamically create age-appropriate, inclusive, and accessible content by compiling metadata-tagged video segments.\n\n**Reduce operational costs**\n\nStreamline production and versioning workflows, reducing content duplication and storage usage.\n\n### The Challenges\n\nCustomised video experiences have not been widely available due to current workflows and technology limitations. To make the transition from linear to multi-version, the following needed to be addressed.\n\n**Metadata enrichment, transport and use**\n\nMore efficient processes and technologies are needed to produce the granular level of metadata required for customisation. A mechanism must be developed to ensure existing metadata is preserved and easily combined with generated metadata. It then needs to be delivered in a format to identify the video segments that align with viewing preferences.\n\n**Production and workflows**\n\nThe approach to production, content management, preparation, and storage would be improved by being adapted to accommodate componentised rather than full-length content.\n\n**Segment compilation and delivery**\n\nSystems and rules need to be developed to combine video segments seamlessly and retaining editorial control over the narrative\n\n### Project Delivery\n\nThe team successfully developed a new methodology and technical solution to enable customised content to be delivered at scale. These are key elements of the proof of concept.\n\n**Content selection**\n\nVideo customisation is particularly well suited to magazine programs where the program is made up of standalone segments. For this reason, an episode of the BBC nature program Springwatch was selected to provide a practical demonstration of how content can be customised retrospectively.\n\n**Metadata enrichment**\n\nTemporal metadata is required to segment the program and to identify the required sections of video. To generate metadata at scale and cost-effectively machine learning was applied to identify text, scene timings, and images. All data was displayed on a single timeline for manual verification and to ensure editorial context was maintained.\n\n**Metadata**\n\nCurrent broadcast workflows lose metadata that can used downstream for customisation. To preserve metadata the new metadata framework MetaRex was used to preserve, collate, transport and deliver the Metadata which was used to configure the video segments.\n\n**Platform and player**\n\nThe Infuse video player was created to take temporal metadata to identify video segments and to create playlists from these using predefined rules to create tailored videos. The media assets were taken from IMF packages to create customised versions.\n\n**The result**\n\nThe 59 minute episode of the BBC’s Springwatch, nature programme was used to demonstrate our approach and the technology was used to retrospectively reconfigure/ produce over 3.6 Million versions at a comparable to cost to creating several versions using traditional workflows.\n\n## The Business case for widespread adoption\n\n**Engagement and Value**\n\n**Why do audiences care?**\n\n- Audiences return to content that feels Just Right\n- 1 content package gives millions of audience options\n- Editorial (& Easter Egg) control retained by producer\n- Per-versioning costs drop rapidly with scale, with up to 90% saving\n\n**Social responsibility**\n\n**Why is this a better supply chain?**\n\n- Audiences are able to modify content to a broader set of personal sensitivities.\n- More personalisation. No redundant rendering\n- More accessibility, inclusivity and localisation for a lower cost\n\n**New markets**\n\n**Why invest?**\n\n- New small markets become viable as localisation costs fall\n- Long-tail library content becomes affordable\n- Targeted opt-in advertising related to opt-in content\n\n### FINAL POC RESULTS\n\n[iframe](https://www.youtube.com/embed/zDoiAQOLZGg?si=YEwNhLMIj3azYqfP)\n\n[iframe](https://www.youtube.com/embed/-naOAx170nk?si=-9RtXOqjujznmZkJ)\n\n[iframe](https://www.youtube.com/embed/k9BibEIYes0?si=vnPwHqjRMS-HuTSi)\n\n[iframe](https://www.youtube.com/embed/cyLdIKVPp50?si=528a6d-TcqxNo1yy)\n\n## **What industry leaders have to say about the Responsive Narrative Factory**\n\n**\"The RNF accelerator programme is a very interesting project and one Clearcast followed closely.**\n\n**As we move to more complex dynamic, multi-version and addressable advertising workflows driven by user data, it's important that we're able to improve the end-to-end supply chain by incorporating containerisation formats such as IMF.\"**\n\nHead Of Technology, Clearcast Ltd\n\n**\"I am enthusiastic about the Responsive Narrative Factory (RNF) proof of concept as it has the potential to revolutionize the localization process and experience for AMC Studios International digital streaming audiences worldwide.**\n\n**It is good to see the implementation of Metadata driving IMF elements to deliver tailor-made experiences to meet a diverse set of individual customer preferences, cultural sensitivities, accessibility and regulatory requirements.**\n\n**I believe my colleagues who also lead media distribution supply chain operations would also be thrilled to be able to achieve higher levels of customization while reducing the cost of editing, storing and transporting full length regional versions.\"**\n\nVP - Program Operations bij AMC Studios International\n\n**\"This tops my list of favourite things at IBC.\"**\n\nDirector of Technology - Hollywood Studio.\n\n**\"How are you doing that? It's not possible.\"**\n\nCTO - Major Film School\n\n**\"This changes the way can personalise adverts.\"**\n\nCEP - Advert Metadata Broker\n\n**“The MOST exciting new concept at IBC this year... a long time coming, but this looks like the dawn of a new type of both video production and consumption.”**\n\nProduct Manager – BroadScreen.\n\n### **Champions:**\n\nLoading\n\n- [![BBC R&D](https://cdn.asp.events/CLIENT_IBC_4ED0594D_5056_B739_542FB395BDA17423/sites/IBC-2021/media/accelerator-logos/BBC1.png/fit-in/500x500/filters:no_upscale())](javascript:openRemoteModal('2023-accelerators/bbc-rd','ajax',{},false,'',{\"dimension15\":\"2023-accelerators/bbc-rd\"});)\n\n- [![IET](https://cdn.asp.events/CLIENT_IBC_4ED0594D_5056_B739_542FB395BDA17423/sites/ibc-2023/media/libraries/2023-accelerators/IET-logo.png/fit-in/500x500/filters:no_upscale())](javascript:openRemoteModal('2023-accelerators/iet','ajax',{},false,'',{\"dimension15\":\"2023-accelerators/iet\"});)\n\n\n### **Participants:**\n\nLoading\n\n- [![Cuvo.io](https://cdn.asp.events/CLIENT_IBC_4ED0594D_5056_B739_542FB395BDA17423/sites/ibc-2023/media/libraries/2023-accelerators/Logo_blue-1.png/fit-in/500x500/filters:no_upscale())](javascript:openRemoteModal('2023-accelerators/cuvoio','ajax',{},false,'',{\"dimension15\":\"2023-accelerators/cuvoio\"});)\n\n- [![EZDRM](https://cdn.asp.events/CLIENT_IBC_4ED0594D_5056_B739_542FB395BDA17423/sites/ibc-2023/media/libraries/2023-accelerators/ezdrm-logo-full-color-trademark-RGB.png/fit-in/500x500/filters:no_upscale())](javascript:openRemoteModal('2023-accelerators/ezdrm','ajax',{},false,'',{\"dimension15\":\"2023-accelerators/ezdrm\"});)\n\n- [![Infuse Video](https://cdn.asp.events/CLIENT_IBC_4ED0594D_5056_B739_542FB395BDA17423/sites/ibc-2023/media/libraries/2023-accelerators/logo-color-smallmargin.png/fit-in/500x500/filte","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fshow.ibc.org%2Faccelerator-project-responsive-narrative-factory","id":"https://show.ibc.org/accelerator-project-responsive-narrative-factory","publishedDate":"2025-09-15T00:00:00.000Z","summary":"The webpage highlights the \"Responsive Narrative Factory\" project showcased at IBC2025, focusing on delivering personalized video experiences at scale. The initiative, demonstrated at IBC 2023, aims to enhance viewer engagement by dynamically tailoring content through rich metadata and modular video components. Key objectives include increasing engagement, improving accessibility and inclusivity, and reducing operational costs by streamlining production workflows. The project involves a multidisciplinary team, including BBC and IET, and addresses challenges related to metadata management, content production, and seamless segment delivery. Overall, it represents a significant step toward making personalized broadcasting a mainstream, commercially viable approach.","title":"Accelerator Project: Responsive Narrative Factory | IBC2025 Show 12-15 Sep 2025","url":"https://show.ibc.org/accelerator-project-responsive-narrative-factory"},{"content":"\\[advertisement\\]\n\n[Topics](https://www.geartechnology.com/topics/) \\> Industry News\n\n# Industry News\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9,fo-face/2025/04/18/Renk_UD4Vt_M_X.jpg)](https://www.geartechnology.com/renk-group-ag-completes-acquisition-of-cincinnati-gearing-systems-inc)\n\n**INDUSTRY NEWS** \\| **2025-04-16**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [RENK Group AG Completes Acquisition of Cincinnati Gearing Systems Inc.](https://www.geartechnology.com/renk-group-ag-completes-acquisition-of-cincinnati-gearing-systems-inc)\n\nRENK Group AG, a leading supplier of military and civilian propulsion solutions, has successfully completed the acquisition of Cincinnati Gearing Systems Inc. (“CGS Inc.”), a U.S.-based manufacturer of precision gears and power transmission technologies. Effective immediately, CGS Inc. will operate under the name RENK America Marine & Industry LLC.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9,fo-face/2025/04/04/Henrik_Ager_jthWc5Bf3.png)](https://www.geartechnology.com/hoganas-announces-biochar-investment-in-2024-sustainability-report)\n\n**INDUSTRY NEWS** \\| **2025-04-04**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Höganäs Announces Biochar Investment in 2024 Sustainability Report](https://www.geartechnology.com/hoganas-announces-biochar-investment-in-2024-sustainability-report)\n\nHöganäs is releasing its 2024 sustainability report, highlighting key progress across climate action, safety, responsible sourcing, and diversity and inclusion. One of the major announcements in the report is a strategic investment in biochar, following successful full-scale trials, to enable a significant shift in sponge iron production in Höganäs, Sweden.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9,fo-face/2025/03/25/TFC2025_4Dx-2dldG.png)](https://www.geartechnology.com/stle-2025-tribology-frontiers-conference-opens-call-for-papers)\n\n**INDUSTRY NEWS** \\| **2025-03-25**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [STLE 2025 Tribology Frontiers Conference Opens Call for Papers](https://www.geartechnology.com/stle-2025-tribology-frontiers-conference-opens-call-for-papers)\n\nThe Society of Tribologists and Lubrication Engineers ( [STLE](http://send.stle.org/link.cfm?r=3kqwUxb0ewPd3QuHlxoi_A~~&pe=n_7fzQeMi6Vj7Bh0J07jG4nE58bWDmVLDope-6cPB88w1efoIFHxHTb5op3nvfhVO5xeoPvtnUQaFP0W4tIdRw~~&t=fp6dEg0fHP6Y-gmjBpvaQQ~~)) — the technical society for individuals in the field of tribology and lubrication engineering — is hosting the 2025 [STLE Tribology Frontiers Conference](http://send.stle.org/link.cfm?r=3kqwUxb0ewPd3QuHlxoi_A~~&pe=XVQLe9ZZC9LOzfNl97yt-fvPC1B3uaJLFSL4opWNDetcWDo9tFfi1iSzVUgw7zjaTOyvqmogQ4WM8nPAPdQM9g~~&t=fp6dEg0fHP6Y-gmjBpvaQQ~~) (TFC) on October 14-16 at Argonne National Laboratory in Lemont, IL.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9,fo-face/2025/03/21/FCG_HappyWorkiversary-2-800x418_k3cmWvs97.jpg)](https://www.geartechnology.com/forest-city-gear-celebrates-3-anniversaries-roger-mcmullin-paul-christoph-mark-javurek)\n\n**INDUSTRY NEWS** \\| **2025-03-24**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Forest City Gear Celebrates 3 Anniversaries: Roger McMullin, Paul Christoph, Mark Javurek](https://www.geartechnology.com/forest-city-gear-celebrates-3-anniversaries-roger-mcmullin-paul-christoph-mark-javurek)\n\nForest City Gear, a manufacturer of fine and medium pitch custom gears, proudly recognize several employees for their long-term dedication to the company, including Roger McMullin (10 years), Paul Cristoph (5 years), and Mark Javurek (5 years).\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9,fo-face/2025/03/21/Markus_Stolmar_RCHB6qqOH.jpg)](https://www.geartechnology.com/united-grinding-s-stolmar-elected-to-amt-board-of-directors)\n\n**INDUSTRY NEWS** \\| **2025-03-21**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [United Grinding’s Stolmar Elected to AMT Board of Directors](https://www.geartechnology.com/united-grinding-s-stolmar-elected-to-amt-board-of-directors)\n\nMarkus Stolmar, president and CEO of United Grinding North America Inc., a subsidiary of the United Grinding Group, was recently elected to the Association for Manufacturing Technology (AMT) board of directors. AMT represents U.S.-based providers of manufacturing technology, speeding the pace of innovation, increasing global competitiveness and developing the industry’s advanced workforce of tomorrow.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9,fo-face/2025/03/20/Teradyne_PuJHD3-4x.jpg)](https://www.geartechnology.com/teradyne-robotics-to-debut-ai-accelerator-powered-solutions-at-nvidia-gtc-2025)\n\n**INDUSTRY NEWS** \\| **2025-03-20**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Teradyne Robotics to Debut AI Accelerator-Powered Solutions at NVIDIA GTC 2025](https://www.geartechnology.com/teradyne-robotics-to-debut-ai-accelerator-powered-solutions-at-nvidia-gtc-2025)\n\nTeradyne Robotics and its partners are set to unveil a suite of advanced, AI-driven robotics solutions at NVIDIA GTC 2025 March 17-21. The unveiling represents the first public demonstration of the AI Accelerator in commercially viable applications.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9,fo-face/2025/03/19/KISSsoft25_0Fo6uKU9b.png)](https://www.geartechnology.com/kisssoft-2025-available-in-april)\n\n**INDUSTRY NEWS** \\| **2025-03-19**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [KISSsoft 2025 Available in April](https://www.geartechnology.com/kisssoft-2025-available-in-april)\n\nThe new _KISSsoft Release_ brings innovations and enhancements for results visualization, allowing you to design even more efficiently, precisely, and productively. Within Gleason’s strategy \"Design– Manufacturing – Measure\", KISSsoft continues to add functions for simulating cylindrical gear manufacturing. After all, you target a gear design combining the highest strength with low manufacturing costs. KISSsoft offers a variety of rating procedures. Additionally, bearing ratings methods according to Timken have been added.\n\n\\[advertisement\\]\n\n![](https://www.geartechnology.com/arrow-left.png)![](https://www.geartechnology.com/arrow-right.png)\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9,fo-face/2025/03/14/Done_in_One_fXvKnvWIF.jpg)](https://www.geartechnology.com/experience-klingelnberg-s-innovative-measurement-technology-live)\n\n**INDUSTRY NEWS** \\| **2025-03-14**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Experience Klingelnberg's Innovative Measurement Technology Live](https://www.geartechnology.com/experience-klingelnberg-s-innovative-measurement-technology-live)\n\nFrom May 6–9, 2025, the 37th Control in Stuttgart, Germany, will open its doors and offer trade visitors a comprehensive overview of the latest technologies, processes, products and system solutions in the field of industrial quality assurance. Klingelnberg will be taking part again this year and is pleased to present the Klingelnberg Precision Measuring Centers P 26, P 40 and P 152, among others.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9,fo-face/2025/03/12/Liebherr_ylt3wjkPQ.jpg)](https://www.geartechnology.com/liebherr-launches-new-parts-shop)\n\n**INDUSTRY NEWS** \\| **2025-03-13**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Liebherr Launches New Parts Shop](https://www.geartechnology.com/liebherr-launches-new-parts-shop)\n\nWith the launch of the new Parts Shop, which will replace the ten existing shops, Liebherr is standardizing the extensive range of spare parts. From March 10, 2025, the Parts Shop is pooling together the entire range from all product segments, including earthmoving, material handling technology, deep foundation, maritime cranes, mobile and crawler cranes, tower cranes, concrete technology, gear technology, automation systems, mining and components, in one central location.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9,fo-face/2025/03/12/Gleason-160_pQQ3SUgOU.jpg)](https://www.geartechnology.com/gleason-celebrates-160th-anniversary)\n\n**INDUSTRY NEWS** \\| **2025-03-12**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Gleason Celebrates 160th Anniversary](https://www.geartechnology.com/gleason-celebrates-160th-anniversary)\n\nThis year marks an important milestone as Gleason Corporation celebrates its 160th anniversary. Founded in 1865 by William Gleason, the company has transformed from a small workshop to a global leader in gear technology, offering comprehensive solutions that encompass design software, manufacturing and metrology equipment, tooling, aftermarket services, and an extensive training program.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9,fo-face/2025/03/12/EvanHundley_2ZCtLWalaT.jpg)](https://www.geartechnology.com/ipsen-usa-announces-management-changes-in-customer-service-team)\n\n**INDUSTRY NEWS** \\| **2025-03-01**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Ipsen USA Announces Management Changes in Customer Service Team](https://www.geartechnology.com/ipsen-usa-announces-management-changes-in-customer-service-team)\n\nIpsen is pleased to announce the promotion of Evan Hundley to retrofits manager and the appointment of Lu Chouraki as field service manager.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9,fo-face/2025/03/20/DavidBrownSantasalo_jvN0rfT2N.jpg)](https://www.geartechnology.com/david-brown-santasalo-expands-carbon-footprint-products-reporting-to-girth-gears)\n\n**INDUSTRY NEWS** \\| **2025-03-01**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [David Brown Santasalo Expands Carbon Footprint Products Reporting to Girth Gears](https://www.geartechnology.com/david-brown-santasalo-expands-carbon-footprint-products-reporting-to-girth-gears)\n\nAs a part of David Brown Santasalo’s (DB Santasalo) ongoing commitment to the Science Based Targets initiative (SBTi), th","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.geartechnology.com%2Ftopics%2Findustry-news%3Fpage%3D120","id":"https://www.geartechnology.com/topics/industry-news?page=120","publishedDate":"2025-09-16T00:00:00.000Z","summary":"The webpage provides recent industry news related to gear manufacturing and engineering. Highlights include the acquisition of Cincinnati Gearing Systems Inc. by RENK Group AG, now operating as RENK America Marine & Industry, enhancing their gear and power transmission capabilities. Höganäs announced a strategic investment in biochar in its 2024 sustainability report, aiming to improve sponge iron production and sustainability efforts. The Society of Tribologists and Lubrication Engineers (STLE) is calling for papers for its 2025 Tribology Frontiers Conference at Argonne National Laboratory. Additionally, Forest City Gear celebrated employee anniversaries, recognizing long-term dedication, and United Grinding’s Stolmar was elected to the Association for Manufacturing Technology (AMT) Board of Directors.","title":"Industry News","url":"https://www.geartechnology.com/topics/industry-news?page=120"},{"content":"\\[advertisement\\]\n\n[Topics](https://www.powertransmission.com/topics/) \\> [Departments](https://www.powertransmission.com/topics/departments) \\> Product News\n\n# Product News\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9/2025/04/16/ELE-Tolomatic_NQhyFKwvW3.jpg)](https://www.powertransmission.com/tolomatic-showcases-latest-servo-linear-actuators-at-automate-2025)\n\n**PRODUCT NEWS** \\| **2025-04-16**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Tolomatic Showcases Latest Servo Linear Actuators at Automate 2025](https://www.powertransmission.com/tolomatic-showcases-latest-servo-linear-actuators-at-automate-2025)\n\nTolomatic Inc. will be showcasing its latest servo linear actuators at Automate 2025 (Booth #7004). Featuring an integrated servomotor, Tolomatic servo linear actuators are ideal for applications where dynamic movement, precise positioning and force repeatability are a priority.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9/2025/04/14/AMC_Image_FD100-25-CM-scaled_5O_t965RI.jpg)](https://www.powertransmission.com/advanced-motion-controls-introduces-canopen-communication-to-20-90-vdc-compact-servo-drives)\n\n**PRODUCT NEWS** \\| **2025-04-15**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Advanced Motion Controls Introduces CANopen Communication to 20-90 VDC Compact Servo Drives](https://www.powertransmission.com/advanced-motion-controls-introduces-canopen-communication-to-20-90-vdc-compact-servo-drives)\n\nAdvanced Motion Controls (AMC), whose solutions provide reliability and high performance for any motion control system, announces new, expanded options for its popular FlexPro line of compact servo drives, providing flexibility in communication protocols and supply voltages.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9/2025/04/14/binder_PR_M8-D_nsMTYU6Xu.jpg)](https://www.powertransmission.com/binder-m8-d-connectors-with-single-wires-enhance-transmission-efficiency)\n\n**PRODUCT NEWS** \\| **2025-04-14**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Binder M8-D Connectors with Single Wires Enhance Transmission Efficiency](https://www.powertransmission.com/binder-m8-d-connectors-with-single-wires-enhance-transmission-efficiency)\n\nAs networking and data exchange expand in factory automation, the demand for reliable, flexible connection solutions is on the rise. Binder, a leading provider of industrial circular connectors, is enhancing its portfolio with M8-D-coded connectors featuring single wires. This new variant allows users to customize wiring to their specific needs, delivering improved transmission performance, particularly for applications like the Profinet standard.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9/2025/04/11/Staubli-Connect_sf1cURZaE.jpg)](https://www.powertransmission.com/staubli-expands-manufacturing-footprint-in-north-america-with-it-cooling-connectors)\n\n**PRODUCT NEWS** \\| **2025-04-11**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Stäubli Expands Manufacturing Footprint in North America with IT Cooling Connectors](https://www.powertransmission.com/staubli-expands-manufacturing-footprint-in-north-america-with-it-cooling-connectors)\n\nStäubli has expanded production of its [fluid connectors for datacenters](https://link.mediaoutreach.meltwater.com/ls/click?upn=u001.eCBUoS30ASxbWNEk-2F0zi1HhxLZ5zAIcx7damVuAt0BAamihOc07cZhj-2BPen1vttyLf79QdUJyH8U4OtICocDCZNzMkWIGjMDyz1PLsfqZfCtmgEl9fWLuhAqbmEiOvtnJQ8fFR511Yn-2F5LcVcuCwGw-3D-3DMS5h_oJgIQrZ5iXo1Fp7rL39-2FFbBat8ysAeUL2SM-2BI-2F7S4rKr0HAQygIZs-2BUzeFuJQPWp48lhPIDk-2BuxVh3x-2Fhm0-2BXLfSHGl8V-2FvjZcJOUc-2BTo2MChn-2FTq01hltvTl7vC6-2FZU6GUQf9R79XmMfxQ36aw5y9An5nO46oof9B1d9o4pGDruxHtfA3Pr0YUV5WuH19meVQn6NY7NemIaErSPbVl9KpNYzIjHXIAMg0cz24gKQ-2BVk1KXZppa-2Fsu5NFPJZrS7hbjjKVcwspDhUL4DcDQNYieuqe8Yi-2FfY-2BnmY7KTz2bA9Qb0Mm5ygYVbhizHMkQMou3IpDm3S-2FJjYWrz4dUlO4kLroFlL71InpKVfzZjQphcrElAGu5d101kEgsVQAYDhb) to Duncan, South Carolina. Currently, this line of products is also manufactured at Stäubli’s locations in Faverges, France and Hangzhou, China. This production line marks not only a milestone for the IT cooling business, but also a conscious effort for Stäubli to be close to its customers applications and market.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9/2025/04/11/Rexroth-Module_gVAxE5_-W.jpg)](https://www.powertransmission.com/bosch-rexroth-expanding-linear-axes-portfolio-to-accommodate-heavier-payloads)\n\n**PRODUCT NEWS** \\| **2025-04-11**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Bosch Rexroth Expanding Linear Axes Portfolio to Accommodate Heavier Payloads](https://www.powertransmission.com/bosch-rexroth-expanding-linear-axes-portfolio-to-accommodate-heavier-payloads)\n\nIn response to a growing need to transfer heavier items faster and more efficiently throughout manufacturing processes and a rapidly changing e-commerce landscape in supply chain operations, Bosch Rexroth has expanded its extensive compact modules portfolio. The new size, 280, can support items up to 250 kg, which makes it ideal for transferring battery modules, machine tools, furniture, palletizing gantries, robotic transfer units, and other larger items.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9/2025/04/04/MikiPulley-Contactless_K-NAR9gyf.jpg)](https://www.powertransmission.com/miki-pulley-introduces-contactless-link-magnetic-couplings)\n\n**PRODUCT NEWS** \\| **2025-04-04**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Miki Pulley Introduces Contactless Link Magnetic Couplings](https://www.powertransmission.com/miki-pulley-introduces-contactless-link-magnetic-couplings)\n\nMiki Pulley Magnetic Couplings are shaft couplings that transmit torque from one shaft to another using a magnetic field instead of a physical or mechanical connection. These Magnetic Couplings are non-contact and rely on the attraction and repulsion of magnetic poles to generate rotational power.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9/2025/04/03/TBK_NewRelease_vUcvLOfLd.jpg)](https://www.powertransmission.com/gwj-technology-releases-new-version-of-tbk-software)\n\n**PRODUCT NEWS** \\| **2025-04-03**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [GWJ Technology Releases New Version of TBK Software](https://www.powertransmission.com/gwj-technology-releases-new-version-of-tbk-software)\n\nGWJ Technology GmbH has released a new version of its _TBK_ desktop calculation solution for designing, recalculating and optimizing gears, shafts, bearings, bolted joints, shaft-hub connections and other machine elements.\n\n\\[advertisement\\]\n\n![](https://www.powertransmission.com/arrow-left.png)![](https://www.powertransmission.com/arrow-right.png)\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9/2025/04/02/faulhaber_16_gpt_1500x1200px_RPsvyftBN.jpg)](https://www.powertransmission.com/faulhaber-offers-new-performance-class-with-new-planetary-gearhead-sizes)\n\n**PRODUCT NEWS** \\| **2025-04-02**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Faulhaber Offers New Performance Class with new Planetary Gearhead Sizes](https://www.powertransmission.com/faulhaber-offers-new-performance-class-with-new-planetary-gearhead-sizes)\n\nThe new members of the Faulhaber metal planetary gearhead family are the 14GPT and 16GPT. The compact additions to the series deliver high torque with minimal volume, making them ideal for\n\ndemanding applications where space is limited. They are extremely robust, capable of withstanding frequent and sudden load changes, and operate with high efficiency. The gearheads can be\n\ncombined with a wide range of motors.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9/2025/04/01/STF_SSDC_ULCert-grouppic_EQX4J61Aty.png)](https://www.powertransmission.com/applied-motion-products-stf-stepper-drives-and-ssdc-stepservo-drives-are-now-ul-certified)\n\n**PRODUCT NEWS** \\| **2025-04-01**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Applied Motion Products STF Stepper Drives and SSDC StepSERVO Drives are now UL-Certified](https://www.powertransmission.com/applied-motion-products-stf-stepper-drives-and-ssdc-stepservo-drives-are-now-ul-certified)\n\nThe Applied Motion Products STF Stepper Drives and SSDC StepSERVO Drives are now UL Certified by Underwriters Laboratories. The drives now meet stringent safety standards with certification, reducing risks of electrical hazards, fire, or component failures. The STF Stepper Drives are DC-powered microstepping drives that control two-phase, bipolar step motors.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9/2025/03/31/Gates_Data_Master_MegaFlex_SVOYZbZWo.jpg)](https://www.powertransmission.com/gates-expands-data-center-cooling-portfolio-with-large-diameter-hose)\n\n**PRODUCT NEWS** \\| **2025-03-31**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Gates Expands Data Center Cooling Portfolio with Large-Diameter Hose](https://www.powertransmission.com/gates-expands-data-center-cooling-portfolio-with-large-diameter-hose)\n\nGates has launched the Data Master MegaFlex. This large-diameter cooling hose expands the company’s data center product portfolio, joining the Data Master small-diameter hose, which [launched](https://investors.gates.com/news/press-release-details/2024/GATES-EXPANDS-DATA-CENTER-COOLING-PORTFOLIO-WITH-LAUNCH-OF-NEW-DATA-MASTER-HOSE/default.aspx) in 2024. The Data Master MegaFlex unlocks new opportunities within data centers and will enable Gates to serve a broader set of customers in today’s digital-first world.\n\n[![](https://ik.imagekit.io/agmamedia/tr:w-300,ar-16-9/2025/03/26/Kollmorgen_zEpoY43yM.jpg)](https://www.powertransmission.com/kollmorgen-s-encoder-offers-multi-turn-absolute-feedback)\n\n**PRODUCT NEWS** \\| **2025-03-27**\n\n![](https://ik.imagekit.io/agmamedia/icons/bookmark.png)\n\n## [Kollmorgen’s Encoder Offers Multi-Turn Absolute Feedback](https://www.powertransmission.com/kollmorgen-s-encoder-offers-multi-turn-a","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.powertransmission.com%2Ftopics%2Fproduct-news%3Fpage%3D51","id":"https://www.powertransmission.com/topics/product-news?page=51","publishedDate":"2025-09-16T00:00:00.000Z","summary":"The webpage features recent product news in the automation and industrial transmission sectors. Key highlights include:\n\n- Tolomatic Inc. showcasing its latest servo linear actuators at Automate 2025, designed for precise, dynamic movement with integrated servomotors.\n- Advanced Motion Controls expanding its FlexPro line of compact servo drives with new communication options supporting 20-90 VDC, enhancing system flexibility.\n- Binder introducing M8-D connectors with single wires to improve transmission efficiency and customization in factory automation networks like Profinet.\n- Stäubli expanding manufacturing capabilities in North America, specifically producing fluid connectors with IT cooling features for data centers, enhancing reliability and performance.\n\nOverall, the updates emphasize advancements in actuator technology, versatile communication solutions, and improved connectivity components to meet evolving automation demands.","title":"Product News","url":"https://www.powertransmission.com/topics/product-news?page=51"},{"content":"**[Register for IBC2025](https://show.ibc.org/ibc2025-pass-types) [Get IBC Ready](https://show.ibc.org/ibc2025-ready)**\n\n# Press Releases\n\n### Latest IBC News\n\nLoading\n\n- ## [IBC2025 Innovation Awards celebrate global media and technology breakthroughs](https://show.ibc.org/press-releases-1/ibc2025-innovation-awards-celebrate-global-media-technology-breakthroughs)\n\n\n\n14 Sep 2025\n\n\n\n\n\nAmsterdam, 14 September 2025 – IBC today unveiled the winners of the prestigious IBC2025 Innovation Awards at a ceremony held at the RAI Amsterdam, recognising outstanding advances across content crea ...\n\n\n\n[Read More](https://show.ibc.org/press-releases-1/ibc2025-innovation-awards-celebrate-global-media-technology-breakthroughs)\n\n- ## [Future Talent takes centre stage: IBC2025 Talent Programme puts skills, inclusion and next-gen engagement in focus](https://show.ibc.org/press-releases-1/future-talent-takes-centre-stage-ibc2025)\n\n\n\n08 Sep 2025\n\n\n\n\n\nLondon, 8 September 2025 – IBC today announced details of the IBC2025 Talent Programme, returning for its second year on 12 September with a powerful mix of skills development, entry-level sessions, i ...\n\n\n\n[Read More](https://show.ibc.org/press-releases-1/future-talent-takes-centre-stage-ibc2025)\n\n- ## [IBC2025 reveals new details of Accelerator Programme projects being showcased in Future Tech](https://show.ibc.org/press-releases-1/ibc2025-reveals-new-details-accelerator-programme-projects-being-showcased-future-tech)\n\n\n\n04 Sep 2025\n\n\n\n\n\nLondon, 4 September 2025 – The IBC Accelerator Media Innovation Programme today unveiled the latest details of the eight Proof of Concepts (POCs) and one Incubator project being showcased at IBC2025 ( ...\n\n\n\n[Read More](https://show.ibc.org/press-releases-1/ibc2025-reveals-new-details-accelerator-programme-projects-being-showcased-future-tech)\n\n- ## [IBC2025 to host Conference session and special screening with Q&A of Becoming Led Zeppelin](https://show.ibc.org/press-releases-1/ibc2025-host-conference-session-special-screening-q-becoming-led-zeppelin)\n\n\n\n02 Sep 2025\n\n\n\n\n\nAmsterdam, 2 September 2025 – Becoming Led Zeppelin, a music documentary that holds the record for the highest-grossing opening weekend across IMAX theaters worldwide and is the most watched documenta ...\n\n\n\n[Read More](https://show.ibc.org/press-releases-1/ibc2025-host-conference-session-special-screening-q-becoming-led-zeppelin)\n\n- ## [Thelma Schoonmaker to receive IBC2025 International Honour for Excellence](https://show.ibc.org/press-releases-1/thelma-schoonmaker-receive-ibc2025-international-honour-excellence)\n\n\n\n19 Aug 2025\n\n\n\n\n\nIBC marks Brazilian broadcaster Globo's 100 years of innovation with IBC Special Award Swiss team awarded Best Technical Paper for research on AI for Trusted News London, 19 August 2025 – IBC today an ...\n\n\n\n[Read More](https://show.ibc.org/press-releases-1/thelma-schoonmaker-receive-ibc2025-international-honour-excellence)\n\n- ## [IBC2025 Conference Programme Showcases Media’s Defining Shifts Across Platforms, People and Technology](https://show.ibc.org/press-releases-1/ibc2025-conference-programme-showcases-medias-defining-shifts-platforms-people-technology)\n\n\n\n07 Aug 2025\n\n\n\n\n\nFeatures diverse sessions from Netflix, PGA Tour, Paramount Global, Walt Disney Studios, YouTube, TikTok, Snap, Roku, TelevisaUnivision, JioStar India, kweliTV and more Technical Papers to offer peer- ...\n\n\n\n[Read More](https://show.ibc.org/press-releases-1/ibc2025-conference-programme-showcases-medias-defining-shifts-platforms-people-technology)\n\n- ## [IBC2025 Innovation Awards finalists announced as industry prepares to celebrate tech and social advances](https://show.ibc.org/press-releases-1/ibc2025-innovation-awards-finalists-announced)\n\n\n\n30 Jul 2025\n\n\n\n\n\nLondon, UK – 30 July 2025 – IBC has unveiled the finalists for the IBC2025 Innovation Awards, recognising collaborative projects that address real-world challenges and deliver transformative innovatio ...\n\n\n\n[Read More](https://show.ibc.org/press-releases-1/ibc2025-innovation-awards-finalists-announced)\n\n\n- [1](https://show.ibc.org/press-releases?page=1&azLetterField=)\n- [2](https://show.ibc.org/press-releases?page=2&azLetterField=)\n- [3](https://show.ibc.org/press-releases?page=3&azLetterField=)\n- [4](https://show.ibc.org/press-releases?page=4&azLetterField=)\n\nSearch\n\nSearch\n\nUsername \\*\n\nPassword \\*\n\nLogin","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fshow.ibc.org%2Fpress-releases","id":"https://show.ibc.org/press-releases","publishedDate":"2025-09-15T00:00:00.000Z","summary":"The IBC2025 Show, scheduled for September 12-15, 2025, at RAI Amsterdam, features a range of industry-leading events and recognitions. The webpage highlights recent press releases covering key aspects of the event, including the announcement of the IBC2025 Innovation Awards winners, which celebrate breakthroughs in media and technology. The event emphasizes talent development through the second annual IBC2025 Talent Programme focused on skills, inclusion, and engaging the next generation. Additionally, details of the Accelerator Programme showcase innovative projects in future tech, with eight Proof of Concepts and one Incubator project. The conference program will feature sessions from major industry players like Netflix, Disney, YouTube, TikTok, and more, addressing shifts across platforms, people, and technology. Notable highlights include a special screening and Q&A of the documentary \"Becoming Led Zeppelin,\" and the recognition of Thelma Schoonmaker with the IBC2025 International Honour for Excellence. The event aims to celebrate technological and social advancements in the media industry, fostering innovation, collaboration, and talent development.","title":"IBC Press Releases | IBC2025 Show 12-15 Sep 2025","url":"https://show.ibc.org/press-releases"},{"content":"This article introduces Relay IR – the second generation of NNVM.\nWe expect readers from two kinds of background – those who have a programming language background and deep learning\nframework developers who are familiar with the computational graph representation.\n\nWe briefly summarize the design goal here, and will touch upon these points in the later part of the article.\n\n- Support traditional data flow-style programming and transformations.\n\n- Support functional-style scoping, let-binding and making it a fully featured differentiable language.\n\n- Being able to allow the user to mix the two programming styles.\n\n\n## Build a Computational Graph with Relay [¶](http://tvm.apache.org/tvm.apache.org\\#build-a-computational-graph-with-relay)\n\nTraditional deep learning frameworks use computational graphs as their intermediate representation.\nA computational graph (or dataflow graph), is a directed acyclic graph (DAG) that represents the computation.\nThough dataflow graphs are limited in terms of the computations they are capable of expressing due to\nlacking control flow, their simplicity makes it easier to implement automatic differentiation and\ncompile for heterogeneous execution environments (e.g., executing parts of the graph on specialized hardware).\n\nYou can use Relay to build a computational (dataflow) graph. Specifically, the above code shows how to\nconstruct a simple two-node graph. You can find that the syntax of the example is not that different from existing\ncomputational graph IR like NNVMv1, with the only difference in terms of terminology:\n\n- Existing frameworks usually use graph and subgraph\n\n- Relay uses function e.g. – `fn (%x)`, to indicate the graph\n\n\nEach dataflow node is a CallNode in Relay. The Relay Python DSL allows you to construct a dataflow graph quickly.\nOne thing we want to highlight in the above code – is that we explicitly constructed an Add node with\nboth input point to `%1`. When a deep learning framework evaluates the above program, it will compute\nthe nodes in topological order, and `%1` will only be computed once.\nWhile this fact is very natural to deep learning framework builders, it is something that might\nsurprise a PL researcher in the first place. If we implement a simple visitor to print out the result and\ntreat the result as nested Call expression, it becomes `log(%x) + log(%x)`.\n\nSuch ambiguity is caused by different interpretations of program semantics when there is a shared node in the DAG.\nIn a normal functional programming IR, nested expressions are treated as expression trees, without considering the\nfact that the `%1` is actually reused twice in `%2`.\n\nThe Relay IR is mindful of this difference. Usually, deep learning framework users build the computational\ngraph in this fashion, where a DAG node reuse often occurs. As a result, when we print out the Relay program in\nthe text format, we print one CallNode per line and assign a temporary id `(%1, %2)` to each CallNode so each common\nnode can be referenced in later parts of the program.\n\n## Module: Support Multiple Functions (Graphs) [¶](http://tvm.apache.org/tvm.apache.org\\#module-support-multiple-functions-graphs)\n\nSo far we have introduced how can we build a dataflow graph as a function. One might naturally ask: Can we support multiple\nfunctions and enable them to call each other? Relay allows grouping multiple functions together in a module; the code below\nshows an example of a function calling another function.\n\n```\ndef @muladd(%x, %y, %z) {\n %1 = mul(%x, %y)\n %2 = add(%1, %z)\n %2\n}\ndef @myfunc(%x) {\n %1 = @muladd(%x, 1, 2)\n %2 = @muladd(%1, 2, 3)\n %2\n}\n\n```\n\nThe Module can be viewed as a `Map `. Here GlobalVar is just an id that is used to represent the functions\nin the module. `@muladd` and `@myfunc` are GlobalVars in the above example. When a CallNode is used to call another function,\nthe corresponding GlobalVar is stored in the op field of the CallNode. It contains a level of indirection – we need to look up\nbody of the called function from the module using the corresponding GlobalVar. In this particular case, we could also directly\nstore the reference to the Function as op in the CallNode. So, why do we need to introduce GlobalVar? The main reason is that\nGlobalVar decouples the definition/declaration and enables recursion and delayed declaration of the function.\n\n```\ndef @myfunc(%x) {\n %1 = equal(%x, 1)\n if (%1) {\n %x\n } else {\n %2 = sub(%x, 1)\n %3 = @myfunc(%2)\n %4 = add(%3, %3)\n %4\n }\n}\n\n```\n\nIn the above example, `@myfunc` recursively calls itself. Using GlobalVar `@myfunc` to represent the function avoids\nthe cyclic dependency in the data structure.\nAt this point, we have introduced the basic concepts in Relay. Notably, Relay has the following improvements over NNVMv1:\n\n- Succinct text format that eases debugging of writing passes.\n\n- First-class support for subgraphs-functions, in a joint module, this enables further chance of joint optimizations such as inlining and calling convention specification.\n\n- Naive front-end language interop, for example, all the data structure can be visited in Python, which allows quick prototyping of optimizations in Python and mixing them with C++ code.\n\n\n## Let Binding and Scopes [¶](http://tvm.apache.org/tvm.apache.org\\#let-binding-and-scopes)\n\nSo far, we have introduced how to build a computational graph in the good old way used in deep learning frameworks.\nThis section will talk about a new important construct introduced by Relay – let bindings.\n\nLet binding is used in every high-level programming language. In Relay, it is a data structure with three\nfields `Let(var, value, body)`. When we evaluate a let expression, we first evaluate the value part, assign\nit to the var, then return the evaluated result in the body expression.\n\nYou can use a sequence of let bindings to construct a logically equivalent program to a dataflow program.\nThe code example below shows one program with two forms side by side.\n\nThe nested let binding is called A-normal form, and it is commonly used as IRs in functional programming languages.\nNow, please take a close look at the AST structure. While the two programs are semantically identical\n(so are their textual representations, except that A-normal form has let prefix), their AST structures are different.\n\nSince program optimizations take these AST data structures and transform them, the two different structures will\naffect the compiler code we are going to write. For example, if we want to detect a pattern `add(log(x), y)`:\n\n- In the data-flow form, we can first access the add node, then directly look at its first argument to see if it is a log\n\n- In the A-normal form, we cannot directly do the check anymore, because the first input to add is `%v1` – we will need to keep a map from variable to its bound values and look up that map, in order to know that `%v1` is a log.\n\n\nDifferent data structures will impact how you might write transformations, and we need to keep that in mind.\nSo now, as a deep learning framework developer, you might ask, Why do we need let bindings?\nYour PL friends will always tell you that let is important – as PL is a quite established field,\nthere must be some wisdom behind that.\n\n## Why We Might Need Let Binding [¶](http://tvm.apache.org/tvm.apache.org\\#why-we-might-need-let-binding)\n\nOne key usage of let binding is that it specifies the scope of computation. Let us take a look at the following example,\nwhich does not use let bindings.\n\nThe problem comes when we try to decide where we should evaluate node `%1`. In particular, while the text format seems\nto suggest that we should evaluate node `%1` outside the if scope, the AST(as shown in the picture) does not suggest so.\nActually, a dataflow graph never defines its scope of the evaluation. This introduces some ambiguity in the semantics.\n\nThis ambiguity becomes more interesting when we have closures. Consider the following program, which returns a closure.\nWe don’t know where should we compute `%1`; it can be either inside or outside the closure.\n\n```\nfn (%x) {\n %1 = log(%x)\n %2 = fn(%y) {\n add(%y, %1)\n }\n %2\n}\n\n```\n\nA let binding solves this problem, as the computation of the value happens at the let node. In both programs,\nif we change `%1 = log(%x)` to `let %v1 = log(%x)`, we clearly specify the computation location to\nbe outside of the if scope and closure. As you can see let-binding gives a more precise specification of the computation site\nand could be useful when we generate backend code (as such specification is in the IR).\n\nOn the other hand, the dataflow form, which does not specify the scope of computation, does have its own advantages\n– namely, we don’t need to worry about where to put the let when we generate the code. The dataflow form also gives more freedom\nto the later passes to decide where to put the evaluation point. As a result, it might not be a bad idea to use data flow\nform of the program in the initial phases of optimizations when you find it is convenient.\nMany optimizations in Relay today are written to optimize dataflow programs.\n\nHowever, when we lower the IR to an actual runtime program, we need to be precise about the scope of computation.\nIn particular, we want to explicitly specify where the scope of computation should happen when we are using\nsub-functions and closures. Let-binding can be used to solve this problem in later stage execution specific optimizations.\n\n## Implication on IR Transformations [¶](http://tvm.apache.org/tvm.apache.org\\#implication-on-ir-transformations)\n\nHopefully, by now you are familiar with the two kinds of representations.\nMost functional programming languages do their analysis in A-normal form,\nwhere the analyzer does not need to be mindful that the expressions are DAGs.\n\nRelay choose to support both the dataflow form and let bindings. We believe that it is important to let the\nframework developer choose the representation they are familiar with.\nThis does, however, have some implications on how we write pa","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Ftvm.apache.org%2Fdocs%2F%2Fv0.8.0%2Farch%2Frelay_intro.html","id":"https://tvm.apache.org/docs//v0.8.0/arch/relay_intro.html","publishedDate":null,"summary":"This webpage introduces Relay IR, the second-generation intermediate representation for deep learning frameworks within TVM. Designed for both programming language experts and deep learning developers, Relay supports traditional dataflow programming, functional scoping, and differentiable programming, allowing users to combine these styles seamlessly. It facilitates building computational graphs (dataflow graphs) as functions, where each node is a CallNode, enabling efficient automatic differentiation and execution on diverse hardware. Relay also supports multiple functions within modules, allowing function calls and modular graph construction, with a clear representation of shared nodes and reuse. Overall, Relay IR provides a flexible, expressive, and efficient foundation for defining and optimizing neural network computations.","title":"Introduction to Relay IR - Apache TVM","url":"https://tvm.apache.org/docs//v0.8.0/arch/relay_intro.html"},{"content":"March 12, 2025\n\n# What about TVM, XLA, and AI compilers? (Democratizing AI Compute, Part 6)\n\nChris Lattner\n\nIn the early days of AI hardware, writing high-performance GPU code was a manageable—if tedious—task. Engineers could handcraft CUDA kernels in C++ for the key operations they needed, and NVIDIA could [build these into libraries like cuDNN](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed#wave) to drive their lock-in. But as deep learning advanced, this approach completely broke down.\n\nNeural networks grew bigger, architectures became more sophisticated, and researchers demanded ever-faster iteration cycles. The number of [unique operators in frameworks like PyTorch](https://dev-discuss.pytorch.org/t/where-do-the-2000-pytorch-operators-come-from-more-than-you-wanted-to-know/373) exploded—now numbering in the thousands. Manually writing and optimizing each one for every new hardware target? Impossible.\n\nPyTorch operator count by version ( [source](https://dev-discuss.pytorch.org/t/where-do-the-2000-pytorch-operators-come-from-more-than-you-wanted-to-know/373))\n\nThis challenge forced a fundamental shift: instead of writing kernels by hand, what if we had a compiler that could _generate_ them automatically? AI compilers emerged to solve this exact problem, marking a transformation from human-crafted CUDA to machine-generated, hardware-optimized compute.\n\nBut as history has shown, building a successful compiler stack isn’t just a technical challenge—it’s a battle over ecosystems, fragmentation, and control. So what worked? What didn’t? And what can we learn from projects like TVM and OpenXLA?\n\nLet’s dive in. 🚀\n\n# What is an “AI Compiler”?\n\nAt its core, an AI compiler is a system that takes high-level operations—like those in PyTorch or TensorFlow—and automatically transforms them into highly efficient GPU code. One of the most fundamental optimizations it performs is called “ **kernel fusion**. **”** To see why this matters, let’s consider a simple example: [multiplying two matrices](https://en.wikipedia.org/wiki/Matrix_multiplication) (”matmul”) and then applying a ReLU (Rectified Linear Unit) [activation function](https://en.wikipedia.org/wiki/Activation_function). These are simple but important operations that occur in common neural networks.\n\n## **Naïve approach: Two separate kernels**\n\nThe most straightforward (but inefficient) way to do this is to perform matrix multiplication first, store the result in memory, then load it again to apply ReLU.\n\n```\n# Naïve matmul implementation for clarity.defmatmul(A, B):# Initialize result matrix to zero. C = [[0] * N for _ inrange(N)]\nfor i inrange(N):\nfor j inrange(N):\nsum = 0for k inrange(N):\n# Matmul sums the dot product of rows and columns.sum += A[i][k] * B[k][j]\n C[i][j] = sum# store one output valuereturn C\n\n# ReLU clamp negatives to zero with the \"max\" function.defrelu(C):# Allocate result array. result = [[0] * N for _ inrange(N)]\nfor i inrange(N):\nfor j inrange(N):\n# This loads from memory, does a trivial max(0, x) operation,# then stores the result. result[i][j] = max(0, C[i][j])\nreturn result\n\nC = matmul(A, B) # Compute matrix multiplication firstD = relu(C) # Then apply ReLU separately.\n```\n\nThese operations are extremely familiar to engineers that might write a CUDA kernel (though remember that [CUDA uses unwieldy C++ syntax!](https://www.modular.com/blog/democratizing-ai-compute-part-4-cuda-is-the-incumbent-but-is-it-any-good#pythoncuda)), and there are many tricks used for efficient implementation.\n\nWhile the above approach is simple and modular, executing operations like this is **extremely slow** because it writes the entire matrix `C` to memory after `matmul()`, then reads it back again in `relu()`. This memory traffic dominates performance, especially on GPUs, where memory access is more expensive than local compute.\n\n## **Fused kernel: One pass, no extra memory traffic**\n\nThe solution for this is simple: we can “ **fuse”** these two operations into a single kernel, eliminating redundant memory access. Instead of storing C after `matmul()`, we apply `relu()` _immediately_ inside the same loop:\n\n```\n# Fused kernel: Matrix multiplication + ReLU in one passdeffused_matmul_relu(A, B):# Initialize result matrix to zero. C = [[0] * N for _ inrange(N)]\nfor i inrange(N):\nfor j inrange(N):\nsum = 0for k inrange(N):\nsum += A[i][k] * B[k][j] # Compute matmul\n# Apply ReLU in the same loop! C[i][j] = max(0, sum)\nreturn C # Only one read/write cycle\n# Compute in a single pass, no extra memory.C = fused_matmul_relu(A, B)\n```\n\nWhile the benefit of this transformation varies by hardware and matrix size, the results can be profound: sometimes 2x better performance! Why is this the case? By fusing the operations:\n\n✅ We **eliminate an extra memory write/read**, reducing pressure on memory bandwidth.\n\n✅ We **keep data in registers or shared memory**, avoiding slow global memory access.\n\n✅ We **reduce memory usage and allocation/deallocation overhead**, since the intermediate buffer has been removed.\n\nThis is the simplest example of kernel fusion: There are many more powerful transformations, and AI kernel engineers have always pushed the limits of optimization ( [learn more](https://horace.io/brrr_intro.html)). With GenAI driving up compute demand, these optimizations are more critical than ever.\n\n## Great performance, but an exponential complexity explosion!\n\nWhile these sorts of optimizations can be extremely exciting and fun to implement for those who are chasing low cost and state of the art performance, there is a hidden truth: **this approach doesn’t scale**.\n\nModern machine learning toolkits include hundreds of different “operations” like matmul, convolution, add, subtract, divide, etc., as well as dozens of [activation functions](https://en.wikipedia.org/wiki/Activation_function) beyond ReLU. Each neural network needs them to be combined in different ways: this causes an explosion in the number of permutations that need to be implemented (hundreds of operations x hundreds of operations = too many to count). NVIDIA’s libraries like cuDNN provide a fixed list of options to choose from, without generality to new research.\n\nFurthermore, there are other axes of explosion as well: we’ve seen an explosion of new numerics datatypes (e.g. “float8”), and of course, there is also an explosion of the kind of hardware that AI should support.\n\nJust three dimensions of complexity\n\n# Early AI compilers: TVM\n\nThere are many AI compilers, but one of the earliest and most successful is TVM - the “ [Tensor Virtual Machine](https://tvm.apache.org/)”. This system took models from TensorFlow/PyTorch and optimized them for diverse hardware, i.e. by applying kernel fusion automatically. This project started at the University of Washington by [Tianqi Chen](https://tqchen.com/) and [Professor Luis Ceze](https://www.cs.washington.edu/people/faculty/luis-ceze/) in about 2016, and delivered a number of innovative results and performance wins described in [the 2018 paper](https://arxiv.org/abs/1802.04799) that outlines the TVM architecture. It was open sourced and incorporated into the Apache project.\n\nAcross its journey, TVM has been adopted by hardware makers (including public contributions from companies like ARM, Qualcomm, Facebook, Intel, and many others) across embedded, DSP, and many other applications. TVM's core contributors later founded OctoAI, [which NVIDIA acquired in late 2024](https://www.forbes.com/sites/janakirammsv/2024/09/30/nvidia-acquires-octoai-to-dominate-enterprise-generative-ai-solutions/)—giving it control over many of the original TVM developers and, potentially, the project's future.\n\n[Source: Apache TVM](https://tvm.apache.org/2017/10/06/nnvm-compiler-announcement)\n\nTVM is an important step for the AI compiler industry, but what can we learn from it? Here are my key takeaways. **_Disclaimer_**: although TVM was a user of LLVM and I had great interest in it, I was never directly involved. This is my perspective as an outsider.\n\n###### **Wasn’t able to deliver performance on modern hardware**\n\nTVM struggled to deliver peak performance on modern AI hardware, particularly as GPUs evolved toward TensorCores and other specialized acceleration. It added support over time but was often late and failed to fully unlock performance. As such, it suffered from one of [the same problems as OpenCL](https://www.modular.com/blog/democratizing-ai-compute-part-5-what-about-cuda-c-alternatives#evolvingneeds): You can’t deliver performance if you can’t unlock the hardware.\n\n###### **Fragmentation driven by conflicting commercial interests**\n\nUnlike OpenCL, TVM wasn't just a specification—it was an **actual implementation**. This made it far more useful out of the box and attracted hardware vendors. But fragmentation still reared its head: vendors forked the code, made incompatible changes, and struggled to stay in sync, slowing progress. This led to friction executing architectural changes (because downstream vendors complained about their forks being broken), which slowed development.\n\n###### **Agility is required to keep up with rapid AI advances**\n\nA final challenge is that TVM was quite early, but the pace of AI innovation around it was rapid. TensorFlow and PyTorch rapidly evolved due to backing by huge companies like Google, Meta, and [NVIDIA](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed#genaisurge), improving their performance and changing the baselines that TVM compared against. The final nail in the coffin, though, was GenAI, which changed the game. TVM was designed for “TradAI”: a set of relatively simple operators that needed fusion, but GenAI has large and complex algorithms deeply integrated with the hardware— [things like FlashAttention3.](https://www.modular.com/blog/democratizing-ai-compute-part-3-how-did-cuda-succeed#genaisurge) TVM fell progressively behind as the industry evolved.\n\nLess strat","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.modular.com%2Fblog%2Fdemocratizing-ai-compute-part-6-what-about-ai-compilers","id":"https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers","publishedDate":"2025-03-12T00:00:00.000Z","summary":"The article discusses the evolution of AI hardware and the shift from manually writing GPU kernels to using AI compilers that generate optimized code automatically. Traditional approaches, like handcrafting CUDA kernels, became impractical as neural networks grew more complex with thousands of operators. AI compilers, exemplified by projects like TVM and OpenXLA, address this by transforming high-level operations into efficient GPU code through techniques like kernel fusion—combining multiple operations into a single, optimized pass to reduce memory traffic and improve performance. The piece emphasizes the importance of these compilers in democratizing AI compute, highlighting their role in overcoming ecosystem fragmentation and enabling faster, hardware-optimized AI development.","title":"Modular: Democratizing AI Compute, Part 6: What about AI compilers (TVM and XLA)?","url":"https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers"},{"content":"![pic](https://media2.dev.to/dynamic/image/width=256,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)\n\n[Create template](https://dev.to/settings/response-templates)\n\nTemplates let you quickly answer FAQs or store snippets for re-use.\n\nSubmitPreview [Dismiss](https://dev.to/404.html)\n\nCollapseExpand\n\n[![alex-nguyen-duy-anh profile image](https://media2.dev.to/dynamic/image/width=50,height=50,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F2935609%2F9f84f216-73b8-4c4e-abd8-6678edf1bd05.jpg)](https://dev.to/alex-nguyen-duy-anh)\n\n[Alex Nguyen](https://dev.to/alex-nguyen-duy-anh)\n\nAlex Nguyen\n\n[![](https://media2.dev.to/dynamic/image/width=90,height=90,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F2935609%2F9f84f216-73b8-4c4e-abd8-6678edf1bd05.jpg)Alex Nguyen](https://dev.to/alex-nguyen-duy-anh)\n\nFollow\n\nAlgorithmic Trading & Fintech Innovator \\| MQL5, Python & AI-Driven Trading Expert\n\n- Location\n\nHo Chi Minh City, Vietnam\n\n- Joined\n\nMar 12, 2025\n\n\n• [Mar 16](https://dev.to/alex-nguyen-duy-anh/new-open-source-gem-hirgen-fuzz-test-your-deep-learning-compilers-ir-stages-to-uncover-hidden-57mk#comment-2mdj0)\n\nDropdown menu\n\n- [Copy link](https://dev.to/alex-nguyen-duy-anh/new-open-source-gem-hirgen-fuzz-test-your-deep-learning-compilers-ir-stages-to-uncover-hidden-57mk#comment-2mdj0)\n- Hide\n\nFuzzing Deep Learning Compilers with HirGen: [alexnguyenduyanh.substack.com/p/fu...](https://alexnguyenduyanh.substack.com/p/fuzzing-deep-learning-compilers-hirgen)\n\nAre you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's [permalink](https://dev.to/alex-nguyen-duy-anh/new-open-source-gem-hirgen-fuzz-test-your-deep-learning-compilers-ir-stages-to-uncover-hidden-57mk).\n\nHide child comments as well\n\nConfirm\n\nFor further actions, you may consider blocking this person and/or [reporting abuse](https://dev.to/report-abuse)\n\n## Read next\n\n[![shlok2740 profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F709634%2F7c660c40-32c5-42ac-8d81-c8b5d47b5edd.jpg)\\\n\\\n**Descriptive and Inferential Statistics** \\\n\\\nShlok Kumar - Mar 4](https://dev.to/shlok2740/descriptive-and-inferential-statistics-1nm3) [![proflead profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F1014611%2Fae3c3049-1eba-402c-b226-63e5b8a7a065.jpeg)\\\n\\\n**FREE GitHub Copilot Alternative: Gemini Code Assist** \\\n\\\nVladislav Guzey - Mar 4](https://dev.to/proflead/free-github-copilot-alternative-gemini-code-assist-5b5p) [![denis_bratchikov profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F2898523%2Feb45c2e6-68b3-4b3e-bdbb-13dcb8f39578.jpg)\\\n\\\n**AI-Powered Code Refactoring: A Case Study Using Cursor with GPT-4o and Claude 3.7 Sonnet** \\\n\\\nDenis Bratchikov - Mar 4](https://dev.to/denis_bratchikov/ai-powered-code-refactoring-a-case-study-using-cursor-with-gpt-4o-and-claude-37-sonnet-3hh) [![avinashvagh profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F670812%2Fa51db037-1982-427e-81cb-830291327747.jpg)\\\n\\\n**Restarting Blog Writing** \\\n\\\nAvinash Vagh - Mar 8](https://dev.to/avinashvagh/restarting-blog-writing-3cj1)\n\n![DEV Community](https://media2.dev.to/dynamic/image/width=190,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)\n\nWe're a place where coders share, stay up-to-date and grow their careers.\n\n[Log in](https://dev.to/enter) [Create account](https://dev.to/enter?state=new-user)\n\n![](https://assets.dev.to/assets/sparkle-heart-5f9bee3767e18deb1bb725290cb151c25234768a0e9a2bd39370c382d02920cf.svg)![](https://assets.dev.to/assets/multi-unicorn-b44d6f8c23cdd00964192bedc38af3e82463978aa611b4365bd33a0f1f4f3e97.svg)![](https://assets.dev.to/assets/exploding-head-daceb38d627e6ae9b730f36a1e390fca556a4289d5a41abb2c35068ad3e2c4b5.svg)![](https://assets.dev.to/assets/raised-hands-74b2099fd66a39f2d7eed9305ee0f4553df0eb7b4f11b01b6b1b499973048fe5.svg)![](https://assets.dev.to/assets/fire-f60e7a582391810302117f987b22a8ef04a2fe0df7e3258a5f49332df1cec71e.svg)","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdev.to%2Falex-nguyen-duy-anh%2Fnew-open-source-gem-hirgen-fuzz-test-your-deep-learning-compilers-ir-stages-to-uncover-hidden-57mk","id":"https://dev.to/alex-nguyen-duy-anh/new-open-source-gem-hirgen-fuzz-test-your-deep-learning-compilers-ir-stages-to-uncover-hidden-57mk","publishedDate":"2025-03-16T00:00:00.000Z","summary":"The article introduces HirGen, an open-source tool designed for fuzz-testing deep learning compiler Intermediate Representation (IR) stages, such as those in TVM and TensorRT. By applying fuzzing techniques, HirGen uncovers hidden bugs that could compromise model optimizations. Already responsible for identifying 21 bugs in TVM, HirGen helps developers ensure that compiler optimizations do not inadvertently break models. The tool aims to improve the reliability and robustness of deep learning deployment pipelines. Users are encouraged to check out HirGen on GitHub for integration and testing.","title":"New open-source gem: HirGen 🛠️. Fuzz-test your deep learning compilers’ IR stages to uncover hidden bugs. Compatible with TVM/TensorRT, it’s already caught 21 bugs in TVM. Perfect for ensuring optimizations don’t break your models. Check it out on GitHub!","url":"https://dev.to/alex-nguyen-duy-anh/new-open-source-gem-hirgen-fuzz-test-your-deep-learning-compilers-ir-stages-to-uncover-hidden-57mk"},{"content":"- [Docs](https://tvm.apache.org/index.html) >\n- [Relax](https://tvm.apache.org/index.html) >\n- Graph Abstraction for ML Models\n- [Edit on GitHub](https://github.com/apache/tvm/edit/main/docs/deep_dive/relax/abstraction.rst)\n\n# Graph Abstraction for ML Models [](https://tvm.apache.org/tvm.apache.org\\#graph-abstraction-for-ml-models)\n\nGraph abstraction is a key technique used in machine learning (ML) compilers\nto represent and reason about the structure and data flow of ML models. By\nabstracting the model into a graph representation, the compiler can perform\nvarious optimizations to improve performance and efficiency. This tutorial will\ncover the basics of graph abstraction, its key elements of Relax IR, and how it enables optimization in ML compilers.\n\n## What is Graph Abstraction? [](https://tvm.apache.org/tvm.apache.org\\#what-is-graph-abstraction)\n\nGraph abstraction is the process of representing an ML model as a directed graph,\nwhere the nodes represent computational operations (e.g., matrix multiplication,\nconvolution) and the edges represent the flow of data between these operations.\nThis abstraction allows the compiler to analyze the dependencies and\nrelationships between different parts of the model.\n\n```\nfromtvm.scriptimport relax as R\n\n@R.function\ndefmain(\n x: R.Tensor((1, 784), dtype=\"float32\"),\n weight: R.Tensor((784, 256), dtype=\"float32\"),\n bias: R.Tensor((256,), dtype=\"float32\"),\n) -> R.Tensor((1, 256), dtype=\"float32\"):\n with R.dataflow():\n lv0 = R.matmul(x, weight)\n lv1 = R.add(lv0, bias)\n gv = R.nn.relu(lv1)\n R.output(gv)\n return gv\n```\n\n## Key Features of Relax [](https://tvm.apache.org/tvm.apache.org\\#key-features-of-relax)\n\nRelax, the graph representation utilized in Apache TVM’s Unity strategy,\nfacilitates end-to-end optimization of ML models through several crucial\nfeatures:\n\n- **First-class symbolic shape**: Relax employs symbolic shapes to represent\ntensor dimensions, enabling global tracking of dynamic shape relationships\nacross tensor operators and function calls.\n\n- **Multi-level abstractions**: Relax supports cross-level abstractions, from\nhigh-level neural network layers to low-level tensor operations, enabling\noptimizations that span different hierarchies within the model.\n\n- **Composable transformations**: Relax offers a framework for composable\ntransformations that can be selectively applied to different model components.\nThis includes capabilities such as partial lowering and partial specialization,\nproviding flexible customization and optimization options.\n\n\nThese features collectively empower Relax to offer a powerful and adaptable approach\nto ML model optimization within the Apache TVM ecosystem.","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Ftvm.apache.org%2Fdocs%2Fdeep_dive%2Frelax%2Fabstraction.html","id":"https://tvm.apache.org/docs/deep_dive/relax/abstraction.html","publishedDate":"2024-01-01T00:00:00.000Z","summary":"The webpage explains graph abstraction in machine learning compilers, focusing on how ML models are represented as directed graphs where nodes are computational operations and edges show data flow. This abstraction enables the compiler to analyze dependencies and optimize model performance. It highlights Relax, a key IR in Apache TVM, which supports features like symbolic shapes for dynamic tensor dimensions, multi-level abstractions for different model hierarchies, and composable transformations for flexible optimizations. Overall, these techniques facilitate efficient, end-to-end optimization of ML models within TVM.","title":"Graph Abstraction for ML Models — tvm 0.19.dev0 documentation","url":"https://tvm.apache.org/docs/deep_dive/relax/abstraction.html"},{"content":"[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[apache](https://github.com/apache)/ **[tvm](https://github.com/apache/tvm)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fapache%2Ftvm) You must be signed in to change notification settings\n- [Fork\\\n3.6k](https://github.com/login?return_to=%2Fapache%2Ftvm)\n- [Star\\\n12.5k](https://github.com/login?return_to=%2Fapache%2Ftvm)\n\n\nOpen deep learning compiler stack for cpu, gpu and specialized accelerators\n\n[tvm.apache.org/](https://tvm.apache.org/)\n\n### License\n\n[Apache-2.0 license](https://github.com/apache/tvm/blob/main/LICENSE)\n\n[12.5k\\\nstars](https://github.com/apache/tvm/stargazers) [3.6k\\\nforks](https://github.com/apache/tvm/forks) [Branches](https://github.com/apache/tvm/branches) [Tags](https://github.com/apache/tvm/tags) [Activity](https://github.com/apache/tvm/activity)\n\n[Star](https://github.com/login?return_to=%2Fapache%2Ftvm)\n\n[Notifications](https://github.com/login?return_to=%2Fapache%2Ftvm) You must be signed in to change notification settings\n\n# apache/tvm\n\nmain\n\n[Branches](https://github.com/apache/tvm/branches) [Tags](https://github.com/apache/tvm/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit ## History [13,169 Commits](https://github.com/apache/tvm/commits/main/) |\n| [.github](https://github.com/apache/tvm/tree/main/.github) | [.github](https://github.com/apache/tvm/tree/main/.github) |\n| [3rdparty](https://github.com/apache/tvm/tree/main/3rdparty) | [3rdparty](https://github.com/apache/tvm/tree/main/3rdparty) |\n| [apps](https://github.com/apache/tvm/tree/main/apps) | [apps](https://github.com/apache/tvm/tree/main/apps) |\n| [ci](https://github.com/apache/tvm/tree/main/ci) | [ci](https://github.com/apache/tvm/tree/main/ci) |\n| [cmake](https://github.com/apache/tvm/tree/main/cmake) | [cmake](https://github.com/apache/tvm/tree/main/cmake) |\n| [conda](https://github.com/apache/tvm/tree/main/conda) | [conda](https://github.com/apache/tvm/tree/main/conda) |\n| [configs](https://github.com/apache/tvm/tree/main/configs) | [configs](https://github.com/apache/tvm/tree/main/configs) |\n| [docker](https://github.com/apache/tvm/tree/main/docker) | [docker](https://github.com/apache/tvm/tree/main/docker) |\n| [docs](https://github.com/apache/tvm/tree/main/docs) | [docs](https://github.com/apache/tvm/tree/main/docs) |\n| [ffi](https://github.com/apache/tvm/tree/main/ffi) | [ffi](https://github.com/apache/tvm/tree/main/ffi) |\n| [include/tvm](https://github.com/apache/tvm/tree/main/include/tvm) | [include/tvm](https://github.com/apache/tvm/tree/main/include/tvm) |\n| [jvm](https://github.com/apache/tvm/tree/main/jvm) | [jvm](https://github.com/apache/tvm/tree/main/jvm) |\n| [licenses](https://github.com/apache/tvm/tree/main/licenses) | [licenses](https://github.com/apache/tvm/tree/main/licenses) |\n| [python](https://github.com/apache/tvm/tree/main/python) | [python](https://github.com/apache/tvm/tree/main/python) |\n| [src](https://github.com/apache/tvm/tree/main/src) | [src](https://github.com/apache/tvm/tree/main/src) |\n| [tests](https://github.com/apache/tvm/tree/main/tests) | [tests](https://github.com/apache/tvm/tree/main/tests) |\n| [web](https://github.com/apache/tvm/tree/main/web) | [web](https://github.com/apache/tvm/tree/main/web) |\n| [.asf.yaml](https://github.com/apache/tvm/blob/main/.asf.yaml) | [.asf.yaml](https://github.com/apache/tvm/blob/main/.asf.yaml) |\n| [.clang-format](https://github.com/apache/tvm/blob/main/.clang-format) | [.clang-format](https://github.com/apache/tvm/blob/main/.clang-format) |\n| [.gitattributes](https://github.com/apache/tvm/blob/main/.gitattributes) | [.gitattributes](https://github.com/apache/tvm/blob/main/.gitattributes) |\n| [.gitignore](https://github.com/apache/tvm/blob/main/.gitignore) | [.gitignore](https://github.com/apache/tvm/blob/main/.gitignore) |\n| [.gitmodules](https://github.com/apache/tvm/blob/main/.gitmodules) | [.gitmodules](https://github.com/apache/tvm/blob/main/.gitmodules) |\n| [.pre-commit-config.yaml](https://github.com/apache/tvm/blob/main/.pre-commit-config.yaml) | [.pre-commit-config.yaml](https://github.com/apache/tvm/blob/main/.pre-commit-config.yaml) |\n| [CMakeLists.txt](https://github.com/apache/tvm/blob/main/CMakeLists.txt) | [CMakeLists.txt](https://github.com/apache/tvm/blob/main/CMakeLists.txt) |\n| [CONTRIBUTORS.md](https://github.com/apache/tvm/blob/main/CONTRIBUTORS.md) | [CONTRIBUTORS.md](https://github.com/apache/tvm/blob/main/CONTRIBUTORS.md) |\n| [KEYS](https://github.com/apache/tvm/blob/main/KEYS) | [KEYS](https://github.com/apache/tvm/blob/main/KEYS) |\n| [LICENSE](https://github.com/apache/tvm/blob/main/LICENSE) | [LICENSE](https://github.com/apache/tvm/blob/main/LICENSE) |\n| [Makefile](https://github.com/apache/tvm/blob/main/Makefile) | [Makefile](https://github.com/apache/tvm/blob/main/Makefile) |\n| [NOTICE](https://github.com/apache/tvm/blob/main/NOTICE) | [NOTICE](https://github.com/apache/tvm/blob/main/NOTICE) |\n| [README.md](https://github.com/apache/tvm/blob/main/README.md) | [README.md](https://github.com/apache/tvm/blob/main/README.md) |\n| [conftest.py](https://github.com/apache/tvm/blob/main/conftest.py) | [conftest.py](https://github.com/apache/tvm/blob/main/conftest.py) |\n| [mypy.ini](https://github.com/apache/tvm/blob/main/mypy.ini) | [mypy.ini](https://github.com/apache/tvm/blob/main/mypy.ini) |\n| [pyproject.toml](https://github.com/apache/tvm/blob/main/pyproject.toml) | [pyproject.toml](https://github.com/apache/tvm/blob/main/pyproject.toml) |\n| [version.py](https://github.com/apache/tvm/blob/main/version.py) | [version.py](https://github.com/apache/tvm/blob/main/version.py) |\n| View all files |\n\n## Repository files navigation\n\n# Open Deep Learning Compiler Stack\n\n[Documentation](https://tvm.apache.org/docs) \\|\n[Contributors](https://github.com/apache/tvm/blob/main/CONTRIBUTORS.md) \\|\n[Community](https://tvm.apache.org/community) \\|\n[Release Notes](https://github.com/apache/tvm/blob/main/NEWS.md)\n\nApache TVM is a compiler stack for deep learning systems. It is designed to close the gap between the\nproductivity-focused deep learning frameworks and the performance- and efficiency-focused hardware backends.\nTVM works with deep learning frameworks to provide end-to-end compilation for different backends.\n\n## License\n\nTVM is licensed under the [Apache-2.0](https://github.com/apache/tvm/blob/main/LICENSE) license.\n\n## Getting Started\n\nCheck out the [TVM Documentation](https://tvm.apache.org/docs/) site for installation instructions, tutorials, examples, and more.\nThe [Getting Started with TVM](https://tvm.apache.org/docs/get_started/overview.html) tutorial is a great\nplace to start.\n\n## Contribute to TVM\n\nTVM adopts the Apache committer model. We aim to create an open-source project maintained and owned by the community.\nCheck out the [Contributor Guide](https://tvm.apache.org/docs/contribute/).\n\n## History and Acknowledgement\n\nTVM started as a research project for deep learning compilation.\nThe first version of the project benefited a lot from the following projects:\n\n- [Halide](https://github.com/halide/Halide): Part of TVM's TIR and arithmetic simplification module\noriginates from Halide. We also learned and adapted some parts of the lowering pipeline from Halide.\n- [Loopy](https://github.com/inducer/loopy): use of integer set analysis and its loop transformation primitives.\n- [Theano](https://github.com/Theano/Theano): the design inspiration of symbolic scan operator for recurrence.\n\nSince then, the project has gone through several rounds of redesigns.\nThe current design is also drastically different from the initial design, following the\ndevelopment trend of the ML compiler community.\n\nThe most recent version focuses on a cross-level design with TensorIR as the tensor-level representation\nand Relax as the graph-level representation and Python-first transformations.\nThe project's current design goal is to make the ML compiler accessible by enabling most\ntransformations to be customizable in Python and bringing a cross-level representation that can jointly\noptimize computational graphs, tensor programs, and libraries. The project is also a foundation\ninfra for building Python-first vertical compilers for domains, such as LLMs.\n\n## About\n\nOpen deep learning compiler stack for cpu, gpu and specialized accelerators\n\n[tvm.apache.org/](https://tvm.apache.org/)\n\n### Topics\n\n[javascript](https://github.com/topics/javascript) [machine-learning](https://github.com/topics/machine-learning) [performance](https://github.com/topics/performance) [deep-learning](https://github.com/topics/deep-learning) [metal](https://github.com/topics/metal) [compiler](https://github.com/topics/compiler) [gpu](https://github.com/topics/gpu) [vulkan](https://github.com/topics/vulkan) [opencl](https://github.com/topics/opencl) [tensor](https://github.com/topics/tensor) [spirv](https://github.com/topics/spirv) [rocm](https://github.com/topics/rocm) [tvm](https://github.com/topics/tvm)\n\n### Resources\n\n[Readme](https://github.com/github.com#readme-ov-file)\n\n### License\n\n[Apache-2.0 license](https://github.com/github.com#Apache-2.0-1-ov-file)\n\n### Code of conduct\n\n[Code of conduct](https://github.com/github.com#coc-ov-file)\n\n### Security policy\n\n[Security policy](https://github.com/github.com#security-ov-file)\n\n### Uh oh!\n\nThere was an error while loading. Please reload this page.\n\n[Activity](https://github.com/apache/tvm/activity)\n\n[Custom properties](https://github.com/apache/tvm/custom-properties)\n\n### Stars\n\n[**12.5k**\\\nstars](https://github.com/apache/tvm/stargazers)\n\n### Watchers\n\n[**376**\\\nwatching](https://githu","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fgithub.com%2Fapache%2Ftvm","id":"https://github.com/apache/tvm","publishedDate":null,"summary":"The GitHub repository for Apache TVM is an open-source deep learning compiler stack designed to optimize machine learning models across various hardware platforms, including CPUs, GPUs, and specialized accelerators. With over 12,500 stars and 3,600 forks, it is a widely adopted project that facilitates efficient deployment of deep learning models. The repository contains extensive code, documentation, and tools organized into directories such as apps, cmake, docker, docs, python, and src. TVM aims to enable high-performance model compilation and execution, making it a valuable resource for developers working on deploying deep learning models across diverse hardware environments.","title":"apache/tvm: Open deep learning compiler stack for cpu ... - GitHub","url":"https://github.com/apache/tvm"},{"content":"Failed to fetch content","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fdiscuss.tvm.apache.org%2Ft%2Fphasing-out-legacy-components%2F17703","id":"https://discuss.tvm.apache.org/t/phasing-out-legacy-components/17703","publishedDate":"2024-09-15T00:00:00.000Z","summary":"Failed to fetch content","title":"Phasing out Legacy Components - Apache TVM Discuss","url":"https://discuss.tvm.apache.org/t/phasing-out-legacy-components/17703"},{"content":"April 8, 2025\n\n# What about the MLIR compiler infrastructure? (Democratizing AI Compute, Part 8)\n\nChris Lattner\n\nBy 2018, AI software had a system fragmentation problem. TensorFlow, PyTorch, JAX, Glow, ONNX, TensorFlow-Lite, XLA, TVM—the list kept growing, and each framework invented its own tangled web of “AI graphs” with different “ops.” The ecosystem was **splintering into silos**, each racing to optimize for different hardware while reinventing the same ideas with subtle variations. Complexity was exploding, and something had to give.\n\nAt the time, I was helping scale Google’s TPUs (and several other internal ASICs) in support of TensorFlow, and it was clear we couldn’t keep reinventing compiler infrastructure from scratch for every project. We needed a better foundation. Fortunately, I had years of experience building LLVM—and Jeff Dean as my manager. Jeff, [a legendary engineer](https://en.wikipedia.org/wiki/Jeff_Dean) and a compiler PhD himself, saw the same problem.\n\nIn a 1:1 conversation, Jeff said something like:\n\n> “Hey Chris, I agree we have a compiler problem. Why don’t you go build a new compiler to unify this mess?”\n\nAnd so, [MLIR](https://en.wikipedia.org/wiki/MLIR_(software)) was born—a **modular, extensible compiler infrastructure** designed to bring order to the chaos. It brought forth a foundation that could scale across hardware platforms, software frameworks, and the rapidly evolving needs of machine learning. It aimed to unify these systems, and provide a technology platform that could harmonize compute from many different hardware makers.\n\nBut unification is _hard_. What started as a technical project quickly turned into a battleground: **open-source governance**, **corporate rivalries**, and **competing visions** all collided. What could have been a straightforward engineering win became something much more complicated.\n\nToday, MLIR is embedded in nearly every major AI stack—including [**CUDA**](https://x.com/JokerEph/status/1902758983116657112)—yet it still hasn’t delivered on the dream of democratized AI compute.\n\nThis is the story of MLIR: how it started, what it changed, and the power struggles along the way.\n\n# MLIR, the Origin Story\n\nModern AI systems rely on complex graphs of operations—matrix multiplications, convolutions, attention mechanisms, and more—all strung together into computational pipelines. To optimize and transform these efficiently requires a solid compiler foundation, as discussed in [part 6](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers).\n\nBut in 2018, most AI frameworks were reinventing compiler technology—and often doing it poorly. Basic techniques like [**Static Single Assignment (SSA)**](https://en.wikipedia.org/wiki/Static_single-assignment_form) were [missing from many](https://www.tensorflow.org/guide/graph_optimization). Each framework had its own ad-hoc graph system, bolted together with hacks that didn’t scale. The result was a fragmented, inefficient ecosystem, riddled with duplication.\n\nI knew we needed a better approach, so I pulled four like-minded folks into a small room at Google. We spent days white-boarding, sketching out what a modern, scalable compiler infrastructure for AI might look like. Our central question: **Could we build a unified representation that could support every AI framework, every hardware backend, and every kind of optimization—from algebraic simplification to** [**polyhedral analysis**](https://en.wikipedia.org/wiki/Frameworks_supporting_the_polyhedral_model) **?**\n\nCirca 2018: Yours truly and four colleagues gather in front of a whiteboard to brainstorm a next-generation compiler\n\nThe breakthrough idea we created is now known as **MLIR dialects**—a way to cleanly separate **domain-specific** concerns from the **core infrastructure** of a compiler. Rather than forcing every user to adopt a rigid, one-size-fits-all intermediate representation (like [LLVM](https://llvm.org/pubs/2002-12-LattnerMSThesis.html) and other compilers), MLIR would let compiler engineers define their own representations—custom ops, types, and semantics—tailored to their domain.\n\n> _Aside: I’m not diving deep on how MLIR works in this post. If you’re curious, check out the_ [_original technical keynote_](https://llvm.org/devmtg/2019-04/talks.html#Keynote_1) _or one of the_ [_many tutorials online_](https://lowlevelbits.org/compiling-ruby-part-3/) _._\n\nAt the time, this was a radical departure from how most compilers were built. Traditional infrastructures were monolithic—forcing all frontends and passes into a single, rigid model. But MLIR embraced **heterogeneity** from day one. It let multiple levels of abstraction coexist, transform, and interoperate seamlessly.\n\nThat modularity was the key. Instead of reimplementing the same infrastructure over and over, MLIR gave developers a shared foundation—whether they were working with TensorFlow graphs, PyTorch IR, or custom TPU ops. It made it possible to build specialized compilers without starting from scratch, and it enabled **true composability** across the AI compiler stack.\n\nMLIR wasn’t just another compiler: It was a framework for building **many** compilers.\n\n# MLIR Growth Within Google and Beyond\n\nMLIR began as a research project inside [**Google Brain**](https://research.google.com/teams/brain/) as a focused team trying to rethink how AI compilers should work. My team was heads-down on the fundamentals: designing the IR, implementing transformations, and validating that the core ideas actually worked. Meanwhile, Google’s open culture and MLIR’s modular design made it easy for others to pick it up and experiment. Before long, MLIR took on a life of its own.\n\nAcross Google, teams working on **custom ASICs** saw the potential. MLIR gave them a structured way to express and optimize hardware-specific operations. **Application-focused teams** started using it for **mobile AI**, and the **TensorFlow team** brought MLIR into **TensorFlow Lite**. Even individual researchers, fascinated by MLIR’s flexibility, began using it to prototype novel compiler techniques.\n\nWhat followed was a **mini-explosion** of use cases. Every new application brought fresh feedback, often while we were still deep in iteration mode. Crucially, this validated our dialect-first approach—proving that MLIR could scale across wildly different domains, from edge devices to datacenter accelerators. Eventually, we reached a tipping point: MLIR was becoming a critical piece of infrastructure across many projects.\n\nMany of us wanted MLIR to reach its full potential—to go beyond Google’s first-party use cases.\n\nAbove: a well-known meme within the MLIR community (Credit: Mehdi Amini)\n\nSo we took the leap: we **open-sourced MLIR** and [contributed it to the **LLVM Foundation**](https://blog.google/technology/ai/mlir-accelerating-ai-open-source-infrastructure/), making it available for the entire industry. To support adoption, we organized regular “ [open design meetings](https://mlir.llvm.org/talks/#open-design-meeting-presentations),” where external contributors could participate in MLIR’s evolution and benefit from the engineering investment behind it. This open collaboration helped catalyze MLIR’s global momentum, especially among compiler developers hungry for a modern infrastructure.\n\n**With this as fuel, MLIR took off:** It is now the [foundation for many major AI projects](https://mlir.llvm.org/users/): [OpenXLA](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers), [Triton](https://www.modular.com/blog/democratizing-ai-compute-part-7-what-about-triton-and-python-edsls), and even parts of CUDA itself. It’s also powering compilers in quantum computing, hardware design ( [via CIRCT](https://circt.llvm.org/)), and [many other domains](https://mlir.llvm.org/users/). Companies around the world—from scrappy startups to hyperscalers—started building their next-generation compilers using MLIR. Much of MLIR’s early growth and success was **directly attributable to Google’s leadership and open approach**—something I think the industry still under-appreciates.\n\nYet for all that success, the grand vision remained out of reach. The ecosystem is still fractured. CUDA still reigns supreme. The dream of truly democratized AI compute remains just that—a dream.\n\nSo what happened? Why did MLIR succeed _technically_, but fail to break the CUDA lock-in?\n\nTo understand that, we need to talk about the **politics, power struggles, and compromises** that shaped MLIR’s evolution.\n\n# The Race to Claim an End-to-end AI Solution\n\nFrom the outset, MLIR was conceived as **general-purpose compiler infrastructure**—a framework designed to allow for **domain-specific compilers**. The goal was flexibility and modularity—MLIR was never just about Machine Learning. In fact, the “ML” in MLIR stood for [_everything but Machine Learning_](https://www.youtube.com/watch?si=PV49fAovBS3pkKo8&t=352&v=qzljG6DKgic&feature=youtu.be) (yep, compiler jokes are nerdy!). However, the AI community was hungry for something more. The AI world wanted an **end-to-end compiler**—something that could map TensorFlow or PyTorch models cleanly to a broad range of hardware.\n\nThe race was on to build the first end-to-end MLIR-based AI solution\n\nAs MLIR gained traction, teams inside and outside Google began racing to build an **end-to-end AI solution** on top of it. Other projects—like [OpenXLA](https://www.modular.com/blog/democratizing-ai-compute-part-6-what-about-ai-compilers), [TritonLang](https://www.modular.com/blog/democratizing-ai-compute-part-7-what-about-triton-and-python-edsls) and many others—adopted MLIR as an implementation detail to strengthen their own stacks. This raised a question: Everyone wanted to be the next-gen AI stack—so who would get there first?\n\nThe race was on. Years later, we know the unfortunate answer: **nobody**.\n\n##### **MLIR’s AI Dialect Explosion**\n\nContributing MLIR to the [LLVM Foundation](","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fwww.modular.com%2Fblog%2Fdemocratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure","id":"https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure","publishedDate":"2025-04-08T00:00:00.000Z","summary":"The webpage discusses the origins, development, and ongoing challenges of MLIR (Multi-Level Intermediate Representation), a modular and extensible compiler infrastructure created to address the fragmentation in AI software frameworks. Around 2018, the AI ecosystem was plagued by numerous incompatible systems like TensorFlow, PyTorch, and ONNX, each with its own graph representations and hardware optimizations, leading to inefficiency and complexity. Recognizing this, Chris Lattner and his team at Google designed MLIR to unify these disparate systems through a flexible, dialect-based approach that separates domain-specific concerns from core compiler infrastructure. Despite widespread adoption—including integration with CUDA—MLIR has yet to fully realize its goal of democratizing AI compute, facing challenges from governance, corporate rivalries, and competing visions. Overall, MLIR represents a significant step toward harmonizing AI hardware and software, but its potential is still being realized amidst ongoing power struggles.","title":"What about the MLIR compiler infrastructure? (Democratizing AI ...","url":"https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure"},{"content":"[Skip to content](https://github.com/github.com#start-of-content)\n\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\n\n[wzh99](https://github.com/wzh99)/ **[relay-mlir](https://github.com/wzh99/relay-mlir)** Public\n\n- [Notifications](https://github.com/login?return_to=%2Fwzh99%2Frelay-mlir) You must be signed in to change notification settings\n- [Fork\\\n7](https://github.com/login?return_to=%2Fwzh99%2Frelay-mlir)\n- [Star\\\n58](https://github.com/login?return_to=%2Fwzh99%2Frelay-mlir)\n\n\nAn MLIR-based toy DL compiler for TVM Relay.\n\n[58\\\nstars](https://github.com/wzh99/relay-mlir/stargazers) [7\\\nforks](https://github.com/wzh99/relay-mlir/forks) [Branches](https://github.com/wzh99/relay-mlir/branches) [Tags](https://github.com/wzh99/relay-mlir/tags) [Activity](https://github.com/wzh99/relay-mlir/activity)\n\n[Star](https://github.com/login?return_to=%2Fwzh99%2Frelay-mlir)\n\n[Notifications](https://github.com/login?return_to=%2Fwzh99%2Frelay-mlir) You must be signed in to change notification settings\n\n# wzh99/relay-mlir\n\nmain\n\n[Branches](https://github.com/wzh99/relay-mlir/branches) [Tags](https://github.com/wzh99/relay-mlir/tags)\n\nGo to file\n\nCode\n\nOpen more actions menu\n\n## Folders and files\n\n| Name | Name | Last commit message | Last commit date |\n| --- | --- | --- | --- |\n| ## Latest commit ## History [33 Commits](https://github.com/wzh99/relay-mlir/commits/main/) |\n| [bin](https://github.com/wzh99/relay-mlir/tree/main/bin) | [bin](https://github.com/wzh99/relay-mlir/tree/main/bin) |\n| [example](https://github.com/wzh99/relay-mlir/tree/main/example) | [example](https://github.com/wzh99/relay-mlir/tree/main/example) |\n| [include](https://github.com/wzh99/relay-mlir/tree/main/include) | [include](https://github.com/wzh99/relay-mlir/tree/main/include) |\n| [lib](https://github.com/wzh99/relay-mlir/tree/main/lib) | [lib](https://github.com/wzh99/relay-mlir/tree/main/lib) |\n| [.gitignore](https://github.com/wzh99/relay-mlir/blob/main/.gitignore) | [.gitignore](https://github.com/wzh99/relay-mlir/blob/main/.gitignore) |\n| [CMakeLists.txt](https://github.com/wzh99/relay-mlir/blob/main/CMakeLists.txt) | [CMakeLists.txt](https://github.com/wzh99/relay-mlir/blob/main/CMakeLists.txt) |\n| [README.md](https://github.com/wzh99/relay-mlir/blob/main/README.md) | [README.md](https://github.com/wzh99/relay-mlir/blob/main/README.md) |\n| View all files |\n\n## Repository files navigation\n\n# Relay-MLIR\n\n## Introduction\n\nThis repository contains an MLIR-based toy DL compiler which compiles TVM Relay IR to LLVM IR for x86-64 target. Relay is chosen as the input of this compiler because it has a simple and concise text representation of a DL computation graph.\n\nThis project is intended to:\n\n- Demonstrate how to build a simple DL compiler with MLIR;\n- Implement some key components introduced in the [MLC](https://github.com/mlc-ai/mlc-en) course.\n\nThis project is NOT intended to:\n\n- Build a full-fledged DL compiler for research or production;\n- Achieve comparable performance with mainstream DL compilers.\n\n## Dependency\n\n- A C++20 compatible compiler\n- LLVM 15.0 with MLIR\n- TVM 0.8\n- [fmt](https://github.com/fmtlib/fmt)\n\nSince TVM does not provide a CMake package, you may have to manually copy its C++ headers and libraries to the build directory of this project. Besides, fmt remains to be a dependency of this project until [C++20 text formatting library](https://en.cppreference.com/w/cpp/utility/format) is fully supported in mainstream implementations of the C++ standard library.\n\n## Example\n\nTake this [two-layer MLP](https://github.com/wzh99/relay-mlir/blob/main/example/mlp.txt) as an example.\n\n```\n#[version = \"0.0.5\"]\ndef @main(%x: Tensor[(2, 784), float32], %w1: Tensor[(128, 784), float32], %b1: Tensor[(128), float32], %w2: Tensor[(10, 128), float32], %b2: Tensor[(10), float32]) {\n %0 = nn.dense(%x, %w1, units=None);\n %1 = nn.bias_add(%0, %b1, axis=1);\n %2 = nn.relu(%1);\n %3 = nn.dense(%2, %w2, units=None);\n nn.bias_add(%3, %b2, axis=1)\n}\n\n```\n\nCurrently I have implemented `Relay -> Affine -> SCF+OpenMP -> LLVM` lowering path. Check [`bin/sample.cpp`](https://github.com/wzh99/relay-mlir/blob/main/bin/sample.cpp) for source code. Other paths (such as the ones with Linalg) will be implemented later.\n\nFirst, the Relay IR shown above is imported to MLIR and the tensor shapes are inferred.\n\n**Relay** _\\[click to expand\\]_\n\n```\nmodule {\n func.func @main(%arg0: tensor<2x784xf32>, %arg1: tensor<128x784xf32>, %arg2: tensor<128xf32>, %arg3: tensor<10x128xf32>, %arg4: tensor<10xf32>) -> tensor<2x10xf32> {\n %0 = \"relay.nn.dense\"(%arg0, %arg1) : (tensor<2x784xf32>, tensor<128x784xf32>) -> tensor<2x128xf32>\n %1 = \"relay.nn.bias_add\"(%0, %arg2) {axis = 1 : si64} : (tensor<2x128xf32>, tensor<128xf32>) -> tensor<2x128xf32>\n %2 = \"relay.nn.relu\"(%1) : (tensor<2x128xf32>) -> tensor<2x128xf32>\n %3 = \"relay.nn.dense\"(%2, %arg3) : (tensor<2x128xf32>, tensor<10x128xf32>) -> tensor<2x10xf32>\n %4 = \"relay.nn.bias_add\"(%3, %arg4) {axis = 1 : si64} : (tensor<2x10xf32>, tensor<10xf32>) -> tensor<2x10xf32>\n return %4 : tensor<2x10xf32>\n }\n}\n```\n\nNext, the operations are fused into groups. A new function is created for each fusion group.\n\n**Relay (fused)** _\\[click to expand\\]_\n\n```\nmodule {\n func.func @main(%arg0: tensor<2x784xf32>, %arg1: tensor<128x784xf32>, %arg2: tensor<128xf32>, %arg3: tensor<10x128xf32>, %arg4: tensor<10xf32>) -> tensor<2x10xf32> {\n %0 = call @fused_0(%arg0, %arg1, %arg2) : (tensor<2x784xf32>, tensor<128x784xf32>, tensor<128xf32>) -> tensor<2x128xf32>\n %1 = call @fused_1(%0, %arg3, %arg4) : (tensor<2x128xf32>, tensor<10x128xf32>, tensor<10xf32>) -> tensor<2x10xf32>\n return %1 : tensor<2x10xf32>\n }\n func.func @fused_0(%arg0: tensor<2x784xf32>, %arg1: tensor<128x784xf32>, %arg2: tensor<128xf32>) -> tensor<2x128xf32> attributes {primitive = true} {\n %0 = \"relay.nn.dense\"(%arg0, %arg1) : (tensor<2x784xf32>, tensor<128x784xf32>) -> tensor<2x128xf32>\n %1 = \"relay.nn.bias_add\"(%0, %arg2) {axis = 1 : si64} : (tensor<2x128xf32>, tensor<128xf32>) -> tensor<2x128xf32>\n %2 = \"relay.nn.relu\"(%1) : (tensor<2x128xf32>) -> tensor<2x128xf32>\n return %2 : tensor<2x128xf32>\n }\n func.func @fused_1(%arg0: tensor<2x128xf32>, %arg1: tensor<10x128xf32>, %arg2: tensor<10xf32>) -> tensor<2x10xf32> attributes {primitive = true} {\n %0 = \"relay.nn.dense\"(%arg0, %arg1) : (tensor<2x128xf32>, tensor<10x128xf32>) -> tensor<2x10xf32>\n %1 = \"relay.nn.bias_add\"(%0, %arg2) {axis = 1 : si64} : (tensor<2x10xf32>, tensor<10xf32>) -> tensor<2x10xf32>\n return %1 : tensor<2x10xf32>\n }\n}\n```\n\nWe can then lower Relay dialect to Affine dialect. Note that the buffers ( `memref` s) are allocated and deallocated based on their lifetimes.\n\n**Affine** _\\[click to expand\\]_\n\n```\nmodule {\n func.func @main(%arg0: memref<2x784xf32>, %arg1: memref<128x784xf32>, %arg2: memref<128xf32>, %arg3: memref<10x128xf32>, %arg4: memref<10xf32>, %arg5: memref<2x10xf32>) attributes {num_inputs = 5 : i64} {\n %0 = memref.alloc() : memref<2x128xf32>\n call @fused_0_lowered(%arg0, %arg1, %arg2, %0) : (memref<2x784xf32>, memref<128x784xf32>, memref<128xf32>, memref<2x128xf32>) -> ()\n call @fused_1_lowered(%0, %arg3, %arg4, %arg5) : (memref<2x128xf32>, memref<10x128xf32>, memref<10xf32>, memref<2x10xf32>) -> ()\n memref.dealloc %0 : memref<2x128xf32>\n return\n }\n func.func @fused_0_lowered(%arg0: memref<2x784xf32>, %arg1: memref<128x784xf32>, %arg2: memref<128xf32>, %arg3: memref<2x128xf32>) attributes {num_inputs = 3 : i64, primitive = true} {\n %0 = memref.alloc() : memref<2x128xf32>\n affine.for %arg4 = 0 to 2 {\n affine.for %arg5 = 0 to 128 {\n %cst = arith.constant 0.000000e+00 : f32\n %2 = affine.for %arg6 = 0 to 784 iter_args(%arg7 = %cst) -> (f32) {\n %3 = affine.load %arg0[%arg4, %arg6] : memref<2x784xf32>\n %4 = affine.load %arg1[%arg5, %arg6] : memref<128x784xf32>\n %5 = arith.mulf %3, %4 : f32\n %6 = arith.addf %arg7, %5 : f32\n affine.yield %6 : f32\n }\n affine.store %2, %0[%arg4, %arg5] : memref<2x128xf32>\n }\n }\n %1 = memref.alloc() : memref<2x128xf32>\n affine.for %arg4 = 0 to 2 {\n affine.for %arg5 = 0 to 128 {\n %2 = affine.load %0[%arg4, %arg5] : memref<2x128xf32>\n %3 = affine.load %arg2[%arg5] : memref<128xf32>\n %4 = arith.addf %2, %3 : f32\n affine.store %4, %1[%arg4, %arg5] : memref<2x128xf32>\n }\n }\n memref.dealloc %0 : memref<2x128xf32>\n affine.for %arg4 = 0 to 2 {\n affine.for %arg5 = 0 to 128 {\n %2 = affine.load %1[%arg4, %arg5] : memref<2x128xf32>\n %cst = arith.constant 0.000000e+00 : f32\n %3 = arith.maxf %2, %cst : f32\n affine.store %3, %arg3[%arg4, %arg5] : memref<2x128xf32>\n }\n }\n memref.dealloc %1 : memref<2x128xf32>\n return\n }\n func.func @fused_1_lowered(%arg0: memref<2x128xf32>, %arg1: memref<10x128xf32>, %arg2: memref<10xf32>, %arg3: memref<2x10xf32>) attributes {num_inputs = 3 : i64, primitive = true} {\n %0 = memref.alloc() : memref<2x10xf32>\n affine.for %arg4 = 0 to 2 {\n affine.for %arg5 = 0 to 10 {\n %cst = arith.constant 0.000000e+00 : f32\n %1 = affine.for %arg6 = 0 to 128 iter_args(%arg7 = %cst) -> (f32) {\n %2 = affine.load %arg0[%arg4, %arg6] : memref<2x128xf32>\n %3 = affine.load %arg1[%arg5, %arg6] : memref<10x128xf32>\n %4 = arith.mulf %2, %3 : f32\n %5 = arith.addf %arg7, %4 : f32\n affine.yield %5 : f32\n }\n affine.store %1, %0[%arg4, %arg5] : memref<2x10xf32>\n }\n }\n affine.for %arg4 = 0 to 2 {\n affine.for %arg5 = 0 to 10 {\n %1 = affine.load %0[%arg4, %arg5] : memref<2x10xf32>\n %2 = affine.load %arg2[%arg5] : memref<10xf32>\n %3 = arith.addf %1, %2 : f32\n affine.store %3, %arg3[%arg4, %arg5] : memref<2x10xf32>\n }\n }\n memref.dealloc %0 : memref<2x10xf32>\n return\n }\n}\n```\n\nSeveral optimizations are performed on affine loops, including loop fusion, scalar replacement, parallelization and vectorization.\n\n**Affine (optimized)** _\\[click to expand\\]_\n\n```\nmodule {\n func.func @main(%arg0: memref<2x784xf32>, %arg1: memref<128x7","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fgithub.com%2Fwzh99%2Frelay-mlir","id":"https://github.com/wzh99/relay-mlir","publishedDate":null,"summary":"The GitHub repository \"relay-mlir\" by wzh99 presents a toy deep learning compiler based on MLIR that translates TVM Relay IR into LLVM IR targeting x86-64 architectures. Designed primarily as an educational demonstration, it showcases the process of building a simple DL compiler using MLIR, focusing on key components introduced in the MLC course. The project emphasizes simplicity over performance, aiming to illustrate compiler construction concepts rather than creating a production-ready tool. It depends on LLVM 15.0, TVM 0.8, and C++20-compatible compilers, with additional dependencies like fmt. The repository includes example models, such as a two-layer MLP, to demonstrate its functionality. It serves as a valuable resource for understanding the compilation pipeline from Relay IR to executable code via MLIR.","title":"An MLIR-based toy DL compiler for TVM Relay. - GitHub","url":"https://github.com/wzh99/relay-mlir"},{"content":"## What is MLIR and LLVM?\n\nLLVM is a powerful compiler infrastructure project that has revolutionized how we build programming languages and tools. At its core, LLVM provides a collection of modular compiler and toolchain technologies that can be used to develop frontend compilers for any programming language and generate optimized code for many target architectures.\n\n**MLIR is a higher level framework that builds on LLVM and is used for building**\n**compilers and machine learning frameworks.** MLIR is the newest project within the LLVM ecosystem, designed to address the challenges of modern hardware accelerators and machine learning frameworks. While LLVM IR is powerful, it operates at a relatively low level. MLIR extends these concepts to support multiple levels of abstraction.\n\nUnfortunatly, the modern compiler stack is often something that one has to learn from reading RFPs, papers and mailing lists. With this tutorial I'll try to give a concise overview of the modern compiler stack and how MLIR fits into it, and how to integrate into the Python machine learning ecosystem.\n\n- [Part 1 - What is MLIR?](https://vectorfold.studio/blog/introduction-to-mlir-and-modern-compiler-infrastructure)\n- [Part 2 - Memory in MLIR](https://vectorfold.studio/blog/memory-in-mlir)\n- [Part 3 - Affine Dialect and OpenMP](https://vectorfold.studio/blog/affine_openmp)\n- [Part 4 - Linear Algebra](https://vectorfold.studio/blog/linear-algebra)\n- [Part 5 - Neural Networks](https://vectorfold.studio/blog/neural-networks)\n- [Part 6 - e-graphs and Term Rewriting](https://vectorfold.studio/blog/egglog)\n- [Part 7 - Transformer Architecture](https://vectorfold.studio/blog/transformers)\n- Part 8 - NVidia GPU Execution\n- Part 9 - Superoptimizing Deep Learning\n\nAt VectorFold Studios our clients leverage our deep optimization expertise to supercharge their AI inference workloads. We specialize in transforming high-level models into efficient kernels, producing finely-tuned models that maximizes GPU and TPU performance to deliver exceptional throughput for client's production deployments. Interally we use MLIR extensivley and so we're going to write a series of articles explaining how it works and how to use it.\n\n### Why MLIR?\n\nMLIR was designed to tackle the shortcomings of older compilers like LLVM and GCC, especially when it comes to modern hardware accelerators and AI workloads. While LLVM does a great job for traditional CPU targets, it falls short with the variety of specialized architectures popping up in AI and machine learning.\n\nToday's AI systems often mix CPUs, GPUs, TPUs, and custom ASICs. MLIR's flexible architecture makes it easier to work across these different platforms, providing solid support for heterogeneous hardware.\n\nThe **dialect** system in MLIR can represent and optimize operations at various levels, from high-level ML tasks down to hardware-specific instructions. This means we can implement optimizations tailored to specific domains, making AI workloads run more efficiently.\n\nInstead of needing separate compilers for each type of accelerator or framework, MLIR offers a unified infrastructure that can be extended for different use cases. Traditional compilers weren't built with AI in mind, but MLIR has first-class support for tensor operations, neural networks, transformers, and other ML constructs.\n\nMLIR has become the go-to tech for specialized machine learning accelerators, finding applications in signal processing, quantum computing, homomorphic encryption, FPGAs, and custom silicon. Its ability to create domain-specific compilers is a game-changer for those \"weird domains\" that don't fit the traditional CPU and GPU mold. Plus, it can be easily embedded in other languages, allowing us to build domain-specific languages tailored for specific tasks—something we'll dive into in this blog.\n\n### Key Components of the Modern Compiler Stack\n\n1. **LLVM**: The heart of LLVM is its intermediate representation - a low-level programming language similar to assembly but with important high-level information preserved. This IR serves as a bridge between source languages and target architectures.\n\n2. **MLIR**: MLIR is a new \"multi-level\" intermediate representation that is more expressive and can represent higher-level concepts like control flow, dataflow, and parallelism and be lowered into LLVM IR.\n\n3. **Optimization Passes**: Both MLIR and LLVM provide a vast set of optimization passes that work on the IR level, allowing any language using LLVM to benefit from sophisticated optimizations. Most are written by grad students and PhDs on the latest and greatest research in compiler optimization. This is what makes LLVM such a powerful tool.\n\n4. **E-Graphs** Equality saturation is a technique for building optimizing compilers using e-graphs, which originated from the automated theorem proving world; it operates by applying a set of rewrites using e-matching until the e-graph is saturated, continuing the saturation process until one of several halting conditions is met. After the rewriting phase, an optimal term is extracted from the e-graph based on a cost function, typically related to the size of the syntax tree or performance considerations, allowing for more efficient optimizations by exploring a broader set of potential transformations and selecting the best one based on defined criteria.\n\n\nA modern compiler then weaves these components together to make up a pipeline.\n\n1. Source code is parsed into a surface language.\n2. Surface language is translated to a core language.\n3. Core langauge can be optimized by core-to-core transformations (optionally using tools like e-graphs).\n4. Core language is lowered to MLIR.\n5. MLIR performs high-level optimizations\n6. MLIR is lowered to LLVM IR\n7. LLVM performs low-level optimizations and code generation\n\nMany programmming languages implement the \"mid-level\" IR as a way to bridge the gap between the source language and LLVM. For example, Rust, Swift, Tensorflow all follow this pattern. With MLIR we have can have a new generation of programming languages and frameworks that can reuse the same infrastructure and perhaps become more interoperable, in much the same way that LLVM has become the de facto backend for many programming languages.\n\n### Installing MLIR\n\nInstalling MLIR is a bit painful, it requires a few dependencies to be installed first.\n\n```\n# MacOS\nbrewinstallcmakeninjaccache\n\n# Linux\napt-getinstallcmakeninja-buildccache\n```\n\nThen we can clone the LLVM repository and build MLIR from source. This can take a _long_ time to build, I recommend having a cup of tea while you add entropy to the universe.\n\n```\ngitclonehttps://github.com/llvm/llvm-project\nmkdirllvm-project/build\ncdllvm-project/build\ncmake-GNinja../llvm\\\n-DLLVM_ENABLE_PROJECTS=mlir\\\n-DLLVM_BUILD_EXAMPLES=ON\\\n-DLLVM_TARGETS_TO_BUILD=\"Native;ARM;X86\"\\\n-DCMAKE_BUILD_TYPE=Release\\\n-DLLVM_ENABLE_ASSERTIONS=ON\\\n-DCMAKE_C_COMPILER=clang-DCMAKE_CXX_COMPILER=clang++\\\n-DLLVM_CCACHE_BUILD=ON\ncmake--build.--targetcheck-mlir\ncmake--build.--targetinstall\n```\n\nCheck the version of MLIR installed. This tutorial is using version 21.0.0.\n\n```\nmlir-opt--version\nLLVM (http://llvm.org/):\nLLVMversion21.0.0git\nOptimizedbuildwithassertions.\n```\n\nAt some point in the near future the Homebrew version of LLVM will be updated to include MLIR, at which point this will be a lot easier. If you're reading this at some point in a better future you can just try this and it might work.\n\n### MLIR Concepts\n\nMLIR is a programming language in its own right, it's very low-level but it contains many of the core primitives you might find in other languages.\n\n1. **Modules**: The top-level container for MLIR operations.\n\n1. **Functions**: A function is a collection of operations that are executed in a specific order.\n\n```\nfunc.func @my_function(%arg0: i32, %arg1: i32) -> i32 {\n // Operations\n}\n```\n\n1. **Dialects**: Domain-specific sets of operations and types in MLIR.\n\nThe major ones well use are:\n\n- `scf`: The structured control flow (scf) dialect provides operations that represent structured control flow constructs, such as loops and conditionals, ensuring a clear and maintainable flow of execution.\n- `func`: This dialect encompasses operations related to function definitions and calls, facilitating the organization and modularization of code through high-level function abstractions.\n- `memref`: The memref dialect is dedicated to memory reference operations, allowing for efficient management and manipulation of memory buffers, essential for performance-critical applications.\n- `index`: The index dialect specializes in operations for handling index computations, which are crucial for addressing elements in arrays and tensors, particularly in loop iterations and data access patterns.\n- `tensor`: This dialect provides operations for creating and manipulating multi-dimensional arrays (tensors), enabling high-level mathematical expressions and transformations in a side-effect-free manner.\n- `linalg`: The linear algebra (linalg) dialect offers a set of operations specifically designed for linear algebra computations, facilitating efficient implementations of matrix and vector operations.\n- `omp`: The OpenMP (omp) dialect includes operations that support parallel programming models, allowing developers to express parallelism in a way that is compatible with OpenMP standards.\n- `arith`: The arithmetic (arith) dialect contains fundamental mathematical operations for both integer and floating-point types, including basic arithmetic, bitwise operations, and comparisons, applicable to scalars, vectors, and tensors.\n- `affine`: The affine dialect provides a framework for expressing affine operations and analyses, enabling optimizations that leverage the predictable behavior of affine expressions.\n- `gpu`: The GPU dialect is specialized for GPU programming, providing operations for parallel execution on heterogeneous architectures.\n\nAdditionally we'll use some hardware specific","favicon":"https://api.ray.so/favicon?url=https%3A%2F%2Fvectorfold.studio%2Fblog%2Fintroduction-to-mlir-and-modern-compiler-infrastructure","id":"https://vectorfold.studio/blog/introduction-to-mlir-and-modern-compiler-infrastructure","publishedDate":"2025-03-07T00:00:00.000Z","summary":"The webpage provides an overview of MLIR (Multi-Level Intermediate Representation) and its role within the LLVM compiler ecosystem. LLVM is a modular infrastructure for developing optimized compilers across various architectures, primarily for traditional CPU targets. MLIR extends LLVM's capabilities by supporting multiple levels of abstraction, making it especially suited for modern hardware accelerators and machine learning workloads. It addresses limitations of older compilers like LLVM and GCC in handling heterogeneous AI hardware such as GPUs, TPUs, and custom ASICs. MLIR's flexible architecture, including its dialect system, enables domain-specific optimizations for neural networks, transformers, and tensor operations, facilitating efficient deployment across diverse platforms. The webpage emphasizes MLIR's significance in modern AI and hardware acceleration, its adaptability for embedding in other languages, and its growing adoption in specialized domains like signal processing and quantum computing. The series of articles aims to explain MLIR's architecture, usage, and integration into the machine learning ecosystem.","title":"Introduction to MLIR and Modern Compilers","url":"https://vectorfold.studio/blog/introduction-to-mlir-and-modern-compiler-infrastructure"}],"status":"completed","toolCallId":"call_iYb9erMkThB9TzhWgRrKaJMW","toolName":"webSearch","type":"tool_call"},{"reasoning":"","type":"reasoning"},{"text":"Short answer: Relay is still present in Apache TVM, but its role in day-to-day AI infrastructure has diminished as the TVM project shifted focus to the newer Relax IR and TensorIR in the “TVM Unity” direction. In production, most large orgs standardize on framework-native stacks (PyTorch 2.x Inductor/Triton, TensorRT, OpenXLA/XLA, ONNX Runtime) and only a minority use TVM/Relay at scale today.\n\nMore detail:\n- Within TVM itself:\n  - Relay remains the high-level, functional IR described in the 2019 paper and is still in the codebase and docs. It’s used by existing frontends and legacy models.\n  - TVM’s current roadmap emphasizes Relax (a newer graph/program IR with first-class symbolic shapes and progressive lowering) plus TensorIR for kernel-level optimization. Many new features, LLM-centric flows, and Python-first transformations target Relax/TIR rather than Relay.\n  - Community discussions and docs describe phasing out legacy components and pushing toward Relax/TIR for end-to-end pipelines; Relay isn’t “removed,” but it’s no longer the primary innovation focus.\n\n- In broader industry:\n  - Production inference stacks commonly rely on:\n    - NVIDIA: TensorRT/Torch-TensorRT, cuDNN/cuBLAS, Triton Inference Server.\n    - PyTorch 2.x: Dynamo/Inductor with Triton kernels, which captured a lot of “compiler” mindshare.\n    - Google/Alphabet ecosystem: OpenXLA/XLA, StableHLO/MLIR.\n    - ONNX Runtime: widely used for portability and hardware provider EPs.\n  - TVM (and thus Relay in older flows) sees targeted use at companies needing custom hardware support, embedded/edge deployments, or cross-vendor portability—often in research, startups, or specific product lines. Public adopters have included various chip vendors and embedded players, but this is a smaller slice compared to the dominant vendor stacks above.\n\n- Practical takeaway if you’re choosing today:\n  - If you’re on mainstream NVIDIA/AMD/CPU and primarily PyTorch: PyTorch Inductor/Triton or TensorRT usually yields better engineering fit and support.\n  - If you need portable compilation across diverse or custom accelerators, or want heavy program-level control: use TVM’s modern path (Relax + TensorIR). Relay can still work for legacy models, but new projects should plan around Relax.\n\nIn short: Relay influenced many later designs and is still available in TVM, but current “AI infra” use is limited; the TVM community’s momentum and most new development are centered on Relax/TensorIR rather than Relay.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0dad08b8b25758750068c87a728edc819c8d7a2cb9e0b4223f"}},"resumableStreamId":"STREAM:b5a205aa-1daf-4f1b-bfd9-18b1f543ec88","role":"assistant","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","timeToFirstToken":1.347,"tokens":557,"tokensPerSecond":19.56307951671818,"updated_at":1757968996399,"userId":"google:104458561188320708185","id":"j971ze5he8125wd1pyzz5m7ehs7qm3ba"},{"_creationTime":1757969050632.262,"_id":"j973pjnpnxye0kwvpdpen5pc9s7qn4kg","attachmentIds":[],"content":"how does TVM used Relay in its infrastructure","created_at":1757969050632,"messageId":"999aa8ff-1871-4f7f-8104-6286d97b79e1","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","updated_at":1757969050632,"userId":"google:104458561188320708185","id":"j973pjnpnxye0kwvpdpen5pc9s7qn4kg","providerMetadata":{}},{"_creationTime":1757969050632.2622,"_id":"j97dye9yeadpye5gd7wwz87n9s7qmxsq","attachmentIds":[],"byok":false,"content":"TVM used Relay as its high-level, program/graph IR that sits between frontends (model import) and low-level kernel generation. Here’s how Relay fit into TVM’s infrastructure and compilation flow:\n\nPosition in the stack\n- Frontends: Models from PyTorch (via TorchScript/FX), TensorFlow, MXNet, and ONNX were imported into Relay.\n- Relay core: A functional, statically typed IR used for whole-model transformations, shape/type inference, and automatic differentiation (when needed).\n- Lowering boundary: Relay “primitive” functions (often fused subgraphs) were lowered into TensorIR (TIR), TVM’s tensor-level IR.\n- Backends: TIR was scheduled and code-generated for targets like LLVM/CPU, CUDA/ROCm GPUs, Metal/Vulkan, or vendor libraries (cuDNN/cuBLAS) via external codegen.\n- Runtime: The TVM runtime executed the compiled module on the chosen device, handling memory, kernel launches, and cross-language invocation.\n\nWhat Relay did\n- Graph/program optimizations:\n  - Type and shape inference, constant folding, dead code elimination, inlining, common subexpression elimination.\n  - Operator fusion and layout transformations to reduce memory traffic and kernel launches.\n  - Quantization-aware transformations and passes like pattern-based rewrites.\n- Expressivity and control flow:\n  - Supported higher-order functions, conditionals, loops/recursion, enabling dynamic models.\n- Automatic differentiation:\n  - Source-to-source AD on Relay programs for training or fine-tuning scenarios.\n- Partitioning and external codegen:\n  - Relay could partition subgraphs to external providers (e.g., TensorRT, cuDNN/cuBLAS, OneDNN) while compiling the remainder through TVM pipelines.\n- Target-aware compilation:\n  - Relay passes considered the target to guide fusion/layout and then emitted TIR primitives optimized for that hardware.\n\nEnd-to-end build flow (typical)\n1) Import model to Relay from ONNX/PyTorch/etc.\n2) Run Relay optimization passes (folding, fusion, layout changes, quantization).\n3) Lower each fused Relay function to TIR.\n4) Schedule TIR (manually, AutoTVM/Ansor/MetaSchedule) for the target.\n5) Codegen to device binaries or runtime modules (e.g., CUDA PTX, LLVM object).\n6) Deploy and execute via TVM runtime.\n\nHow it interacted with GPUs\n- Relay itself didn’t generate GPU kernels; it formed fused compute regions and chose layouts amenable to GPUs.\n- After lowering, TIR mapped loop axes to GPU blocks/threads, used shared/local memory scopes, and was autotuned for the specific GPU.\n- Optional partitioning routed some ops to vendor libs (cuDNN/cuBLAS), with glue code to integrate results.\n\nWhere it lives relative to newer components\n- Relay was the original high-level IR. Newer TVM “Unity” work emphasizes Relax (a successor graph/program IR with first-class symbolic shapes and better integration with Python) alongside TensorIR.\n- Many legacy paths and frontends still operate through Relay, and Relay-based models coexist with Relax/TIR in the runtime through the same packed-function ABI.\n\nKey takeaway\nIn TVM’s infrastructure, Relay functioned as the high-level optimization and fusion layer: it imported and transformed models, produced fused compute regions, and handed them to TIR for scheduling and codegen across CPUs, GPUs, and accelerators. It enabled whole-program reasoning at the top, while TVM handled low-level performance at the bottom.","created_at":1757969050633,"messageId":"0edcabc7-7454-44a7-8c56-8238174a156d","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"TVM used Relay as its high-level, program/graph IR that sits between frontends (model import) and low-level kernel generation. Here’s how Relay fit into TVM’s infrastructure and compilation flow:\n\nPosition in the stack\n- Frontends: Models from PyTorch (via TorchScript/FX), TensorFlow, MXNet, and ONNX were imported into Relay.\n- Relay core: A functional, statically typed IR used for whole-model transformations, shape/type inference, and automatic differentiation (when needed).\n- Lowering boundary: Relay “primitive” functions (often fused subgraphs) were lowered into TensorIR (TIR), TVM’s tensor-level IR.\n- Backends: TIR was scheduled and code-generated for targets like LLVM/CPU, CUDA/ROCm GPUs, Metal/Vulkan, or vendor libraries (cuDNN/cuBLAS) via external codegen.\n- Runtime: The TVM runtime executed the compiled module on the chosen device, handling memory, kernel launches, and cross-language invocation.\n\nWhat Relay did\n- Graph/program optimizations:\n  - Type and shape inference, constant folding, dead code elimination, inlining, common subexpression elimination.\n  - Operator fusion and layout transformations to reduce memory traffic and kernel launches.\n  - Quantization-aware transformations and passes like pattern-based rewrites.\n- Expressivity and control flow:\n  - Supported higher-order functions, conditionals, loops/recursion, enabling dynamic models.\n- Automatic differentiation:\n  - Source-to-source AD on Relay programs for training or fine-tuning scenarios.\n- Partitioning and external codegen:\n  - Relay could partition subgraphs to external providers (e.g., TensorRT, cuDNN/cuBLAS, OneDNN) while compiling the remainder through TVM pipelines.\n- Target-aware compilation:\n  - Relay passes considered the target to guide fusion/layout and then emitted TIR primitives optimized for that hardware.\n\nEnd-to-end build flow (typical)\n1) Import model to Relay from ONNX/PyTorch/etc.\n2) Run Relay optimization passes (folding, fusion, layout changes, quantization).\n3) Lower each fused Relay function to TIR.\n4) Schedule TIR (manually, AutoTVM/Ansor/MetaSchedule) for the target.\n5) Codegen to device binaries or runtime modules (e.g., CUDA PTX, LLVM object).\n6) Deploy and execute via TVM runtime.\n\nHow it interacted with GPUs\n- Relay itself didn’t generate GPU kernels; it formed fused compute regions and chose layouts amenable to GPUs.\n- After lowering, TIR mapped loop axes to GPU blocks/threads, used shared/local memory scopes, and was autotuned for the specific GPU.\n- Optional partitioning routed some ops to vendor libs (cuDNN/cuBLAS), with glue code to integrate results.\n\nWhere it lives relative to newer components\n- Relay was the original high-level IR. Newer TVM “Unity” work emphasizes Relax (a successor graph/program IR with first-class symbolic shapes and better integration with Python) alongside TensorIR.\n- Many legacy paths and frontends still operate through Relay, and Relay-based models coexist with Relax/TIR in the runtime through the same packed-function ABI.\n\nKey takeaway\nIn TVM’s infrastructure, Relay functioned as the high-level optimization and fusion layer: it imported and transformed models, produced fused compute regions, and handed them to TIR for scheduling and codegen across CPUs, GPUs, and accelerators. It enabled whole-program reasoning at the top, while TVM handled low-level performance at the bottom.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0fb2338c05fa55510068c87a9b45a881a3a0cb9b183f93cfe6"}},"resumableStreamId":"STREAM:ecebbdfb-732d-4ace-a40a-370c09e69227","role":"assistant","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","timeToFirstToken":0.713,"tokens":733,"tokensPerSecond":57.50372636698831,"updated_at":1757969050633,"userId":"google:104458561188320708185","id":"j97dye9yeadpye5gd7wwz87n9s7qmxsq"},{"_creationTime":1757969073867.36,"_id":"j97fatvb4226st9skqfhgny8ch7qnf8r","attachmentIds":[],"content":"how efficient in Relay","created_at":1757969073871,"messageId":"5ba4e66c-9694-495e-b678-db3059df672d","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","updated_at":1757969073871,"userId":"google:104458561188320708185","id":"j97fatvb4226st9skqfhgny8ch7qnf8r","providerMetadata":{}},{"_creationTime":1757969073867.3604,"_id":"j97cq6384sas7c2a5ahneyny5x7qm1sx","attachmentIds":[],"byok":false,"content":"Do you mean “How efficient is Relay?” Here’s a concise, practical view:\n\nWhat Relay optimizes well\n- Operator fusion and layout transforms: Reduces kernel launches and memory traffic; can yield large wins on GPUs/CPUs versus unfused graphs.\n- Cross-op algebraic simplifications and constant folding: Removes overhead and simplifies compute.\n- Target-aware lowering to TensorIR + auto-tuning: When paired with AutoTVM/Ansor/MetaSchedule, Relay-produced kernels can reach near hand-tuned performance on many ops and models.\n- External library partitioning: Can delegate GEMMs/conv to cuBLAS/cuDNN/OneDNN and fuse the rest, often matching strong baselines.\n\nWhere efficiency can vary\n- Kernel quality depends on scheduling/tuning: Without auto-tuning, performance can be far from peak; tuning time can be nontrivial.\n- Dynamic control flow: Supported, but some optimizations (fusion, layout) are harder across highly dynamic patterns, which may limit peak throughput versus static pipelines.\n- Modern TVM direction: Newer flows (Relax + TensorIR in “TVM Unity”) often outperform legacy Relay pipelines due to better symbolic shape handling and improved scheduling integration.\n- Vendor-specialized stacks: On NVIDIA, TensorRT or PyTorch Inductor+Triton can outperform Relay-based flows out of the box for many transformer and CNN models, especially with FP8/INT8 pipelines tightly integrated with hardware.\n\nTypical outcomes reported by users\n- With proper tuning, Relay/TIR pipelines can be within a few percent of vendor libraries on common ops and competitive end-to-end on CNNs and many transformer blocks.\n- On non-mainstream hardware or custom accelerators, Relay’s portability plus tuning can exceed framework defaults because you can co-design schedules.\n- Cold performance (no tuning) can lag specialized stacks; warm performance after meta-schedule tuning is much stronger.\n\nHow to get the best efficiency from Relay today\n- Use fusion and layout passes configured for your target; enable partitioning to cuDNN/cuBLAS on NVIDIA where beneficial.\n- Run MetaSchedule/Ansor tuning on representative shapes/batches.\n- Quantize where appropriate (INT8) using TVM’s QAT/PTQ flows.\n- Consider migrating to Relax for new projects; it retains Relay’s ideas but integrates better with TIR and symbolic shapes.\n\nBottom line\nRelay can be efficient, especially when combined with TVM’s auto-tuning and external libraries. However, compared to today’s leading vendor-specific compilers, Relay usually requires more tuning effort and may trail the very latest, hardware-specific optimizations. For new work, the TVM ecosystem’s efficiency story is increasingly centered on Relax + TensorIR rather than Relay.","created_at":1757969073872,"messageId":"42005f3e-6079-49ea-8539-46f17cf1fab7","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Do you mean “How efficient is Relay?” Here’s a concise, practical view:\n\nWhat Relay optimizes well\n- Operator fusion and layout transforms: Reduces kernel launches and memory traffic; can yield large wins on GPUs/CPUs versus unfused graphs.\n- Cross-op algebraic simplifications and constant folding: Removes overhead and simplifies compute.\n- Target-aware lowering to TensorIR + auto-tuning: When paired with AutoTVM/Ansor/MetaSchedule, Relay-produced kernels can reach near hand-tuned performance on many ops and models.\n- External library partitioning: Can delegate GEMMs/conv to cuBLAS/cuDNN/OneDNN and fuse the rest, often matching strong baselines.\n\nWhere efficiency can vary\n- Kernel quality depends on scheduling/tuning: Without auto-tuning, performance can be far from peak; tuning time can be nontrivial.\n- Dynamic control flow: Supported, but some optimizations (fusion, layout) are harder across highly dynamic patterns, which may limit peak throughput versus static pipelines.\n- Modern TVM direction: Newer flows (Relax + TensorIR in “TVM Unity”) often outperform legacy Relay pipelines due to better symbolic shape handling and improved scheduling integration.\n- Vendor-specialized stacks: On NVIDIA, TensorRT or PyTorch Inductor+Triton can outperform Relay-based flows out of the box for many transformer and CNN models, especially with FP8/INT8 pipelines tightly integrated with hardware.\n\nTypical outcomes reported by users\n- With proper tuning, Relay/TIR pipelines can be within a few percent of vendor libraries on common ops and competitive end-to-end on CNNs and many transformer blocks.\n- On non-mainstream hardware or custom accelerators, Relay’s portability plus tuning can exceed framework defaults because you can co-design schedules.\n- Cold performance (no tuning) can lag specialized stacks; warm performance after meta-schedule tuning is much stronger.\n\nHow to get the best efficiency from Relay today\n- Use fusion and layout passes configured for your target; enable partitioning to cuDNN/cuBLAS on NVIDIA where beneficial.\n- Run MetaSchedule/Ansor tuning on representative shapes/batches.\n- Quantize where appropriate (INT8) using TVM’s QAT/PTQ flows.\n- Consider migrating to Relax for new projects; it retains Relay’s ideas but integrates better with TIR and symbolic shapes.\n\nBottom line\nRelay can be efficient, especially when combined with TVM’s auto-tuning and external libraries. However, compared to today’s leading vendor-specific compilers, Relay usually requires more tuning effort and may trail the very latest, hardware-specific optimizations. For new work, the TVM ecosystem’s efficiency story is increasingly centered on Relax + TensorIR rather than Relay.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0b694e9a434feb860068c87ab29844819cac4647cc9b6706a5"}},"resumableStreamId":"STREAM:8972aa25-9efc-4f40-a44e-1f7a9e45c87f","role":"assistant","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","timeToFirstToken":0.918,"tokens":559,"tokensPerSecond":39.07997762863535,"updated_at":1757969073872,"userId":"google:104458561188320708185","id":"j97cq6384sas7c2a5ahneyny5x7qm1sx"},{"_creationTime":1757969118016.7502,"_id":"j979a7w1q78s7mj8tfzk7f4v517qn6aa","attachmentIds":[],"content":"how does functional programming play into Relay","created_at":1757969118017,"messageId":"31cae366-66dc-47c9-b41c-aee71d002e89","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","updated_at":1757969118017,"userId":"google:104458561188320708185","id":"j979a7w1q78s7mj8tfzk7f4v517qn6aa","providerMetadata":{}},{"_creationTime":1757969118016.7505,"_id":"j977b3e7pgms1yj87140gmmr6h7qnhr9","attachmentIds":[],"byok":false,"content":"Functional programming is central to Relay’s design and is the key to unifying dynamic expressivity with strong static optimization. Here’s how it plays in:\n\nCore language model\n- First-class functions: Functions are values—you can pass them as arguments, return them, store them—enabling higher-order programming (e.g., map, fold, custom layers as functions).\n- Pure, side-effect–free core: Computation is treated as pure by default, which makes transformations like inlining, beta-reduction, CSE, and DCE safe and predictable.\n- Lexical scoping and closures: Relay supports closures, so functions can capture environments, enabling modular composition without sacrificing analyzability.\n- Explicit control flow as expressions: Conditionals, pattern matching, and recursion/loops are expressed as ordinary expressions, not special graph nodes. This keeps dynamic control flow within a uniform, optimizable IR.\n\nStatic reasoning via types\n- Static typing with shape-dependent types: Types carry tensor dtypes and shapes (possibly symbolic). This is a functional-style, strongly typed system that enables compile-time checks and guides optimizations.\n- Type/shape inference: Like many functional languages, Relay infers types, relieving annotation burden while retaining static guarantees.\n- Effect/alias analysis layered on purity: Because the core is functional, it’s easier to reason about when mutations (e.g., buffers) happen, enabling safe fusion and memory planning.\n\nCompiler transformations enabled by FP\n- Equational reasoning: Purity and referential transparency allow algebraic simplifications and reassociation without changing semantics.\n- Inlining and specialization: Higher-order functions can be inlined and specialized for concrete shapes/dtypes, improving performance.\n- Partial evaluation/constant folding: Functional structure makes it straightforward to evaluate known subprograms at compile time.\n- Whole-program AD as a source-to-source transform: Reverse-mode automatic differentiation is implemented by transforming functional programs, so gradients work through higher-order functions and control flow without hardcoding per-operator rules.\n\nBridging dynamic and static\n- Dynamic expressivity: You can write models with data-dependent branches, recursion, or variable sequence lengths using ordinary functional constructs.\n- Static optimization: The type/shape system plus purity enables aggressive compile-time optimization, then Relay lowers to TensorIR for schedule-level tuning.\n\nInterplay with lower levels\n- Boundary to TIR: After functional-level optimizations (fusion, layout transforms), Relay produces “primitive” functions that represent fused regions. These are then lowered to TensorIR, where imperative loop nests and memory scopes are introduced for hardware mapping.\n- External codegen: Functional partitioning lets the compiler carve out subgraphs for vendor libraries (e.g., cuDNN) while keeping the rest in the same functional IR.\n\nPractical benefits\n- Modularity and reuse: Define layers and combinators as ordinary functions; the compiler can still optimize them away when appropriate.\n- Robustness of transforms: FP semantics reduce corner cases in graph rewrites and make analyses (like dead code or common subexpression elimination) more reliable.\n- Extensibility: New ops or patterns can be expressed as functions/combinators without fighting a fixed op-graph abstraction.\n\nIn short: Relay is a functional, statically typed IR where purity, first-class functions, and shape-aware types enable both dynamic model expressivity and strong, compositional compiler optimizations. Functional programming isn’t just a style choice—it’s the mechanism that lets Relay act like a real programming language while still compiling down to fast kernels.","created_at":1757969118018,"messageId":"74ba5600-72ab-48be-aff7-5d23fe1e1e8f","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Functional programming is central to Relay’s design and is the key to unifying dynamic expressivity with strong static optimization. Here’s how it plays in:\n\nCore language model\n- First-class functions: Functions are values—you can pass them as arguments, return them, store them—enabling higher-order programming (e.g., map, fold, custom layers as functions).\n- Pure, side-effect–free core: Computation is treated as pure by default, which makes transformations like inlining, beta-reduction, CSE, and DCE safe and predictable.\n- Lexical scoping and closures: Relay supports closures, so functions can capture environments, enabling modular composition without sacrificing analyzability.\n- Explicit control flow as expressions: Conditionals, pattern matching, and recursion/loops are expressed as ordinary expressions, not special graph nodes. This keeps dynamic control flow within a uniform, optimizable IR.\n\nStatic reasoning via types\n- Static typing with shape-dependent types: Types carry tensor dtypes and shapes (possibly symbolic). This is a functional-style, strongly typed system that enables compile-time checks and guides optimizations.\n- Type/shape inference: Like many functional languages, Relay infers types, relieving annotation burden while retaining static guarantees.\n- Effect/alias analysis layered on purity: Because the core is functional, it’s easier to reason about when mutations (e.g., buffers) happen, enabling safe fusion and memory planning.\n\nCompiler transformations enabled by FP\n- Equational reasoning: Purity and referential transparency allow algebraic simplifications and reassociation without changing semantics.\n- Inlining and specialization: Higher-order functions can be inlined and specialized for concrete shapes/dtypes, improving performance.\n- Partial evaluation/constant folding: Functional structure makes it straightforward to evaluate known subprograms at compile time.\n- Whole-program AD as a source-to-source transform: Reverse-mode automatic differentiation is implemented by transforming functional programs, so gradients work through higher-order functions and control flow without hardcoding per-operator rules.\n\nBridging dynamic and static\n- Dynamic expressivity: You can write models with data-dependent branches, recursion, or variable sequence lengths using ordinary functional constructs.\n- Static optimization: The type/shape system plus purity enables aggressive compile-time optimization, then Relay lowers to TensorIR for schedule-level tuning.\n\nInterplay with lower levels\n- Boundary to TIR: After functional-level optimizations (fusion, layout transforms), Relay produces “primitive” functions that represent fused regions. These are then lowered to TensorIR, where imperative loop nests and memory scopes are introduced for hardware mapping.\n- External codegen: Functional partitioning lets the compiler carve out subgraphs for vendor libraries (e.g., cuDNN) while keeping the rest in the same functional IR.\n\nPractical benefits\n- Modularity and reuse: Define layers and combinators as ordinary functions; the compiler can still optimize them away when appropriate.\n- Robustness of transforms: FP semantics reduce corner cases in graph rewrites and make analyses (like dead code or common subexpression elimination) more reliable.\n- Extensibility: New ops or patterns can be expressed as functions/combinators without fighting a fixed op-graph abstraction.\n\nIn short: Relay is a functional, statically typed IR where purity, first-class functions, and shape-aware types enable both dynamic model expressivity and strong, compositional compiler optimizations. Functional programming isn’t just a style choice—it’s the mechanism that lets Relay act like a real programming language while still compiling down to fast kernels.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_04c7b830ac20c75d0068c87adeb994819fb03c90d7f08fb8c8"}},"resumableStreamId":"STREAM:188f1ef5-6b83-483b-8318-aaa81d911639","role":"assistant","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","timeToFirstToken":2.566,"tokens":710,"tokensPerSecond":40.506617982656316,"updated_at":1757969118018,"userId":"google:104458561188320708185","id":"j977b3e7pgms1yj87140gmmr6h7qnhr9"},{"_creationTime":1757969277703.4683,"_id":"j978t2fktbettba5n2bxmq54n57qngn3","attachmentIds":[],"content":"explain how relay optimize eager mode?","created_at":1757969277703,"messageId":"30a051e3-4f38-4172-9919-ad123bdcc8b0","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","updated_at":1757969277703,"userId":"google:104458561188320708185","id":"j978t2fktbettba5n2bxmq54n57qngn3","providerMetadata":{}},{"_creationTime":1757969277703.4685,"_id":"j97epc83veq1ptz33nn208stkh7qmppn","attachmentIds":[],"byok":false,"content":"Relay doesn’t “optimize eager mode” in the sense of speeding up Python eager execution directly. Instead, it takes models that are written or traced from eager frameworks and compiles them into an optimized, statically analyzable program. In other words: it turns eager-style models into an IR where the compiler can apply whole‑program optimizations and then lower to fast kernels.\n\nHow Relay gets from eager to optimized\n- Import from eager frameworks:\n  - You write/train in an eager framework (e.g., PyTorch, TensorFlow eager).\n  - A frontend (TorchScript/FX export, ONNX export, etc.) captures the computation and converts it into a Relay program.\n- Make it a real program:\n  - Relay represents the model as a functional program with explicit control flow, first-class functions, and shape-typed tensors. This representation preserves dynamic behavior but gives the compiler structure to analyze.\n\nOptimizations Relay applies to “eager-originated” graphs\n- Constant folding and partial evaluation:\n  - Evaluates subexpressions known at compile time (e.g., shapes, constants, frozen weights after export), removing runtime overhead introduced by eager tracing or scripting.\n- Common subexpression elimination and dead code elimination:\n  - Removes redundant computations that might appear due to modular/eager coding patterns or framework graph captures.\n- Operator fusion:\n  - Fuses chains like conv → bias-add → relu into a single “primitive” function, drastically reducing kernel launches and global memory traffic typical in eager execution.\n- Layout and dataflow rewrites:\n  - Reorders or transforms tensor layouts (e.g., NCHW↔NHWC) to match backend kernels, and simplifies reshapes/transposes inserted during eager composition.\n- Type/shape inference and specialization:\n  - Specializes functions for concrete dtypes and shapes inferred from the captured model, enabling more aggressive lowering and removing dynamic checks.\n- Control-flow-aware optimization:\n  - Because control flow is first-class, Relay can optimize across if/loop constructs introduced by eager code, not just within a flat op DAG.\n- Quantization passes:\n  - Applies PTQ/QAT transformations to reduce precision safely, something that’s hard to do purely in eager mode without a compiler view.\n- Memory planning:\n  - Plans buffer reuse and in-place updates at compile time to reduce allocations and copies that eager stacks often incur.\n\nFrom program to fast kernels\n- Lowering to TensorIR (TIR):\n  - Each fused Relay region becomes a TIR primitive with explicit loops and memory scopes.\n- Scheduling and autotuning:\n  - AutoTVM/Ansor/MetaSchedule search schedules (tiling, thread/block binding, caching in shared memory, vectorization) to reach near hand-tuned performance on GPUs/CPUs.\n- External libraries:\n  - Partition subgraphs to cuDNN/cuBLAS/OneDNN/TensorRT where beneficial, while compiling the remainder through TVM.\n\nWhat this means relative to eager mode\n- Eager mode is great for flexibility and debugging but pays overheads: many small kernels, framework runtime checks, Python dispatch, redundant ops, suboptimal layouts.\n- Relay removes those overheads by compiling the captured eager computation into a fused, specialized, target-aware executable, while still supporting dynamic constructs when present.\n\nTypical impact\n- Fewer kernel launches and less memory traffic due to fusion.\n- Better kernel quality via schedule tuning.\n- Lower runtime overhead by eliminating interpreter/framework layers and redundant ops.\n\nBottom line: Relay doesn’t speed up the eager interpreter itself; it compiles the computation you wrote in eager style into an optimized program with fusion, specialization, and tuned kernels, delivering static-graph-like performance while preserving the ability to represent dynamic behavior.","created_at":1757969277704,"messageId":"94ca8cf1-3d9f-482a-81ec-64fa1979d8c1","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Relay doesn’t “optimize eager mode” in the sense of speeding up Python eager execution directly. Instead, it takes models that are written or traced from eager frameworks and compiles them into an optimized, statically analyzable program. In other words: it turns eager-style models into an IR where the compiler can apply whole‑program optimizations and then lower to fast kernels.\n\nHow Relay gets from eager to optimized\n- Import from eager frameworks:\n  - You write/train in an eager framework (e.g., PyTorch, TensorFlow eager).\n  - A frontend (TorchScript/FX export, ONNX export, etc.) captures the computation and converts it into a Relay program.\n- Make it a real program:\n  - Relay represents the model as a functional program with explicit control flow, first-class functions, and shape-typed tensors. This representation preserves dynamic behavior but gives the compiler structure to analyze.\n\nOptimizations Relay applies to “eager-originated” graphs\n- Constant folding and partial evaluation:\n  - Evaluates subexpressions known at compile time (e.g., shapes, constants, frozen weights after export), removing runtime overhead introduced by eager tracing or scripting.\n- Common subexpression elimination and dead code elimination:\n  - Removes redundant computations that might appear due to modular/eager coding patterns or framework graph captures.\n- Operator fusion:\n  - Fuses chains like conv → bias-add → relu into a single “primitive” function, drastically reducing kernel launches and global memory traffic typical in eager execution.\n- Layout and dataflow rewrites:\n  - Reorders or transforms tensor layouts (e.g., NCHW↔NHWC) to match backend kernels, and simplifies reshapes/transposes inserted during eager composition.\n- Type/shape inference and specialization:\n  - Specializes functions for concrete dtypes and shapes inferred from the captured model, enabling more aggressive lowering and removing dynamic checks.\n- Control-flow-aware optimization:\n  - Because control flow is first-class, Relay can optimize across if/loop constructs introduced by eager code, not just within a flat op DAG.\n- Quantization passes:\n  - Applies PTQ/QAT transformations to reduce precision safely, something that’s hard to do purely in eager mode without a compiler view.\n- Memory planning:\n  - Plans buffer reuse and in-place updates at compile time to reduce allocations and copies that eager stacks often incur.\n\nFrom program to fast kernels\n- Lowering to TensorIR (TIR):\n  - Each fused Relay region becomes a TIR primitive with explicit loops and memory scopes.\n- Scheduling and autotuning:\n  - AutoTVM/Ansor/MetaSchedule search schedules (tiling, thread/block binding, caching in shared memory, vectorization) to reach near hand-tuned performance on GPUs/CPUs.\n- External libraries:\n  - Partition subgraphs to cuDNN/cuBLAS/OneDNN/TensorRT where beneficial, while compiling the remainder through TVM.\n\nWhat this means relative to eager mode\n- Eager mode is great for flexibility and debugging but pays overheads: many small kernels, framework runtime checks, Python dispatch, redundant ops, suboptimal layouts.\n- Relay removes those overheads by compiling the captured eager computation into a fused, specialized, target-aware executable, while still supporting dynamic constructs when present.\n\nTypical impact\n- Fewer kernel launches and less memory traffic due to fusion.\n- Better kernel quality via schedule tuning.\n- Lower runtime overhead by eliminating interpreter/framework layers and redundant ops.\n\nBottom line: Relay doesn’t speed up the eager interpreter itself; it compiles the computation you wrote in eager style into an optimized program with fusion, specialization, and tuned kernels, delivering static-graph-like performance while preserving the ability to represent dynamic behavior.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0bdd8e7eb290b3ef0068c87b7e8ae881a0b038d4bbe40be692"}},"resumableStreamId":"STREAM:fc1723e5-1bb7-4fe2-b802-3016adb13b97","role":"assistant","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","timeToFirstToken":0.731,"tokens":761,"tokensPerSecond":68.70091179922362,"updated_at":1757969277704,"userId":"google:104458561188320708185","id":"j97epc83veq1ptz33nn208stkh7qmppn"},{"_creationTime":1757969303180.9458,"_id":"j970ak1xvf16m1fat0g2fev3nh7qm3jp","attachmentIds":[],"content":"how does Relay speed up LLM training and inference?","created_at":1757969303181,"messageId":"21c65cfd-83c5-4ffc-941c-d82199592f63","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"role":"user","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","updated_at":1757969303181,"userId":"google:104458561188320708185","id":"j970ak1xvf16m1fat0g2fev3nh7qm3jp","providerMetadata":{}},{"_creationTime":1757969303180.946,"_id":"j977xqr84q74b0qxh3dh95pc8d7qnr80","attachmentIds":[],"byok":false,"content":"Relay helps speed up LLM training and inference by turning a high-level model (often authored in an eager framework) into an optimized program with fused compute, specialized shapes/dtypes, and high-performance kernels. The main levers:\n\nProgram-level optimizations\n- Operator fusion across blocks: Fuse patterns like matmul → bias → dropout → residual → layernorm (or subchains within) to reduce kernel launches and global memory traffic—critical for memory-bound LLM layers.\n- Layout and algebraic rewrites: Simplify reshape/transposes introduced by framework code, canonicalize attention subgraphs, and pick layouts that match backend kernels (e.g., optimizing QKV packing).\n- Constant folding and specialization: Fold static constants (e.g., rotary embedding tables, causal masks when static) and specialize functions for concrete sequence lengths, dtypes, and tensor ranks.\n- Control-flow-aware transforms: Handle variable-length decoding loops and conditional paths while still performing fusion/simplification where safe.\n- Quantization and mixed precision: Apply PTQ/QAT passes (INT8/INT4, FP16/BF16) and propagate reduced precision through compatible subgraphs to increase throughput and reduce memory use.\n\nKernel-level performance via TIR + tuning\n- Lowering to TensorIR: Each fused Relay region becomes a TIR primitive with explicit loops and memory scopes suitable for GPUs/CPUs/accelerators.\n- Auto-scheduling/auto-tuning: MetaSchedule/Ansor find schedules with tiling, shared-memory caching, tensorization (e.g., WMMA/Tensor Cores), vectorization, and unrolling that push matmuls/attn to near vendor-library speed.\n- Tensor-core utilization: Pattern-match GEMMs and attention blocks to use tensor-core intrinsics on NVIDIA (or equivalent on other HW), including FP16/BF16/INT8 paths.\n- Memory planning: Reuse buffers across layers and compress intermediates to cut peak memory—important for long context and large batch inference.\n\nAttention- and transformer-specific wins\n- Fused attention: Compile composite attention (Q/K/V projection, scaling, masking, softmax, value matmul) into fewer kernels; optionally partition to vendor libraries (e.g., FlashAttention-like patterns if available or via external codegen).\n- LayerNorm and activation fusion: Combine norm + bias + activation where possible; reduce memory round-trips.\n- Rotary/ALiBi handling: Precompute or fold positional encodings where shapes allow; hoist invariants out of decode loops.\n\nDecoding optimizations for inference\n- KV-cache–aware compilation: Treat cache loads/stores explicitly to align memory layout and prefetching; fuse small ops around the cache updates to lower per-token latency.\n- Static-then-dynamic specialization: Build specialized kernels for common sequence lengths (e.g., 128/256/512/4k) and fall back to symbolic shapes for outliers, balancing throughput and flexibility.\n- Reduced launch overhead: Larger fused kernels and runtime planning reduce per-token overhead, a major factor in autoregressive decoding.\n\nTraining-time benefits\n- Whole-graph AD: Relay’s source-to-source reverse-mode AD differentiates through control flow and higher-order code, enabling global optimization of the forward+backward.\n- Fused backward kernels: The same fusion and lowering apply to gradients (e.g., fused dGEMM + epilogue ops, fused dLayerNorm), reducing memory traffic in backward.\n- Mixed-precision and loss-scaling flows: Compiler-enforced dtype propagation and safe cast placement help maintain numerical stability while maximizing tensor-core use.\n\nInteroperability with vendor libraries\n- External codegen partitioning: Route large GEMMs/conv-like projections to cuBLAS/cuDNN/rocBLAS/OneDNN or TensorRT when advantageous, while compiling the rest; combine with TVM-tuned kernels for non-library ops.\n\nWhat to expect in practice\n- With tuning: End-to-end speedups from fewer kernel launches, better memory locality, and tensor-core utilization; competitive with vendor stacks on many subgraphs.\n- Without tuning: Performance improves from fusion and simplification, but peak throughput usually requires MetaSchedule/Ansor tuning.\n- Modern TVM direction: Newer TVM flows (Relax + TensorIR) tend to deliver stronger LLM results than legacy Relay-only pipelines, but the mechanisms above are the same—program-level fusion/specialization plus tuned kernels.\n\nBottom line: Relay accelerates LLMs by compiling the transformer program into fewer, larger, tensor-core–friendly kernels, specializing shapes/dtypes, optimizing attention and KV-cache dataflow, and then auto-tuning the resulting kernels for the target hardware, for both training and inference.","created_at":1757969303182,"messageId":"661b9cdb-c776-4db2-9839-e08fc194504f","model":"gpt-5-chat","modelParams":{"includeSearch":true,"reasoningEffort":"medium"},"parts":[{"reasoning":"","type":"reasoning"},{"text":"Relay helps speed up LLM training and inference by turning a high-level model (often authored in an eager framework) into an optimized program with fused compute, specialized shapes/dtypes, and high-performance kernels. The main levers:\n\nProgram-level optimizations\n- Operator fusion across blocks: Fuse patterns like matmul → bias → dropout → residual → layernorm (or subchains within) to reduce kernel launches and global memory traffic—critical for memory-bound LLM layers.\n- Layout and algebraic rewrites: Simplify reshape/transposes introduced by framework code, canonicalize attention subgraphs, and pick layouts that match backend kernels (e.g., optimizing QKV packing).\n- Constant folding and specialization: Fold static constants (e.g., rotary embedding tables, causal masks when static) and specialize functions for concrete sequence lengths, dtypes, and tensor ranks.\n- Control-flow-aware transforms: Handle variable-length decoding loops and conditional paths while still performing fusion/simplification where safe.\n- Quantization and mixed precision: Apply PTQ/QAT passes (INT8/INT4, FP16/BF16) and propagate reduced precision through compatible subgraphs to increase throughput and reduce memory use.\n\nKernel-level performance via TIR + tuning\n- Lowering to TensorIR: Each fused Relay region becomes a TIR primitive with explicit loops and memory scopes suitable for GPUs/CPUs/accelerators.\n- Auto-scheduling/auto-tuning: MetaSchedule/Ansor find schedules with tiling, shared-memory caching, tensorization (e.g., WMMA/Tensor Cores), vectorization, and unrolling that push matmuls/attn to near vendor-library speed.\n- Tensor-core utilization: Pattern-match GEMMs and attention blocks to use tensor-core intrinsics on NVIDIA (or equivalent on other HW), including FP16/BF16/INT8 paths.\n- Memory planning: Reuse buffers across layers and compress intermediates to cut peak memory—important for long context and large batch inference.\n\nAttention- and transformer-specific wins\n- Fused attention: Compile composite attention (Q/K/V projection, scaling, masking, softmax, value matmul) into fewer kernels; optionally partition to vendor libraries (e.g., FlashAttention-like patterns if available or via external codegen).\n- LayerNorm and activation fusion: Combine norm + bias + activation where possible; reduce memory round-trips.\n- Rotary/ALiBi handling: Precompute or fold positional encodings where shapes allow; hoist invariants out of decode loops.\n\nDecoding optimizations for inference\n- KV-cache–aware compilation: Treat cache loads/stores explicitly to align memory layout and prefetching; fuse small ops around the cache updates to lower per-token latency.\n- Static-then-dynamic specialization: Build specialized kernels for common sequence lengths (e.g., 128/256/512/4k) and fall back to symbolic shapes for outliers, balancing throughput and flexibility.\n- Reduced launch overhead: Larger fused kernels and runtime planning reduce per-token overhead, a major factor in autoregressive decoding.\n\nTraining-time benefits\n- Whole-graph AD: Relay’s source-to-source reverse-mode AD differentiates through control flow and higher-order code, enabling global optimization of the forward+backward.\n- Fused backward kernels: The same fusion and lowering apply to gradients (e.g., fused dGEMM + epilogue ops, fused dLayerNorm), reducing memory traffic in backward.\n- Mixed-precision and loss-scaling flows: Compiler-enforced dtype propagation and safe cast placement help maintain numerical stability while maximizing tensor-core use.\n\nInteroperability with vendor libraries\n- External codegen partitioning: Route large GEMMs/conv-like projections to cuBLAS/cuDNN/rocBLAS/OneDNN or TensorRT when advantageous, while compiling the rest; combine with TVM-tuned kernels for non-library ops.\n\nWhat to expect in practice\n- With tuning: End-to-end speedups from fewer kernel launches, better memory locality, and tensor-core utilization; competitive with vendor stacks on many subgraphs.\n- Without tuning: Performance improves from fusion and simplification, but peak throughput usually requires MetaSchedule/Ansor tuning.\n- Modern TVM direction: Newer TVM flows (Relax + TensorIR) tend to deliver stronger LLM results than legacy Relay-only pipelines, but the mechanisms above are the same—program-level fusion/specialization plus tuned kernels.\n\nBottom line: Relay accelerates LLMs by compiling the transformer program into fewer, larger, tensor-core–friendly kernels, specializing shapes/dtypes, optimizing attention and KV-cache dataflow, and then auto-tuning the resulting kernels for the target hardware, for both training and inference.","type":"text"}],"providerMetadata":{"openai":{"responseId":"resp_0de48e2f2445af050068c87b97e224819c931e9200e6999be6"}},"resumableStreamId":"STREAM:7475b2a8-4646-4297-8680-1f03d9abef64","role":"assistant","status":"done","threadId":"366d04f1-b26e-4f64-b89c-62575b2136c0","timeToFirstToken":1.12,"tokens":957,"tokensPerSecond":53.02526595744681,"updated_at":1757969303182,"userId":"google:104458561188320708185","id":"j977xqr84q74b0qxh3dh95pc8d7qnr80"}],"version":"6.0.0"}